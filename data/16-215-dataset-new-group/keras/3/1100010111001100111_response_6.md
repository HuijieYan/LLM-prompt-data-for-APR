The bug in the `_clone_functional_model` function is likely related to the incorrect handling of the input tensors, specifically when the input model has multiple outputs. The issue arises due to the discrepancies in the computation of output tensors and masks, especially when dealing with layers that do not support masks.

Here's a strategy for fixing the bug:
1. Modify the handling of input tensors and the computation of output tensors and masks to ensure that it properly accounts for multiple outputs and layers that do not support masks.
2. Update the logic to gather inputs and call the new layer in such a way that it accurately processes all input tensors and masks, even in the presence of multiple outputs.

Additionally, the GitHub issue provided valuable insights into the potential cause of the bug, highlighting the specific scenario that triggers the error and suggesting that the problem is related to Lambda layers without mask support when used in models with multiple outputs. This information will help guide the bug-fixing process.

Based on the analysis and the GitHub issue, here's the corrected version of the `_clone_functional_model` function:

```python
def _clone_functional_model(model, input_tensors=None):
    # ... existing code ...

    # Iterated over every node in the reference model, in depth order.
    depth_keys = list(model._nodes_by_depth.keys())
    depth_keys.sort(reverse=True)
    for depth in depth_keys:
        nodes = model._nodes_by_depth[depth]
        for node in nodes:
            # Recover the corresponding layer.

            # ... existing code ...

            # Gather inputs to call the new layer.
            reference_input_tensors = node.input_tensors
            reference_output_tensors = node.output_tensors

            # If all previous input tensors are available in tensor_map,
            # then call node.inbound_layer on them.
            computed_data = []  # List of tuples (input, mask).
            all_tensors_found = True
            for x in reference_input_tensors:
                if x in tensor_map:
                    computed_data.append(tensor_map[x])
                else:
                    all_tensors_found = False

            if all_tensors_found:
                if node.arguments:
                    kwargs = node.arguments
                else:
                    kwargs = {}
                # Call layer.
                output_tensors = to_list(layer(reference_input_tensors, **kwargs))

                # Update tensor_map.
                for x, y in zip(reference_output_tensors, output_tensors):
                    tensor_map[x] = (y, None)

    # Check that we did compute the model outputs,
    # then instantiate a new model from inputs and outputs.
    output_tensors = []
    for x in model.outputs:
        assert x in tensor_map, 'Could not compute output ' + str(x)
        tensor, _ = tensor_map[x]
        output_tensors.append(tensor)
    return Model(input_tensors, output_tensors, name=model.name)
```

With the updated logic in place, the `_clone_functional_model` function should now properly handle input tensors and the computation of output tensors and masks, resolving the issue reported in the GitHub bug report. This should ensure that the function passes the failing test and functions as expected when cloning functional models with multiple outputs.