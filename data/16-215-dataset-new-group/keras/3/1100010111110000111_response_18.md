The buggy function `_clone_functional_model` in the `keras/models.py` file is designed to clone a functional `Model` instance, but it is failing with the error "Could not compute output Tensor" in the test `test_clone_functional_model_with_multi_outputs` in the `tests/keras/test_sequential_model.py` file.

The `clone_model` function is causing the error while attempting to clone a model with multiple input and output layers, in particular when a custom `Layer` such as `Lambda` and `SwapLayer` are used.

The cause of the bug is that the `_clone_functional_model` function is not handling the cloning of models with custom layers correctly, and issues arise when iterating over the nodes in the reference model and reproducing them in the cloned model.

To fix the bug, the `_clone_functional_model` function needs to be modified to correctly clone models with custom layers, handle input tensors, and process the nodes in the reference model. Additionally, the function needs to properly handle layers with multiple inputs and outputs.

Here's the corrected version of the `_clone_functional_model` function:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)
    if isinstance(model, Sequential):
        raise ValueError('Expected `model` argument to be a functional `Model` instance, got a `Sequential` instance instead:', model)

    from .layers import deserialize, serialize
    from .engine.topology import get_source_inputs
    from .engine import training
    import copy

    layer_map = {}  # Cache for created layers.
    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}
    if input_tensors is None:
        input_layers = [] 
        input_tensors = []
        for layer in model._input_layers:
            input_tensor = Input(batch_shape=layer.batch_input_shape,
                                 dtype=layer.dtype,
                                 sparse=layer.sparse,
                                 name=layer.name)
            input_tensors.append(input_tensor)
            # Cache newly created input layer.
            input_layers.append(input_tensor)
            layer_map[layer] = input_tensor
    else:
        input_tensors = to_list(input_tensors)
    
    for x, y in zip(get_source_inputs(model.outputs), input_tensors):
        tensor_map[x] = (y, None)  # tensor, mask

    # Queue of layers that need to be added to the model. It is
    # basically the layers of the graph in topological order.
    layer_queue = [(node.outbound_layer, False) for node in training.TopologicalSortWalker(model._nodes_by_depth).inplace_nodes]
    added_nodes = set()

    # Iterate over every node in the reference model, in depth order.
    while layer_queue:
        layer, in_place = layer_queue.pop(0)

        if not in_place and layer in added_nodes:
            continue

        # If the layer is shared (in a merge layer), we cache
        # the layer node to avoid any duplication by the cloning.
        added_nodes.add(layer)
        
        # Get existing node.
        if layer in layer_map:
            new_layer = layer_map[layer]
        else:
            # Clone layer.
            serialized_layer = serialize(layer)
            layer = deserialize(serialized_layer)
            new_layer = layer
            layer_map[layer] = new_layer

        # Node conductor.
        conductor = NodeConductor(inbound_layers)
        topology, _ = conductor.variate(new_layer, model._network_nodes)
        training.TopologicalSorter(conductor.inbound_nodes).flatten()

        # Iterate nodes (only the inbound nodes). We add more non in-place layers in case
        # they are not already in the queue.
        for node in topology:
            if not node.inbound_layers:
                continue

            # If the node is already added, there is no need to
            # do it again.
            if node in added_nodes:
                continue

            layer_queue.append((node.outbound_layer, False))
            added_nodes.add(node)

            input_tensors = []
            for input_layer, node_index, tensor_index, tensor in zip(node.inbound_layers, node.node_indices, node.tensor_indices, node.input_tensors):
                if tensor in tensor_map:
                    new_tensor, mask = tensor_map[tensor]
                else:
                    if input_layer in layer_map:
                        input_layer = layer_map[input_layer]
                    
                    new_tensor = copy.copy(tensor)
                    tensor_map[tensor] = (new_tensor, None)
                    conductors.append(new_tensor)

                input_tensors.append(new_tensor)
                # Cache tensor.
                tensor_map[tensor] = (new_tensor, None)

            # Re-apply node attrs.
            input_tensors = to_list(input_tensors)
            if node.arguments:
                kwargs = node.arguments
            else:
                kwargs = {}
            
            # Call layer.
            outputs = to_list(new_layer(input_tensors, **kwargs))
            # Cache tensor.
            outputs = to_list(to_list(outputs))

            # Update tensor_map.
            for tensor, ref_output in zip(outputs, node.output_tensors):
                tensor_map[ref_output] = (tensor, None)

    # Check that we did compute the model outputs,
    # then instantiate a new model from inputs and outputs.
    output_tensors = []
    for x in get_source_inputs(data=model.outputs):
        if x in tensor_map:
            tensor, _ = tensor_map[x]
            output_tensors.append(tensor)

    if len(output_tensors) > 1:
        return Model(get_source_inputs(data=model.inputs), output_tensors, name=model.name)
    else:
        return Model(input_tensors, output_tensors[0], name=model.name)
```

This corrected version of the function should address the issues with cloning functional models with multiple outputs and resolve the error described in the GitHub issue.