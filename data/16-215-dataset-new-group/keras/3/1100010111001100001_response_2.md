The bug in the function `_clone_functional_model` is that it fails to correctly handle the case when the model has multiple input tensors, resulting in the incorrect cloning of the model. This causes the failing test `test_clone_functional_model_with_multi_outputs` to produce inconsistent predictions between the original and cloned models.

The bug is likely located in the section of the code that handles the creation of input tensors and input layers. The function fails to correctly map the input tensors to input layers and handle the case where a model has multiple inputs.

To fix the bug, we need to ensure that the input tensors and input layers are correctly mapped and handled for models with multiple inputs.

Here is the corrected version of the `_clone_functional_model` function:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)
    if isinstance(model, Sequential):
        raise ValueError('Expected `model` argument to be a functional `Model` instance, got a `Sequential` instance instead:', model)

    layer_map = {} 
    tensor_map = {}  
    input_layers = model.inputs  # use the existing input layers
    
    if input_tensors is not None:
        if len(input_tensors) != len(input_layers):
            raise ValueError('Number of input tensors should match the number of input layers')

        for i, x in enumerate(input_tensors):
            if not K.is_keras_tensor(x):
                name = model.inputs[i].name
                input_tensor = Input(tensor=x, name='input_wrapper_for_' + name)
                input_layers[i] = input_tensor  # update the existing input layer with new input tensor
                x._keras_history = (input_tensor, 0, 0)  # update the history
        input_tensors = input_layers  # update input_tensors

    for i, x in enumerate(input_tensors):
        tensor_map[model.inputs[i]] = (x, None)

    for layer in model.layers:
        if layer in layer_map:
            continue
        new_layer = layer.__class__.from_config(layer.get_config())
        layer_map[layer] = new_layer

    for node in model._nodes_by_depth[0]:
        layer = node.outbound_layer

        if layer not in layer_map:
            new_layer = layer.__class__.from_config(layer.get_config())
            layer_map[layer] = new_layer
            layer = new_layer
        else:
            layer = layer_map[layer]
        
        reference_input_tensors = node.input_tensors
        reference_output_tensors = node.output_tensors
        
        computed_data = []  
        for x in reference_input_tensors:
            if x in tensor_map:
                computed_data.append(tensor_map[x])

        if len(computed_data) == len(reference_input_tensors):
            kwargs = node.arguments if node.arguments else {}
            output_tensors = to_list(layer([x[0] for x in computed_data], **kwargs))
            tensor_map.update(zip(reference_output_tensors, output_tensors))

    # Check that we did compute the model outputs
    output_tensors = [tensor_map[x][0] for x in model.outputs]
    return Model(input_tensors, output_tensors, name=model.name)
```

In the corrected version, we ensure that the input layers and input tensors are properly mapped and handled, and that the layer creation and connection process is correctly executed. This should resolve the issue with cloning models with multiple input tensors.

After applying these changes, the failing test `test_clone_functional_model_with_multi_outputs` should pass.