## Corrected Version

```python
def _clone_functional_model(model, input_tensors=None):
    """Clone a functional `Model` instance.

    Model cloning is similar to calling a model on new inputs,
    except that it creates new layers (and thus new weights) instead
    of sharing the weights of the existing layers.

    # Arguments
        model: Instance of `Model`.
        input_tensors: optional list of input tensors
            to build the model upon. If not provided,
            placeholders will be created.

    # Returns
        An instance of `Model` reproducing the behavior
        of the original model, on top of new inputs tensors,
        using newly instantiated weights.

    # Raises
        ValueError: in case of invalid `model` argument value.
    """
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)
    if isinstance(model, Sequential):
        raise ValueError('Expected `model` argument to be a functional `Model` instance, got a `Sequential` instance instead:', model)

    layer_map = {}  # Cache for created layers.
    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}
    if input_tensors is None:
        # Create placeholders to build the model on top of.
        input_layers = []
        input_tensors = []
        for layer in model._input_layers:
            batch_shape = layer.batch_input_shape
            dtype = layer.dtype
            sparse = layer.sparse
            name = layer.name
            input_tensor = Input(batch_shape=batch_shape,
                                 dtype=dtype,
                                 sparse=sparse,
                                 name=name)
            input_tensors.append(input_tensor)
            # Cache newly created input layer.
            newly_created_input_layer = input_tensor
            layer_map[layer] = newly_created_input_layer
            input_layers.append(layer)
        for _original, _cloned in zip(model._input_layers, input_layers):
            layer_map[_original] = _cloned
    else:
        # Make sure that all input tensors come from a Keras layer.
        # If tensor comes from an input layer: cache the input layer.
        input_tensors = to_list(input_tensors)
        _input_tensors = []
        for i, x in enumerate(input_tensors):
            if not K.is_keras_tensor(x):
                name = model._input_layers[i].name
                input_tensor = Input(tensor=x,
                                     name='input_wrapper_for_' + name)
                _input_tensors.append(input_tensor)
                # Cache newly created input layer.
                original_input_layer = input_tensors[i]
                newly_created_input_layer = input_tensor
                layer_map[original_input_layer] = newly_created_input_layer
            else:
                _input_tensors.append(x)
        input_tensors = _input_tensors

    for x, y in zip(model.inputs, input_tensors):
        tensor_map[x] = (y, None)  # tensor, mask
    
    # Correct the logic for processing nodes and layers
    from .. import layers
    layer_classes = dir(layers)
    for layer_classes_key in layer_classes:
        layer = getattr(layers, layer_classes_key, None)
        if hasattr(layer, 'from_config'):
            try:
                if layer not in layer_map:
                        # Clone layer.
                        new_layer = layer.from_config(layer.get_config())
                        layer_map[layer] = new_layer
                else:
                    # Reuse previously cloned layer.
                    continue
            except:
                # Ignore any errors if the current layer cannot be cloned with from_config method
                continue

    # Iterated over every node in the reference model, in depth order.
    depth_keys = list(model._nodes_by_depth.keys())
    depth_keys.sort(reverse=True)
    for depth in depth_keys:
        nodes = model._nodes_by_depth[depth]
        for node in nodes:
            layer = node.outbound_layer
            
            # Get or create layer.
            if layer not in layer_map:
                # Clone layer.
                new_layer = layer.__class__.from_config(layer.get_config())
                layer_map[layer] = new_layer
                layer = new_layer
        
            # Gather inputs to call the new layer.
            reference_input_tensors = node.input_tensors
            reference_output_tensors = node.output_tensors
            
            # If all previous input tensors are available in tensor_map,
            # then call node.inbound_layer on them.
            computed_tensors = []
            output_tensors = []
            for x in reference_input_tensors:
                if x in tensor_map:
                    computed_tensors.append(tensor_map[x][0])
            # Call layer.
            output_tensors = to_list(layer(computed_tensors))
            # Update tensor_map.
            for x, y in zip(reference_output_tensors, output_tensors):
                tensor_map[x] = (y, None)

    # Check that we did compute the model outputs,
    # then instantiate a new model from inputs and outputs.
    output_tensors = []
    for x in model.outputs:
        assert x in tensor_map, 'Could not compute output ' + str(x)
        tensor, _ = tensor_map[x]
        output_tensors.append(tensor)
    return Model(input_tensors, output_tensors, name=model.name)
```
The bug was caused by incorrect handling of layers and nodes during the cloning process. The corrected code ensures that all layers are properly cloned and that the input and output tensors are correctly mapped. This should resolve the issues reported in the failing test and GitHub issue.