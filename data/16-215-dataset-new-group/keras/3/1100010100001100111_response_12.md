The bug in the `_clone_functional_model` function seems to be related to the creation of input placeholders and the mapping of tensor inputs when using `clone_model` with multi_gpu_model. The issue on GitHub describes a specific scenario where the model cloning process fails due to an error in computing the output tensor. The issue also points out that the cloning process is affected by the use of layers without mask support.

The cause of the bug is that the `clone_model` function incorrectly handles the input tensors and the creation of input layers, leading to a mismatch in the output mask support. This results in an error when trying to compute the output tensor for models with layers that do not support masks.

To fix the bug, we can modify the `_clone_functional_model` function to correctly handle input tensors and input layers when cloning a model. Additionally, we need to ensure that the mask support for each layer is appropriately handled during the cloning process.

Here's the corrected version of the `_clone_functional_model` function:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)
    if isinstance(model, Sequential):
        raise ValueError('Expected `model` argument to be a functional `Model` instance, got a `Sequential` instance instead:', model)

    layer_map = {}  # Cache for created layers.
    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}

    if input_tensors is None:
        input_tensors = []
        for layer in model._input_layers:
            input_tensor = Input(batch_shape=layer.batch_input_shape,
                                 dtype=layer.dtype,
                                 sparse=layer.sparse,
                                 name=layer.name)
            input_tensors.append(input_tensor)
            # Cache newly created input layer.
            layer_map[layer] = input_tensor._keras_history[0]

    for x, y in zip(model.inputs, input_tensors):
        tensor_map[x] = (y, None)  # tensor, mask

    depth_keys = list(model._nodes_by_depth.keys())
    depth_keys.sort(reverse=True)
    for depth in depth_keys:
        nodes = model._nodes_by_depth[depth]
        for node in nodes:
            layer = node.outbound_layer

            if layer not in layer_map:
                new_layer = layer.__class__.from_config(layer.get_config())
                layer_map[layer] = new_layer
                layer = new_layer
            else:
                layer = layer_map[layer]
                if isinstance(layer, InputLayer):
                    continue

            reference_input_tensors = node.input_tensors
            computed_data = []
            for x in reference_input_tensors:
                if x in tensor_map:
                    computed_data.append(tensor_map[x])

            if len(computed_data) == len(reference_input_tensors):
                if node.arguments:
                    kwargs = node.arguments
                else:
                    kwargs = {}

                computed_tensors = [x[0] for x in computed_data]
                output_tensors = to_list(layer(computed_tensors, **kwargs))

                for x, y in zip(node.output_tensors, output_tensors):
                    tensor_map[x] = (y, None)

    output_tensors = []
    for x in model.outputs:
        assert x in tensor_map, 'Could not compute output ' + str(x)
        tensor, _ = tensor_map[x]
        output_tensors.append(tensor)

    return Model(input_tensors, output_tensors, name=model.name)
```

In this corrected version, the function first handles the input tensors and creates new input layers if needed. It then correctly maps the input tensors and ensures that mask support is handled appropriately for each layer during the cloning process.

This should resolve the issue reported on GitHub and allow for a successful model cloning process with `clone_model` and `multi_gpu_model`.