The error message suggests that the model outputs were not properly computed during the cloning process. This likely indicates a bug in the `_clone_functional_model` function, which is failing to properly map the input and output tensors of the model being cloned. 

The cause of the bug seems to be related to the logic for cloning layers and mapping input/output tensors in the `_clone_functional_model` function. It's likely that the code is not properly updating the `layer_map` and `tensor_map`, resulting in missing mappings for the layer and its associated tensors.

To fix the bug, we should carefully review the logic for creating and mapping the input/output tensors, as well as the cloning process for layers, ensuring that the mapping between original and cloned layers and their associated tensors is properly maintained.

Here's the corrected version of the `_clone_functional_model` function:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)

    if isinstance(model, Sequential):
        raise ValueError('Expected `model` argument to be a functional `Model` instance, got a `Sequential` instance instead:', model)

    layer_map = {}  # Cache for created layers.
    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}

    # Create new input tensors if not provided
    if input_tensors is None:
        input_tensors = [Input(batch_shape=layer.input.shape[1:]) for layer in model._input_layers]
    else:
        input_tensors = to_list(input_tensors)

    # Map the input tensors to the model's input layers
    for original, cloned in zip(model._input_layers, input_tensors):
        layer_map[original] = cloned

    # Iterate over every node in the reference model, in depth order
    for depth, nodes in model._nodes_by_depth.items():
        for node in nodes:
            layer = node.outbound_layer

            # Get or create layer
            if layer not in layer_map:
                # Clone layer
                new_layer = layer.__class__.from_config(layer.get_config())
                layer_map[layer] = new_layer
                layer = new_layer
            else:
                # Reuse previously cloned layer
                layer = layer_map[layer]
                # Don't call InputLayer multiple times
                if isinstance(layer, InputLayer):
                    continue

            # Gather inputs to call the new layer
            reference_input_tensors = node.input_tensors

            # If all previous input tensors are available in tensor_map, then call node.inbound_layer on them
            if all(x in tensor_map for x in reference_input_tensors):
                # Get the corresponding input tensors for the new layer
                inputs = [tensor_map[x][0] for x in reference_input_tensors]

                # Call layer
                output_tensors = to_list(
                    layer.call(inputs, **node.arguments))

                # Update tensor_map
                for x, y in zip(node.output_tensors, output_tensors):
                    tensor_map[x] = (y, None)  # tensor, mask

    # Check that we did compute the model outputs, then instantiate a new model from inputs and outputs
    output_tensors = [tensor_map[x][0] for x in model.outputs]

    return Model(input_tensors, output_tensors, name=model.name)
```

In the corrected version, we have revised the logic for creating and mapping input/output tensors, as well as the process for cloning layers. We ensure that the mappings between original and cloned layers and their associated tensors are correctly maintained throughout the process. This should address the issue of missing mappings and ensure that the model outputs are properly computed during the cloning process.