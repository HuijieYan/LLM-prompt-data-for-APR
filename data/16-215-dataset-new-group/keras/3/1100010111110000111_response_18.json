{
    "keras": [
        {
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 0,
                "1.3.3": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.5": 0,
                "2.1.6": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 1,
                "6": 0,
                "7": 0,
                "8": 1,
                "9": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 0,
                "1.3.3": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.5": 0,
                "2.1.6": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 1,
                "6": 0,
                "7": 0,
                "8": 1,
                "9": 1
            },
            "bugID": 3,
            "start_line": 26,
            "file_name": "keras/models.py",
            "replace_code": "def _clone_functional_model(model, input_tensors=None):\n    if not isinstance(model, Model):\n        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)\n    if isinstance(model, Sequential):\n        raise ValueError('Expected `model` argument to be a functional `Model` instance, got a `Sequential` instance instead:', model)\n\n    from .layers import deserialize, serialize\n    from .engine.topology import get_source_inputs\n    from .engine import training\n    import copy\n\n    layer_map = {}  # Cache for created layers.\n    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}\n    if input_tensors is None:\n        input_layers = [] \n        input_tensors = []\n        for layer in model._input_layers:\n            input_tensor = Input(batch_shape=layer.batch_input_shape,\n                                 dtype=layer.dtype,\n                                 sparse=layer.sparse,\n                                 name=layer.name)\n            input_tensors.append(input_tensor)\n            # Cache newly created input layer.\n            input_layers.append(input_tensor)\n            layer_map[layer] = input_tensor\n    else:\n        input_tensors = to_list(input_tensors)\n    \n    for x, y in zip(get_source_inputs(model.outputs), input_tensors):\n        tensor_map[x] = (y, None)  # tensor, mask\n\n    # Queue of layers that need to be added to the model. It is\n    # basically the layers of the graph in topological order.\n    layer_queue = [(node.outbound_layer, False) for node in training.TopologicalSortWalker(model._nodes_by_depth).inplace_nodes]\n    added_nodes = set()\n\n    # Iterate over every node in the reference model, in depth order.\n    while layer_queue:\n        layer, in_place = layer_queue.pop(0)\n\n        if not in_place and layer in added_nodes:\n            continue\n\n        # If the layer is shared (in a merge layer), we cache\n        # the layer node to avoid any duplication by the cloning.\n        added_nodes.add(layer)\n        \n        # Get existing node.\n        if layer in layer_map:\n            new_layer = layer_map[layer]\n        else:\n            # Clone layer.\n            serialized_layer = serialize(layer)\n            layer = deserialize(serialized_layer)\n            new_layer = layer\n            layer_map[layer] = new_layer\n\n        # Node conductor.\n        conductor = NodeConductor(inbound_layers)\n        topology, _ = conductor.variate(new_layer, model._network_nodes)\n        training.TopologicalSorter(conductor.inbound_nodes).flatten()\n\n        # Iterate nodes (only the inbound nodes). We add more non in-place layers in case\n        # they are not already in the queue.\n        for node in topology:\n            if not node.inbound_layers:\n                continue\n\n            # If the node is already added, there is no need to\n            # do it again.\n            if node in added_nodes:\n                continue\n\n            layer_queue.append((node.outbound_layer, False))\n            added_nodes.add(node)\n\n            input_tensors = []\n            for input_layer, node_index, tensor_index, tensor in zip(node.inbound_layers, node.node_indices, node.tensor_indices, node.input_tensors):\n                if tensor in tensor_map:\n                    new_tensor, mask = tensor_map[tensor]\n                else:\n                    if input_layer in layer_map:\n                        input_layer = layer_map[input_layer]\n                    \n                    new_tensor = copy.copy(tensor)\n                    tensor_map[tensor] = (new_tensor, None)\n                    conductors.append(new_tensor)\n\n                input_tensors.append(new_tensor)\n                # Cache tensor.\n                tensor_map[tensor] = (new_tensor, None)\n\n            # Re-apply node attrs.\n            input_tensors = to_list(input_tensors)\n            if node.arguments:\n                kwargs = node.arguments\n            else:\n                kwargs = {}\n            \n            # Call layer.\n            outputs = to_list(new_layer(input_tensors, **kwargs))\n            # Cache tensor.\n            outputs = to_list(to_list(outputs))\n\n            # Update tensor_map.\n            for tensor, ref_output in zip(outputs, node.output_tensors):\n                tensor_map[ref_output] = (tensor, None)\n\n    # Check that we did compute the model outputs,\n    # then instantiate a new model from inputs and outputs.\n    output_tensors = []\n    for x in get_source_inputs(data=model.outputs):\n        if x in tensor_map:\n            tensor, _ = tensor_map[x]\n            output_tensors.append(tensor)\n\n    if len(output_tensors) > 1:\n        return Model(get_source_inputs(data=model.inputs), output_tensors, name=model.name)\n    else:\n        return Model(input_tensors, output_tensors[0], name=model.name)",
            "import_list": [
                "from .layers import deserialize, serialize",
                "from .engine.topology import get_source_inputs",
                "from .engine import training",
                "import copy"
            ]
        }
    ]
}