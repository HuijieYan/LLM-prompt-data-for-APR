To debug the issue, let's start by analyzing the current implementation and the failing test case. The failing test case `test_clone_functional_model_with_multi_outputs` from the `test_sequential_model.py` file is trying to clone a functional model with multiple outputs and then compare the predictions from the original model and the cloned model.

The error message indicates that the model cloning process is unable to compute the output, specifically the tensor named `swap_layer_1/Identity:0`. This suggests that there is an issue with the model cloning process, as it fails to properly handle the cloning of a model with multiple outputs.

Looking at the runtime values and types of the input parameters, the `model` contains input layers, input tensors, nodes by depth, outputs, and a name. The variables `layer_map` and `tensor_map` store the mapping of layers and tensors during the cloning process. The function seems to initialize placeholders if `input_tensors` are not provided, cache input layers, and then iterate through the nodes to compute the new model outputs. However, it encounters an issue where it cannot compute the output tensor.

One potential strategy for fixing the bug is to carefully handle the creation and mapping of layers and tensors during the model cloning process. This includes ensuring that multiple outputs and their respective masks are properly handled, and the layers are cloned appropriately.

Here's a corrected version of the `_clone_functional_model` function to address the issue:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)
    
    layer_map = {}  
    tensor_map = {}  
    
    if input_tensors is None:
        input_layers = []
        input_tensors = []
        for layer in model._input_layers:
            input_tensor = Input(batch_shape=layer.batch_input_shape,
                                 dtype=layer.dtype,
                                 sparse=layer.sparse,
                                 name=layer.name)
            input_tensors.append(input_tensor)
            layer_map[layer] = input_tensor

    for x, y in zip(model.inputs, input_tensors):
        tensor_map[x] = (y, None)  
    
    for depth, nodes in model._nodes_by_depth.items():
        for node in nodes:
            layer = node.outbound_layer
            if layer not in layer_map:
                new_layer = layer.__class__.from_config(layer.get_config())
                layer_map[layer] = new_layer

            reference_input_tensors = node.input_tensors
            reference_output_tensors = node.output_tensors
            
            computed_data = []  
            for x in reference_input_tensors:
                if x in tensor_map:
                    computed_data.append(tensor_map[x])

            if len(computed_data) == len(reference_input_tensors):
                kwargs = node.arguments if node.arguments else {}
                computed_tensors = [x[0] for x in computed_data]
                computed_masks = [x[1] for x in computed_data]
                output_tensors = to_list(
                    layer(computed_tensors, **kwargs))
                if has_arg(layer.call, 'mask'):
                    kwargs['mask'] = computed_masks
                    output_masks = to_list(
                        layer.compute_mask(computed_tensors, computed_masks))
                else:
                    output_masks = [None] * len(output_tensors)

                for x, y, mask in zip(reference_output_tensors, output_tensors, output_masks):
                    tensor_map[x] = (y, mask)

    output_tensors = [tensor_map[x][0] for x in model.outputs]
    return Model(input_tensors, output_tensors, name=model.name)
```
In this corrected version, the function ensures that the input tensors and their mappings are properly handled. It also iterates through the nodes and effectively computes the output tensors and their corresponding masks.

By implementing these changes, the model cloning process should now be able to handle models with multiple outputs and produce the correct output tensors. This should resolve the issue reported in the GitHub thread and make the failing test case pass.