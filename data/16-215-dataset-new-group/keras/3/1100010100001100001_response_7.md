The cause of the bug in the provided function lies in the incorrect handling of input tensors and the cloning of layers. The function fails to properly create input tensors and clone layers, resulting in an incorrect return value.

To fix the bug, we need to ensure that the input tensors are created and handled correctly, and that the layers are cloned properly in a manner that preserves the model's behavior. We also need to ensure that all output tensors are correctly computed.

Here's the corrected version of the function:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)
    if isinstance(model, Sequential):
        raise ValueError('Expected `model` argument to be a functional `Model` instance, got a `Sequential` instance instead:', model)

    layer_map = {}
    tensor_map = {}
    if input_tensors is None:
        input_tensors = [Input(batch_shape=layer.batch_input_shape, dtype=layer.dtype, sparse=layer.sparse, name=layer.name) for layer in model._input_layers]
        layer_map = {orig: new for orig, new in zip(model._input_layers, input_tensors)}
    else:
        for i, tensor in enumerate(input_tensors):
            if not K.is_keras_tensor(tensor):
                name = model._input_layers[i].name
                input_tensor = Input(tensor=tensor, name='input_wrapper_for_' + name)
                input_tensors[i] = input_tensor
                layer_map[model._input_layers[i]] = input_tensor

    for x, y in zip(model.inputs, input_tensors):
        tensor_map[x] = (y, None)

    for depth in sorted(model._nodes_by_depth.keys(), reverse=True):
        for node in model._nodes_by_depth[depth]:
            layer = node.outbound_layer
            if layer not in layer_map:
                new_layer = layer.__class__.from_config(layer.get_config())
                layer_map[layer] = new_layer

            ref_input_tensors = node.input_tensors
            ref_output_tensors = node.output_tensors

            computed_data = []
            for tensor in ref_input_tensors:
                if tensor in tensor_map:
                    computed_data.append(tensor_map[tensor])

            if len(computed_data) == len(ref_input_tensors):
                kwargs = node.arguments if node.arguments else {}
                computed_tensors, computed_masks = zip(*computed_data) if computed_data else ([], [])
                if 'mask' in kwargs and len(computed_tensors) == 1:
                    kwargs['mask'] = computed_masks[0]
                output_tensors = to_list(layer_map[layer](computed_tensors, **kwargs))
                output_masks = to_list(layer_map[layer].compute_mask(computed_tensors, computed_masks))
                for orig_tensor, new_tensor, mask in zip(ref_output_tensors, output_tensors, output_masks):
                    tensor_map[orig_tensor] = (new_tensor, mask)

    output_tensors = [tensor_map[x][0] for x in model.outputs]

    return Model(input_tensors, output_tensors, name=model.name)
```

In this corrected version, the function properly creates input tensors if `input_tensors` is not provided, and also correctly handles input tensors that are provided. It clones layers as needed and computes the output tensors at the end. This should address the previously encountered issues with the buggy function.