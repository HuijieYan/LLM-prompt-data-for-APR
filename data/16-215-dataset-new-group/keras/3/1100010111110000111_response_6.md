The bug in the `_clone_functional_model` function is causing an error when using the `clone_model` function from Keras. The error message is "AssertionError: Could not compute output Tensor("swap_layer_1/Identity:0", shape=(?, 4), dtype=float32)", and the issue raised in GitHub also reports a similar error. The issue seems to be related to the inability to compute the output tensor due to an incorrect handling of the layers.

The bug likely originates from the logic to create new layers and handle input tensors, clone layers, and call the new layer within the `_clone_functional_model` function.

To fix the bug, the `_clone_functional_model` function should be refactored to correctly create new layers, handle input tensors, and clone layers. Additionally, the function should handle layer output comprehensively to ensure that the error reported in the failing test and the GitHub issue does not occur.

Here's the corrected version of the `_clone_functional_model` function based on the analysis provided:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)

    layer_map = {}  # Cache for created layers.
    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}
    
    if input_tensors is None:
        input_layers = []
        input_tensors = []
        
        for layer in model._input_layers:
            input_tensor = Input(batch_shape=layer.batch_input_shape,
                                 dtype=layer.dtype,
                                 sparse=layer.sparse,
                                 name=layer.name)
            input_tensors.append(input_tensor)
            # Cache newly created input layer.
            layer_map[layer] = input_tensor
        
    else:
        input_tensors = to_list(input_tensors)
        
        for i, x in enumerate(input_tensors):
            input_layer = model._input_layers[i]
            
            if not K.is_keras_tensor(x):
                input_tensor = Input(tensor=x, name='input_wrapper_for_' + input_layer.name)
                layer_map[input_layer] = input_tensor
                input_tensors[i] = input_tensor

        input_tensors = to_list(input_tensors)
                
    tensor_map = {original_tensor: new_tensor for original_tensor, new_tensor in zip(model.inputs, input_tensors)}
    
    for layer in model._input_layers:
        layer_map[layer] = input_tensor
    
    for layer in model.layers:
        layer_config = layer.get_config()
        new_layer = layer.__class__.from_config(layer_config)
        layer_map[layer] = new_layer
    
    for node in model._nodes_by_depth:
        reference_output_tensors = node.output_tensors
        computed_data = []
        
        for x in node.input_tensors:
            if x in tensor_map:
                computed_data.append(tensor_map[x])

        if len(computed_data) == len(node.input_tensors):
            kwargs = node.arguments if node.arguments else {}
            computed_tensors = [x[0] for x in computed_data]
            computed_masks = [x[1] for x in computed_data]

            if has_arg(node.outbound_layer.call, 'mask'):
                kwargs['mask'] = computed_masks

            output_tensors = to_list(node.outbound_layer(computed_tensors, **kwargs))

            if has_arg(node.outbound_layer.call, 'mask'):
                output_masks = to_list(node.outbound_layer.compute_mask(computed_tensors, computed_masks))
                for output_tensor, output_mask in zip(reference_output_tensors, output_tensors, output_masks):
                    tensor_map[output_tensor] = (output_tensor, output_mask)
            else:
                for output_tensor in reference_output_tensors:
                    tensor_map[output_tensor] = (output_tensor, None)
                    
    output_tensors = [tensor_map[x][0] for x in model.outputs]
    
    return Model(input_tensors, output_tensors, name=model.name)
```

The corrected version of the `_clone_functional_model` function provided above addresses the bug and should resolve the issue reported in the GitHub.