### Bug Fix Strategy:
The bug lies in the `_clone_functional_model` function, specifically in the section where it iterates through the model's nodes and tries to compute the output tensors. The failing test indicates an issue with computing the output tensors, and the GitHub issue further details how the error could appear when using a functional model with a layer that has multiple outputs without mask support.

To fix the bug, we need to address how the function handles the output masks and ensure that it correctly computes and handles the output tensors, especially when dealing with layers that have multiple outputs without mask support.

### The corrected version of the function:

```python
def _clone_functional_model(model, input_tensors=None):
    # ... (existing code)

    # Iterated over every node in the reference model, in depth order.
    depth_keys = list(model._nodes_by_depth.keys())
    depth_keys.sort(reverse=True)
    for depth in depth_keys:
        nodes = model._nodes_by_depth[depth]
        for node in nodes:
            # Recover the corresponding layer.
            layer = node.outbound_layer

            # Get or create layer.
            if layer not in layer_map:
                # Clone layer.
                new_layer = layer.__class__.from_config(layer.get_config())
                layer_map[layer] = new_layer
                layer = new_layer
            else:
                # Reuse previously cloned layer.
                layer = layer_map[layer]
                # Don't call InputLayer multiple times.
                if isinstance(layer, InputLayer):
                    continue

            # Gather inputs to call the new layer.
            reference_input_tensors = node.input_tensors
            reference_output_tensors = node.output_tensors

            # If all previous input tensors are available in tensor_map,
            # then call node.inbound_layer on them.
            computed_data = []  # List of tuples (input, mask).
            all_input_tensors_in_map = True  # Flag to check if all input tensors are available
            for x in reference_input_tensors:
                if x in tensor_map:
                    computed_data.append(tensor_map[x])
                else:
                    all_input_tensors_in_map = False
                    break

            if all_input_tensors_in_map:
                # Call layer.
                if node.arguments:
                    kwargs = node.arguments
                else:
                    kwargs = {}
                computed_tensors = [x[0] for x in computed_data]
                if has_arg(layer.call, 'mask'):
                    kwargs['mask'] = [x[1] for x in computed_data if x[1] is not None]
                output_tensors = to_list(
                    layer(computed_tensors, **kwargs))
                if has_arg(layer.call, 'mask'):
                    output_masks = to_list(
                        layer.compute_mask(computed_tensors,
                                           [x[1] for x in computed_data if x[1] is not None]))
                else:
                    output_masks = [None] * len(output_tensors)
                # Update tensor_map.
                for x, y, mask in zip(reference_output_tensors,
                                      output_tensors,
                                      output_masks):
                    tensor_map[x] = (y, mask)

    # Check that we did compute the model outputs,
    # then instantiate a new model from inputs and outputs.
    output_tensors = []
    for x in model.outputs:
        assert x in tensor_map, 'Could not compute output ' + str(x)
        tensor, _ = tensor_map[x]
        output_tensors.append(tensor)
    return Model(input_tensors, output_tensors, name=model.name)
```

This corrected version addresses the issue of computing the output tensors and masks correctly, especially when dealing with layers that have multiple outputs without mask support. It also handles the possibility that some input tensors may not have masks, ensuring a more robust cloning process for functional models.