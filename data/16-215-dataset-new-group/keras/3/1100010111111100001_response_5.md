The error message and the runtime input/output variable values indicate that the bug is likely in the logic for handling input tensors and creating placeholders. The error message also reveals that the model outputs are not being computed correctly. 

To fix the bug, we need to ensure that the input tensors are correctly handled and that the model outputs are successfully computed. We should also check that the model argument is a functional Model instance and not a Sequential instance.

Here's the corrected version of the _clone_functional_model function:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, but got {}'.format(type(model)))
    if isinstance(model, Sequential):
        raise ValueError('Expected `model` argument to be a functional `Model` instance, but got a `Sequential` instance instead: {}'.format(model))

    layer_map = {}  # Cache for created layers
    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}
    if input_tensors is None:
        input_tensors = [K.placeholder(shape=layer.batch_input_shape[1:], dtype=layer.dtype) for layer in model._input_layers]
    else:
        input_tensors = to_list(input_tensors)
        for x in input_tensors:
            if not K.is_keras_tensor(x):
                x = K.placeholder(shape=x.shape[1:], dtype=x.dtype)
    
    for x, y in zip(model.inputs, input_tensors):
        tensor_map[x] = (y, None)  # tensor, mask

    depth_keys = list(model._nodes_by_depth.keys())
    depth_keys.sort(reverse=True)
    for depth in depth_keys:
        nodes = model._nodes_by_depth[depth]
        for node in nodes:
            layer = node.outbound_layer

            if layer not in layer_map:
                new_layer = layer.__class__.from_config(layer.get_config())
                layer_map[layer] = new_layer
            else:
                layer = layer_map[layer]
                if isinstance(layer, InputLayer):
                    continue

            reference_input_tensors = node.input_tensors
            reference_output_tensors = node.output_tensors

            computed_data = []
            for x in reference_input_tensors:
                if x in tensor_map:
                    computed_data.append(tensor_map[x])

            if len(computed_data) == len(reference_input_tensors):
                kwargs = node.arguments if node.arguments else {}
                computed_tensors = [x[0] for x in computed_data]
                computed_masks = [x[1] for x in computed_data] if any(x[1] for x in computed_data) else None

                output_tensors = to_list(layer(computed_tensors, **kwargs))
                output_masks = to_list(layer.compute_mask(computed_tensors, computed_masks))

                for x, y, mask in zip(reference_output_tensors, output_tensors, output_masks):
                    tensor_map[x] = (y, mask)

    output_tensors = [tensor_map[x][0] for x in model.outputs]

    return Model(input_tensors, output_tensors, name=model.name)
```

In this corrected version, we handle the creation of input_tensors correctly and ensure that they are added to the tensor_map. Additionally, we make sure that the model outputs are successfully computed and added to the output_tensors list for the new model. These changes should fix the bug and make the function pass the failing test.