Please fix the function/method provided below and provide the corrected function/method as the output.


# Buggy function source code
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/scrapy_32/scrapy/crawler.py

# relative function's signature in this file
def __init__(self, spidercls, settings):
    # ... omitted code ...
    pass

# relative function's signature in this file
def __init__(self, settings):
    # ... omitted code ...
    pass

# relative function's signature in this file
def __init__(self, settings):
    # ... omitted code ...
    pass

# relative function's signature in this file
def _signal_shutdown(self, signum, _):
    # ... omitted code ...
    pass

# class declaration containing the buggy function
class CrawlerProcess(CrawlerRunner):
    """
    A class to run multiple scrapy crawlers in a process simultaneously.
    
    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support
    for starting a Twisted `reactor`_ and handling shutdown signals, like the
    keyboard interrupt command Ctrl-C. It also configures top-level logging.
    
    This utility should be a better fit than
    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another
    Twisted `reactor`_ within your application.
    
    The CrawlerProcess object must be instantiated with a
    :class:`~scrapy.settings.Settings` object.
    
    This class shouldn't be needed (since Scrapy is responsible of using it
    accordingly) unless writing scripts that manually handle the crawling
    process. See :ref:`run-from-script` for an example.
    """

    # ... omitted code ...


    # signature of a relative function in this class
    def __init__(self, settings):
        # ... omitted code ...
        pass

    # signature of a relative function in this class
    def _signal_shutdown(self, signum, _):
        # ... omitted code ...
        pass



    # this is the buggy function you need to fix
    def __init__(self, settings):
        super(CrawlerProcess, self).__init__(settings)
        install_shutdown_handlers(self._signal_shutdown)
        configure_logging(settings)
        log_scrapy_info(settings)
    
```

# Variable runtime value and type inside buggy function
## Buggy case 1
### input parameter runtime value and type for buggy function
self, value: `<scrapy.crawler.CrawlerProcess object at 0x10b2c20d0>`, type: `CrawlerProcess`

settings, value: `{'foo': 'bar'}`, type: `dict`

### variable runtime value and type before buggy function return
self.settings, value: `<scrapy.settings.Settings object at 0x10b2c2700>`, type: `Settings`



# A test function for the buggy function
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/scrapy_32/tests/test_crawler.py

    def test_crawler_process_accepts_dict(self):
        runner = CrawlerProcess({'foo': 'bar'})
        self.assertEqual(runner.settings['foo'], 'bar')
        self.assertEqual(
            runner.settings['RETRY_ENABLED'],
            default_settings.RETRY_ENABLED
        )
        self.assertIsInstance(runner.settings, Settings)
```

## Error message from test function
```text
self = <tests.test_crawler.CrawlerProcessTest testMethod=test_crawler_process_accepts_dict>

    def test_crawler_process_accepts_dict(self):
>       runner = CrawlerProcess({'foo': 'bar'})

/Volumes/SSD2T/bgp_envs/repos/scrapy_32/tests/test_crawler.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Volumes/SSD2T/bgp_envs/repos/scrapy_32/scrapy/crawler.py:213: in __init__
    log_scrapy_info(settings)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

settings = {'foo': 'bar'}

    def log_scrapy_info(settings):
        logger.info("Scrapy %(version)s started (bot: %(bot)s)",
>                   {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})
E       KeyError: 'BOT_NAME'

/Volumes/SSD2T/bgp_envs/repos/scrapy_32/scrapy/utils/log.py:108: KeyError

```


