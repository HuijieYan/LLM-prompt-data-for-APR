{
    "1.1.1": "def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    return (\n        to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n\n        # default encoding for path component SHOULD be UTF-8\n        quote(to_bytes(parts.path, path_encoding), _safe_chars),\n        quote(to_bytes(parts.params, path_encoding), _safe_chars),\n\n        # encoding of query and fragment follows page encoding\n        # or form-charset (if known and passed)\n        quote(to_bytes(parts.query, encoding), _safe_chars),\n        quote(to_bytes(parts.fragment, encoding), _safe_chars)\n    )\n",
    "1.1.2": null,
    "1.2.1": null,
    "1.2.2": null,
    "1.2.3": null,
    "1.2.4": null,
    "1.2.5": null,
    "1.3.1": "scrapy/utils/url.py",
    "1.3.2": null,
    "1.4.1": [
        "    def test_canonicalize_url_idna_exceptions(self):\n        # missing DNS label\n        self.assertEqual(\n            canonicalize_url(u\"http://.example.com/r\u00e9sum\u00e9?q=r\u00e9sum\u00e9\"),\n            \"http://.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\")\n\n        # DNS label too long\n        self.assertEqual(\n            canonicalize_url(\n                u\"http://www.{label}.com/r\u00e9sum\u00e9?q=r\u00e9sum\u00e9\".format(\n                    label=u\"example\"*11)),\n            \"http://www.{label}.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\".format(\n                    label=u\"example\"*11))"
    ],
    "1.4.2": [
        "tests/test_utils_url.py"
    ],
    "2.1.1": [
        [
            "E                   UnicodeError: label empty or too long",
            "E       UnicodeError: encoding with 'idna' codec failed (UnicodeError: label empty or too long)"
        ]
    ],
    "2.1.2": [
        [
            "self = <encodings.idna.Codec object at 0x10d8d0d00>, input = '.example.com'\nerrors = 'strict'\n\n    def encode(self, input, errors='strict'):\n    \n        if errors != 'strict':\n            # IDNA is quite clear that implementations must be strict\n            raise UnicodeError(\"unsupported error handling \"+errors)\n    \n        if not input:\n            return b'', 0\n    \n        try:\n            result = input.encode('ascii')\n        except UnicodeEncodeError:\n            pass\n        else:\n            # ASCII name: fast path\n            labels = result.split(b'.')\n            for label in labels[:-1]:\n                if not (0 < len(label) < 64):\n>                   raise UnicodeError(\"label empty or too long\")",
            "\n/usr/local/Cellar/python@3.8/3.8.18_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/encodings/idna.py:163: UnicodeError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_utils_url.CanonicalizeUrlTest testMethod=test_canonicalize_url_idna_exceptions>\n\n    def test_canonicalize_url_idna_exceptions(self):\n        # missing DNS label\n        self.assertEqual(\n>           canonicalize_url(u\"http://.example.com/r\u00e9sum\u00e9?q=r\u00e9sum\u00e9\"),\n            \"http://.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\")\n\n/Volumes/SSD2T/bgp_envs/repos/scrapy_15/tests/test_utils_url.py:271: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/Volumes/SSD2T/bgp_envs/repos/scrapy_15/scrapy/utils/url.py:84: in canonicalize_url\n    scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nparts = ParseResult(scheme='http', netloc='.example.com', path='/r\u00e9sum\u00e9', params='', query='q=r\u00e9sum\u00e9', fragment='')\nencoding = None, path_encoding = 'utf8'\n\n    def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n        return (\n            to_native_str(parts.scheme),\n>           to_native_str(parts.netloc.encode('idna')),\n    \n            # default encoding for path component SHOULD be UTF-8\n            quote(to_bytes(parts.path, path_encoding), _safe_chars),\n            quote(to_bytes(parts.params, path_encoding), _safe_chars),\n    \n            # encoding of query and fragment follows page encoding\n            # or form-charset (if known and passed)\n            quote(to_bytes(parts.query, encoding), _safe_chars),\n            quote(to_bytes(parts.fragment, encoding), _safe_chars)\n        )",
            "\n/Volumes/SSD2T/bgp_envs/repos/scrapy_15/scrapy/utils/url.py:46: UnicodeError"
        ]
    ],
    "2.1.3": [
        [
            {
                "parts.scheme": {
                    "value": "'http'",
                    "shape": "4",
                    "omitted": false
                },
                "parts": {
                    "value": "ParseResult(scheme='http', netloc='.example.com', path='/r\u00e9sum\u00e9', params='', query='q=r\u00e9sum\u00e9', fragment='')",
                    "shape": "6",
                    "omitted": false
                },
                "parts.netloc": {
                    "value": "'.example.com'",
                    "shape": "12",
                    "omitted": false
                },
                "parts.path": {
                    "value": "'/r\u00e9sum\u00e9'",
                    "shape": "7",
                    "omitted": false
                },
                "path_encoding": {
                    "value": "'utf8'",
                    "shape": "4",
                    "omitted": false
                },
                "parts.params": {
                    "value": "''",
                    "shape": "0",
                    "omitted": false
                },
                "parts.query": {
                    "value": "'q=r\u00e9sum\u00e9'",
                    "shape": "8",
                    "omitted": false
                },
                "parts.fragment": {
                    "value": "''",
                    "shape": "0",
                    "omitted": false
                }
            },
            {}
        ]
    ],
    "2.1.4": [
        [
            {
                "parts.scheme": "str",
                "parts": "ParseResult",
                "parts.netloc": "str",
                "parts.path": "str",
                "path_encoding": "str",
                "parts.params": "str",
                "parts.query": "str",
                "parts.fragment": "str"
            },
            {}
        ]
    ],
    "2.1.5": [
        [
            {
                "parts.netloc": {
                    "value": "'.example.com'",
                    "shape": "12",
                    "omitted": false
                },
                "parts": {
                    "value": "ParseResult(scheme='http', netloc='.example.com', path='/r\u00e9sum\u00e9', params='', query='q=r\u00e9sum\u00e9', fragment='')",
                    "shape": "6",
                    "omitted": false
                },
                "parts.scheme": {
                    "value": "'http'",
                    "shape": "4",
                    "omitted": false
                },
                "parts.path": {
                    "value": "'/r\u00e9sum\u00e9'",
                    "shape": "7",
                    "omitted": false
                },
                "path_encoding": {
                    "value": "'utf8'",
                    "shape": "4",
                    "omitted": false
                },
                "parts.params": {
                    "value": "''",
                    "shape": "0",
                    "omitted": false
                },
                "parts.query": {
                    "value": "'q=r\u00e9sum\u00e9'",
                    "shape": "8",
                    "omitted": false
                },
                "parts.fragment": {
                    "value": "''",
                    "shape": "0",
                    "omitted": false
                }
            },
            {
                "netloc": {
                    "value": "'.example.com'",
                    "shape": "12",
                    "omitted": false
                }
            }
        ],
        [
            {
                "parts.netloc": {
                    "value": "'www.exampleexampleexampleexampleexampleexampleexampleexampleexampleexampleexample.com'",
                    "shape": "85",
                    "omitted": false
                },
                "parts": {
                    "value": "ParseResult(scheme='http', netloc='www.exampleexampleexampleexampleexampleexampleexampleexampleexampleexampleexample.com', path='/r\u00e9sum\u00e9', params='', query='q=r\u00e9sum\u00e9', fragment='')",
                    "shape": "6",
                    "omitted": false
                },
                "parts.scheme": {
                    "value": "'http'",
                    "shape": "4",
                    "omitted": false
                },
                "parts.path": {
                    "value": "'/r\u00e9sum\u00e9'",
                    "shape": "7",
                    "omitted": false
                },
                "path_encoding": {
                    "value": "'utf8'",
                    "shape": "4",
                    "omitted": false
                },
                "parts.params": {
                    "value": "''",
                    "shape": "0",
                    "omitted": false
                },
                "parts.query": {
                    "value": "'q=r\u00e9sum\u00e9'",
                    "shape": "8",
                    "omitted": false
                },
                "parts.fragment": {
                    "value": "''",
                    "shape": "0",
                    "omitted": false
                }
            },
            {
                "netloc": {
                    "value": "'www.exampleexampleexampleexampleexampleexampleexampleexampleexampleexampleexample.com'",
                    "shape": "85",
                    "omitted": false
                }
            }
        ]
    ],
    "2.1.6": [
        [
            {
                "parts.netloc": "str",
                "parts": "ParseResult",
                "parts.scheme": "str",
                "parts.path": "str",
                "path_encoding": "str",
                "parts.params": "str",
                "parts.query": "str",
                "parts.fragment": "str"
            },
            {
                "netloc": "str"
            }
        ],
        [
            {
                "parts.netloc": "str",
                "parts": "ParseResult",
                "parts.scheme": "str",
                "parts.path": "str",
                "path_encoding": "str",
                "parts.params": "str",
                "parts.query": "str",
                "parts.fragment": "str"
            },
            {
                "netloc": "str"
            }
        ]
    ],
    "3.1.1": [
        "Unicode Link Extractor\n"
    ],
    "3.1.2": [
        "When using the following to extract all of the links from a response:\n\nself.link_extractor = LinkExtractor()\n...\nlinks = self.link_extractor.extract_links(response)\nOn rare occasions, the following error is thrown:\n\n2016-05-25 12:13:55,432 [root] [ERROR]  Error on http://detroit.curbed.com/2016/5/5/11605132/tiny-house-designer-show, traceback: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 1203, in mainLoop\n    self.runUntilCurrent()\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 825, in runUntilCurrent\n    call.func(*call.args, **call.kw)\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 393, in callback\n    self._startRunCallbacks(result)\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 501, in _startRunCallbacks\n    self._runCallbacks()\n--- <exception caught here> ---\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 588, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/var/www/html/DomainCrawler/DomainCrawler/spiders/hybrid_spider.py\", line 223, in parse\n    items.extend(self._extract_requests(response))\n  File \"/var/www/html/DomainCrawler/DomainCrawler/spiders/hybrid_spider.py\", line 477, in _extract_requests\n    links = self.link_extractor.extract_links(response)\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/linkextractors/lxmlhtml.py\", line 111, in extract_links\n    all_links.extend(self._process_links(links))\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/linkextractors/__init__.py\", line 103, in _process_links\n    link.url = canonicalize_url(urlparse(link.url))\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/utils/url.py\", line 85, in canonicalize_url\n    parse_url(url), encoding=encoding)\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/utils/url.py\", line 46, in _safe_ParseResult\n    to_native_str(parts.netloc.encode('idna')),\n  File \"/usr/local/lib/python2.7/encodings/idna.py\", line 164, in encode\n    result.append(ToASCII(label))\n  File \"/usr/local/lib/python2.7/encodings/idna.py\", line 73, in ToASCII\n    raise UnicodeError(\"label empty or too long\")\nexceptions.UnicodeError: label empty or too long\nI was able to find some information concerning the error from here.\nMy question is: What is the best way to handle this? Even if there is one bad link in the response, I'd want all of the other good links to be extracted.\n"
    ]
}