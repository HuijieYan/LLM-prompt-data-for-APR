{
    "1.1.1": "def follow(self, url, callback=None, method='GET', headers=None, body=None,\n           cookies=None, meta=None, encoding='utf-8', priority=0,\n           dont_filter=False, errback=None):\n    # type: (...) -> Request\n    \"\"\"\n    Return a :class:`~.Request` instance to follow a link ``url``.\n    It accepts the same arguments as ``Request.__init__`` method,\n    but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,\n    not only an absolute URL.\n    \n    :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow` \n    method which supports selectors in addition to absolute/relative URLs\n    and Link objects.\n    \"\"\"\n    if isinstance(url, Link):\n        url = url.url\n    url = self.urljoin(url)\n    return Request(url, callback,\n                   method=method,\n                   headers=headers,\n                   body=body,\n                   cookies=cookies,\n                   meta=meta,\n                   encoding=encoding,\n                   priority=priority,\n                   dont_filter=dont_filter,\n                   errback=errback)\n",
    "1.1.2": "Return a :class:`~.Request` instance to follow a link ``url``.\nIt accepts the same arguments as ``Request.__init__`` method,\nbut ``url`` can be a relative URL or a ``scrapy.link.Link`` object,\nnot only an absolute URL.\n\n:class:`~.TextResponse` provides a :meth:`~.TextResponse.follow` \nmethod which supports selectors in addition to absolute/relative URLs\nand Link objects.",
    "1.2.1": "class Response(object_ref)",
    "1.2.2": null,
    "1.2.3": [
        "meta(self)",
        "urljoin(self, url)"
    ],
    "1.2.4": null,
    "1.2.5": null,
    "1.3.1": "scrapy/http/response/__init__.py",
    "1.3.2": [
        "meta(self)",
        "urljoin(self, url)"
    ],
    "1.4.1": [
        "    def test_follow_None_url(self):\n        r = self.response_class(\"http://example.com\")\n        self.assertRaises(ValueError, r.follow, None)"
    ],
    "1.4.2": [
        "tests/test_http_response.py"
    ],
    "2.1.1": [
        [
            "E       AssertionError: ValueError not raised by follow"
        ]
    ],
    "2.1.2": [
        [
            "self = <tests.test_http_response.BaseResponseTest testMethod=test_follow_None_url>\n\n    def test_follow_None_url(self):\n        r = self.response_class(\"http://example.com\")\n>       self.assertRaises(ValueError, r.follow, None)",
            "\n/Volumes/SSD2T/bgp_envs/repos/scrapy_5/tests/test_http_response.py:160: AssertionError"
        ]
    ],
    "2.1.3": [
        [
            {
                "self.urljoin": "<bound method Response.urljoin of <200 http://example.com>>",
                "self": "<200 http://example.com>",
                "method": "'GET'",
                "encoding": "'utf-8'",
                "priority": "0",
                "dont_filter": "False"
            },
            {
                "url": "'http://example.com'"
            }
        ]
    ],
    "2.1.4": [
        [
            {
                "self.urljoin": "method",
                "self": "Response",
                "method": "str",
                "encoding": "str",
                "priority": "int",
                "dont_filter": "bool"
            },
            {
                "url": "str"
            }
        ]
    ],
    "2.1.5": null,
    "2.1.6": null,
    "3.1.1": [
        "[suggest ] response.follow should raise a exception when called on None or an empty string, instead of crawling the current page again\n"
    ],
    "3.1.2": [
        "response.follow will raise a exception when url='' or none in stead of crawl the (base) page itself again.\n\nnone will use follow to crawl the source(base) page again right? all parsers will be passed without warning if that way.\n\nthanks\n"
    ]
}