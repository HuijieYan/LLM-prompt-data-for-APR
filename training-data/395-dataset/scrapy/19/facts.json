{
    "1.1.1": "def get_origin_req_host(self):\n    return urlparse_cached(self.request).hostname\n",
    "1.1.2": null,
    "1.2.1": "class WrappedRequest(object)",
    "1.2.2": "Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class\n\nsee http://docs.python.org/library/urllib2.html#urllib2.Request",
    "1.2.3": null,
    "1.2.4": null,
    "1.2.5": null,
    "1.3.1": "scrapy/http/cookies.py",
    "1.3.2": null,
    "1.4.1": [
        "    def test_get_origin_req_host(self):\n        self.assertEqual(self.wrapped.get_origin_req_host(), 'www.example.com')\n        self.assertEqual(self.wrapped.origin_req_host, 'www.example.com')"
    ],
    "1.4.2": [
        "tests/test_http_cookies.py"
    ],
    "2.1.1": [
        [
            "E       AttributeError: 'WrappedRequest' object has no attribute 'origin_req_host'"
        ]
    ],
    "2.1.2": [
        [
            "self = <tests.test_http_cookies.WrappedRequestTest testMethod=test_get_origin_req_host>\n\n    def test_get_origin_req_host(self):\n        self.assertEqual(self.wrapped.get_origin_req_host(), 'www.example.com')\n>       self.assertEqual(self.wrapped.origin_req_host, 'www.example.com')",
            "\n/Volumes/SSD2T/bgp_envs/repos/scrapy_19/tests/test_http_cookies.py:38: AttributeError"
        ]
    ],
    "2.1.3": [
        [
            {
                "self.request": {
                    "value": "<GET http://www.example.com/page.html>",
                    "shape": null,
                    "omitted": false
                }
            },
            {}
        ]
    ],
    "2.1.4": [
        [
            {
                "self.request": "Request"
            },
            {}
        ]
    ],
    "2.1.5": [
        [
            {
                "self.request": {
                    "value": "<GET http://www.example.com/page.html>",
                    "shape": null,
                    "omitted": false
                }
            },
            {}
        ]
    ],
    "2.1.6": [
        [
            {
                "self.request": "Request"
            },
            {}
        ]
    ],
    "3.1.1": [
        "PY3: Fail to download the second or later requests to hosts using secure cookies\n"
    ],
    "3.1.2": [
        "Environment\nMac OS X 10.10.5\nPython 3.4.2\nScrapy 1.1.0rc1\nTwisted 15.5.0\nSteps to Reproduce\nSave the following spider as secure_cookie_spider.py.\n\nimport scrapy\n\n\nclass SecureCookieSpider(scrapy.Spider):\n   name = 'secure_cookie_spider'\n   start_urls = [\n       'https://github.com/',\n   ]\n\n   def parse(self, response):\n       # Request the same url again\n       yield scrapy.Request(url=response.url, callback=self.parse_second_request)\n\n   def parse_second_request(self, response):\n       pass\nRun the following command.\n\n$ scrapy runspider secure_cookie_spider.py\nExpected Results\nNo error is reported.\n\nActual Results\nFail to download the second request with AttributeError: 'WrappedRequest' object has no attribute 'type'.\n\n$ scrapy runspider secure_cookie_spider.py\n2016-02-07 11:57:11 [scrapy] INFO: Scrapy 1.1.0rc1 started (bot: scrapybot)\n2016-02-07 11:57:11 [scrapy] INFO: Overridden settings: {}\n2016-02-07 11:57:11 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.logstats.LogStats']\n2016-02-07 11:57:11 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-02-07 11:57:11 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-02-07 11:57:11 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-02-07 11:57:11 [scrapy] INFO: Spider opened\n2016-02-07 11:57:11 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-02-07 11:57:12 [scrapy] DEBUG: Crawled (200) <GET https://github.com/> (referer: None)\n2016-02-07 11:57:12 [scrapy] ERROR: Error downloading <GET https://github.com/>\nTraceback (most recent call last):\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/twisted/internet/defer.py\", line 1128, in _inlineCallbacks\n    result = g.send(result)\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/core/downloader/middleware.py\", line 37, in process_request\n    response = yield method(request=request, spider=spider)\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/downloadermiddlewares/cookies.py\", line 39, in process_request\n    jar.add_cookie_header(request)\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/http/cookies.py\", line 42, in add_cookie_header\n    cookies += self.jar._cookies_for_domain(host, wreq)\n  File \"/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/cookiejar.py\", line 1242, in _cookies_for_domain\n    if not self._policy.return_ok(cookie, request):\n  File \"/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/cookiejar.py\", line 1077, in return_ok\n    if not fn(cookie, request):\n  File \"/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/cookiejar.py\", line 1103, in return_ok_secure\n    if cookie.secure and request.type != \"https\":\nAttributeError: 'WrappedRequest' object has no attribute 'type'\n2016-02-07 11:57:12 [scrapy] INFO: Closing spider (finished)\n2016-02-07 11:57:12 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/exception_count': 1,\n 'downloader/exception_type_count/builtins.AttributeError': 1,\n 'downloader/request_bytes': 211,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 9735,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 2, 7, 2, 57, 12, 757829),\n 'log_count/DEBUG': 1,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 7,\n 'request_depth_max': 1,\n 'response_received_count': 1,\n 'scheduler/dequeued': 2,\n 'scheduler/dequeued/memory': 2,\n 'scheduler/enqueued': 2,\n 'scheduler/enqueued/memory': 2,\n 'start_time': datetime.datetime(2016, 2, 7, 2, 57, 11, 384330)}\n2016-02-07 11:57:12 [scrapy] INFO: Spider closed (finished)\nNote that no error is reported in Python 2.\n"
    ]
}