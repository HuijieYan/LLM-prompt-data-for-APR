{
    "1.1.1": "def _robots_error(self, failure, netloc):\n    self._parsers.pop(netloc).callback(None)\n",
    "1.1.2": null,
    "1.2.1": "class RobotsTxtMiddleware(object)",
    "1.2.2": null,
    "1.2.3": null,
    "1.3.1": "/Volumes/SSD2T/bgp_envs_non_pandas/repos/scrapy_21/scrapy/downloadermiddlewares/robotstxt.py",
    "1.3.2": null,
    "1.4.1": [
        "    def test_robotstxt_immediate_error(self):\n        self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n        err = error.DNSLookupError('Robotstxt address not found')\n        def immediate_failure(request, spider):\n            deferred = Deferred()\n            deferred.errback(failure.Failure(err))\n            return deferred\n        self.crawler.engine.download.side_effect = immediate_failure\n\n        middleware = RobotsTxtMiddleware(self.crawler)\n        return self.assertNotIgnored(Request('http://site.local'), middleware)"
    ],
    "1.4.2": [
        "/Volumes/SSD2T/bgp_envs_non_pandas/repos/scrapy_21/tests/test_downloadermiddleware_robotstxt.py"
    ],
    "2.1.1": [
        [
            "E       KeyError: 'site.local'"
        ]
    ],
    "2.1.2": [
        [
            "f = <bound method RobotsTxtMiddleware.robot_parser of <scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware object at 0x104656310>>\nargs = (<GET http://site.local>, None), kw = {}\n\n    def maybeDeferred(f, *args, **kw):\n        \"\"\"\n        Invoke a function that may or may not return a L{Deferred}.\n    \n        Call the given function with the given arguments.  If the returned\n        object is a L{Deferred}, return it.  If the returned object is a L{Failure},\n        wrap it with L{fail} and return it.  Otherwise, wrap it in L{succeed} and\n        return it.  If an exception is raised, convert it to a L{Failure}, wrap it\n        in L{fail}, and then return it.\n    \n        @type f: Any callable\n        @param f: The callable to invoke\n    \n        @param args: The arguments to pass to C{f}\n        @param kw: The keyword arguments to pass to C{f}\n    \n        @rtype: L{Deferred}\n        @return: The result of the function call, wrapped in a L{Deferred} if\n        necessary.\n        \"\"\"\n        try:\n>           result = f(*args, **kw)\n\n/Volumes/SSD2T/bgp_envs_non_pandas/envs/scrapy_21/lib/python3.8/site-packages/twisted/internet/defer.py:151: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware object at 0x104656310>\nrequest = <GET http://site.local>, spider = None\n\n    def robot_parser(self, request, spider):\n        url = urlparse_cached(request)\n        netloc = url.netloc\n    \n        if netloc not in self._parsers:\n            self._parsers[netloc] = Deferred()\n            robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={'dont_obey_robotstxt': True}\n            )\n            dfd = self.crawler.engine.download(robotsreq, spider)\n            dfd.addCallback(self._parse_robots, netloc)\n            dfd.addErrback(self._logerror, robotsreq, spider)\n            dfd.addErrback(self._robots_error, netloc)\n    \n>       if isinstance(self._parsers[netloc], Deferred):",
            "\n/Volumes/SSD2T/bgp_envs_non_pandas/repos/scrapy_21/scrapy/downloadermiddlewares/robotstxt.py:65: KeyError"
        ]
    ],
    "2.1.3": null,
    "2.1.4": null,
    "2.1.5": null,
    "2.1.6": null,
    "3.1.1": [
        "\n"
    ],
    "3.1.2": [
        "\n"
    ]
}