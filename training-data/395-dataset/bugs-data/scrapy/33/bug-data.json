{
    "scrapy:33": {
        "/Volumes/SSD2T/bgp_envs/repos/scrapy_33/scrapy/core/engine.py": {
            "buggy_functions": [
                {
                    "function_name": "_next_request_from_scheduler",
                    "function_code": "def _next_request_from_scheduler(self, spider):\n    slot = self.slot\n    request = slot.scheduler.next_request()\n    if not request:\n        return\n    d = self._download(request, spider)\n    d.addBoth(self._handle_downloader_output, request, spider)\n    d.addErrback(lambda f: logger.info('Error while handling downloader output',\n                                       extra={'spider': spider, 'failure': f}))\n    d.addBoth(lambda _: slot.remove_request(request))\n    d.addErrback(lambda f: logger.info('Error while removing request from slot',\n                                       extra={'spider': spider, 'failure': f}))\n    d.addBoth(lambda _: slot.nextcall.schedule())\n    d.addErrback(lambda f: logger.info('Error while scheduling new request',\n                                       extra={'spider': spider, 'failure': f}))\n    return d\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 130,
                    "end_line": 145,
                    "variables": {
                        "slot": [
                            131,
                            132,
                            139,
                            142
                        ],
                        "self.slot": [
                            131
                        ],
                        "self": [
                            136,
                            131,
                            135
                        ],
                        "request": [
                            132,
                            133,
                            135,
                            136,
                            139
                        ],
                        "slot.scheduler.next_request": [
                            132
                        ],
                        "slot.scheduler": [
                            132
                        ],
                        "d": [
                            135,
                            136,
                            137,
                            139,
                            140,
                            142,
                            143,
                            145
                        ],
                        "self._download": [
                            135
                        ],
                        "spider": [
                            135,
                            136,
                            138,
                            141,
                            144
                        ],
                        "d.addBoth": [
                            136,
                            139,
                            142
                        ],
                        "self._handle_downloader_output": [
                            136
                        ],
                        "d.addErrback": [
                            137,
                            140,
                            143
                        ],
                        "logger.info": [
                            137,
                            140,
                            143
                        ],
                        "logger": [
                            137,
                            140,
                            143
                        ],
                        "f": [
                            144,
                            138,
                            141
                        ],
                        "slot.remove_request": [
                            139
                        ],
                        "slot.nextcall.schedule": [
                            142
                        ],
                        "slot.nextcall": [
                            142
                        ]
                    },
                    "filtered_variables": {
                        "slot": [
                            131,
                            132,
                            139,
                            142
                        ],
                        "self.slot": [
                            131
                        ],
                        "self": [
                            136,
                            131,
                            135
                        ],
                        "request": [
                            132,
                            133,
                            135,
                            136,
                            139
                        ],
                        "slot.scheduler.next_request": [
                            132
                        ],
                        "slot.scheduler": [
                            132
                        ],
                        "d": [
                            135,
                            136,
                            137,
                            139,
                            140,
                            142,
                            143,
                            145
                        ],
                        "self._download": [
                            135
                        ],
                        "spider": [
                            135,
                            136,
                            138,
                            141,
                            144
                        ],
                        "d.addBoth": [
                            136,
                            139,
                            142
                        ],
                        "self._handle_downloader_output": [
                            136
                        ],
                        "d.addErrback": [
                            137,
                            140,
                            143
                        ],
                        "logger.info": [
                            137,
                            140,
                            143
                        ],
                        "logger": [
                            137,
                            140,
                            143
                        ],
                        "f": [
                            144,
                            138,
                            141
                        ],
                        "slot.remove_request": [
                            139
                        ],
                        "slot.nextcall.schedule": [
                            142
                        ],
                        "slot.nextcall": [
                            142
                        ]
                    },
                    "diff_line_number": 138,
                    "class_data": {
                        "signature": "class ExecutionEngine(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, crawler, spider_closed_callback):\n    self.crawler = crawler\n    self.settings = crawler.settings\n    self.signals = crawler.signals\n    self.logformatter = crawler.logformatter\n    self.slot = None\n    self.spider = None\n    self.running = False\n    self.paused = False\n    self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n    downloader_cls = load_object(self.settings['DOWNLOADER'])\n    self.downloader = downloader_cls(crawler)\n    self.scraper = Scraper(crawler)\n    self._spider_closed_callback = spider_closed_callback",
                            "@defer.inlineCallbacks\ndef start(self):\n    \"\"\"Start the execution engine\"\"\"\n    assert not self.running, 'Engine already running'\n    self.start_time = time()\n    yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n    self.running = True\n    self._closewait = defer.Deferred()\n    yield self._closewait",
                            "def stop(self):\n    \"\"\"Stop the execution engine gracefully\"\"\"\n    assert self.running, 'Engine not running'\n    self.running = False\n    dfd = self._close_all_spiders()\n    return dfd.addBoth(lambda _: self._finish_stopping_engine())",
                            "def pause(self):\n    \"\"\"Pause the execution engine\"\"\"\n    self.paused = True",
                            "def unpause(self):\n    \"\"\"Resume the execution engine\"\"\"\n    self.paused = False",
                            "def _next_request(self, spider):\n    slot = self.slot\n    if not slot:\n        return\n    if self.paused:\n        slot.nextcall.schedule(5)\n        return\n    while not self._needs_backout(spider):\n        if not self._next_request_from_scheduler(spider):\n            break\n    if slot.start_requests and (not self._needs_backout(spider)):\n        try:\n            request = next(slot.start_requests)\n        except StopIteration:\n            slot.start_requests = None\n        except Exception:\n            slot.start_requests = None\n            logger.error('Error while obtaining start requests', exc_info=True, extra={'spider': spider})\n        else:\n            self.crawl(request, spider)\n    if self.spider_is_idle(spider) and slot.close_if_idle:\n        self._spider_idle(spider)",
                            "def _needs_backout(self, spider):\n    slot = self.slot\n    return not self.running or slot.closing or self.downloader.needs_backout() or self.scraper.slot.needs_backout()",
                            "def _next_request_from_scheduler(self, spider):\n    slot = self.slot\n    request = slot.scheduler.next_request()\n    if not request:\n        return\n    d = self._download(request, spider)\n    d.addBoth(self._handle_downloader_output, request, spider)\n    d.addErrback(lambda f: logger.info('Error while handling downloader output', extra={'spider': spider, 'failure': f}))\n    d.addBoth(lambda _: slot.remove_request(request))\n    d.addErrback(lambda f: logger.info('Error while removing request from slot', extra={'spider': spider, 'failure': f}))\n    d.addBoth(lambda _: slot.nextcall.schedule())\n    d.addErrback(lambda f: logger.info('Error while scheduling new request', extra={'spider': spider, 'failure': f}))\n    return d",
                            "def _handle_downloader_output(self, response, request, spider):\n    assert isinstance(response, (Request, Response, Failure)), response\n    if isinstance(response, Request):\n        self.crawl(response, spider)\n        return\n    d = self.scraper.enqueue_scrape(response, request, spider)\n    d.addErrback(lambda f: logger.error('Error while enqueuing downloader output', extra={'spider': spider, 'failure': f}))\n    return d",
                            "def spider_is_idle(self, spider):\n    scraper_idle = self.scraper.slot.is_idle()\n    pending = self.slot.scheduler.has_pending_requests()\n    downloading = bool(self.downloader.active)\n    pending_start_requests = self.slot.start_requests is not None\n    idle = scraper_idle and (not (pending or downloading or pending_start_requests))\n    return idle",
                            "@property\ndef open_spiders(self):\n    return [self.spider] if self.spider else []",
                            "def has_capacity(self):\n    \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n    return not bool(self.slot)",
                            "def crawl(self, request, spider):\n    assert spider in self.open_spiders, 'Spider %r not opened when crawling: %s' % (spider.name, request)\n    self.schedule(request, spider)\n    self.slot.nextcall.schedule()",
                            "def schedule(self, request, spider):\n    self.signals.send_catch_log(signal=signals.request_scheduled, request=request, spider=spider)\n    if not self.slot.scheduler.enqueue_request(request):\n        self.signals.send_catch_log(signal=signals.request_dropped, request=request, spider=spider)",
                            "def download(self, request, spider):\n    slot = self.slot\n    slot.add_request(request)\n    d = self._download(request, spider)\n    d.addBoth(self._downloaded, slot, request, spider)\n    return d",
                            "def _downloaded(self, response, slot, request, spider):\n    slot.remove_request(request)\n    return self.download(response, spider) if isinstance(response, Request) else response",
                            "def _download(self, request, spider):\n    slot = self.slot\n    slot.add_request(request)\n\n    def _on_success(response):\n        assert isinstance(response, (Response, Request))\n        if isinstance(response, Response):\n            response.request = request\n            logkws = self.logformatter.crawled(request, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            self.signals.send_catch_log(signal=signals.response_received, response=response, request=request, spider=spider)\n        return response\n\n    def _on_complete(_):\n        slot.nextcall.schedule()\n        return _\n    dwld = self.downloader.fetch(request, spider)\n    dwld.addCallbacks(_on_success)\n    dwld.addBoth(_on_complete)\n    return dwld",
                            "@defer.inlineCallbacks\ndef open_spider(self, spider, start_requests=(), close_if_idle=True):\n    assert self.has_capacity(), 'No free spider slot when opening %r' % spider.name\n    logger.info('Spider opened', extra={'spider': spider})\n    nextcall = CallLaterOnce(self._next_request, spider)\n    scheduler = self.scheduler_cls.from_crawler(self.crawler)\n    start_requests = (yield self.scraper.spidermw.process_start_requests(start_requests, spider))\n    slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n    self.slot = slot\n    self.spider = spider\n    yield scheduler.open(spider)\n    yield self.scraper.open_spider(spider)\n    self.crawler.stats.open_spider(spider)\n    yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n    slot.nextcall.schedule()",
                            "def _spider_idle(self, spider):\n    \"\"\"Called when a spider gets idle. This function is called when there\n    are no remaining pages to download or schedule. It can be called\n    multiple times. If some extension raises a DontCloseSpider exception\n    (in the spider_idle signal handler) the spider is not closed until the\n    next loop and this function is guaranteed to be called (at least) once\n    again for this spider.\n    \"\"\"\n    res = self.signals.send_catch_log(signal=signals.spider_idle, spider=spider, dont_log=DontCloseSpider)\n    if any((isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) for _, x in res)):\n        self.slot.nextcall.schedule(5)\n        return\n    if self.spider_is_idle(spider):\n        self.close_spider(spider, reason='finished')",
                            "def close_spider(self, spider, reason='cancelled'):\n    \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n    slot = self.slot\n    if slot.closing:\n        return slot.closing\n    logger.info('Closing spider (%(reason)s)', {'reason': reason}, extra={'spider': spider})\n    dfd = slot.close()\n\n    def log_failure(msg):\n\n        def errback(failure):\n            logger.error(msg, extra={'spider': spider, 'failure': failure})\n        return errback\n    dfd.addBoth(lambda _: self.downloader.close())\n    dfd.addErrback(log_failure('Downloader close failure'))\n    dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n    dfd.addErrback(log_failure('Scraper close failure'))\n    dfd.addBoth(lambda _: slot.scheduler.close(reason))\n    dfd.addErrback(log_failure('Scheduler close failure'))\n    dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(signal=signals.spider_closed, spider=spider, reason=reason))\n    dfd.addErrback(log_failure('Error while sending spider_close signal'))\n    dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n    dfd.addErrback(log_failure('Stats close failure'))\n    dfd.addBoth(lambda _: logger.info('Spider closed (%(reason)s)', {'reason': reason}, extra={'spider': spider}))\n    dfd.addBoth(lambda _: setattr(self, 'slot', None))\n    dfd.addErrback(log_failure('Error while unassigning slot'))\n    dfd.addBoth(lambda _: setattr(self, 'spider', None))\n    dfd.addErrback(log_failure('Error while unassigning spider'))\n    dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n    return dfd",
                            "def _close_all_spiders(self):\n    dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n    dlist = defer.DeferredList(dfds)\n    return dlist",
                            "@defer.inlineCallbacks\ndef _finish_stopping_engine(self):\n    yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n    self._closewait.callback(None)",
                            "def _on_success(response):\n    assert isinstance(response, (Response, Request))\n    if isinstance(response, Response):\n        response.request = request\n        logkws = self.logformatter.crawled(request, response, spider)\n        logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n        self.signals.send_catch_log(signal=signals.response_received, response=response, request=request, spider=spider)\n    return response",
                            "def _on_complete(_):\n    slot.nextcall.schedule()\n    return _",
                            "def log_failure(msg):\n\n    def errback(failure):\n        logger.error(msg, extra={'spider': spider, 'failure': failure})\n    return errback",
                            "def errback(failure):\n    logger.error(msg, extra={'spider': spider, 'failure': failure})"
                        ],
                        "constructor_variables": [
                            "paused",
                            "_spider_closed_callback",
                            "downloader_cls",
                            "downloader",
                            "logformatter",
                            "scheduler_cls",
                            "slot",
                            "settings",
                            "running",
                            "crawler",
                            "spider",
                            "signals",
                            "scraper"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, crawler, spider_closed_callback)",
                            "start(self)",
                            "stop(self)",
                            "pause(self)",
                            "unpause(self)",
                            "_next_request(self, spider)",
                            "_needs_backout(self, spider)",
                            "_next_request_from_scheduler(self, spider)",
                            "_handle_downloader_output(self, response, request, spider)",
                            "spider_is_idle(self, spider)",
                            "open_spiders(self)",
                            "has_capacity(self)",
                            "crawl(self, request, spider)",
                            "schedule(self, request, spider)",
                            "download(self, request, spider)",
                            "_downloaded(self, response, slot, request, spider)",
                            "_download(self, request, spider)",
                            "open_spider(self, spider, start_requests=(), close_if_idle=True)",
                            "_spider_idle(self, spider)",
                            "close_spider(self, spider, reason='cancelled')",
                            "_close_all_spiders(self)",
                            "_finish_stopping_engine(self)",
                            "_on_success(response)",
                            "_on_complete(_)",
                            "log_failure(msg)",
                            "errback(failure)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "_handle_downloader_output",
                    "function_code": "def _handle_downloader_output(self, response, request, spider):\n    assert isinstance(response, (Request, Response, Failure)), response\n    # downloader middleware can return requests (for example, redirects)\n    if isinstance(response, Request):\n        self.crawl(response, spider)\n        return\n    # response is a Response or Failure\n    d = self.scraper.enqueue_scrape(response, request, spider)\n    d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',\n                                        extra={'spider': spider, 'failure': f}))\n    return d\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 147,
                    "end_line": 157,
                    "variables": {
                        "isinstance": [
                            148,
                            150
                        ],
                        "response": [
                            154,
                            148,
                            150,
                            151
                        ],
                        "Request": [
                            148,
                            150
                        ],
                        "Response": [
                            148
                        ],
                        "Failure": [
                            148
                        ],
                        "self.crawl": [
                            151
                        ],
                        "self": [
                            154,
                            151
                        ],
                        "spider": [
                            154,
                            156,
                            151
                        ],
                        "d": [
                            154,
                            155,
                            157
                        ],
                        "self.scraper.enqueue_scrape": [
                            154
                        ],
                        "self.scraper": [
                            154
                        ],
                        "request": [
                            154
                        ],
                        "d.addErrback": [
                            155
                        ],
                        "logger.error": [
                            155
                        ],
                        "logger": [
                            155
                        ],
                        "f": [
                            156
                        ]
                    },
                    "filtered_variables": {
                        "response": [
                            154,
                            148,
                            150,
                            151
                        ],
                        "Request": [
                            148,
                            150
                        ],
                        "Response": [
                            148
                        ],
                        "Failure": [
                            148
                        ],
                        "self.crawl": [
                            151
                        ],
                        "self": [
                            154,
                            151
                        ],
                        "spider": [
                            154,
                            156,
                            151
                        ],
                        "d": [
                            154,
                            155,
                            157
                        ],
                        "self.scraper.enqueue_scrape": [
                            154
                        ],
                        "self.scraper": [
                            154
                        ],
                        "request": [
                            154
                        ],
                        "d.addErrback": [
                            155
                        ],
                        "logger.error": [
                            155
                        ],
                        "logger": [
                            155
                        ],
                        "f": [
                            156
                        ]
                    },
                    "diff_line_number": 156,
                    "class_data": {
                        "signature": "class ExecutionEngine(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, crawler, spider_closed_callback):\n    self.crawler = crawler\n    self.settings = crawler.settings\n    self.signals = crawler.signals\n    self.logformatter = crawler.logformatter\n    self.slot = None\n    self.spider = None\n    self.running = False\n    self.paused = False\n    self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n    downloader_cls = load_object(self.settings['DOWNLOADER'])\n    self.downloader = downloader_cls(crawler)\n    self.scraper = Scraper(crawler)\n    self._spider_closed_callback = spider_closed_callback",
                            "@defer.inlineCallbacks\ndef start(self):\n    \"\"\"Start the execution engine\"\"\"\n    assert not self.running, 'Engine already running'\n    self.start_time = time()\n    yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n    self.running = True\n    self._closewait = defer.Deferred()\n    yield self._closewait",
                            "def stop(self):\n    \"\"\"Stop the execution engine gracefully\"\"\"\n    assert self.running, 'Engine not running'\n    self.running = False\n    dfd = self._close_all_spiders()\n    return dfd.addBoth(lambda _: self._finish_stopping_engine())",
                            "def pause(self):\n    \"\"\"Pause the execution engine\"\"\"\n    self.paused = True",
                            "def unpause(self):\n    \"\"\"Resume the execution engine\"\"\"\n    self.paused = False",
                            "def _next_request(self, spider):\n    slot = self.slot\n    if not slot:\n        return\n    if self.paused:\n        slot.nextcall.schedule(5)\n        return\n    while not self._needs_backout(spider):\n        if not self._next_request_from_scheduler(spider):\n            break\n    if slot.start_requests and (not self._needs_backout(spider)):\n        try:\n            request = next(slot.start_requests)\n        except StopIteration:\n            slot.start_requests = None\n        except Exception:\n            slot.start_requests = None\n            logger.error('Error while obtaining start requests', exc_info=True, extra={'spider': spider})\n        else:\n            self.crawl(request, spider)\n    if self.spider_is_idle(spider) and slot.close_if_idle:\n        self._spider_idle(spider)",
                            "def _needs_backout(self, spider):\n    slot = self.slot\n    return not self.running or slot.closing or self.downloader.needs_backout() or self.scraper.slot.needs_backout()",
                            "def _next_request_from_scheduler(self, spider):\n    slot = self.slot\n    request = slot.scheduler.next_request()\n    if not request:\n        return\n    d = self._download(request, spider)\n    d.addBoth(self._handle_downloader_output, request, spider)\n    d.addErrback(lambda f: logger.info('Error while handling downloader output', extra={'spider': spider, 'failure': f}))\n    d.addBoth(lambda _: slot.remove_request(request))\n    d.addErrback(lambda f: logger.info('Error while removing request from slot', extra={'spider': spider, 'failure': f}))\n    d.addBoth(lambda _: slot.nextcall.schedule())\n    d.addErrback(lambda f: logger.info('Error while scheduling new request', extra={'spider': spider, 'failure': f}))\n    return d",
                            "def _handle_downloader_output(self, response, request, spider):\n    assert isinstance(response, (Request, Response, Failure)), response\n    if isinstance(response, Request):\n        self.crawl(response, spider)\n        return\n    d = self.scraper.enqueue_scrape(response, request, spider)\n    d.addErrback(lambda f: logger.error('Error while enqueuing downloader output', extra={'spider': spider, 'failure': f}))\n    return d",
                            "def spider_is_idle(self, spider):\n    scraper_idle = self.scraper.slot.is_idle()\n    pending = self.slot.scheduler.has_pending_requests()\n    downloading = bool(self.downloader.active)\n    pending_start_requests = self.slot.start_requests is not None\n    idle = scraper_idle and (not (pending or downloading or pending_start_requests))\n    return idle",
                            "@property\ndef open_spiders(self):\n    return [self.spider] if self.spider else []",
                            "def has_capacity(self):\n    \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n    return not bool(self.slot)",
                            "def crawl(self, request, spider):\n    assert spider in self.open_spiders, 'Spider %r not opened when crawling: %s' % (spider.name, request)\n    self.schedule(request, spider)\n    self.slot.nextcall.schedule()",
                            "def schedule(self, request, spider):\n    self.signals.send_catch_log(signal=signals.request_scheduled, request=request, spider=spider)\n    if not self.slot.scheduler.enqueue_request(request):\n        self.signals.send_catch_log(signal=signals.request_dropped, request=request, spider=spider)",
                            "def download(self, request, spider):\n    slot = self.slot\n    slot.add_request(request)\n    d = self._download(request, spider)\n    d.addBoth(self._downloaded, slot, request, spider)\n    return d",
                            "def _downloaded(self, response, slot, request, spider):\n    slot.remove_request(request)\n    return self.download(response, spider) if isinstance(response, Request) else response",
                            "def _download(self, request, spider):\n    slot = self.slot\n    slot.add_request(request)\n\n    def _on_success(response):\n        assert isinstance(response, (Response, Request))\n        if isinstance(response, Response):\n            response.request = request\n            logkws = self.logformatter.crawled(request, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            self.signals.send_catch_log(signal=signals.response_received, response=response, request=request, spider=spider)\n        return response\n\n    def _on_complete(_):\n        slot.nextcall.schedule()\n        return _\n    dwld = self.downloader.fetch(request, spider)\n    dwld.addCallbacks(_on_success)\n    dwld.addBoth(_on_complete)\n    return dwld",
                            "@defer.inlineCallbacks\ndef open_spider(self, spider, start_requests=(), close_if_idle=True):\n    assert self.has_capacity(), 'No free spider slot when opening %r' % spider.name\n    logger.info('Spider opened', extra={'spider': spider})\n    nextcall = CallLaterOnce(self._next_request, spider)\n    scheduler = self.scheduler_cls.from_crawler(self.crawler)\n    start_requests = (yield self.scraper.spidermw.process_start_requests(start_requests, spider))\n    slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n    self.slot = slot\n    self.spider = spider\n    yield scheduler.open(spider)\n    yield self.scraper.open_spider(spider)\n    self.crawler.stats.open_spider(spider)\n    yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n    slot.nextcall.schedule()",
                            "def _spider_idle(self, spider):\n    \"\"\"Called when a spider gets idle. This function is called when there\n    are no remaining pages to download or schedule. It can be called\n    multiple times. If some extension raises a DontCloseSpider exception\n    (in the spider_idle signal handler) the spider is not closed until the\n    next loop and this function is guaranteed to be called (at least) once\n    again for this spider.\n    \"\"\"\n    res = self.signals.send_catch_log(signal=signals.spider_idle, spider=spider, dont_log=DontCloseSpider)\n    if any((isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) for _, x in res)):\n        self.slot.nextcall.schedule(5)\n        return\n    if self.spider_is_idle(spider):\n        self.close_spider(spider, reason='finished')",
                            "def close_spider(self, spider, reason='cancelled'):\n    \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n    slot = self.slot\n    if slot.closing:\n        return slot.closing\n    logger.info('Closing spider (%(reason)s)', {'reason': reason}, extra={'spider': spider})\n    dfd = slot.close()\n\n    def log_failure(msg):\n\n        def errback(failure):\n            logger.error(msg, extra={'spider': spider, 'failure': failure})\n        return errback\n    dfd.addBoth(lambda _: self.downloader.close())\n    dfd.addErrback(log_failure('Downloader close failure'))\n    dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n    dfd.addErrback(log_failure('Scraper close failure'))\n    dfd.addBoth(lambda _: slot.scheduler.close(reason))\n    dfd.addErrback(log_failure('Scheduler close failure'))\n    dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(signal=signals.spider_closed, spider=spider, reason=reason))\n    dfd.addErrback(log_failure('Error while sending spider_close signal'))\n    dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n    dfd.addErrback(log_failure('Stats close failure'))\n    dfd.addBoth(lambda _: logger.info('Spider closed (%(reason)s)', {'reason': reason}, extra={'spider': spider}))\n    dfd.addBoth(lambda _: setattr(self, 'slot', None))\n    dfd.addErrback(log_failure('Error while unassigning slot'))\n    dfd.addBoth(lambda _: setattr(self, 'spider', None))\n    dfd.addErrback(log_failure('Error while unassigning spider'))\n    dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n    return dfd",
                            "def _close_all_spiders(self):\n    dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n    dlist = defer.DeferredList(dfds)\n    return dlist",
                            "@defer.inlineCallbacks\ndef _finish_stopping_engine(self):\n    yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n    self._closewait.callback(None)",
                            "def _on_success(response):\n    assert isinstance(response, (Response, Request))\n    if isinstance(response, Response):\n        response.request = request\n        logkws = self.logformatter.crawled(request, response, spider)\n        logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n        self.signals.send_catch_log(signal=signals.response_received, response=response, request=request, spider=spider)\n    return response",
                            "def _on_complete(_):\n    slot.nextcall.schedule()\n    return _",
                            "def log_failure(msg):\n\n    def errback(failure):\n        logger.error(msg, extra={'spider': spider, 'failure': failure})\n    return errback",
                            "def errback(failure):\n    logger.error(msg, extra={'spider': spider, 'failure': failure})"
                        ],
                        "constructor_variables": [
                            "paused",
                            "_spider_closed_callback",
                            "downloader_cls",
                            "downloader",
                            "logformatter",
                            "scheduler_cls",
                            "slot",
                            "settings",
                            "running",
                            "crawler",
                            "spider",
                            "signals",
                            "scraper"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, crawler, spider_closed_callback)",
                            "start(self)",
                            "stop(self)",
                            "pause(self)",
                            "unpause(self)",
                            "_next_request(self, spider)",
                            "_needs_backout(self, spider)",
                            "_next_request_from_scheduler(self, spider)",
                            "_handle_downloader_output(self, response, request, spider)",
                            "spider_is_idle(self, spider)",
                            "open_spiders(self)",
                            "has_capacity(self)",
                            "crawl(self, request, spider)",
                            "schedule(self, request, spider)",
                            "download(self, request, spider)",
                            "_downloaded(self, response, slot, request, spider)",
                            "_download(self, request, spider)",
                            "open_spider(self, spider, start_requests=(), close_if_idle=True)",
                            "_spider_idle(self, spider)",
                            "close_spider(self, spider, reason='cancelled')",
                            "_close_all_spiders(self)",
                            "_finish_stopping_engine(self)",
                            "_on_success(response)",
                            "_on_complete(_)",
                            "log_failure(msg)",
                            "errback(failure)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "close_spider",
                    "function_code": "def close_spider(self, spider, reason='cancelled'):\n    \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n\n    slot = self.slot\n    if slot.closing:\n        return slot.closing\n    logger.info(\"Closing spider (%(reason)s)\",\n                {'reason': reason},\n                extra={'spider': spider})\n\n    dfd = slot.close()\n\n    def log_failure(msg):\n        def errback(failure):\n            logger.error(msg, extra={'spider': spider, 'failure': failure})\n        return errback\n\n    dfd.addBoth(lambda _: self.downloader.close())\n    dfd.addErrback(log_failure('Downloader close failure'))\n\n    dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n    dfd.addErrback(log_failure('Scraper close failure'))\n\n    dfd.addBoth(lambda _: slot.scheduler.close(reason))\n    dfd.addErrback(log_failure('Scheduler close failure'))\n\n    dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n        signal=signals.spider_closed, spider=spider, reason=reason))\n    dfd.addErrback(log_failure('Error while sending spider_close signal'))\n\n    dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n    dfd.addErrback(log_failure('Stats close failure'))\n\n    dfd.addBoth(lambda _: logger.info(\"Spider closed (%(reason)s)\",\n                                      {'reason': reason},\n                                      extra={'spider': spider}))\n\n    dfd.addBoth(lambda _: setattr(self, 'slot', None))\n    dfd.addErrback(log_failure('Error while unassigning slot'))\n\n    dfd.addBoth(lambda _: setattr(self, 'spider', None))\n    dfd.addErrback(log_failure('Error while unassigning spider'))\n\n    dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n    return dfd\n",
                    "decorators": [],
                    "docstring": "Close (cancel) spider and clear all its outstanding requests",
                    "start_line": 257,
                    "end_line": 302,
                    "variables": {
                        "slot": [
                            260,
                            261,
                            262,
                            267,
                            280
                        ],
                        "self.slot": [
                            260
                        ],
                        "self": [
                            260,
                            294,
                            297,
                            300,
                            274,
                            277,
                            283,
                            287
                        ],
                        "slot.closing": [
                            261,
                            262
                        ],
                        "logger.info": [
                            290,
                            263
                        ],
                        "logger": [
                            290,
                            271,
                            263
                        ],
                        "reason": [
                            291,
                            264,
                            280,
                            284,
                            287
                        ],
                        "spider": [
                            292,
                            265,
                            300,
                            271,
                            277,
                            284,
                            287
                        ],
                        "dfd": [
                            288,
                            290,
                            294,
                            295,
                            297,
                            298,
                            267,
                            300,
                            302,
                            274,
                            275,
                            277,
                            278,
                            280,
                            281,
                            283,
                            285,
                            287
                        ],
                        "slot.close": [
                            267
                        ],
                        "logger.error": [
                            271
                        ],
                        "msg": [
                            271
                        ],
                        "failure": [
                            271
                        ],
                        "errback": [
                            272
                        ],
                        "dfd.addBoth": [
                            290,
                            294,
                            297,
                            300,
                            274,
                            277,
                            280,
                            283,
                            287
                        ],
                        "self.downloader.close": [
                            274
                        ],
                        "self.downloader": [
                            274
                        ],
                        "dfd.addErrback": [
                            288,
                            295,
                            298,
                            275,
                            278,
                            281,
                            285
                        ],
                        "log_failure": [
                            288,
                            295,
                            298,
                            275,
                            278,
                            281,
                            285
                        ],
                        "self.scraper.close_spider": [
                            277
                        ],
                        "self.scraper": [
                            277
                        ],
                        "slot.scheduler.close": [
                            280
                        ],
                        "slot.scheduler": [
                            280
                        ],
                        "self.signals.send_catch_log_deferred": [
                            283
                        ],
                        "self.signals": [
                            283
                        ],
                        "signals.spider_closed": [
                            284
                        ],
                        "signals": [
                            284
                        ],
                        "self.crawler.stats.close_spider": [
                            287
                        ],
                        "self.crawler.stats": [
                            287
                        ],
                        "self.crawler": [
                            287
                        ],
                        "setattr": [
                            297,
                            294
                        ],
                        "self._spider_closed_callback": [
                            300
                        ]
                    },
                    "filtered_variables": {
                        "slot": [
                            260,
                            261,
                            262,
                            267,
                            280
                        ],
                        "self.slot": [
                            260
                        ],
                        "self": [
                            260,
                            294,
                            297,
                            300,
                            274,
                            277,
                            283,
                            287
                        ],
                        "slot.closing": [
                            261,
                            262
                        ],
                        "logger.info": [
                            290,
                            263
                        ],
                        "logger": [
                            290,
                            271,
                            263
                        ],
                        "reason": [
                            291,
                            264,
                            280,
                            284,
                            287
                        ],
                        "spider": [
                            292,
                            265,
                            300,
                            271,
                            277,
                            284,
                            287
                        ],
                        "dfd": [
                            288,
                            290,
                            294,
                            295,
                            297,
                            298,
                            267,
                            300,
                            302,
                            274,
                            275,
                            277,
                            278,
                            280,
                            281,
                            283,
                            285,
                            287
                        ],
                        "slot.close": [
                            267
                        ],
                        "logger.error": [
                            271
                        ],
                        "msg": [
                            271
                        ],
                        "failure": [
                            271
                        ],
                        "errback": [
                            272
                        ],
                        "dfd.addBoth": [
                            290,
                            294,
                            297,
                            300,
                            274,
                            277,
                            280,
                            283,
                            287
                        ],
                        "self.downloader.close": [
                            274
                        ],
                        "self.downloader": [
                            274
                        ],
                        "dfd.addErrback": [
                            288,
                            295,
                            298,
                            275,
                            278,
                            281,
                            285
                        ],
                        "log_failure": [
                            288,
                            295,
                            298,
                            275,
                            278,
                            281,
                            285
                        ],
                        "self.scraper.close_spider": [
                            277
                        ],
                        "self.scraper": [
                            277
                        ],
                        "slot.scheduler.close": [
                            280
                        ],
                        "slot.scheduler": [
                            280
                        ],
                        "self.signals.send_catch_log_deferred": [
                            283
                        ],
                        "self.signals": [
                            283
                        ],
                        "signals.spider_closed": [
                            284
                        ],
                        "signals": [
                            284
                        ],
                        "self.crawler.stats.close_spider": [
                            287
                        ],
                        "self.crawler.stats": [
                            287
                        ],
                        "self.crawler": [
                            287
                        ],
                        "self._spider_closed_callback": [
                            300
                        ]
                    },
                    "diff_line_number": 271,
                    "class_data": {
                        "signature": "class ExecutionEngine(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, crawler, spider_closed_callback):\n    self.crawler = crawler\n    self.settings = crawler.settings\n    self.signals = crawler.signals\n    self.logformatter = crawler.logformatter\n    self.slot = None\n    self.spider = None\n    self.running = False\n    self.paused = False\n    self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n    downloader_cls = load_object(self.settings['DOWNLOADER'])\n    self.downloader = downloader_cls(crawler)\n    self.scraper = Scraper(crawler)\n    self._spider_closed_callback = spider_closed_callback",
                            "@defer.inlineCallbacks\ndef start(self):\n    \"\"\"Start the execution engine\"\"\"\n    assert not self.running, 'Engine already running'\n    self.start_time = time()\n    yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n    self.running = True\n    self._closewait = defer.Deferred()\n    yield self._closewait",
                            "def stop(self):\n    \"\"\"Stop the execution engine gracefully\"\"\"\n    assert self.running, 'Engine not running'\n    self.running = False\n    dfd = self._close_all_spiders()\n    return dfd.addBoth(lambda _: self._finish_stopping_engine())",
                            "def pause(self):\n    \"\"\"Pause the execution engine\"\"\"\n    self.paused = True",
                            "def unpause(self):\n    \"\"\"Resume the execution engine\"\"\"\n    self.paused = False",
                            "def _next_request(self, spider):\n    slot = self.slot\n    if not slot:\n        return\n    if self.paused:\n        slot.nextcall.schedule(5)\n        return\n    while not self._needs_backout(spider):\n        if not self._next_request_from_scheduler(spider):\n            break\n    if slot.start_requests and (not self._needs_backout(spider)):\n        try:\n            request = next(slot.start_requests)\n        except StopIteration:\n            slot.start_requests = None\n        except Exception:\n            slot.start_requests = None\n            logger.error('Error while obtaining start requests', exc_info=True, extra={'spider': spider})\n        else:\n            self.crawl(request, spider)\n    if self.spider_is_idle(spider) and slot.close_if_idle:\n        self._spider_idle(spider)",
                            "def _needs_backout(self, spider):\n    slot = self.slot\n    return not self.running or slot.closing or self.downloader.needs_backout() or self.scraper.slot.needs_backout()",
                            "def _next_request_from_scheduler(self, spider):\n    slot = self.slot\n    request = slot.scheduler.next_request()\n    if not request:\n        return\n    d = self._download(request, spider)\n    d.addBoth(self._handle_downloader_output, request, spider)\n    d.addErrback(lambda f: logger.info('Error while handling downloader output', extra={'spider': spider, 'failure': f}))\n    d.addBoth(lambda _: slot.remove_request(request))\n    d.addErrback(lambda f: logger.info('Error while removing request from slot', extra={'spider': spider, 'failure': f}))\n    d.addBoth(lambda _: slot.nextcall.schedule())\n    d.addErrback(lambda f: logger.info('Error while scheduling new request', extra={'spider': spider, 'failure': f}))\n    return d",
                            "def _handle_downloader_output(self, response, request, spider):\n    assert isinstance(response, (Request, Response, Failure)), response\n    if isinstance(response, Request):\n        self.crawl(response, spider)\n        return\n    d = self.scraper.enqueue_scrape(response, request, spider)\n    d.addErrback(lambda f: logger.error('Error while enqueuing downloader output', extra={'spider': spider, 'failure': f}))\n    return d",
                            "def spider_is_idle(self, spider):\n    scraper_idle = self.scraper.slot.is_idle()\n    pending = self.slot.scheduler.has_pending_requests()\n    downloading = bool(self.downloader.active)\n    pending_start_requests = self.slot.start_requests is not None\n    idle = scraper_idle and (not (pending or downloading or pending_start_requests))\n    return idle",
                            "@property\ndef open_spiders(self):\n    return [self.spider] if self.spider else []",
                            "def has_capacity(self):\n    \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n    return not bool(self.slot)",
                            "def crawl(self, request, spider):\n    assert spider in self.open_spiders, 'Spider %r not opened when crawling: %s' % (spider.name, request)\n    self.schedule(request, spider)\n    self.slot.nextcall.schedule()",
                            "def schedule(self, request, spider):\n    self.signals.send_catch_log(signal=signals.request_scheduled, request=request, spider=spider)\n    if not self.slot.scheduler.enqueue_request(request):\n        self.signals.send_catch_log(signal=signals.request_dropped, request=request, spider=spider)",
                            "def download(self, request, spider):\n    slot = self.slot\n    slot.add_request(request)\n    d = self._download(request, spider)\n    d.addBoth(self._downloaded, slot, request, spider)\n    return d",
                            "def _downloaded(self, response, slot, request, spider):\n    slot.remove_request(request)\n    return self.download(response, spider) if isinstance(response, Request) else response",
                            "def _download(self, request, spider):\n    slot = self.slot\n    slot.add_request(request)\n\n    def _on_success(response):\n        assert isinstance(response, (Response, Request))\n        if isinstance(response, Response):\n            response.request = request\n            logkws = self.logformatter.crawled(request, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            self.signals.send_catch_log(signal=signals.response_received, response=response, request=request, spider=spider)\n        return response\n\n    def _on_complete(_):\n        slot.nextcall.schedule()\n        return _\n    dwld = self.downloader.fetch(request, spider)\n    dwld.addCallbacks(_on_success)\n    dwld.addBoth(_on_complete)\n    return dwld",
                            "@defer.inlineCallbacks\ndef open_spider(self, spider, start_requests=(), close_if_idle=True):\n    assert self.has_capacity(), 'No free spider slot when opening %r' % spider.name\n    logger.info('Spider opened', extra={'spider': spider})\n    nextcall = CallLaterOnce(self._next_request, spider)\n    scheduler = self.scheduler_cls.from_crawler(self.crawler)\n    start_requests = (yield self.scraper.spidermw.process_start_requests(start_requests, spider))\n    slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n    self.slot = slot\n    self.spider = spider\n    yield scheduler.open(spider)\n    yield self.scraper.open_spider(spider)\n    self.crawler.stats.open_spider(spider)\n    yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n    slot.nextcall.schedule()",
                            "def _spider_idle(self, spider):\n    \"\"\"Called when a spider gets idle. This function is called when there\n    are no remaining pages to download or schedule. It can be called\n    multiple times. If some extension raises a DontCloseSpider exception\n    (in the spider_idle signal handler) the spider is not closed until the\n    next loop and this function is guaranteed to be called (at least) once\n    again for this spider.\n    \"\"\"\n    res = self.signals.send_catch_log(signal=signals.spider_idle, spider=spider, dont_log=DontCloseSpider)\n    if any((isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) for _, x in res)):\n        self.slot.nextcall.schedule(5)\n        return\n    if self.spider_is_idle(spider):\n        self.close_spider(spider, reason='finished')",
                            "def close_spider(self, spider, reason='cancelled'):\n    \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n    slot = self.slot\n    if slot.closing:\n        return slot.closing\n    logger.info('Closing spider (%(reason)s)', {'reason': reason}, extra={'spider': spider})\n    dfd = slot.close()\n\n    def log_failure(msg):\n\n        def errback(failure):\n            logger.error(msg, extra={'spider': spider, 'failure': failure})\n        return errback\n    dfd.addBoth(lambda _: self.downloader.close())\n    dfd.addErrback(log_failure('Downloader close failure'))\n    dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n    dfd.addErrback(log_failure('Scraper close failure'))\n    dfd.addBoth(lambda _: slot.scheduler.close(reason))\n    dfd.addErrback(log_failure('Scheduler close failure'))\n    dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(signal=signals.spider_closed, spider=spider, reason=reason))\n    dfd.addErrback(log_failure('Error while sending spider_close signal'))\n    dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n    dfd.addErrback(log_failure('Stats close failure'))\n    dfd.addBoth(lambda _: logger.info('Spider closed (%(reason)s)', {'reason': reason}, extra={'spider': spider}))\n    dfd.addBoth(lambda _: setattr(self, 'slot', None))\n    dfd.addErrback(log_failure('Error while unassigning slot'))\n    dfd.addBoth(lambda _: setattr(self, 'spider', None))\n    dfd.addErrback(log_failure('Error while unassigning spider'))\n    dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n    return dfd",
                            "def _close_all_spiders(self):\n    dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n    dlist = defer.DeferredList(dfds)\n    return dlist",
                            "@defer.inlineCallbacks\ndef _finish_stopping_engine(self):\n    yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n    self._closewait.callback(None)",
                            "def _on_success(response):\n    assert isinstance(response, (Response, Request))\n    if isinstance(response, Response):\n        response.request = request\n        logkws = self.logformatter.crawled(request, response, spider)\n        logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n        self.signals.send_catch_log(signal=signals.response_received, response=response, request=request, spider=spider)\n    return response",
                            "def _on_complete(_):\n    slot.nextcall.schedule()\n    return _",
                            "def log_failure(msg):\n\n    def errback(failure):\n        logger.error(msg, extra={'spider': spider, 'failure': failure})\n    return errback",
                            "def errback(failure):\n    logger.error(msg, extra={'spider': spider, 'failure': failure})"
                        ],
                        "constructor_variables": [
                            "paused",
                            "_spider_closed_callback",
                            "downloader_cls",
                            "downloader",
                            "logformatter",
                            "scheduler_cls",
                            "slot",
                            "settings",
                            "running",
                            "crawler",
                            "spider",
                            "signals",
                            "scraper"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, crawler, spider_closed_callback)",
                            "start(self)",
                            "stop(self)",
                            "pause(self)",
                            "unpause(self)",
                            "_next_request(self, spider)",
                            "_needs_backout(self, spider)",
                            "_next_request_from_scheduler(self, spider)",
                            "_handle_downloader_output(self, response, request, spider)",
                            "spider_is_idle(self, spider)",
                            "open_spiders(self)",
                            "has_capacity(self)",
                            "crawl(self, request, spider)",
                            "schedule(self, request, spider)",
                            "download(self, request, spider)",
                            "_downloaded(self, response, slot, request, spider)",
                            "_download(self, request, spider)",
                            "open_spider(self, spider, start_requests=(), close_if_idle=True)",
                            "_spider_idle(self, spider)",
                            "close_spider(self, spider, reason='cancelled')",
                            "_close_all_spiders(self)",
                            "_finish_stopping_engine(self)",
                            "_on_success(response)",
                            "_on_complete(_)",
                            "log_failure(msg)",
                            "errback(failure)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "from scrapy.utils.log import logformatter_adapter",
                    "start_line": 19,
                    "end_line": 20
                }
            ],
            "inscope_functions": [
                "def __init__(self, start_requests, close_if_idle, nextcall, scheduler):\n    self.closing = False\n    self.inprogress = set() # requests in progress\n    self.start_requests = iter(start_requests)\n    self.close_if_idle = close_if_idle\n    self.nextcall = nextcall\n    self.scheduler = scheduler",
                "def add_request(self, request):\n    self.inprogress.add(request)",
                "def remove_request(self, request):\n    self.inprogress.remove(request)\n    self._maybe_fire_closing()",
                "def close(self):\n    self.closing = defer.Deferred()\n    self._maybe_fire_closing()\n    return self.closing",
                "def _maybe_fire_closing(self):\n    if self.closing and not self.inprogress:\n        if self.nextcall:\n            self.nextcall.cancel()\n        self.closing.callback(None)",
                "def __init__(self, crawler, spider_closed_callback):\n    self.crawler = crawler\n    self.settings = crawler.settings\n    self.signals = crawler.signals\n    self.logformatter = crawler.logformatter\n    self.slot = None\n    self.spider = None\n    self.running = False\n    self.paused = False\n    self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n    downloader_cls = load_object(self.settings['DOWNLOADER'])\n    self.downloader = downloader_cls(crawler)\n    self.scraper = Scraper(crawler)\n    self._spider_closed_callback = spider_closed_callback",
                "@defer.inlineCallbacks\ndef start(self):\n    \"\"\"Start the execution engine\"\"\"\n    assert not self.running, \"Engine already running\"\n    self.start_time = time()\n    yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n    self.running = True\n    self._closewait = defer.Deferred()\n    yield self._closewait",
                "def stop(self):\n    \"\"\"Stop the execution engine gracefully\"\"\"\n    assert self.running, \"Engine not running\"\n    self.running = False\n    dfd = self._close_all_spiders()\n    return dfd.addBoth(lambda _: self._finish_stopping_engine())",
                "def pause(self):\n    \"\"\"Pause the execution engine\"\"\"\n    self.paused = True",
                "def unpause(self):\n    \"\"\"Resume the execution engine\"\"\"\n    self.paused = False",
                "def _next_request(self, spider):\n    slot = self.slot\n    if not slot:\n        return\n\n    if self.paused:\n        slot.nextcall.schedule(5)\n        return\n\n    while not self._needs_backout(spider):\n        if not self._next_request_from_scheduler(spider):\n            break\n\n    if slot.start_requests and not self._needs_backout(spider):\n        try:\n            request = next(slot.start_requests)\n        except StopIteration:\n            slot.start_requests = None\n        except Exception:\n            slot.start_requests = None\n            logger.error('Error while obtaining start requests',\n                         exc_info=True, extra={'spider': spider})\n        else:\n            self.crawl(request, spider)\n\n    if self.spider_is_idle(spider) and slot.close_if_idle:\n        self._spider_idle(spider)",
                "def _needs_backout(self, spider):\n    slot = self.slot\n    return not self.running \\\n        or slot.closing \\\n        or self.downloader.needs_backout() \\\n        or self.scraper.slot.needs_backout()",
                "def _next_request_from_scheduler(self, spider):\n    slot = self.slot\n    request = slot.scheduler.next_request()\n    if not request:\n        return\n    d = self._download(request, spider)\n    d.addBoth(self._handle_downloader_output, request, spider)\n    d.addErrback(lambda f: logger.info('Error while handling downloader output',\n                                       extra={'spider': spider, 'failure': f}))\n    d.addBoth(lambda _: slot.remove_request(request))\n    d.addErrback(lambda f: logger.info('Error while removing request from slot',\n                                       extra={'spider': spider, 'failure': f}))\n    d.addBoth(lambda _: slot.nextcall.schedule())\n    d.addErrback(lambda f: logger.info('Error while scheduling new request',\n                                       extra={'spider': spider, 'failure': f}))\n    return d",
                "def _handle_downloader_output(self, response, request, spider):\n    assert isinstance(response, (Request, Response, Failure)), response\n    # downloader middleware can return requests (for example, redirects)\n    if isinstance(response, Request):\n        self.crawl(response, spider)\n        return\n    # response is a Response or Failure\n    d = self.scraper.enqueue_scrape(response, request, spider)\n    d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',\n                                        extra={'spider': spider, 'failure': f}))\n    return d",
                "def spider_is_idle(self, spider):\n    scraper_idle = self.scraper.slot.is_idle()\n    pending = self.slot.scheduler.has_pending_requests()\n    downloading = bool(self.downloader.active)\n    pending_start_requests = self.slot.start_requests is not None\n    idle = scraper_idle and not (pending or downloading or pending_start_requests)\n    return idle",
                "@property\ndef open_spiders(self):\n    return [self.spider] if self.spider else []",
                "def has_capacity(self):\n    \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n    return not bool(self.slot)",
                "def crawl(self, request, spider):\n    assert spider in self.open_spiders, \\\n        \"Spider %r not opened when crawling: %s\" % (spider.name, request)\n    self.schedule(request, spider)\n    self.slot.nextcall.schedule()",
                "def schedule(self, request, spider):\n    self.signals.send_catch_log(signal=signals.request_scheduled,\n            request=request, spider=spider)\n    if not self.slot.scheduler.enqueue_request(request):\n        self.signals.send_catch_log(signal=signals.request_dropped,\n                                    request=request, spider=spider)",
                "def download(self, request, spider):\n    slot = self.slot\n    slot.add_request(request)\n    d = self._download(request, spider)\n    d.addBoth(self._downloaded, slot, request, spider)\n    return d",
                "def _downloaded(self, response, slot, request, spider):\n    slot.remove_request(request)\n    return self.download(response, spider) \\\n            if isinstance(response, Request) else response",
                "def _download(self, request, spider):\n    slot = self.slot\n    slot.add_request(request)\n    def _on_success(response):\n        assert isinstance(response, (Response, Request))\n        if isinstance(response, Response):\n            response.request = request # tie request to response received\n            logkws = self.logformatter.crawled(request, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            self.signals.send_catch_log(signal=signals.response_received, \\\n                response=response, request=request, spider=spider)\n        return response\n\n    def _on_complete(_):\n        slot.nextcall.schedule()\n        return _\n\n    dwld = self.downloader.fetch(request, spider)\n    dwld.addCallbacks(_on_success)\n    dwld.addBoth(_on_complete)\n    return dwld",
                "@defer.inlineCallbacks\ndef open_spider(self, spider, start_requests=(), close_if_idle=True):\n    assert self.has_capacity(), \"No free spider slot when opening %r\" % \\\n        spider.name\n    logger.info(\"Spider opened\", extra={'spider': spider})\n    nextcall = CallLaterOnce(self._next_request, spider)\n    scheduler = self.scheduler_cls.from_crawler(self.crawler)\n    start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n    slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n    self.slot = slot\n    self.spider = spider\n    yield scheduler.open(spider)\n    yield self.scraper.open_spider(spider)\n    self.crawler.stats.open_spider(spider)\n    yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n    slot.nextcall.schedule()",
                "def _spider_idle(self, spider):\n    \"\"\"Called when a spider gets idle. This function is called when there\n    are no remaining pages to download or schedule. It can be called\n    multiple times. If some extension raises a DontCloseSpider exception\n    (in the spider_idle signal handler) the spider is not closed until the\n    next loop and this function is guaranteed to be called (at least) once\n    again for this spider.\n    \"\"\"\n    res = self.signals.send_catch_log(signal=signals.spider_idle, \\\n        spider=spider, dont_log=DontCloseSpider)\n    if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n            for _, x in res):\n        self.slot.nextcall.schedule(5)\n        return\n\n    if self.spider_is_idle(spider):\n        self.close_spider(spider, reason='finished')",
                "def close_spider(self, spider, reason='cancelled'):\n    \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n\n    slot = self.slot\n    if slot.closing:\n        return slot.closing\n    logger.info(\"Closing spider (%(reason)s)\",\n                {'reason': reason},\n                extra={'spider': spider})\n\n    dfd = slot.close()\n\n    def log_failure(msg):\n        def errback(failure):\n            logger.error(msg, extra={'spider': spider, 'failure': failure})\n        return errback\n\n    dfd.addBoth(lambda _: self.downloader.close())\n    dfd.addErrback(log_failure('Downloader close failure'))\n\n    dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n    dfd.addErrback(log_failure('Scraper close failure'))\n\n    dfd.addBoth(lambda _: slot.scheduler.close(reason))\n    dfd.addErrback(log_failure('Scheduler close failure'))\n\n    dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n        signal=signals.spider_closed, spider=spider, reason=reason))\n    dfd.addErrback(log_failure('Error while sending spider_close signal'))\n\n    dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n    dfd.addErrback(log_failure('Stats close failure'))\n\n    dfd.addBoth(lambda _: logger.info(\"Spider closed (%(reason)s)\",\n                                      {'reason': reason},\n                                      extra={'spider': spider}))\n\n    dfd.addBoth(lambda _: setattr(self, 'slot', None))\n    dfd.addErrback(log_failure('Error while unassigning slot'))\n\n    dfd.addBoth(lambda _: setattr(self, 'spider', None))\n    dfd.addErrback(log_failure('Error while unassigning spider'))\n\n    dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n    return dfd",
                "def _close_all_spiders(self):\n    dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n    dlist = defer.DeferredList(dfds)\n    return dlist",
                "@defer.inlineCallbacks\ndef _finish_stopping_engine(self):\n    yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n    self._closewait.callback(None)",
                "def _on_success(response):\n    assert isinstance(response, (Response, Request))\n    if isinstance(response, Response):\n        response.request = request # tie request to response received\n        logkws = self.logformatter.crawled(request, response, spider)\n        logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n        self.signals.send_catch_log(signal=signals.response_received, \\\n            response=response, request=request, spider=spider)\n    return response",
                "def _on_complete(_):\n    slot.nextcall.schedule()\n    return _",
                "def log_failure(msg):\n    def errback(failure):\n        logger.error(msg, extra={'spider': spider, 'failure': failure})\n    return errback",
                "def errback(failure):\n    logger.error(msg, extra={'spider': spider, 'failure': failure})"
            ],
            "inscope_function_signatures": [
                "__init__(self, start_requests, close_if_idle, nextcall, scheduler)",
                "add_request(self, request)",
                "remove_request(self, request)",
                "close(self)",
                "_maybe_fire_closing(self)",
                "__init__(self, crawler, spider_closed_callback)",
                "start(self)",
                "stop(self)",
                "pause(self)",
                "unpause(self)",
                "_next_request(self, spider)",
                "_needs_backout(self, spider)",
                "_next_request_from_scheduler(self, spider)",
                "_handle_downloader_output(self, response, request, spider)",
                "spider_is_idle(self, spider)",
                "open_spiders(self)",
                "has_capacity(self)",
                "crawl(self, request, spider)",
                "schedule(self, request, spider)",
                "download(self, request, spider)",
                "_downloaded(self, response, slot, request, spider)",
                "_download(self, request, spider)",
                "open_spider(self, spider, start_requests=(), close_if_idle=True)",
                "_spider_idle(self, spider)",
                "close_spider(self, spider, reason='cancelled')",
                "_close_all_spiders(self)",
                "_finish_stopping_engine(self)",
                "_on_success(response)",
                "_on_complete(_)",
                "log_failure(msg)",
                "errback(failure)"
            ],
            "variables_in_file": {
                "logger": [
                    226,
                    290,
                    263,
                    137,
                    140,
                    143,
                    208,
                    271,
                    115,
                    21,
                    155
                ],
                "logging.getLogger": [
                    21
                ],
                "logging": [
                    21
                ],
                "__name__": [
                    21
                ],
                "object": [
                    24,
                    53
                ],
                "self.closing": [
                    42,
                    44,
                    47,
                    50,
                    27
                ],
                "self": [
                    27,
                    28,
                    29,
                    30,
                    31,
                    32,
                    35,
                    38,
                    39,
                    42,
                    43,
                    44,
                    47,
                    48,
                    49,
                    50,
                    56,
                    57,
                    58,
                    59,
                    60,
                    61,
                    62,
                    63,
                    64,
                    65,
                    66,
                    67,
                    68,
                    73,
                    74,
                    75,
                    76,
                    77,
                    78,
                    82,
                    83,
                    84,
                    85,
                    89,
                    93,
                    96,
                    100,
                    104,
                    105,
                    108,
                    118,
                    120,
                    121,
                    124,
                    125,
                    127,
                    128,
                    131,
                    135,
                    136,
                    151,
                    154,
                    160,
                    161,
                    162,
                    163,
                    169,
                    173,
                    176,
                    178,
                    179,
                    182,
                    184,
                    185,
                    189,
                    191,
                    192,
                    197,
                    201,
                    207,
                    209,
                    217,
                    224,
                    227,
                    228,
                    229,
                    231,
                    232,
                    234,
                    235,
                    236,
                    247,
                    251,
                    254,
                    255,
                    260,
                    274,
                    277,
                    283,
                    287,
                    294,
                    297,
                    300,
                    305,
                    311,
                    312
                ],
                "self.inprogress": [
                    35,
                    28,
                    38,
                    47
                ],
                "set": [
                    28
                ],
                "self.start_requests": [
                    29
                ],
                "iter": [
                    29
                ],
                "start_requests": [
                    229,
                    29,
                    230
                ],
                "self.close_if_idle": [
                    30
                ],
                "close_if_idle": [
                    30,
                    230
                ],
                "self.nextcall": [
                    48,
                    49,
                    31
                ],
                "nextcall": [
                    227,
                    230,
                    31
                ],
                "self.scheduler": [
                    32
                ],
                "scheduler": [
                    32,
                    233,
                    228,
                    230
                ],
                "self.inprogress.add": [
                    35
                ],
                "request": [
                    132,
                    133,
                    135,
                    136,
                    139,
                    154,
                    35,
                    38,
                    177,
                    178,
                    183,
                    184,
                    186,
                    190,
                    191,
                    192,
                    196,
                    202,
                    206,
                    207,
                    210,
                    217,
                    110,
                    118
                ],
                "self.inprogress.remove": [
                    38
                ],
                "self._maybe_fire_closing": [
                    43,
                    39
                ],
                "defer.Deferred": [
                    42,
                    77
                ],
                "defer": [
                    70,
                    42,
                    77,
                    306,
                    309,
                    222
                ],
                "self.nextcall.cancel": [
                    49
                ],
                "self.closing.callback": [
                    50
                ],
                "self.crawler": [
                    56,
                    235,
                    228,
                    287
                ],
                "crawler": [
                    66,
                    67,
                    56,
                    57,
                    58,
                    59
                ],
                "self.settings": [
                    64,
                    57,
                    65
                ],
                "crawler.settings": [
                    57
                ],
                "self.signals": [
                    75,
                    236,
                    209,
                    182,
                    247,
                    185,
                    58,
                    283,
                    311
                ],
                "crawler.signals": [
                    58
                ],
                "self.logformatter": [
                    59,
                    207
                ],
                "crawler.logformatter": [
                    59
                ],
                "self.slot": [
                    96,
                    161,
                    131,
                    163,
                    260,
                    231,
                    201,
                    173,
                    60,
                    179,
                    184,
                    251,
                    124,
                    189
                ],
                "self.spider": [
                    232,
                    169,
                    61
                ],
                "self.running": [
                    73,
                    76,
                    82,
                    83,
                    125,
                    62
                ],
                "self.paused": [
                    89,
                    100,
                    93,
                    63
                ],
                "self.scheduler_cls": [
                    64,
                    228
                ],
                "load_object": [
                    64,
                    65
                ],
                "downloader_cls": [
                    65,
                    66
                ],
                "self.downloader": [
                    162,
                    66,
                    274,
                    217,
                    127
                ],
                "self.scraper": [
                    128,
                    160,
                    67,
                    229,
                    234,
                    277,
                    154
                ],
                "Scraper": [
                    67
                ],
                "self._spider_closed_callback": [
                    68,
                    300
                ],
                "spider_closed_callback": [
                    68
                ],
                "self.start_time": [
                    74
                ],
                "time": [
                    74
                ],
                "self.signals.send_catch_log_deferred": [
                    283,
                    75,
                    236,
                    311
                ],
                "signals.engine_started": [
                    75
                ],
                "signals": [
                    75,
                    236,
                    209,
                    182,
                    247,
                    185,
                    284,
                    311
                ],
                "self._closewait": [
                    312,
                    77,
                    78
                ],
                "defer.inlineCallbacks": [
                    309,
                    222,
                    70
                ],
                "dfd": [
                    267,
                    274,
                    275,
                    277,
                    278,
                    280,
                    281,
                    283,
                    285,
                    287,
                    288,
                    290,
                    294,
                    295,
                    297,
                    298,
                    300,
                    302,
                    84,
                    85
                ],
                "self._close_all_spiders": [
                    84
                ],
                "dfd.addBoth": [
                    290,
                    294,
                    297,
                    300,
                    274,
                    277,
                    85,
                    280,
                    283,
                    287
                ],
                "self._finish_stopping_engine": [
                    85
                ],
                "slot": [
                    131,
                    132,
                    260,
                    261,
                    262,
                    139,
                    267,
                    142,
                    280,
                    189,
                    190,
                    192,
                    196,
                    201,
                    202,
                    214,
                    96,
                    97,
                    101,
                    230,
                    231,
                    108,
                    237,
                    110,
                    112,
                    114,
                    120,
                    124,
                    126
                ],
                "slot.nextcall.schedule": [
                    237,
                    214,
                    101,
                    142
                ],
                "slot.nextcall": [
                    237,
                    214,
                    101,
                    142
                ],
                "self._needs_backout": [
                    104,
                    108
                ],
                "spider": [
                    135,
                    136,
                    265,
                    138,
                    141,
                    271,
                    144,
                    277,
                    151,
                    154,
                    156,
                    284,
                    287,
                    292,
                    300,
                    176,
                    177,
                    178,
                    183,
                    186,
                    191,
                    192,
                    197,
                    207,
                    208,
                    210,
                    217,
                    248,
                    225,
                    226,
                    227,
                    229,
                    104,
                    105,
                    232,
                    233,
                    108,
                    234,
                    235,
                    236,
                    116,
                    118,
                    120,
                    121,
                    254,
                    255
                ],
                "self._next_request_from_scheduler": [
                    105
                ],
                "slot.start_requests": [
                    112,
                    114,
                    108,
                    110
                ],
                "next": [
                    110
                ],
                "StopIteration": [
                    111
                ],
                "Exception": [
                    113
                ],
                "logger.error": [
                    115,
                    155,
                    271
                ],
                "self.crawl": [
                    118,
                    151
                ],
                "self.spider_is_idle": [
                    120,
                    254
                ],
                "slot.close_if_idle": [
                    120
                ],
                "self._spider_idle": [
                    121
                ],
                "slot.closing": [
                    261,
                    126,
                    262
                ],
                "self.downloader.needs_backout": [
                    127
                ],
                "self.scraper.slot.needs_backout": [
                    128
                ],
                "self.scraper.slot": [
                    128,
                    160
                ],
                "slot.scheduler.next_request": [
                    132
                ],
                "slot.scheduler": [
                    280,
                    132
                ],
                "d": [
                    192,
                    193,
                    135,
                    136,
                    137,
                    139,
                    140,
                    142,
                    143,
                    145,
                    154,
                    155,
                    157,
                    191
                ],
                "self._download": [
                    191,
                    135
                ],
                "d.addBoth": [
                    136,
                    139,
                    142,
                    192
                ],
                "self._handle_downloader_output": [
                    136
                ],
                "d.addErrback": [
                    137,
                    155,
                    140,
                    143
                ],
                "logger.info": [
                    226,
                    290,
                    263,
                    137,
                    140,
                    143
                ],
                "f": [
                    144,
                    138,
                    156,
                    141
                ],
                "slot.remove_request": [
                    139,
                    196
                ],
                "isinstance": [
                    198,
                    204,
                    205,
                    148,
                    150,
                    249
                ],
                "response": [
                    197,
                    198,
                    204,
                    205,
                    206,
                    207,
                    210,
                    211,
                    148,
                    150,
                    151,
                    154
                ],
                "Request": [
                    204,
                    148,
                    198,
                    150
                ],
                "Response": [
                    204,
                    148,
                    205
                ],
                "Failure": [
                    249,
                    148
                ],
                "self.scraper.enqueue_scrape": [
                    154
                ],
                "scraper_idle": [
                    160,
                    164
                ],
                "self.scraper.slot.is_idle": [
                    160
                ],
                "pending": [
                    161,
                    164
                ],
                "self.slot.scheduler.has_pending_requests": [
                    161
                ],
                "self.slot.scheduler": [
                    184,
                    161
                ],
                "downloading": [
                    162,
                    164
                ],
                "bool": [
                    162,
                    173
                ],
                "self.downloader.active": [
                    162
                ],
                "pending_start_requests": [
                    163,
                    164
                ],
                "self.slot.start_requests": [
                    163
                ],
                "idle": [
                    164,
                    165
                ],
                "property": [
                    167
                ],
                "self.open_spiders": [
                    176,
                    305
                ],
                "spider.name": [
                    177,
                    225
                ],
                "self.schedule": [
                    178
                ],
                "self.slot.nextcall.schedule": [
                    179,
                    251
                ],
                "self.slot.nextcall": [
                    179,
                    251
                ],
                "self.signals.send_catch_log": [
                    185,
                    209,
                    182,
                    247
                ],
                "signals.request_scheduled": [
                    182
                ],
                "self.slot.scheduler.enqueue_request": [
                    184
                ],
                "signals.request_dropped": [
                    185
                ],
                "slot.add_request": [
                    202,
                    190
                ],
                "self._downloaded": [
                    192
                ],
                "self.download": [
                    197
                ],
                "response.request": [
                    206
                ],
                "logkws": [
                    208,
                    207
                ],
                "self.logformatter.crawled": [
                    207
                ],
                "logger.log": [
                    208
                ],
                "logformatter_adapter": [
                    208
                ],
                "signals.response_received": [
                    209
                ],
                "_": [
                    250,
                    215
                ],
                "dwld": [
                    217,
                    218,
                    219,
                    220
                ],
                "self.downloader.fetch": [
                    217
                ],
                "dwld.addCallbacks": [
                    218
                ],
                "_on_success": [
                    218
                ],
                "dwld.addBoth": [
                    219
                ],
                "_on_complete": [
                    219
                ],
                "self.has_capacity": [
                    224
                ],
                "CallLaterOnce": [
                    227
                ],
                "self._next_request": [
                    227
                ],
                "self.scheduler_cls.from_crawler": [
                    228
                ],
                "self.scraper.spidermw.process_start_requests": [
                    229
                ],
                "self.scraper.spidermw": [
                    229
                ],
                "Slot": [
                    230
                ],
                "scheduler.open": [
                    233
                ],
                "self.scraper.open_spider": [
                    234
                ],
                "self.crawler.stats.open_spider": [
                    235
                ],
                "self.crawler.stats": [
                    235,
                    287
                ],
                "signals.spider_opened": [
                    236
                ],
                "res": [
                    250,
                    247
                ],
                "signals.spider_idle": [
                    247
                ],
                "DontCloseSpider": [
                    248,
                    249
                ],
                "any": [
                    249
                ],
                "x": [
                    249,
                    250
                ],
                "x.value": [
                    249
                ],
                "self.close_spider": [
                    305,
                    255
                ],
                "reason": [
                    291,
                    264,
                    280,
                    284,
                    287
                ],
                "slot.close": [
                    267
                ],
                "msg": [
                    271
                ],
                "failure": [
                    271
                ],
                "errback": [
                    272
                ],
                "self.downloader.close": [
                    274
                ],
                "dfd.addErrback": [
                    288,
                    295,
                    298,
                    275,
                    278,
                    281,
                    285
                ],
                "log_failure": [
                    288,
                    295,
                    298,
                    275,
                    278,
                    281,
                    285
                ],
                "self.scraper.close_spider": [
                    277
                ],
                "slot.scheduler.close": [
                    280
                ],
                "signals.spider_closed": [
                    284
                ],
                "self.crawler.stats.close_spider": [
                    287
                ],
                "setattr": [
                    297,
                    294
                ],
                "dfds": [
                    305,
                    306
                ],
                "s": [
                    305
                ],
                "dlist": [
                    306,
                    307
                ],
                "defer.DeferredList": [
                    306
                ],
                "signals.engine_stopped": [
                    311
                ],
                "self._closewait.callback": [
                    312
                ]
            },
            "filtered_variables_in_file": {
                "logger": [
                    226,
                    290,
                    263,
                    137,
                    140,
                    143,
                    208,
                    271,
                    115,
                    21,
                    155
                ],
                "logging.getLogger": [
                    21
                ],
                "logging": [
                    21
                ],
                "self.closing": [
                    42,
                    44,
                    47,
                    50,
                    27
                ],
                "self": [
                    27,
                    28,
                    29,
                    30,
                    31,
                    32,
                    35,
                    38,
                    39,
                    42,
                    43,
                    44,
                    47,
                    48,
                    49,
                    50,
                    56,
                    57,
                    58,
                    59,
                    60,
                    61,
                    62,
                    63,
                    64,
                    65,
                    66,
                    67,
                    68,
                    73,
                    74,
                    75,
                    76,
                    77,
                    78,
                    82,
                    83,
                    84,
                    85,
                    89,
                    93,
                    96,
                    100,
                    104,
                    105,
                    108,
                    118,
                    120,
                    121,
                    124,
                    125,
                    127,
                    128,
                    131,
                    135,
                    136,
                    151,
                    154,
                    160,
                    161,
                    162,
                    163,
                    169,
                    173,
                    176,
                    178,
                    179,
                    182,
                    184,
                    185,
                    189,
                    191,
                    192,
                    197,
                    201,
                    207,
                    209,
                    217,
                    224,
                    227,
                    228,
                    229,
                    231,
                    232,
                    234,
                    235,
                    236,
                    247,
                    251,
                    254,
                    255,
                    260,
                    274,
                    277,
                    283,
                    287,
                    294,
                    297,
                    300,
                    305,
                    311,
                    312
                ],
                "self.inprogress": [
                    35,
                    28,
                    38,
                    47
                ],
                "self.start_requests": [
                    29
                ],
                "start_requests": [
                    229,
                    29,
                    230
                ],
                "self.close_if_idle": [
                    30
                ],
                "close_if_idle": [
                    30,
                    230
                ],
                "self.nextcall": [
                    48,
                    49,
                    31
                ],
                "nextcall": [
                    227,
                    230,
                    31
                ],
                "self.scheduler": [
                    32
                ],
                "scheduler": [
                    32,
                    233,
                    228,
                    230
                ],
                "self.inprogress.add": [
                    35
                ],
                "request": [
                    132,
                    133,
                    135,
                    136,
                    139,
                    154,
                    35,
                    38,
                    177,
                    178,
                    183,
                    184,
                    186,
                    190,
                    191,
                    192,
                    196,
                    202,
                    206,
                    207,
                    210,
                    217,
                    110,
                    118
                ],
                "self.inprogress.remove": [
                    38
                ],
                "self._maybe_fire_closing": [
                    43,
                    39
                ],
                "defer.Deferred": [
                    42,
                    77
                ],
                "defer": [
                    70,
                    42,
                    77,
                    306,
                    309,
                    222
                ],
                "self.nextcall.cancel": [
                    49
                ],
                "self.closing.callback": [
                    50
                ],
                "self.crawler": [
                    56,
                    235,
                    228,
                    287
                ],
                "crawler": [
                    66,
                    67,
                    56,
                    57,
                    58,
                    59
                ],
                "self.settings": [
                    64,
                    57,
                    65
                ],
                "crawler.settings": [
                    57
                ],
                "self.signals": [
                    75,
                    236,
                    209,
                    182,
                    247,
                    185,
                    58,
                    283,
                    311
                ],
                "crawler.signals": [
                    58
                ],
                "self.logformatter": [
                    59,
                    207
                ],
                "crawler.logformatter": [
                    59
                ],
                "self.slot": [
                    96,
                    161,
                    131,
                    163,
                    260,
                    231,
                    201,
                    173,
                    60,
                    179,
                    184,
                    251,
                    124,
                    189
                ],
                "self.spider": [
                    232,
                    169,
                    61
                ],
                "self.running": [
                    73,
                    76,
                    82,
                    83,
                    125,
                    62
                ],
                "self.paused": [
                    89,
                    100,
                    93,
                    63
                ],
                "self.scheduler_cls": [
                    64,
                    228
                ],
                "load_object": [
                    64,
                    65
                ],
                "downloader_cls": [
                    65,
                    66
                ],
                "self.downloader": [
                    162,
                    66,
                    274,
                    217,
                    127
                ],
                "self.scraper": [
                    128,
                    160,
                    67,
                    229,
                    234,
                    277,
                    154
                ],
                "Scraper": [
                    67
                ],
                "self._spider_closed_callback": [
                    68,
                    300
                ],
                "spider_closed_callback": [
                    68
                ],
                "self.start_time": [
                    74
                ],
                "time": [
                    74
                ],
                "self.signals.send_catch_log_deferred": [
                    283,
                    75,
                    236,
                    311
                ],
                "signals.engine_started": [
                    75
                ],
                "signals": [
                    75,
                    236,
                    209,
                    182,
                    247,
                    185,
                    284,
                    311
                ],
                "self._closewait": [
                    312,
                    77,
                    78
                ],
                "defer.inlineCallbacks": [
                    309,
                    222,
                    70
                ],
                "dfd": [
                    267,
                    274,
                    275,
                    277,
                    278,
                    280,
                    281,
                    283,
                    285,
                    287,
                    288,
                    290,
                    294,
                    295,
                    297,
                    298,
                    300,
                    302,
                    84,
                    85
                ],
                "self._close_all_spiders": [
                    84
                ],
                "dfd.addBoth": [
                    290,
                    294,
                    297,
                    300,
                    274,
                    277,
                    85,
                    280,
                    283,
                    287
                ],
                "self._finish_stopping_engine": [
                    85
                ],
                "slot": [
                    131,
                    132,
                    260,
                    261,
                    262,
                    139,
                    267,
                    142,
                    280,
                    189,
                    190,
                    192,
                    196,
                    201,
                    202,
                    214,
                    96,
                    97,
                    101,
                    230,
                    231,
                    108,
                    237,
                    110,
                    112,
                    114,
                    120,
                    124,
                    126
                ],
                "slot.nextcall.schedule": [
                    237,
                    214,
                    101,
                    142
                ],
                "slot.nextcall": [
                    237,
                    214,
                    101,
                    142
                ],
                "self._needs_backout": [
                    104,
                    108
                ],
                "spider": [
                    135,
                    136,
                    265,
                    138,
                    141,
                    271,
                    144,
                    277,
                    151,
                    154,
                    156,
                    284,
                    287,
                    292,
                    300,
                    176,
                    177,
                    178,
                    183,
                    186,
                    191,
                    192,
                    197,
                    207,
                    208,
                    210,
                    217,
                    248,
                    225,
                    226,
                    227,
                    229,
                    104,
                    105,
                    232,
                    233,
                    108,
                    234,
                    235,
                    236,
                    116,
                    118,
                    120,
                    121,
                    254,
                    255
                ],
                "self._next_request_from_scheduler": [
                    105
                ],
                "slot.start_requests": [
                    112,
                    114,
                    108,
                    110
                ],
                "logger.error": [
                    115,
                    155,
                    271
                ],
                "self.crawl": [
                    118,
                    151
                ],
                "self.spider_is_idle": [
                    120,
                    254
                ],
                "slot.close_if_idle": [
                    120
                ],
                "self._spider_idle": [
                    121
                ],
                "slot.closing": [
                    261,
                    126,
                    262
                ],
                "self.downloader.needs_backout": [
                    127
                ],
                "self.scraper.slot.needs_backout": [
                    128
                ],
                "self.scraper.slot": [
                    128,
                    160
                ],
                "slot.scheduler.next_request": [
                    132
                ],
                "slot.scheduler": [
                    280,
                    132
                ],
                "d": [
                    192,
                    193,
                    135,
                    136,
                    137,
                    139,
                    140,
                    142,
                    143,
                    145,
                    154,
                    155,
                    157,
                    191
                ],
                "self._download": [
                    191,
                    135
                ],
                "d.addBoth": [
                    136,
                    139,
                    142,
                    192
                ],
                "self._handle_downloader_output": [
                    136
                ],
                "d.addErrback": [
                    137,
                    155,
                    140,
                    143
                ],
                "logger.info": [
                    226,
                    290,
                    263,
                    137,
                    140,
                    143
                ],
                "f": [
                    144,
                    138,
                    156,
                    141
                ],
                "slot.remove_request": [
                    139,
                    196
                ],
                "response": [
                    197,
                    198,
                    204,
                    205,
                    206,
                    207,
                    210,
                    211,
                    148,
                    150,
                    151,
                    154
                ],
                "Request": [
                    204,
                    148,
                    198,
                    150
                ],
                "Response": [
                    204,
                    148,
                    205
                ],
                "Failure": [
                    249,
                    148
                ],
                "self.scraper.enqueue_scrape": [
                    154
                ],
                "scraper_idle": [
                    160,
                    164
                ],
                "self.scraper.slot.is_idle": [
                    160
                ],
                "pending": [
                    161,
                    164
                ],
                "self.slot.scheduler.has_pending_requests": [
                    161
                ],
                "self.slot.scheduler": [
                    184,
                    161
                ],
                "downloading": [
                    162,
                    164
                ],
                "self.downloader.active": [
                    162
                ],
                "pending_start_requests": [
                    163,
                    164
                ],
                "self.slot.start_requests": [
                    163
                ],
                "idle": [
                    164,
                    165
                ],
                "self.open_spiders": [
                    176,
                    305
                ],
                "spider.name": [
                    177,
                    225
                ],
                "self.schedule": [
                    178
                ],
                "self.slot.nextcall.schedule": [
                    179,
                    251
                ],
                "self.slot.nextcall": [
                    179,
                    251
                ],
                "self.signals.send_catch_log": [
                    185,
                    209,
                    182,
                    247
                ],
                "signals.request_scheduled": [
                    182
                ],
                "self.slot.scheduler.enqueue_request": [
                    184
                ],
                "signals.request_dropped": [
                    185
                ],
                "slot.add_request": [
                    202,
                    190
                ],
                "self._downloaded": [
                    192
                ],
                "self.download": [
                    197
                ],
                "response.request": [
                    206
                ],
                "logkws": [
                    208,
                    207
                ],
                "self.logformatter.crawled": [
                    207
                ],
                "logger.log": [
                    208
                ],
                "logformatter_adapter": [
                    208
                ],
                "signals.response_received": [
                    209
                ],
                "_": [
                    250,
                    215
                ],
                "dwld": [
                    217,
                    218,
                    219,
                    220
                ],
                "self.downloader.fetch": [
                    217
                ],
                "dwld.addCallbacks": [
                    218
                ],
                "_on_success": [
                    218
                ],
                "dwld.addBoth": [
                    219
                ],
                "_on_complete": [
                    219
                ],
                "self.has_capacity": [
                    224
                ],
                "CallLaterOnce": [
                    227
                ],
                "self._next_request": [
                    227
                ],
                "self.scheduler_cls.from_crawler": [
                    228
                ],
                "self.scraper.spidermw.process_start_requests": [
                    229
                ],
                "self.scraper.spidermw": [
                    229
                ],
                "Slot": [
                    230
                ],
                "scheduler.open": [
                    233
                ],
                "self.scraper.open_spider": [
                    234
                ],
                "self.crawler.stats.open_spider": [
                    235
                ],
                "self.crawler.stats": [
                    235,
                    287
                ],
                "signals.spider_opened": [
                    236
                ],
                "res": [
                    250,
                    247
                ],
                "signals.spider_idle": [
                    247
                ],
                "DontCloseSpider": [
                    248,
                    249
                ],
                "x": [
                    249,
                    250
                ],
                "x.value": [
                    249
                ],
                "self.close_spider": [
                    305,
                    255
                ],
                "reason": [
                    291,
                    264,
                    280,
                    284,
                    287
                ],
                "slot.close": [
                    267
                ],
                "msg": [
                    271
                ],
                "failure": [
                    271
                ],
                "errback": [
                    272
                ],
                "self.downloader.close": [
                    274
                ],
                "dfd.addErrback": [
                    288,
                    295,
                    298,
                    275,
                    278,
                    281,
                    285
                ],
                "log_failure": [
                    288,
                    295,
                    298,
                    275,
                    278,
                    281,
                    285
                ],
                "self.scraper.close_spider": [
                    277
                ],
                "slot.scheduler.close": [
                    280
                ],
                "signals.spider_closed": [
                    284
                ],
                "self.crawler.stats.close_spider": [
                    287
                ],
                "dfds": [
                    305,
                    306
                ],
                "s": [
                    305
                ],
                "dlist": [
                    306,
                    307
                ],
                "defer.DeferredList": [
                    306
                ],
                "signals.engine_stopped": [
                    311
                ],
                "self._closewait.callback": [
                    312
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs/repos/scrapy_33/scrapy/core/scraper.py": {
            "buggy_functions": [
                {
                    "function_name": "enqueue_scrape",
                    "function_code": "def enqueue_scrape(self, response, request, spider):\n    slot = self.slot\n    dfd = slot.add_response_request(response, request)\n    def finish_scraping(_):\n        slot.finish_response(response, request)\n        self._check_if_closing(spider, slot)\n        self._scrape_next(spider, slot)\n        return _\n    dfd.addBoth(finish_scraping)\n    dfd.addErrback(\n        lambda f: logger.error('Scraper bug processing %(request)s',\n                               {'request': request},\n                               extra={'spider': spider, 'failure': f}))\n    self._scrape_next(spider, slot)\n    return dfd\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 98,
                    "end_line": 112,
                    "variables": {
                        "slot": [
                            99,
                            100,
                            102,
                            103,
                            104,
                            111
                        ],
                        "self.slot": [
                            99
                        ],
                        "self": [
                            104,
                            99,
                            111,
                            103
                        ],
                        "dfd": [
                            112,
                            106,
                            107,
                            100
                        ],
                        "slot.add_response_request": [
                            100
                        ],
                        "response": [
                            100,
                            102
                        ],
                        "request": [
                            100,
                            109,
                            102
                        ],
                        "slot.finish_response": [
                            102
                        ],
                        "self._check_if_closing": [
                            103
                        ],
                        "spider": [
                            104,
                            111,
                            110,
                            103
                        ],
                        "self._scrape_next": [
                            104,
                            111
                        ],
                        "_": [
                            105
                        ],
                        "dfd.addBoth": [
                            106
                        ],
                        "finish_scraping": [
                            106
                        ],
                        "dfd.addErrback": [
                            107
                        ],
                        "logger.error": [
                            108
                        ],
                        "logger": [
                            108
                        ],
                        "f": [
                            110
                        ]
                    },
                    "filtered_variables": {
                        "slot": [
                            99,
                            100,
                            102,
                            103,
                            104,
                            111
                        ],
                        "self.slot": [
                            99
                        ],
                        "self": [
                            104,
                            99,
                            111,
                            103
                        ],
                        "dfd": [
                            112,
                            106,
                            107,
                            100
                        ],
                        "slot.add_response_request": [
                            100
                        ],
                        "response": [
                            100,
                            102
                        ],
                        "request": [
                            100,
                            109,
                            102
                        ],
                        "slot.finish_response": [
                            102
                        ],
                        "self._check_if_closing": [
                            103
                        ],
                        "spider": [
                            104,
                            111,
                            110,
                            103
                        ],
                        "self._scrape_next": [
                            104,
                            111
                        ],
                        "_": [
                            105
                        ],
                        "dfd.addBoth": [
                            106
                        ],
                        "finish_scraping": [
                            106
                        ],
                        "dfd.addErrback": [
                            107
                        ],
                        "logger.error": [
                            108
                        ],
                        "logger": [
                            108
                        ],
                        "f": [
                            110
                        ]
                    },
                    "diff_line_number": 110,
                    "class_data": {
                        "signature": "class Scraper(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, crawler):\n    self.slot = None\n    self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n    itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n    self.itemproc = itemproc_cls.from_crawler(crawler)\n    self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n    self.crawler = crawler\n    self.signals = crawler.signals\n    self.logformatter = crawler.logformatter",
                            "@defer.inlineCallbacks\ndef open_spider(self, spider):\n    \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n    self.slot = Slot()\n    yield self.itemproc.open_spider(spider)",
                            "def close_spider(self, spider):\n    \"\"\"Close a spider being scraped and release its resources\"\"\"\n    slot = self.slot\n    slot.closing = defer.Deferred()\n    slot.closing.addCallback(self.itemproc.close_spider)\n    self._check_if_closing(spider, slot)\n    return slot.closing",
                            "def is_idle(self):\n    \"\"\"Return True if there isn't any more spiders to process\"\"\"\n    return not self.slot",
                            "def _check_if_closing(self, spider, slot):\n    if slot.closing and slot.is_idle():\n        slot.closing.callback(spider)",
                            "def enqueue_scrape(self, response, request, spider):\n    slot = self.slot\n    dfd = slot.add_response_request(response, request)\n\n    def finish_scraping(_):\n        slot.finish_response(response, request)\n        self._check_if_closing(spider, slot)\n        self._scrape_next(spider, slot)\n        return _\n    dfd.addBoth(finish_scraping)\n    dfd.addErrback(lambda f: logger.error('Scraper bug processing %(request)s', {'request': request}, extra={'spider': spider, 'failure': f}))\n    self._scrape_next(spider, slot)\n    return dfd",
                            "def _scrape_next(self, spider, slot):\n    while slot.queue:\n        response, request, deferred = slot.next_response_request_deferred()\n        self._scrape(response, request, spider).chainDeferred(deferred)",
                            "def _scrape(self, response, request, spider):\n    \"\"\"Handle the downloaded response or failure trough the spider\n    callback/errback\"\"\"\n    assert isinstance(response, (Response, Failure))\n    dfd = self._scrape2(response, request, spider)\n    dfd.addErrback(self.handle_spider_error, request, response, spider)\n    dfd.addCallback(self.handle_spider_output, request, response, spider)\n    return dfd",
                            "def _scrape2(self, request_result, request, spider):\n    \"\"\"Handle the different cases of request's result been a Response or a\n    Failure\"\"\"\n    if not isinstance(request_result, Failure):\n        return self.spidermw.scrape_response(self.call_spider, request_result, request, spider)\n    else:\n        dfd = self.call_spider(request_result, request, spider)\n        return dfd.addErrback(self._log_download_errors, request_result, request, spider)",
                            "def call_spider(self, result, request, spider):\n    result.request = request\n    dfd = defer_result(result)\n    dfd.addCallbacks(request.callback or spider.parse, request.errback)\n    return dfd.addCallback(iterate_spider_output)",
                            "def handle_spider_error(self, _failure, request, response, spider):\n    exc = _failure.value\n    if isinstance(exc, CloseSpider):\n        self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n        return\n    referer = request.headers.get('Referer')\n    logger.error('Spider error processing %(request)s (referer: %(referer)s)', {'request': request, 'referer': referer}, extra={'spider': spider, 'failure': _failure})\n    self.signals.send_catch_log(signal=signals.spider_error, failure=_failure, response=response, spider=spider)\n    self.crawler.stats.inc_value('spider_exceptions/%s' % _failure.value.__class__.__name__, spider=spider)",
                            "def handle_spider_output(self, result, request, response, spider):\n    if not result:\n        return defer_succeed(None)\n    it = iter_errback(result, self.handle_spider_error, request, response, spider)\n    dfd = parallel(it, self.concurrent_items, self._process_spidermw_output, request, response, spider)\n    return dfd",
                            "def _process_spidermw_output(self, output, request, response, spider):\n    \"\"\"Process each Request/Item (given in the output parameter) returned\n    from the given spider\n    \"\"\"\n    if isinstance(output, Request):\n        self.crawler.engine.crawl(request=output, spider=spider)\n    elif isinstance(output, (BaseItem, dict)):\n        self.slot.itemproc_size += 1\n        dfd = self.itemproc.process_item(output, spider)\n        dfd.addBoth(self._itemproc_finished, output, response, spider)\n        return dfd\n    elif output is None:\n        pass\n    else:\n        typename = type(output).__name__\n        logger.error('Spider must return Request, BaseItem, dict or None, got %(typename)r in %(request)s', {'request': request, 'typename': typename}, extra={'spider': spider})",
                            "def _log_download_errors(self, spider_failure, download_failure, request, spider):\n    \"\"\"Log and silence errors that come from the engine (typically download\n    errors that got propagated thru here)\n    \"\"\"\n    if isinstance(download_failure, Failure) and (not download_failure.check(IgnoreRequest)):\n        if download_failure.frames:\n            logger.error('Error downloading %(request)s', {'request': request}, extra={'spider': spider, 'failure': download_failure})\n        else:\n            errmsg = download_failure.getErrorMessage()\n            if errmsg:\n                logger.error('Error downloading %(request)s: %(errmsg)s', {'request': request, 'errmsg': errmsg}, extra={'spider': spider})\n    if spider_failure is not download_failure:\n        return spider_failure",
                            "def _itemproc_finished(self, output, item, response, spider):\n    \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n    \"\"\"\n    self.slot.itemproc_size -= 1\n    if isinstance(output, Failure):\n        ex = output.value\n        if isinstance(ex, DropItem):\n            logkws = self.logformatter.dropped(item, ex, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            return self.signals.send_catch_log_deferred(signal=signals.item_dropped, item=item, response=response, spider=spider, exception=output.value)\n        else:\n            logger.error('Error processing %(item)s', {'item': item}, extra={'spider': spider, 'failure': output})\n    else:\n        logkws = self.logformatter.scraped(output, response, spider)\n        logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n        return self.signals.send_catch_log_deferred(signal=signals.item_scraped, item=output, response=response, spider=spider)",
                            "def finish_scraping(_):\n    slot.finish_response(response, request)\n    self._check_if_closing(spider, slot)\n    self._scrape_next(spider, slot)\n    return _"
                        ],
                        "constructor_variables": [
                            "itemproc",
                            "logformatter",
                            "spidermw",
                            "concurrent_items",
                            "slot",
                            "itemproc_cls",
                            "crawler",
                            "signals"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, crawler)",
                            "open_spider(self, spider)",
                            "close_spider(self, spider)",
                            "is_idle(self)",
                            "_check_if_closing(self, spider, slot)",
                            "enqueue_scrape(self, response, request, spider)",
                            "_scrape_next(self, spider, slot)",
                            "_scrape(self, response, request, spider)",
                            "_scrape2(self, request_result, request, spider)",
                            "call_spider(self, result, request, spider)",
                            "handle_spider_error(self, _failure, request, response, spider)",
                            "handle_spider_output(self, result, request, response, spider)",
                            "_process_spidermw_output(self, output, request, response, spider)",
                            "_log_download_errors(self, spider_failure, download_failure, request, spider)",
                            "_itemproc_finished(self, output, item, response, spider)",
                            "finish_scraping(_)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "handle_spider_error",
                    "function_code": "def handle_spider_error(self, _failure, request, response, spider):\n    exc = _failure.value\n    if isinstance(exc, CloseSpider):\n        self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n        return\n    referer = request.headers.get('Referer')\n    logger.error(\n        \"Spider error processing %(request)s (referer: %(referer)s)\",\n        {'request': request, 'referer': referer},\n        extra={'spider': spider, 'failure': _failure}\n    )\n    self.signals.send_catch_log(\n        signal=signals.spider_error,\n        failure=_failure, response=response,\n        spider=spider\n    )\n    self.crawler.stats.inc_value(\n        \"spider_exceptions/%s\" % _failure.value.__class__.__name__,\n        spider=spider\n    )\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 147,
                    "end_line": 166,
                    "variables": {
                        "exc": [
                            148,
                            149,
                            150
                        ],
                        "_failure.value": [
                            164,
                            148
                        ],
                        "_failure": [
                            160,
                            156,
                            164,
                            148
                        ],
                        "isinstance": [
                            149
                        ],
                        "CloseSpider": [
                            149
                        ],
                        "self.crawler.engine.close_spider": [
                            150
                        ],
                        "self.crawler.engine": [
                            150
                        ],
                        "self.crawler": [
                            163,
                            150
                        ],
                        "self": [
                            158,
                            163,
                            150
                        ],
                        "spider": [
                            161,
                            156,
                            165,
                            150
                        ],
                        "exc.reason": [
                            150
                        ],
                        "referer": [
                            152,
                            155
                        ],
                        "request.headers.get": [
                            152
                        ],
                        "request.headers": [
                            152
                        ],
                        "request": [
                            152,
                            155
                        ],
                        "logger.error": [
                            153
                        ],
                        "logger": [
                            153
                        ],
                        "self.signals.send_catch_log": [
                            158
                        ],
                        "self.signals": [
                            158
                        ],
                        "signals.spider_error": [
                            159
                        ],
                        "signals": [
                            159
                        ],
                        "response": [
                            160
                        ],
                        "self.crawler.stats.inc_value": [
                            163
                        ],
                        "self.crawler.stats": [
                            163
                        ],
                        "_failure.value.__class__.__name__": [
                            164
                        ],
                        "_failure.value.__class__": [
                            164
                        ]
                    },
                    "filtered_variables": {
                        "exc": [
                            148,
                            149,
                            150
                        ],
                        "_failure.value": [
                            164,
                            148
                        ],
                        "_failure": [
                            160,
                            156,
                            164,
                            148
                        ],
                        "CloseSpider": [
                            149
                        ],
                        "self.crawler.engine.close_spider": [
                            150
                        ],
                        "self.crawler.engine": [
                            150
                        ],
                        "self.crawler": [
                            163,
                            150
                        ],
                        "self": [
                            158,
                            163,
                            150
                        ],
                        "spider": [
                            161,
                            156,
                            165,
                            150
                        ],
                        "exc.reason": [
                            150
                        ],
                        "referer": [
                            152,
                            155
                        ],
                        "request.headers.get": [
                            152
                        ],
                        "request.headers": [
                            152
                        ],
                        "request": [
                            152,
                            155
                        ],
                        "logger.error": [
                            153
                        ],
                        "logger": [
                            153
                        ],
                        "self.signals.send_catch_log": [
                            158
                        ],
                        "self.signals": [
                            158
                        ],
                        "signals.spider_error": [
                            159
                        ],
                        "signals": [
                            159
                        ],
                        "response": [
                            160
                        ],
                        "self.crawler.stats.inc_value": [
                            163
                        ],
                        "self.crawler.stats": [
                            163
                        ],
                        "_failure.value.__class__.__name__": [
                            164
                        ],
                        "_failure.value.__class__": [
                            164
                        ]
                    },
                    "diff_line_number": 156,
                    "class_data": {
                        "signature": "class Scraper(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, crawler):\n    self.slot = None\n    self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n    itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n    self.itemproc = itemproc_cls.from_crawler(crawler)\n    self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n    self.crawler = crawler\n    self.signals = crawler.signals\n    self.logformatter = crawler.logformatter",
                            "@defer.inlineCallbacks\ndef open_spider(self, spider):\n    \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n    self.slot = Slot()\n    yield self.itemproc.open_spider(spider)",
                            "def close_spider(self, spider):\n    \"\"\"Close a spider being scraped and release its resources\"\"\"\n    slot = self.slot\n    slot.closing = defer.Deferred()\n    slot.closing.addCallback(self.itemproc.close_spider)\n    self._check_if_closing(spider, slot)\n    return slot.closing",
                            "def is_idle(self):\n    \"\"\"Return True if there isn't any more spiders to process\"\"\"\n    return not self.slot",
                            "def _check_if_closing(self, spider, slot):\n    if slot.closing and slot.is_idle():\n        slot.closing.callback(spider)",
                            "def enqueue_scrape(self, response, request, spider):\n    slot = self.slot\n    dfd = slot.add_response_request(response, request)\n\n    def finish_scraping(_):\n        slot.finish_response(response, request)\n        self._check_if_closing(spider, slot)\n        self._scrape_next(spider, slot)\n        return _\n    dfd.addBoth(finish_scraping)\n    dfd.addErrback(lambda f: logger.error('Scraper bug processing %(request)s', {'request': request}, extra={'spider': spider, 'failure': f}))\n    self._scrape_next(spider, slot)\n    return dfd",
                            "def _scrape_next(self, spider, slot):\n    while slot.queue:\n        response, request, deferred = slot.next_response_request_deferred()\n        self._scrape(response, request, spider).chainDeferred(deferred)",
                            "def _scrape(self, response, request, spider):\n    \"\"\"Handle the downloaded response or failure trough the spider\n    callback/errback\"\"\"\n    assert isinstance(response, (Response, Failure))\n    dfd = self._scrape2(response, request, spider)\n    dfd.addErrback(self.handle_spider_error, request, response, spider)\n    dfd.addCallback(self.handle_spider_output, request, response, spider)\n    return dfd",
                            "def _scrape2(self, request_result, request, spider):\n    \"\"\"Handle the different cases of request's result been a Response or a\n    Failure\"\"\"\n    if not isinstance(request_result, Failure):\n        return self.spidermw.scrape_response(self.call_spider, request_result, request, spider)\n    else:\n        dfd = self.call_spider(request_result, request, spider)\n        return dfd.addErrback(self._log_download_errors, request_result, request, spider)",
                            "def call_spider(self, result, request, spider):\n    result.request = request\n    dfd = defer_result(result)\n    dfd.addCallbacks(request.callback or spider.parse, request.errback)\n    return dfd.addCallback(iterate_spider_output)",
                            "def handle_spider_error(self, _failure, request, response, spider):\n    exc = _failure.value\n    if isinstance(exc, CloseSpider):\n        self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n        return\n    referer = request.headers.get('Referer')\n    logger.error('Spider error processing %(request)s (referer: %(referer)s)', {'request': request, 'referer': referer}, extra={'spider': spider, 'failure': _failure})\n    self.signals.send_catch_log(signal=signals.spider_error, failure=_failure, response=response, spider=spider)\n    self.crawler.stats.inc_value('spider_exceptions/%s' % _failure.value.__class__.__name__, spider=spider)",
                            "def handle_spider_output(self, result, request, response, spider):\n    if not result:\n        return defer_succeed(None)\n    it = iter_errback(result, self.handle_spider_error, request, response, spider)\n    dfd = parallel(it, self.concurrent_items, self._process_spidermw_output, request, response, spider)\n    return dfd",
                            "def _process_spidermw_output(self, output, request, response, spider):\n    \"\"\"Process each Request/Item (given in the output parameter) returned\n    from the given spider\n    \"\"\"\n    if isinstance(output, Request):\n        self.crawler.engine.crawl(request=output, spider=spider)\n    elif isinstance(output, (BaseItem, dict)):\n        self.slot.itemproc_size += 1\n        dfd = self.itemproc.process_item(output, spider)\n        dfd.addBoth(self._itemproc_finished, output, response, spider)\n        return dfd\n    elif output is None:\n        pass\n    else:\n        typename = type(output).__name__\n        logger.error('Spider must return Request, BaseItem, dict or None, got %(typename)r in %(request)s', {'request': request, 'typename': typename}, extra={'spider': spider})",
                            "def _log_download_errors(self, spider_failure, download_failure, request, spider):\n    \"\"\"Log and silence errors that come from the engine (typically download\n    errors that got propagated thru here)\n    \"\"\"\n    if isinstance(download_failure, Failure) and (not download_failure.check(IgnoreRequest)):\n        if download_failure.frames:\n            logger.error('Error downloading %(request)s', {'request': request}, extra={'spider': spider, 'failure': download_failure})\n        else:\n            errmsg = download_failure.getErrorMessage()\n            if errmsg:\n                logger.error('Error downloading %(request)s: %(errmsg)s', {'request': request, 'errmsg': errmsg}, extra={'spider': spider})\n    if spider_failure is not download_failure:\n        return spider_failure",
                            "def _itemproc_finished(self, output, item, response, spider):\n    \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n    \"\"\"\n    self.slot.itemproc_size -= 1\n    if isinstance(output, Failure):\n        ex = output.value\n        if isinstance(ex, DropItem):\n            logkws = self.logformatter.dropped(item, ex, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            return self.signals.send_catch_log_deferred(signal=signals.item_dropped, item=item, response=response, spider=spider, exception=output.value)\n        else:\n            logger.error('Error processing %(item)s', {'item': item}, extra={'spider': spider, 'failure': output})\n    else:\n        logkws = self.logformatter.scraped(output, response, spider)\n        logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n        return self.signals.send_catch_log_deferred(signal=signals.item_scraped, item=output, response=response, spider=spider)",
                            "def finish_scraping(_):\n    slot.finish_response(response, request)\n    self._check_if_closing(spider, slot)\n    self._scrape_next(spider, slot)\n    return _"
                        ],
                        "constructor_variables": [
                            "itemproc",
                            "logformatter",
                            "spidermw",
                            "concurrent_items",
                            "slot",
                            "itemproc_cls",
                            "crawler",
                            "signals"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, crawler)",
                            "open_spider(self, spider)",
                            "close_spider(self, spider)",
                            "is_idle(self)",
                            "_check_if_closing(self, spider, slot)",
                            "enqueue_scrape(self, response, request, spider)",
                            "_scrape_next(self, spider, slot)",
                            "_scrape(self, response, request, spider)",
                            "_scrape2(self, request_result, request, spider)",
                            "call_spider(self, result, request, spider)",
                            "handle_spider_error(self, _failure, request, response, spider)",
                            "handle_spider_output(self, result, request, response, spider)",
                            "_process_spidermw_output(self, output, request, response, spider)",
                            "_log_download_errors(self, spider_failure, download_failure, request, spider)",
                            "_itemproc_finished(self, output, item, response, spider)",
                            "finish_scraping(_)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "_log_download_errors",
                    "function_code": "def _log_download_errors(self, spider_failure, download_failure, request, spider):\n    \"\"\"Log and silence errors that come from the engine (typically download\n    errors that got propagated thru here)\n    \"\"\"\n    if (isinstance(download_failure, Failure) and\n            not download_failure.check(IgnoreRequest)):\n        if download_failure.frames:\n            logger.error('Error downloading %(request)s',\n                         {'request': request},\n                         extra={'spider': spider, 'failure': download_failure})\n        else:\n            errmsg = download_failure.getErrorMessage()\n            if errmsg:\n                logger.error('Error downloading %(request)s: %(errmsg)s',\n                             {'request': request, 'errmsg': errmsg},\n                             extra={'spider': spider})\n\n    if spider_failure is not download_failure:\n        return spider_failure\n",
                    "decorators": [],
                    "docstring": "Log and silence errors that come from the engine (typically download\nerrors that got propagated thru here)",
                    "start_line": 196,
                    "end_line": 214,
                    "variables": {
                        "isinstance": [
                            200
                        ],
                        "download_failure": [
                            200,
                            201,
                            202,
                            205,
                            207,
                            213
                        ],
                        "Failure": [
                            200
                        ],
                        "download_failure.check": [
                            201
                        ],
                        "IgnoreRequest": [
                            201
                        ],
                        "download_failure.frames": [
                            202
                        ],
                        "logger.error": [
                            209,
                            203
                        ],
                        "logger": [
                            209,
                            203
                        ],
                        "request": [
                            210,
                            204
                        ],
                        "spider": [
                            211,
                            205
                        ],
                        "errmsg": [
                            208,
                            210,
                            207
                        ],
                        "download_failure.getErrorMessage": [
                            207
                        ],
                        "spider_failure": [
                            213,
                            214
                        ]
                    },
                    "filtered_variables": {
                        "download_failure": [
                            200,
                            201,
                            202,
                            205,
                            207,
                            213
                        ],
                        "Failure": [
                            200
                        ],
                        "download_failure.check": [
                            201
                        ],
                        "IgnoreRequest": [
                            201
                        ],
                        "download_failure.frames": [
                            202
                        ],
                        "logger.error": [
                            209,
                            203
                        ],
                        "logger": [
                            209,
                            203
                        ],
                        "request": [
                            210,
                            204
                        ],
                        "spider": [
                            211,
                            205
                        ],
                        "errmsg": [
                            208,
                            210,
                            207
                        ],
                        "download_failure.getErrorMessage": [
                            207
                        ],
                        "spider_failure": [
                            213,
                            214
                        ]
                    },
                    "diff_line_number": 205,
                    "class_data": {
                        "signature": "class Scraper(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, crawler):\n    self.slot = None\n    self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n    itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n    self.itemproc = itemproc_cls.from_crawler(crawler)\n    self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n    self.crawler = crawler\n    self.signals = crawler.signals\n    self.logformatter = crawler.logformatter",
                            "@defer.inlineCallbacks\ndef open_spider(self, spider):\n    \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n    self.slot = Slot()\n    yield self.itemproc.open_spider(spider)",
                            "def close_spider(self, spider):\n    \"\"\"Close a spider being scraped and release its resources\"\"\"\n    slot = self.slot\n    slot.closing = defer.Deferred()\n    slot.closing.addCallback(self.itemproc.close_spider)\n    self._check_if_closing(spider, slot)\n    return slot.closing",
                            "def is_idle(self):\n    \"\"\"Return True if there isn't any more spiders to process\"\"\"\n    return not self.slot",
                            "def _check_if_closing(self, spider, slot):\n    if slot.closing and slot.is_idle():\n        slot.closing.callback(spider)",
                            "def enqueue_scrape(self, response, request, spider):\n    slot = self.slot\n    dfd = slot.add_response_request(response, request)\n\n    def finish_scraping(_):\n        slot.finish_response(response, request)\n        self._check_if_closing(spider, slot)\n        self._scrape_next(spider, slot)\n        return _\n    dfd.addBoth(finish_scraping)\n    dfd.addErrback(lambda f: logger.error('Scraper bug processing %(request)s', {'request': request}, extra={'spider': spider, 'failure': f}))\n    self._scrape_next(spider, slot)\n    return dfd",
                            "def _scrape_next(self, spider, slot):\n    while slot.queue:\n        response, request, deferred = slot.next_response_request_deferred()\n        self._scrape(response, request, spider).chainDeferred(deferred)",
                            "def _scrape(self, response, request, spider):\n    \"\"\"Handle the downloaded response or failure trough the spider\n    callback/errback\"\"\"\n    assert isinstance(response, (Response, Failure))\n    dfd = self._scrape2(response, request, spider)\n    dfd.addErrback(self.handle_spider_error, request, response, spider)\n    dfd.addCallback(self.handle_spider_output, request, response, spider)\n    return dfd",
                            "def _scrape2(self, request_result, request, spider):\n    \"\"\"Handle the different cases of request's result been a Response or a\n    Failure\"\"\"\n    if not isinstance(request_result, Failure):\n        return self.spidermw.scrape_response(self.call_spider, request_result, request, spider)\n    else:\n        dfd = self.call_spider(request_result, request, spider)\n        return dfd.addErrback(self._log_download_errors, request_result, request, spider)",
                            "def call_spider(self, result, request, spider):\n    result.request = request\n    dfd = defer_result(result)\n    dfd.addCallbacks(request.callback or spider.parse, request.errback)\n    return dfd.addCallback(iterate_spider_output)",
                            "def handle_spider_error(self, _failure, request, response, spider):\n    exc = _failure.value\n    if isinstance(exc, CloseSpider):\n        self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n        return\n    referer = request.headers.get('Referer')\n    logger.error('Spider error processing %(request)s (referer: %(referer)s)', {'request': request, 'referer': referer}, extra={'spider': spider, 'failure': _failure})\n    self.signals.send_catch_log(signal=signals.spider_error, failure=_failure, response=response, spider=spider)\n    self.crawler.stats.inc_value('spider_exceptions/%s' % _failure.value.__class__.__name__, spider=spider)",
                            "def handle_spider_output(self, result, request, response, spider):\n    if not result:\n        return defer_succeed(None)\n    it = iter_errback(result, self.handle_spider_error, request, response, spider)\n    dfd = parallel(it, self.concurrent_items, self._process_spidermw_output, request, response, spider)\n    return dfd",
                            "def _process_spidermw_output(self, output, request, response, spider):\n    \"\"\"Process each Request/Item (given in the output parameter) returned\n    from the given spider\n    \"\"\"\n    if isinstance(output, Request):\n        self.crawler.engine.crawl(request=output, spider=spider)\n    elif isinstance(output, (BaseItem, dict)):\n        self.slot.itemproc_size += 1\n        dfd = self.itemproc.process_item(output, spider)\n        dfd.addBoth(self._itemproc_finished, output, response, spider)\n        return dfd\n    elif output is None:\n        pass\n    else:\n        typename = type(output).__name__\n        logger.error('Spider must return Request, BaseItem, dict or None, got %(typename)r in %(request)s', {'request': request, 'typename': typename}, extra={'spider': spider})",
                            "def _log_download_errors(self, spider_failure, download_failure, request, spider):\n    \"\"\"Log and silence errors that come from the engine (typically download\n    errors that got propagated thru here)\n    \"\"\"\n    if isinstance(download_failure, Failure) and (not download_failure.check(IgnoreRequest)):\n        if download_failure.frames:\n            logger.error('Error downloading %(request)s', {'request': request}, extra={'spider': spider, 'failure': download_failure})\n        else:\n            errmsg = download_failure.getErrorMessage()\n            if errmsg:\n                logger.error('Error downloading %(request)s: %(errmsg)s', {'request': request, 'errmsg': errmsg}, extra={'spider': spider})\n    if spider_failure is not download_failure:\n        return spider_failure",
                            "def _itemproc_finished(self, output, item, response, spider):\n    \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n    \"\"\"\n    self.slot.itemproc_size -= 1\n    if isinstance(output, Failure):\n        ex = output.value\n        if isinstance(ex, DropItem):\n            logkws = self.logformatter.dropped(item, ex, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            return self.signals.send_catch_log_deferred(signal=signals.item_dropped, item=item, response=response, spider=spider, exception=output.value)\n        else:\n            logger.error('Error processing %(item)s', {'item': item}, extra={'spider': spider, 'failure': output})\n    else:\n        logkws = self.logformatter.scraped(output, response, spider)\n        logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n        return self.signals.send_catch_log_deferred(signal=signals.item_scraped, item=output, response=response, spider=spider)",
                            "def finish_scraping(_):\n    slot.finish_response(response, request)\n    self._check_if_closing(spider, slot)\n    self._scrape_next(spider, slot)\n    return _"
                        ],
                        "constructor_variables": [
                            "itemproc",
                            "logformatter",
                            "spidermw",
                            "concurrent_items",
                            "slot",
                            "itemproc_cls",
                            "crawler",
                            "signals"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, crawler)",
                            "open_spider(self, spider)",
                            "close_spider(self, spider)",
                            "is_idle(self)",
                            "_check_if_closing(self, spider, slot)",
                            "enqueue_scrape(self, response, request, spider)",
                            "_scrape_next(self, spider, slot)",
                            "_scrape(self, response, request, spider)",
                            "_scrape2(self, request_result, request, spider)",
                            "call_spider(self, result, request, spider)",
                            "handle_spider_error(self, _failure, request, response, spider)",
                            "handle_spider_output(self, result, request, response, spider)",
                            "_process_spidermw_output(self, output, request, response, spider)",
                            "_log_download_errors(self, spider_failure, download_failure, request, spider)",
                            "_itemproc_finished(self, output, item, response, spider)",
                            "finish_scraping(_)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "_itemproc_finished",
                    "function_code": "def _itemproc_finished(self, output, item, response, spider):\n    \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n    \"\"\"\n    self.slot.itemproc_size -= 1\n    if isinstance(output, Failure):\n        ex = output.value\n        if isinstance(ex, DropItem):\n            logkws = self.logformatter.dropped(item, ex, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            return self.signals.send_catch_log_deferred(\n                signal=signals.item_dropped, item=item, response=response,\n                spider=spider, exception=output.value)\n        else:\n            logger.error('Error processing %(item)s', {'item': item},\n                         extra={'spider': spider, 'failure': output})\n    else:\n        logkws = self.logformatter.scraped(output, response, spider)\n        logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n        return self.signals.send_catch_log_deferred(\n            signal=signals.item_scraped, item=output, response=response,\n            spider=spider)\n",
                    "decorators": [],
                    "docstring": "ItemProcessor finished for the given ``item`` and returned ``output``\n        ",
                    "start_line": 216,
                    "end_line": 236,
                    "variables": {
                        "self.slot.itemproc_size": [
                            219
                        ],
                        "self.slot": [
                            219
                        ],
                        "self": [
                            225,
                            232,
                            234,
                            219,
                            223
                        ],
                        "isinstance": [
                            220,
                            222
                        ],
                        "output": [
                            227,
                            230,
                            232,
                            235,
                            220,
                            221
                        ],
                        "Failure": [
                            220
                        ],
                        "ex": [
                            221,
                            222,
                            223
                        ],
                        "output.value": [
                            227,
                            221
                        ],
                        "DropItem": [
                            222
                        ],
                        "logkws": [
                            224,
                            232,
                            233,
                            223
                        ],
                        "self.logformatter.dropped": [
                            223
                        ],
                        "self.logformatter": [
                            232,
                            223
                        ],
                        "item": [
                            226,
                            229,
                            223
                        ],
                        "response": [
                            232,
                            226,
                            235,
                            223
                        ],
                        "spider": [
                            224,
                            227,
                            230,
                            232,
                            233,
                            236,
                            223
                        ],
                        "logger.log": [
                            224,
                            233
                        ],
                        "logger": [
                            224,
                            233,
                            229
                        ],
                        "logformatter_adapter": [
                            224,
                            233
                        ],
                        "self.signals.send_catch_log_deferred": [
                            225,
                            234
                        ],
                        "self.signals": [
                            225,
                            234
                        ],
                        "signals.item_dropped": [
                            226
                        ],
                        "signals": [
                            226,
                            235
                        ],
                        "logger.error": [
                            229
                        ],
                        "self.logformatter.scraped": [
                            232
                        ],
                        "signals.item_scraped": [
                            235
                        ]
                    },
                    "filtered_variables": {
                        "self.slot.itemproc_size": [
                            219
                        ],
                        "self.slot": [
                            219
                        ],
                        "self": [
                            225,
                            232,
                            234,
                            219,
                            223
                        ],
                        "output": [
                            227,
                            230,
                            232,
                            235,
                            220,
                            221
                        ],
                        "Failure": [
                            220
                        ],
                        "ex": [
                            221,
                            222,
                            223
                        ],
                        "output.value": [
                            227,
                            221
                        ],
                        "DropItem": [
                            222
                        ],
                        "logkws": [
                            224,
                            232,
                            233,
                            223
                        ],
                        "self.logformatter.dropped": [
                            223
                        ],
                        "self.logformatter": [
                            232,
                            223
                        ],
                        "item": [
                            226,
                            229,
                            223
                        ],
                        "response": [
                            232,
                            226,
                            235,
                            223
                        ],
                        "spider": [
                            224,
                            227,
                            230,
                            232,
                            233,
                            236,
                            223
                        ],
                        "logger.log": [
                            224,
                            233
                        ],
                        "logger": [
                            224,
                            233,
                            229
                        ],
                        "logformatter_adapter": [
                            224,
                            233
                        ],
                        "self.signals.send_catch_log_deferred": [
                            225,
                            234
                        ],
                        "self.signals": [
                            225,
                            234
                        ],
                        "signals.item_dropped": [
                            226
                        ],
                        "signals": [
                            226,
                            235
                        ],
                        "logger.error": [
                            229
                        ],
                        "self.logformatter.scraped": [
                            232
                        ],
                        "signals.item_scraped": [
                            235
                        ]
                    },
                    "diff_line_number": 230,
                    "class_data": {
                        "signature": "class Scraper(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, crawler):\n    self.slot = None\n    self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n    itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n    self.itemproc = itemproc_cls.from_crawler(crawler)\n    self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n    self.crawler = crawler\n    self.signals = crawler.signals\n    self.logformatter = crawler.logformatter",
                            "@defer.inlineCallbacks\ndef open_spider(self, spider):\n    \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n    self.slot = Slot()\n    yield self.itemproc.open_spider(spider)",
                            "def close_spider(self, spider):\n    \"\"\"Close a spider being scraped and release its resources\"\"\"\n    slot = self.slot\n    slot.closing = defer.Deferred()\n    slot.closing.addCallback(self.itemproc.close_spider)\n    self._check_if_closing(spider, slot)\n    return slot.closing",
                            "def is_idle(self):\n    \"\"\"Return True if there isn't any more spiders to process\"\"\"\n    return not self.slot",
                            "def _check_if_closing(self, spider, slot):\n    if slot.closing and slot.is_idle():\n        slot.closing.callback(spider)",
                            "def enqueue_scrape(self, response, request, spider):\n    slot = self.slot\n    dfd = slot.add_response_request(response, request)\n\n    def finish_scraping(_):\n        slot.finish_response(response, request)\n        self._check_if_closing(spider, slot)\n        self._scrape_next(spider, slot)\n        return _\n    dfd.addBoth(finish_scraping)\n    dfd.addErrback(lambda f: logger.error('Scraper bug processing %(request)s', {'request': request}, extra={'spider': spider, 'failure': f}))\n    self._scrape_next(spider, slot)\n    return dfd",
                            "def _scrape_next(self, spider, slot):\n    while slot.queue:\n        response, request, deferred = slot.next_response_request_deferred()\n        self._scrape(response, request, spider).chainDeferred(deferred)",
                            "def _scrape(self, response, request, spider):\n    \"\"\"Handle the downloaded response or failure trough the spider\n    callback/errback\"\"\"\n    assert isinstance(response, (Response, Failure))\n    dfd = self._scrape2(response, request, spider)\n    dfd.addErrback(self.handle_spider_error, request, response, spider)\n    dfd.addCallback(self.handle_spider_output, request, response, spider)\n    return dfd",
                            "def _scrape2(self, request_result, request, spider):\n    \"\"\"Handle the different cases of request's result been a Response or a\n    Failure\"\"\"\n    if not isinstance(request_result, Failure):\n        return self.spidermw.scrape_response(self.call_spider, request_result, request, spider)\n    else:\n        dfd = self.call_spider(request_result, request, spider)\n        return dfd.addErrback(self._log_download_errors, request_result, request, spider)",
                            "def call_spider(self, result, request, spider):\n    result.request = request\n    dfd = defer_result(result)\n    dfd.addCallbacks(request.callback or spider.parse, request.errback)\n    return dfd.addCallback(iterate_spider_output)",
                            "def handle_spider_error(self, _failure, request, response, spider):\n    exc = _failure.value\n    if isinstance(exc, CloseSpider):\n        self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n        return\n    referer = request.headers.get('Referer')\n    logger.error('Spider error processing %(request)s (referer: %(referer)s)', {'request': request, 'referer': referer}, extra={'spider': spider, 'failure': _failure})\n    self.signals.send_catch_log(signal=signals.spider_error, failure=_failure, response=response, spider=spider)\n    self.crawler.stats.inc_value('spider_exceptions/%s' % _failure.value.__class__.__name__, spider=spider)",
                            "def handle_spider_output(self, result, request, response, spider):\n    if not result:\n        return defer_succeed(None)\n    it = iter_errback(result, self.handle_spider_error, request, response, spider)\n    dfd = parallel(it, self.concurrent_items, self._process_spidermw_output, request, response, spider)\n    return dfd",
                            "def _process_spidermw_output(self, output, request, response, spider):\n    \"\"\"Process each Request/Item (given in the output parameter) returned\n    from the given spider\n    \"\"\"\n    if isinstance(output, Request):\n        self.crawler.engine.crawl(request=output, spider=spider)\n    elif isinstance(output, (BaseItem, dict)):\n        self.slot.itemproc_size += 1\n        dfd = self.itemproc.process_item(output, spider)\n        dfd.addBoth(self._itemproc_finished, output, response, spider)\n        return dfd\n    elif output is None:\n        pass\n    else:\n        typename = type(output).__name__\n        logger.error('Spider must return Request, BaseItem, dict or None, got %(typename)r in %(request)s', {'request': request, 'typename': typename}, extra={'spider': spider})",
                            "def _log_download_errors(self, spider_failure, download_failure, request, spider):\n    \"\"\"Log and silence errors that come from the engine (typically download\n    errors that got propagated thru here)\n    \"\"\"\n    if isinstance(download_failure, Failure) and (not download_failure.check(IgnoreRequest)):\n        if download_failure.frames:\n            logger.error('Error downloading %(request)s', {'request': request}, extra={'spider': spider, 'failure': download_failure})\n        else:\n            errmsg = download_failure.getErrorMessage()\n            if errmsg:\n                logger.error('Error downloading %(request)s: %(errmsg)s', {'request': request, 'errmsg': errmsg}, extra={'spider': spider})\n    if spider_failure is not download_failure:\n        return spider_failure",
                            "def _itemproc_finished(self, output, item, response, spider):\n    \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n    \"\"\"\n    self.slot.itemproc_size -= 1\n    if isinstance(output, Failure):\n        ex = output.value\n        if isinstance(ex, DropItem):\n            logkws = self.logformatter.dropped(item, ex, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            return self.signals.send_catch_log_deferred(signal=signals.item_dropped, item=item, response=response, spider=spider, exception=output.value)\n        else:\n            logger.error('Error processing %(item)s', {'item': item}, extra={'spider': spider, 'failure': output})\n    else:\n        logkws = self.logformatter.scraped(output, response, spider)\n        logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n        return self.signals.send_catch_log_deferred(signal=signals.item_scraped, item=output, response=response, spider=spider)",
                            "def finish_scraping(_):\n    slot.finish_response(response, request)\n    self._check_if_closing(spider, slot)\n    self._scrape_next(spider, slot)\n    return _"
                        ],
                        "constructor_variables": [
                            "itemproc",
                            "logformatter",
                            "spidermw",
                            "concurrent_items",
                            "slot",
                            "itemproc_cls",
                            "crawler",
                            "signals"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, crawler)",
                            "open_spider(self, spider)",
                            "close_spider(self, spider)",
                            "is_idle(self)",
                            "_check_if_closing(self, spider, slot)",
                            "enqueue_scrape(self, response, request, spider)",
                            "_scrape_next(self, spider, slot)",
                            "_scrape(self, response, request, spider)",
                            "_scrape2(self, request_result, request, spider)",
                            "call_spider(self, result, request, spider)",
                            "handle_spider_error(self, _failure, request, response, spider)",
                            "handle_spider_output(self, result, request, response, spider)",
                            "_process_spidermw_output(self, output, request, response, spider)",
                            "_log_download_errors(self, spider_failure, download_failure, request, spider)",
                            "_itemproc_finished(self, output, item, response, spider)",
                            "finish_scraping(_)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "from scrapy.utils.log import logformatter_adapter\nfrom scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest",
                    "start_line": 13,
                    "end_line": 14
                }
            ],
            "inscope_functions": [
                "def __init__(self, max_active_size=5000000):\n    self.max_active_size = max_active_size\n    self.queue = deque()\n    self.active = set()\n    self.active_size = 0\n    self.itemproc_size = 0\n    self.closing = None",
                "def add_response_request(self, response, request):\n    deferred = defer.Deferred()\n    self.queue.append((response, request, deferred))\n    if isinstance(response, Response):\n        self.active_size += max(len(response.body), self.MIN_RESPONSE_SIZE)\n    else:\n        self.active_size += self.MIN_RESPONSE_SIZE\n    return deferred",
                "def next_response_request_deferred(self):\n    response, request, deferred = self.queue.popleft()\n    self.active.add(request)\n    return response, request, deferred",
                "def finish_response(self, response, request):\n    self.active.remove(request)\n    if isinstance(response, Response):\n        self.active_size -= max(len(response.body), self.MIN_RESPONSE_SIZE)\n    else:\n        self.active_size -= self.MIN_RESPONSE_SIZE",
                "def is_idle(self):\n    return not (self.queue or self.active)",
                "def needs_backout(self):\n    return self.active_size > self.max_active_size",
                "def __init__(self, crawler):\n    self.slot = None\n    self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n    itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n    self.itemproc = itemproc_cls.from_crawler(crawler)\n    self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n    self.crawler = crawler\n    self.signals = crawler.signals\n    self.logformatter = crawler.logformatter",
                "@defer.inlineCallbacks\ndef open_spider(self, spider):\n    \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n    self.slot = Slot()\n    yield self.itemproc.open_spider(spider)",
                "def close_spider(self, spider):\n    \"\"\"Close a spider being scraped and release its resources\"\"\"\n    slot = self.slot\n    slot.closing = defer.Deferred()\n    slot.closing.addCallback(self.itemproc.close_spider)\n    self._check_if_closing(spider, slot)\n    return slot.closing",
                "def is_idle(self):\n    \"\"\"Return True if there isn't any more spiders to process\"\"\"\n    return not self.slot",
                "def _check_if_closing(self, spider, slot):\n    if slot.closing and slot.is_idle():\n        slot.closing.callback(spider)",
                "def enqueue_scrape(self, response, request, spider):\n    slot = self.slot\n    dfd = slot.add_response_request(response, request)\n    def finish_scraping(_):\n        slot.finish_response(response, request)\n        self._check_if_closing(spider, slot)\n        self._scrape_next(spider, slot)\n        return _\n    dfd.addBoth(finish_scraping)\n    dfd.addErrback(\n        lambda f: logger.error('Scraper bug processing %(request)s',\n                               {'request': request},\n                               extra={'spider': spider, 'failure': f}))\n    self._scrape_next(spider, slot)\n    return dfd",
                "def _scrape_next(self, spider, slot):\n    while slot.queue:\n        response, request, deferred = slot.next_response_request_deferred()\n        self._scrape(response, request, spider).chainDeferred(deferred)",
                "def _scrape(self, response, request, spider):\n    \"\"\"Handle the downloaded response or failure trough the spider\n    callback/errback\"\"\"\n    assert isinstance(response, (Response, Failure))\n\n    dfd = self._scrape2(response, request, spider) # returns spiders processed output\n    dfd.addErrback(self.handle_spider_error, request, response, spider)\n    dfd.addCallback(self.handle_spider_output, request, response, spider)\n    return dfd",
                "def _scrape2(self, request_result, request, spider):\n    \"\"\"Handle the different cases of request's result been a Response or a\n    Failure\"\"\"\n    if not isinstance(request_result, Failure):\n        return self.spidermw.scrape_response(\n            self.call_spider, request_result, request, spider)\n    else:\n        # FIXME: don't ignore errors in spider middleware\n        dfd = self.call_spider(request_result, request, spider)\n        return dfd.addErrback(\n            self._log_download_errors, request_result, request, spider)",
                "def call_spider(self, result, request, spider):\n    result.request = request\n    dfd = defer_result(result)\n    dfd.addCallbacks(request.callback or spider.parse, request.errback)\n    return dfd.addCallback(iterate_spider_output)",
                "def handle_spider_error(self, _failure, request, response, spider):\n    exc = _failure.value\n    if isinstance(exc, CloseSpider):\n        self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n        return\n    referer = request.headers.get('Referer')\n    logger.error(\n        \"Spider error processing %(request)s (referer: %(referer)s)\",\n        {'request': request, 'referer': referer},\n        extra={'spider': spider, 'failure': _failure}\n    )\n    self.signals.send_catch_log(\n        signal=signals.spider_error,\n        failure=_failure, response=response,\n        spider=spider\n    )\n    self.crawler.stats.inc_value(\n        \"spider_exceptions/%s\" % _failure.value.__class__.__name__,\n        spider=spider\n    )",
                "def handle_spider_output(self, result, request, response, spider):\n    if not result:\n        return defer_succeed(None)\n    it = iter_errback(result, self.handle_spider_error, request, response, spider)\n    dfd = parallel(it, self.concurrent_items,\n        self._process_spidermw_output, request, response, spider)\n    return dfd",
                "def _process_spidermw_output(self, output, request, response, spider):\n    \"\"\"Process each Request/Item (given in the output parameter) returned\n    from the given spider\n    \"\"\"\n    if isinstance(output, Request):\n        self.crawler.engine.crawl(request=output, spider=spider)\n    elif isinstance(output, (BaseItem, dict)):\n        self.slot.itemproc_size += 1\n        dfd = self.itemproc.process_item(output, spider)\n        dfd.addBoth(self._itemproc_finished, output, response, spider)\n        return dfd\n    elif output is None:\n        pass\n    else:\n        typename = type(output).__name__\n        logger.error('Spider must return Request, BaseItem, dict or None, '\n                     'got %(typename)r in %(request)s',\n                     {'request': request, 'typename': typename},\n                     extra={'spider': spider})",
                "def _log_download_errors(self, spider_failure, download_failure, request, spider):\n    \"\"\"Log and silence errors that come from the engine (typically download\n    errors that got propagated thru here)\n    \"\"\"\n    if (isinstance(download_failure, Failure) and\n            not download_failure.check(IgnoreRequest)):\n        if download_failure.frames:\n            logger.error('Error downloading %(request)s',\n                         {'request': request},\n                         extra={'spider': spider, 'failure': download_failure})\n        else:\n            errmsg = download_failure.getErrorMessage()\n            if errmsg:\n                logger.error('Error downloading %(request)s: %(errmsg)s',\n                             {'request': request, 'errmsg': errmsg},\n                             extra={'spider': spider})\n\n    if spider_failure is not download_failure:\n        return spider_failure",
                "def _itemproc_finished(self, output, item, response, spider):\n    \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n    \"\"\"\n    self.slot.itemproc_size -= 1\n    if isinstance(output, Failure):\n        ex = output.value\n        if isinstance(ex, DropItem):\n            logkws = self.logformatter.dropped(item, ex, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            return self.signals.send_catch_log_deferred(\n                signal=signals.item_dropped, item=item, response=response,\n                spider=spider, exception=output.value)\n        else:\n            logger.error('Error processing %(item)s', {'item': item},\n                         extra={'spider': spider, 'failure': output})\n    else:\n        logkws = self.logformatter.scraped(output, response, spider)\n        logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n        return self.signals.send_catch_log_deferred(\n            signal=signals.item_scraped, item=output, response=response,\n            spider=spider)",
                "def finish_scraping(_):\n    slot.finish_response(response, request)\n    self._check_if_closing(spider, slot)\n    self._scrape_next(spider, slot)\n    return _"
            ],
            "inscope_function_signatures": [
                "__init__(self, max_active_size=5000000)",
                "add_response_request(self, response, request)",
                "next_response_request_deferred(self)",
                "finish_response(self, response, request)",
                "is_idle(self)",
                "needs_backout(self)",
                "__init__(self, crawler)",
                "open_spider(self, spider)",
                "close_spider(self, spider)",
                "is_idle(self)",
                "_check_if_closing(self, spider, slot)",
                "enqueue_scrape(self, response, request, spider)",
                "_scrape_next(self, spider, slot)",
                "_scrape(self, response, request, spider)",
                "_scrape2(self, request_result, request, spider)",
                "call_spider(self, result, request, spider)",
                "handle_spider_error(self, _failure, request, response, spider)",
                "handle_spider_output(self, result, request, response, spider)",
                "_process_spidermw_output(self, output, request, response, spider)",
                "_log_download_errors(self, spider_failure, download_failure, request, spider)",
                "_itemproc_finished(self, output, item, response, spider)",
                "finish_scraping(_)"
            ],
            "variables_in_file": {
                "logger": [
                    224,
                    229,
                    233,
                    203,
                    108,
                    209,
                    20,
                    153,
                    191
                ],
                "logging.getLogger": [
                    20
                ],
                "logging": [
                    20
                ],
                "__name__": [
                    20,
                    190
                ],
                "object": [
                    64,
                    23
                ],
                "MIN_RESPONSE_SIZE": [
                    26
                ],
                "self.max_active_size": [
                    61,
                    29
                ],
                "self": [
                    133,
                    134,
                    137,
                    139,
                    150,
                    29,
                    30,
                    31,
                    32,
                    33,
                    34,
                    158,
                    163,
                    38,
                    40,
                    42,
                    171,
                    172,
                    173,
                    46,
                    47,
                    51,
                    53,
                    181,
                    55,
                    183,
                    184,
                    58,
                    185,
                    61,
                    67,
                    68,
                    70,
                    71,
                    72,
                    73,
                    74,
                    79,
                    80,
                    84,
                    86,
                    87,
                    219,
                    92,
                    223,
                    225,
                    99,
                    103,
                    104,
                    232,
                    234,
                    111,
                    117,
                    124,
                    125,
                    126
                ],
                "max_active_size": [
                    29
                ],
                "self.queue": [
                    38,
                    46,
                    58,
                    30
                ],
                "deque": [
                    30
                ],
                "self.active": [
                    58,
                    51,
                    47,
                    31
                ],
                "set": [
                    31
                ],
                "self.active_size": [
                    32,
                    40,
                    42,
                    53,
                    55,
                    61
                ],
                "self.itemproc_size": [
                    33
                ],
                "self.closing": [
                    34
                ],
                "deferred": [
                    37,
                    38,
                    43,
                    46,
                    48,
                    116,
                    117
                ],
                "defer.Deferred": [
                    85,
                    37
                ],
                "defer": [
                    85,
                    76,
                    37
                ],
                "self.queue.append": [
                    38
                ],
                "response": [
                    160,
                    38,
                    39,
                    40,
                    171,
                    173,
                    46,
                    48,
                    52,
                    53,
                    185,
                    223,
                    226,
                    100,
                    102,
                    232,
                    235,
                    116,
                    117,
                    122,
                    124,
                    125,
                    126
                ],
                "request": [
                    134,
                    137,
                    139,
                    142,
                    144,
                    152,
                    155,
                    38,
                    171,
                    173,
                    46,
                    47,
                    48,
                    51,
                    193,
                    204,
                    210,
                    100,
                    102,
                    109,
                    116,
                    117,
                    124,
                    125,
                    126
                ],
                "isinstance": [
                    132,
                    39,
                    200,
                    52,
                    149,
                    180,
                    182,
                    122,
                    220,
                    222
                ],
                "Response": [
                    122,
                    52,
                    39
                ],
                "max": [
                    40,
                    53
                ],
                "len": [
                    40,
                    53
                ],
                "response.body": [
                    40,
                    53
                ],
                "self.MIN_RESPONSE_SIZE": [
                    40,
                    42,
                    53,
                    55
                ],
                "self.queue.popleft": [
                    46
                ],
                "self.active.add": [
                    47
                ],
                "self.active.remove": [
                    51
                ],
                "self.slot": [
                    99,
                    67,
                    79,
                    84,
                    183,
                    219,
                    92
                ],
                "self.spidermw": [
                    68,
                    133
                ],
                "SpiderMiddlewareManager.from_crawler": [
                    68
                ],
                "SpiderMiddlewareManager": [
                    68
                ],
                "crawler": [
                    68,
                    69,
                    70,
                    71,
                    72,
                    73,
                    74
                ],
                "itemproc_cls": [
                    69,
                    70
                ],
                "load_object": [
                    69
                ],
                "crawler.settings": [
                    69,
                    71
                ],
                "self.itemproc": [
                    80,
                    86,
                    70,
                    184
                ],
                "itemproc_cls.from_crawler": [
                    70
                ],
                "self.concurrent_items": [
                    172,
                    71
                ],
                "crawler.settings.getint": [
                    71
                ],
                "self.crawler": [
                    72,
                    163,
                    181,
                    150
                ],
                "self.signals": [
                    73,
                    234,
                    225,
                    158
                ],
                "crawler.signals": [
                    73
                ],
                "self.logformatter": [
                    232,
                    74,
                    223
                ],
                "crawler.logformatter": [
                    74
                ],
                "Slot": [
                    79
                ],
                "self.itemproc.open_spider": [
                    80
                ],
                "spider": [
                    134,
                    137,
                    139,
                    144,
                    150,
                    156,
                    161,
                    165,
                    171,
                    173,
                    181,
                    184,
                    185,
                    194,
                    205,
                    80,
                    211,
                    87,
                    223,
                    96,
                    224,
                    227,
                    230,
                    103,
                    104,
                    232,
                    233,
                    236,
                    110,
                    111,
                    117,
                    124,
                    125,
                    126
                ],
                "defer.inlineCallbacks": [
                    76
                ],
                "slot": [
                    96,
                    99,
                    100,
                    102,
                    103,
                    104,
                    111,
                    115,
                    84,
                    85,
                    86,
                    87,
                    88,
                    116,
                    95
                ],
                "slot.closing": [
                    96,
                    85,
                    86,
                    88,
                    95
                ],
                "slot.closing.addCallback": [
                    86
                ],
                "self.itemproc.close_spider": [
                    86
                ],
                "self._check_if_closing": [
                    103,
                    87
                ],
                "slot.is_idle": [
                    95
                ],
                "slot.closing.callback": [
                    96
                ],
                "dfd": [
                    100,
                    137,
                    106,
                    107,
                    138,
                    172,
                    174,
                    143,
                    112,
                    144,
                    145,
                    184,
                    185,
                    186,
                    124,
                    125,
                    126,
                    127
                ],
                "slot.add_response_request": [
                    100
                ],
                "slot.finish_response": [
                    102
                ],
                "self._scrape_next": [
                    104,
                    111
                ],
                "_": [
                    105
                ],
                "dfd.addBoth": [
                    185,
                    106
                ],
                "finish_scraping": [
                    106
                ],
                "dfd.addErrback": [
                    138,
                    107,
                    125
                ],
                "logger.error": [
                    229,
                    203,
                    108,
                    209,
                    153,
                    191
                ],
                "f": [
                    110
                ],
                "slot.queue": [
                    115
                ],
                "slot.next_response_request_deferred": [
                    116
                ],
                "chainDeferred": [
                    117
                ],
                "self._scrape": [
                    117
                ],
                "Failure": [
                    200,
                    122,
                    220,
                    132
                ],
                "self._scrape2": [
                    124
                ],
                "self.handle_spider_error": [
                    171,
                    125
                ],
                "dfd.addCallback": [
                    145,
                    126
                ],
                "self.handle_spider_output": [
                    126
                ],
                "request_result": [
                    137,
                    139,
                    132,
                    134
                ],
                "self.spidermw.scrape_response": [
                    133
                ],
                "self.call_spider": [
                    137,
                    134
                ],
                "self._log_download_errors": [
                    139
                ],
                "result.request": [
                    142
                ],
                "result": [
                    169,
                    171,
                    142,
                    143
                ],
                "defer_result": [
                    143
                ],
                "dfd.addCallbacks": [
                    144
                ],
                "request.callback": [
                    144
                ],
                "spider.parse": [
                    144
                ],
                "request.errback": [
                    144
                ],
                "iterate_spider_output": [
                    145
                ],
                "exc": [
                    148,
                    149,
                    150
                ],
                "_failure.value": [
                    164,
                    148
                ],
                "_failure": [
                    160,
                    156,
                    164,
                    148
                ],
                "CloseSpider": [
                    149
                ],
                "self.crawler.engine.close_spider": [
                    150
                ],
                "self.crawler.engine": [
                    181,
                    150
                ],
                "exc.reason": [
                    150
                ],
                "referer": [
                    152,
                    155
                ],
                "request.headers.get": [
                    152
                ],
                "request.headers": [
                    152
                ],
                "self.signals.send_catch_log": [
                    158
                ],
                "signals.spider_error": [
                    159
                ],
                "signals": [
                    226,
                    235,
                    159
                ],
                "self.crawler.stats.inc_value": [
                    163
                ],
                "self.crawler.stats": [
                    163
                ],
                "_failure.value.__class__.__name__": [
                    164
                ],
                "_failure.value.__class__": [
                    164
                ],
                "defer_succeed": [
                    170
                ],
                "it": [
                    171,
                    172
                ],
                "iter_errback": [
                    171
                ],
                "parallel": [
                    172
                ],
                "self._process_spidermw_output": [
                    173
                ],
                "output": [
                    227,
                    230,
                    232,
                    235,
                    180,
                    181,
                    182,
                    184,
                    185,
                    187,
                    220,
                    221,
                    190
                ],
                "Request": [
                    180
                ],
                "self.crawler.engine.crawl": [
                    181
                ],
                "BaseItem": [
                    182
                ],
                "dict": [
                    182
                ],
                "self.slot.itemproc_size": [
                    219,
                    183
                ],
                "self.itemproc.process_item": [
                    184
                ],
                "self._itemproc_finished": [
                    185
                ],
                "typename": [
                    193,
                    190
                ],
                "type": [
                    190
                ],
                "download_failure": [
                    200,
                    201,
                    202,
                    205,
                    207,
                    213
                ],
                "download_failure.check": [
                    201
                ],
                "IgnoreRequest": [
                    201
                ],
                "download_failure.frames": [
                    202
                ],
                "errmsg": [
                    208,
                    210,
                    207
                ],
                "download_failure.getErrorMessage": [
                    207
                ],
                "spider_failure": [
                    213,
                    214
                ],
                "ex": [
                    221,
                    222,
                    223
                ],
                "output.value": [
                    227,
                    221
                ],
                "DropItem": [
                    222
                ],
                "logkws": [
                    224,
                    232,
                    233,
                    223
                ],
                "self.logformatter.dropped": [
                    223
                ],
                "item": [
                    226,
                    229,
                    223
                ],
                "logger.log": [
                    224,
                    233
                ],
                "logformatter_adapter": [
                    224,
                    233
                ],
                "self.signals.send_catch_log_deferred": [
                    225,
                    234
                ],
                "signals.item_dropped": [
                    226
                ],
                "self.logformatter.scraped": [
                    232
                ],
                "signals.item_scraped": [
                    235
                ]
            },
            "filtered_variables_in_file": {
                "logger": [
                    224,
                    229,
                    233,
                    203,
                    108,
                    209,
                    20,
                    153,
                    191
                ],
                "logging.getLogger": [
                    20
                ],
                "logging": [
                    20
                ],
                "MIN_RESPONSE_SIZE": [
                    26
                ],
                "self.max_active_size": [
                    61,
                    29
                ],
                "self": [
                    133,
                    134,
                    137,
                    139,
                    150,
                    29,
                    30,
                    31,
                    32,
                    33,
                    34,
                    158,
                    163,
                    38,
                    40,
                    42,
                    171,
                    172,
                    173,
                    46,
                    47,
                    51,
                    53,
                    181,
                    55,
                    183,
                    184,
                    58,
                    185,
                    61,
                    67,
                    68,
                    70,
                    71,
                    72,
                    73,
                    74,
                    79,
                    80,
                    84,
                    86,
                    87,
                    219,
                    92,
                    223,
                    225,
                    99,
                    103,
                    104,
                    232,
                    234,
                    111,
                    117,
                    124,
                    125,
                    126
                ],
                "max_active_size": [
                    29
                ],
                "self.queue": [
                    38,
                    46,
                    58,
                    30
                ],
                "deque": [
                    30
                ],
                "self.active": [
                    58,
                    51,
                    47,
                    31
                ],
                "self.active_size": [
                    32,
                    40,
                    42,
                    53,
                    55,
                    61
                ],
                "self.itemproc_size": [
                    33
                ],
                "self.closing": [
                    34
                ],
                "deferred": [
                    37,
                    38,
                    43,
                    46,
                    48,
                    116,
                    117
                ],
                "defer.Deferred": [
                    85,
                    37
                ],
                "defer": [
                    85,
                    76,
                    37
                ],
                "self.queue.append": [
                    38
                ],
                "response": [
                    160,
                    38,
                    39,
                    40,
                    171,
                    173,
                    46,
                    48,
                    52,
                    53,
                    185,
                    223,
                    226,
                    100,
                    102,
                    232,
                    235,
                    116,
                    117,
                    122,
                    124,
                    125,
                    126
                ],
                "request": [
                    134,
                    137,
                    139,
                    142,
                    144,
                    152,
                    155,
                    38,
                    171,
                    173,
                    46,
                    47,
                    48,
                    51,
                    193,
                    204,
                    210,
                    100,
                    102,
                    109,
                    116,
                    117,
                    124,
                    125,
                    126
                ],
                "Response": [
                    122,
                    52,
                    39
                ],
                "response.body": [
                    40,
                    53
                ],
                "self.MIN_RESPONSE_SIZE": [
                    40,
                    42,
                    53,
                    55
                ],
                "self.queue.popleft": [
                    46
                ],
                "self.active.add": [
                    47
                ],
                "self.active.remove": [
                    51
                ],
                "self.slot": [
                    99,
                    67,
                    79,
                    84,
                    183,
                    219,
                    92
                ],
                "self.spidermw": [
                    68,
                    133
                ],
                "SpiderMiddlewareManager.from_crawler": [
                    68
                ],
                "SpiderMiddlewareManager": [
                    68
                ],
                "crawler": [
                    68,
                    69,
                    70,
                    71,
                    72,
                    73,
                    74
                ],
                "itemproc_cls": [
                    69,
                    70
                ],
                "load_object": [
                    69
                ],
                "crawler.settings": [
                    69,
                    71
                ],
                "self.itemproc": [
                    80,
                    86,
                    70,
                    184
                ],
                "itemproc_cls.from_crawler": [
                    70
                ],
                "self.concurrent_items": [
                    172,
                    71
                ],
                "crawler.settings.getint": [
                    71
                ],
                "self.crawler": [
                    72,
                    163,
                    181,
                    150
                ],
                "self.signals": [
                    73,
                    234,
                    225,
                    158
                ],
                "crawler.signals": [
                    73
                ],
                "self.logformatter": [
                    232,
                    74,
                    223
                ],
                "crawler.logformatter": [
                    74
                ],
                "Slot": [
                    79
                ],
                "self.itemproc.open_spider": [
                    80
                ],
                "spider": [
                    134,
                    137,
                    139,
                    144,
                    150,
                    156,
                    161,
                    165,
                    171,
                    173,
                    181,
                    184,
                    185,
                    194,
                    205,
                    80,
                    211,
                    87,
                    223,
                    96,
                    224,
                    227,
                    230,
                    103,
                    104,
                    232,
                    233,
                    236,
                    110,
                    111,
                    117,
                    124,
                    125,
                    126
                ],
                "defer.inlineCallbacks": [
                    76
                ],
                "slot": [
                    96,
                    99,
                    100,
                    102,
                    103,
                    104,
                    111,
                    115,
                    84,
                    85,
                    86,
                    87,
                    88,
                    116,
                    95
                ],
                "slot.closing": [
                    96,
                    85,
                    86,
                    88,
                    95
                ],
                "slot.closing.addCallback": [
                    86
                ],
                "self.itemproc.close_spider": [
                    86
                ],
                "self._check_if_closing": [
                    103,
                    87
                ],
                "slot.is_idle": [
                    95
                ],
                "slot.closing.callback": [
                    96
                ],
                "dfd": [
                    100,
                    137,
                    106,
                    107,
                    138,
                    172,
                    174,
                    143,
                    112,
                    144,
                    145,
                    184,
                    185,
                    186,
                    124,
                    125,
                    126,
                    127
                ],
                "slot.add_response_request": [
                    100
                ],
                "slot.finish_response": [
                    102
                ],
                "self._scrape_next": [
                    104,
                    111
                ],
                "_": [
                    105
                ],
                "dfd.addBoth": [
                    185,
                    106
                ],
                "finish_scraping": [
                    106
                ],
                "dfd.addErrback": [
                    138,
                    107,
                    125
                ],
                "logger.error": [
                    229,
                    203,
                    108,
                    209,
                    153,
                    191
                ],
                "f": [
                    110
                ],
                "slot.queue": [
                    115
                ],
                "slot.next_response_request_deferred": [
                    116
                ],
                "chainDeferred": [
                    117
                ],
                "self._scrape": [
                    117
                ],
                "Failure": [
                    200,
                    122,
                    220,
                    132
                ],
                "self._scrape2": [
                    124
                ],
                "self.handle_spider_error": [
                    171,
                    125
                ],
                "dfd.addCallback": [
                    145,
                    126
                ],
                "self.handle_spider_output": [
                    126
                ],
                "request_result": [
                    137,
                    139,
                    132,
                    134
                ],
                "self.spidermw.scrape_response": [
                    133
                ],
                "self.call_spider": [
                    137,
                    134
                ],
                "self._log_download_errors": [
                    139
                ],
                "result.request": [
                    142
                ],
                "result": [
                    169,
                    171,
                    142,
                    143
                ],
                "defer_result": [
                    143
                ],
                "dfd.addCallbacks": [
                    144
                ],
                "request.callback": [
                    144
                ],
                "spider.parse": [
                    144
                ],
                "request.errback": [
                    144
                ],
                "iterate_spider_output": [
                    145
                ],
                "exc": [
                    148,
                    149,
                    150
                ],
                "_failure.value": [
                    164,
                    148
                ],
                "_failure": [
                    160,
                    156,
                    164,
                    148
                ],
                "CloseSpider": [
                    149
                ],
                "self.crawler.engine.close_spider": [
                    150
                ],
                "self.crawler.engine": [
                    181,
                    150
                ],
                "exc.reason": [
                    150
                ],
                "referer": [
                    152,
                    155
                ],
                "request.headers.get": [
                    152
                ],
                "request.headers": [
                    152
                ],
                "self.signals.send_catch_log": [
                    158
                ],
                "signals.spider_error": [
                    159
                ],
                "signals": [
                    226,
                    235,
                    159
                ],
                "self.crawler.stats.inc_value": [
                    163
                ],
                "self.crawler.stats": [
                    163
                ],
                "_failure.value.__class__.__name__": [
                    164
                ],
                "_failure.value.__class__": [
                    164
                ],
                "defer_succeed": [
                    170
                ],
                "it": [
                    171,
                    172
                ],
                "iter_errback": [
                    171
                ],
                "parallel": [
                    172
                ],
                "self._process_spidermw_output": [
                    173
                ],
                "output": [
                    227,
                    230,
                    232,
                    235,
                    180,
                    181,
                    182,
                    184,
                    185,
                    187,
                    220,
                    221,
                    190
                ],
                "Request": [
                    180
                ],
                "self.crawler.engine.crawl": [
                    181
                ],
                "BaseItem": [
                    182
                ],
                "self.slot.itemproc_size": [
                    219,
                    183
                ],
                "self.itemproc.process_item": [
                    184
                ],
                "self._itemproc_finished": [
                    185
                ],
                "typename": [
                    193,
                    190
                ],
                "download_failure": [
                    200,
                    201,
                    202,
                    205,
                    207,
                    213
                ],
                "download_failure.check": [
                    201
                ],
                "IgnoreRequest": [
                    201
                ],
                "download_failure.frames": [
                    202
                ],
                "errmsg": [
                    208,
                    210,
                    207
                ],
                "download_failure.getErrorMessage": [
                    207
                ],
                "spider_failure": [
                    213,
                    214
                ],
                "ex": [
                    221,
                    222,
                    223
                ],
                "output.value": [
                    227,
                    221
                ],
                "DropItem": [
                    222
                ],
                "logkws": [
                    224,
                    232,
                    233,
                    223
                ],
                "self.logformatter.dropped": [
                    223
                ],
                "item": [
                    226,
                    229,
                    223
                ],
                "logger.log": [
                    224,
                    233
                ],
                "logformatter_adapter": [
                    224,
                    233
                ],
                "self.signals.send_catch_log_deferred": [
                    225,
                    234
                ],
                "signals.item_dropped": [
                    226
                ],
                "self.logformatter.scraped": [
                    232
                ],
                "signals.item_scraped": [
                    235
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs/repos/scrapy_33/scrapy/downloadermiddlewares/robotstxt.py": {
            "buggy_functions": [
                {
                    "function_name": "_logerror",
                    "function_code": "def _logerror(self, failure, request, spider):\n    if failure.type is not IgnoreRequest:\n        logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                     {'request': request, 'f_exception': failure.value},\n                     extra={'spider': spider, 'failure': failure})\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 58,
                    "end_line": 62,
                    "variables": {
                        "failure.type": [
                            59
                        ],
                        "failure": [
                            59,
                            61,
                            62
                        ],
                        "IgnoreRequest": [
                            59
                        ],
                        "logger.error": [
                            60
                        ],
                        "logger": [
                            60
                        ],
                        "request": [
                            61
                        ],
                        "failure.value": [
                            61
                        ],
                        "spider": [
                            62
                        ]
                    },
                    "filtered_variables": {
                        "failure.type": [
                            59
                        ],
                        "failure": [
                            59,
                            61,
                            62
                        ],
                        "IgnoreRequest": [
                            59
                        ],
                        "logger.error": [
                            60
                        ],
                        "logger": [
                            60
                        ],
                        "request": [
                            61
                        ],
                        "failure.value": [
                            61
                        ],
                        "spider": [
                            62
                        ]
                    },
                    "diff_line_number": 62,
                    "class_data": {
                        "signature": "class RobotsTxtMiddleware(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, crawler):\n    if not crawler.settings.getbool('ROBOTSTXT_OBEY'):\n        raise NotConfigured\n    self.crawler = crawler\n    self._useragent = crawler.settings.get('USER_AGENT')\n    self._parsers = {}",
                            "@classmethod\ndef from_crawler(cls, crawler):\n    return cls(crawler)",
                            "def process_request(self, request, spider):\n    if request.meta.get('dont_obey_robotstxt'):\n        return\n    rp = self.robot_parser(request, spider)\n    if rp and (not rp.can_fetch(self._useragent, request.url)):\n        logger.debug('Forbidden by robots.txt: %(request)s', {'request': request}, extra={'spider': spider})\n        raise IgnoreRequest",
                            "def robot_parser(self, request, spider):\n    url = urlparse_cached(request)\n    netloc = url.netloc\n    if netloc not in self._parsers:\n        self._parsers[netloc] = None\n        robotsurl = '%s://%s/robots.txt' % (url.scheme, url.netloc)\n        robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY, meta={'dont_obey_robotstxt': True})\n        dfd = self.crawler.engine.download(robotsreq, spider)\n        dfd.addCallback(self._parse_robots)\n        dfd.addErrback(self._logerror, robotsreq, spider)\n    return self._parsers[netloc]",
                            "def _logerror(self, failure, request, spider):\n    if failure.type is not IgnoreRequest:\n        logger.error('Error downloading %(request)s: %(f_exception)s', {'request': request, 'f_exception': failure.value}, extra={'spider': spider, 'failure': failure})",
                            "def _parse_robots(self, response):\n    rp = robotparser.RobotFileParser(response.url)\n    rp.parse(response.body.splitlines())\n    self._parsers[urlparse_cached(response).netloc] = rp"
                        ],
                        "constructor_variables": [
                            "crawler",
                            "_parsers",
                            "_useragent"
                        ],
                        "class_level_variables": [
                            "DOWNLOAD_PRIORITY"
                        ],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, crawler)",
                            "from_crawler(cls, crawler)",
                            "process_request(self, request, spider)",
                            "robot_parser(self, request, spider)",
                            "_logerror(self, failure, request, spider)",
                            "_parse_robots(self, response)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "from scrapy.utils.httpobj import urlparse_cached",
                    "start_line": 13,
                    "end_line": 14
                }
            ],
            "inscope_functions": [
                "def __init__(self, crawler):\n    if not crawler.settings.getbool('ROBOTSTXT_OBEY'):\n        raise NotConfigured\n\n    self.crawler = crawler\n    self._useragent = crawler.settings.get('USER_AGENT')\n    self._parsers = {}",
                "@classmethod\ndef from_crawler(cls, crawler):\n    return cls(crawler)",
                "def process_request(self, request, spider):\n    if request.meta.get('dont_obey_robotstxt'):\n        return\n    rp = self.robot_parser(request, spider)\n    if rp and not rp.can_fetch(self._useragent, request.url):\n        logger.debug(\"Forbidden by robots.txt: %(request)s\",\n                     {'request': request}, extra={'spider': spider})\n        raise IgnoreRequest",
                "def robot_parser(self, request, spider):\n    url = urlparse_cached(request)\n    netloc = url.netloc\n    if netloc not in self._parsers:\n        self._parsers[netloc] = None\n        robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n        robotsreq = Request(\n            robotsurl,\n            priority=self.DOWNLOAD_PRIORITY,\n            meta={'dont_obey_robotstxt': True}\n        )\n        dfd = self.crawler.engine.download(robotsreq, spider)\n        dfd.addCallback(self._parse_robots)\n        dfd.addErrback(self._logerror, robotsreq, spider)\n    return self._parsers[netloc]",
                "def _logerror(self, failure, request, spider):\n    if failure.type is not IgnoreRequest:\n        logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                     {'request': request, 'f_exception': failure.value},\n                     extra={'spider': spider, 'failure': failure})",
                "def _parse_robots(self, response):\n    rp = robotparser.RobotFileParser(response.url)\n    rp.parse(response.body.splitlines())\n    self._parsers[urlparse_cached(response).netloc] = rp"
            ],
            "inscope_function_signatures": [
                "__init__(self, crawler)",
                "from_crawler(cls, crawler)",
                "process_request(self, request, spider)",
                "robot_parser(self, request, spider)",
                "_logerror(self, failure, request, spider)",
                "_parse_robots(self, response)"
            ],
            "variables_in_file": {
                "logger": [
                    60,
                    38,
                    15
                ],
                "logging.getLogger": [
                    15
                ],
                "logging": [
                    15
                ],
                "__name__": [
                    15
                ],
                "object": [
                    18
                ],
                "DOWNLOAD_PRIORITY": [
                    19
                ],
                "crawler.settings.getbool": [
                    22
                ],
                "crawler.settings": [
                    26,
                    22
                ],
                "crawler": [
                    25,
                    26,
                    22,
                    31
                ],
                "NotConfigured": [
                    23
                ],
                "self.crawler": [
                    25,
                    53
                ],
                "self": [
                    67,
                    36,
                    37,
                    45,
                    46,
                    50,
                    53,
                    54,
                    55,
                    56,
                    25,
                    26,
                    27
                ],
                "self._useragent": [
                    26,
                    37
                ],
                "crawler.settings.get": [
                    26
                ],
                "self._parsers": [
                    67,
                    45,
                    46,
                    56,
                    27
                ],
                "cls": [
                    31
                ],
                "classmethod": [
                    29
                ],
                "request.meta.get": [
                    34
                ],
                "request.meta": [
                    34
                ],
                "request": [
                    34,
                    36,
                    37,
                    39,
                    43,
                    61
                ],
                "rp": [
                    65,
                    66,
                    67,
                    36,
                    37
                ],
                "self.robot_parser": [
                    36
                ],
                "spider": [
                    36,
                    39,
                    53,
                    55,
                    62
                ],
                "rp.can_fetch": [
                    37
                ],
                "request.url": [
                    37
                ],
                "logger.debug": [
                    38
                ],
                "IgnoreRequest": [
                    40,
                    59
                ],
                "url": [
                    43,
                    44,
                    47
                ],
                "urlparse_cached": [
                    67,
                    43
                ],
                "netloc": [
                    67,
                    44,
                    45,
                    46,
                    56
                ],
                "url.netloc": [
                    44,
                    47
                ],
                "robotsurl": [
                    49,
                    47
                ],
                "url.scheme": [
                    47
                ],
                "robotsreq": [
                    48,
                    53,
                    55
                ],
                "Request": [
                    48
                ],
                "self.DOWNLOAD_PRIORITY": [
                    50
                ],
                "dfd": [
                    53,
                    54,
                    55
                ],
                "self.crawler.engine.download": [
                    53
                ],
                "self.crawler.engine": [
                    53
                ],
                "dfd.addCallback": [
                    54
                ],
                "self._parse_robots": [
                    54
                ],
                "dfd.addErrback": [
                    55
                ],
                "self._logerror": [
                    55
                ],
                "failure.type": [
                    59
                ],
                "failure": [
                    59,
                    61,
                    62
                ],
                "logger.error": [
                    60
                ],
                "failure.value": [
                    61
                ],
                "robotparser.RobotFileParser": [
                    65
                ],
                "robotparser": [
                    65
                ],
                "response.url": [
                    65
                ],
                "response": [
                    65,
                    66,
                    67
                ],
                "rp.parse": [
                    66
                ],
                "response.body.splitlines": [
                    66
                ],
                "response.body": [
                    66
                ]
            },
            "filtered_variables_in_file": {
                "logger": [
                    60,
                    38,
                    15
                ],
                "logging.getLogger": [
                    15
                ],
                "logging": [
                    15
                ],
                "DOWNLOAD_PRIORITY": [
                    19
                ],
                "crawler.settings.getbool": [
                    22
                ],
                "crawler.settings": [
                    26,
                    22
                ],
                "crawler": [
                    25,
                    26,
                    22,
                    31
                ],
                "NotConfigured": [
                    23
                ],
                "self.crawler": [
                    25,
                    53
                ],
                "self": [
                    67,
                    36,
                    37,
                    45,
                    46,
                    50,
                    53,
                    54,
                    55,
                    56,
                    25,
                    26,
                    27
                ],
                "self._useragent": [
                    26,
                    37
                ],
                "crawler.settings.get": [
                    26
                ],
                "self._parsers": [
                    67,
                    45,
                    46,
                    56,
                    27
                ],
                "cls": [
                    31
                ],
                "request.meta.get": [
                    34
                ],
                "request.meta": [
                    34
                ],
                "request": [
                    34,
                    36,
                    37,
                    39,
                    43,
                    61
                ],
                "rp": [
                    65,
                    66,
                    67,
                    36,
                    37
                ],
                "self.robot_parser": [
                    36
                ],
                "spider": [
                    36,
                    39,
                    53,
                    55,
                    62
                ],
                "rp.can_fetch": [
                    37
                ],
                "request.url": [
                    37
                ],
                "logger.debug": [
                    38
                ],
                "IgnoreRequest": [
                    40,
                    59
                ],
                "url": [
                    43,
                    44,
                    47
                ],
                "urlparse_cached": [
                    67,
                    43
                ],
                "netloc": [
                    67,
                    44,
                    45,
                    46,
                    56
                ],
                "url.netloc": [
                    44,
                    47
                ],
                "robotsurl": [
                    49,
                    47
                ],
                "url.scheme": [
                    47
                ],
                "robotsreq": [
                    48,
                    53,
                    55
                ],
                "Request": [
                    48
                ],
                "self.DOWNLOAD_PRIORITY": [
                    50
                ],
                "dfd": [
                    53,
                    54,
                    55
                ],
                "self.crawler.engine.download": [
                    53
                ],
                "self.crawler.engine": [
                    53
                ],
                "dfd.addCallback": [
                    54
                ],
                "self._parse_robots": [
                    54
                ],
                "dfd.addErrback": [
                    55
                ],
                "self._logerror": [
                    55
                ],
                "failure.type": [
                    59
                ],
                "failure": [
                    59,
                    61,
                    62
                ],
                "logger.error": [
                    60
                ],
                "failure.value": [
                    61
                ],
                "robotparser.RobotFileParser": [
                    65
                ],
                "robotparser": [
                    65
                ],
                "response.url": [
                    65
                ],
                "response": [
                    65,
                    66,
                    67
                ],
                "rp.parse": [
                    66
                ],
                "response.body.splitlines": [
                    66
                ],
                "response.body": [
                    66
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs/repos/scrapy_33/scrapy/extensions/feedexport.py": {
            "buggy_functions": [
                {
                    "function_name": "close_spider",
                    "function_code": "def close_spider(self, spider):\n    slot = self.slot\n    if not slot.itemcount and not self.store_empty:\n        return\n    slot.exporter.finish_exporting()\n    logfmt = \"%%s %(format)s feed (%(itemcount)d items) in: %(uri)s\"\n    log_args = {'format': self.format,\n                'itemcount': slot.itemcount,\n                'uri': slot.uri}\n    d = defer.maybeDeferred(slot.storage.store, slot.file)\n    d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n                                        extra={'spider': spider}))\n    d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                        extra={'spider': spider, 'failure': f}))\n    return d\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 174,
                    "end_line": 188,
                    "variables": {
                        "slot": [
                            175,
                            176,
                            178,
                            181,
                            182,
                            183
                        ],
                        "self.slot": [
                            175
                        ],
                        "self": [
                            176,
                            180,
                            175
                        ],
                        "slot.itemcount": [
                            176,
                            181
                        ],
                        "self.store_empty": [
                            176
                        ],
                        "slot.exporter.finish_exporting": [
                            178
                        ],
                        "slot.exporter": [
                            178
                        ],
                        "logfmt": [
                            184,
                            186,
                            179
                        ],
                        "log_args": [
                            184,
                            186,
                            180
                        ],
                        "self.format": [
                            180
                        ],
                        "slot.uri": [
                            182
                        ],
                        "d": [
                            184,
                            186,
                            188,
                            183
                        ],
                        "defer.maybeDeferred": [
                            183
                        ],
                        "defer": [
                            183
                        ],
                        "slot.storage.store": [
                            183
                        ],
                        "slot.storage": [
                            183
                        ],
                        "slot.file": [
                            183
                        ],
                        "d.addCallback": [
                            184
                        ],
                        "logger.info": [
                            184
                        ],
                        "logger": [
                            184,
                            186
                        ],
                        "spider": [
                            185,
                            187
                        ],
                        "d.addErrback": [
                            186
                        ],
                        "logger.error": [
                            186
                        ],
                        "f": [
                            187
                        ]
                    },
                    "filtered_variables": {
                        "slot": [
                            175,
                            176,
                            178,
                            181,
                            182,
                            183
                        ],
                        "self.slot": [
                            175
                        ],
                        "self": [
                            176,
                            180,
                            175
                        ],
                        "slot.itemcount": [
                            176,
                            181
                        ],
                        "self.store_empty": [
                            176
                        ],
                        "slot.exporter.finish_exporting": [
                            178
                        ],
                        "slot.exporter": [
                            178
                        ],
                        "logfmt": [
                            184,
                            186,
                            179
                        ],
                        "log_args": [
                            184,
                            186,
                            180
                        ],
                        "self.format": [
                            180
                        ],
                        "slot.uri": [
                            182
                        ],
                        "d": [
                            184,
                            186,
                            188,
                            183
                        ],
                        "defer.maybeDeferred": [
                            183
                        ],
                        "defer": [
                            183
                        ],
                        "slot.storage.store": [
                            183
                        ],
                        "slot.storage": [
                            183
                        ],
                        "slot.file": [
                            183
                        ],
                        "d.addCallback": [
                            184
                        ],
                        "logger.info": [
                            184
                        ],
                        "logger": [
                            184,
                            186
                        ],
                        "spider": [
                            185,
                            187
                        ],
                        "d.addErrback": [
                            186
                        ],
                        "logger.error": [
                            186
                        ],
                        "f": [
                            187
                        ]
                    },
                    "diff_line_number": 187,
                    "class_data": {
                        "signature": "class FeedExporter(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, settings):\n    self.settings = settings\n    self.urifmt = settings['FEED_URI']\n    if not self.urifmt:\n        raise NotConfigured\n    self.format = settings['FEED_FORMAT'].lower()\n    self.storages = self._load_components('FEED_STORAGES')\n    self.exporters = self._load_components('FEED_EXPORTERS')\n    if not self._storage_supported(self.urifmt):\n        raise NotConfigured\n    if not self._exporter_supported(self.format):\n        raise NotConfigured\n    self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n    self.export_fields = settings.getlist('FEED_EXPORT_FIELDS')\n    uripar = settings['FEED_URI_PARAMS']\n    self._uripar = load_object(uripar) if uripar else lambda x, y: None",
                            "@classmethod\ndef from_crawler(cls, crawler):\n    o = cls(crawler.settings)\n    crawler.signals.connect(o.open_spider, signals.spider_opened)\n    crawler.signals.connect(o.close_spider, signals.spider_closed)\n    crawler.signals.connect(o.item_scraped, signals.item_scraped)\n    return o",
                            "def open_spider(self, spider):\n    uri = self.urifmt % self._get_uri_params(spider)\n    storage = self._get_storage(uri)\n    file = storage.open(spider)\n    exporter = self._get_exporter(file, fields_to_export=self.export_fields)\n    exporter.start_exporting()\n    self.slot = SpiderSlot(file, exporter, storage, uri)",
                            "def close_spider(self, spider):\n    slot = self.slot\n    if not slot.itemcount and (not self.store_empty):\n        return\n    slot.exporter.finish_exporting()\n    logfmt = '%%s %(format)s feed (%(itemcount)d items) in: %(uri)s'\n    log_args = {'format': self.format, 'itemcount': slot.itemcount, 'uri': slot.uri}\n    d = defer.maybeDeferred(slot.storage.store, slot.file)\n    d.addCallback(lambda _: logger.info(logfmt % 'Stored', log_args, extra={'spider': spider}))\n    d.addErrback(lambda f: logger.error(logfmt % 'Error storing', log_args, extra={'spider': spider, 'failure': f}))\n    return d",
                            "def item_scraped(self, item, spider):\n    slot = self.slot\n    slot.exporter.export_item(item)\n    slot.itemcount += 1\n    return item",
                            "def _load_components(self, setting_prefix):\n    conf = dict(self.settings['%s_BASE' % setting_prefix])\n    conf.update(self.settings[setting_prefix])\n    d = {}\n    for k, v in conf.items():\n        try:\n            d[k] = load_object(v)\n        except NotConfigured:\n            pass\n    return d",
                            "def _exporter_supported(self, format):\n    if format in self.exporters:\n        return True\n    logger.error('Unknown feed format: %(format)s', {'format': format})",
                            "def _storage_supported(self, uri):\n    scheme = urlparse(uri).scheme\n    if scheme in self.storages:\n        try:\n            self._get_storage(uri)\n            return True\n        except NotConfigured:\n            logger.error('Disabled feed storage scheme: %(scheme)s', {'scheme': scheme})\n    else:\n        logger.error('Unknown feed storage scheme: %(scheme)s', {'scheme': scheme})",
                            "def _get_exporter(self, *args, **kwargs):\n    return self.exporters[self.format](*args, **kwargs)",
                            "def _get_storage(self, uri):\n    return self.storages[urlparse(uri).scheme](uri)",
                            "def _get_uri_params(self, spider):\n    params = {}\n    for k in dir(spider):\n        params[k] = getattr(spider, k)\n    ts = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n    params['time'] = ts\n    self._uripar(params, spider)\n    return params"
                        ],
                        "constructor_variables": [
                            "_uripar",
                            "settings",
                            "urifmt",
                            "exporters",
                            "format",
                            "export_fields",
                            "uripar",
                            "store_empty",
                            "storages"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, settings)",
                            "from_crawler(cls, crawler)",
                            "open_spider(self, spider)",
                            "close_spider(self, spider)",
                            "item_scraped(self, item, spider)",
                            "_load_components(self, setting_prefix)",
                            "_exporter_supported(self, format)",
                            "_storage_supported(self, uri)",
                            "_get_exporter(self, *args, **kwargs)",
                            "_get_storage(self, uri)",
                            "_get_uri_params(self, spider)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "from scrapy.utils.python import get_func_args",
                    "start_line": 24,
                    "end_line": 25
                }
            ],
            "inscope_functions": [
                "def __init__(uri):\n    \"\"\"Initialize the storage with the parameters given in the URI\"\"\"",
                "def open(spider):\n    \"\"\"Open the storage for the given spider. It must return a file-like\n    object that will be used for the exporters\"\"\"",
                "def store(file):\n    \"\"\"Store the given file stream\"\"\"",
                "def open(self, spider):\n    return TemporaryFile(prefix='feed-')",
                "def store(self, file):\n    return threads.deferToThread(self._store_in_thread, file)",
                "def _store_in_thread(self, file):\n    raise NotImplementedError",
                "def __init__(self, uri, _stdout=sys.stdout):\n    self._stdout = _stdout",
                "def open(self, spider):\n    return self._stdout",
                "def store(self, file):\n    pass",
                "def __init__(self, uri):\n    self.path = file_uri_to_path(uri)",
                "def open(self, spider):\n    dirname = os.path.dirname(self.path)\n    if dirname and not os.path.exists(dirname):\n        os.makedirs(dirname)\n    return open(self.path, 'ab')",
                "def store(self, file):\n    file.close()",
                "def __init__(self, uri):\n    from scrapy.conf import settings\n    try:\n        import boto\n    except ImportError:\n        raise NotConfigured\n    self.connect_s3 = boto.connect_s3\n    u = urlparse(uri)\n    self.bucketname = u.hostname\n    self.access_key = u.username or settings['AWS_ACCESS_KEY_ID']\n    self.secret_key = u.password or settings['AWS_SECRET_ACCESS_KEY']\n    self.keyname = u.path",
                "def _store_in_thread(self, file):\n    file.seek(0)\n    conn = self.connect_s3(self.access_key, self.secret_key)\n    bucket = conn.get_bucket(self.bucketname, validate=False)\n    key = bucket.new_key(self.keyname)\n    key.set_contents_from_file(file)\n    key.close()",
                "def __init__(self, uri):\n    u = urlparse(uri)\n    self.host = u.hostname\n    self.port = int(u.port or '21')\n    self.username = u.username\n    self.password = u.password\n    self.path = u.path",
                "def _store_in_thread(self, file):\n    file.seek(0)\n    ftp = FTP()\n    ftp.connect(self.host, self.port)\n    ftp.login(self.username, self.password)\n    dirname, filename = posixpath.split(self.path)\n    ftp_makedirs_cwd(ftp, dirname)\n    ftp.storbinary('STOR %s' % filename, file)\n    ftp.quit()",
                "def __init__(self, file, exporter, storage, uri):\n    self.file = file\n    self.exporter = exporter\n    self.storage = storage\n    self.uri = uri\n    self.itemcount = 0",
                "def __init__(self, settings):\n    self.settings = settings\n    self.urifmt = settings['FEED_URI']\n    if not self.urifmt:\n        raise NotConfigured\n    self.format = settings['FEED_FORMAT'].lower()\n    self.storages = self._load_components('FEED_STORAGES')\n    self.exporters = self._load_components('FEED_EXPORTERS')\n    if not self._storage_supported(self.urifmt):\n        raise NotConfigured\n    if not self._exporter_supported(self.format):\n        raise NotConfigured\n    self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n    self.export_fields = settings.getlist('FEED_EXPORT_FIELDS')\n    uripar = settings['FEED_URI_PARAMS']\n    self._uripar = load_object(uripar) if uripar else lambda x, y: None",
                "@classmethod\ndef from_crawler(cls, crawler):\n    o = cls(crawler.settings)\n    crawler.signals.connect(o.open_spider, signals.spider_opened)\n    crawler.signals.connect(o.close_spider, signals.spider_closed)\n    crawler.signals.connect(o.item_scraped, signals.item_scraped)\n    return o",
                "def open_spider(self, spider):\n    uri = self.urifmt % self._get_uri_params(spider)\n    storage = self._get_storage(uri)\n    file = storage.open(spider)\n    exporter = self._get_exporter(file, fields_to_export=self.export_fields)\n    exporter.start_exporting()\n    self.slot = SpiderSlot(file, exporter, storage, uri)",
                "def close_spider(self, spider):\n    slot = self.slot\n    if not slot.itemcount and not self.store_empty:\n        return\n    slot.exporter.finish_exporting()\n    logfmt = \"%%s %(format)s feed (%(itemcount)d items) in: %(uri)s\"\n    log_args = {'format': self.format,\n                'itemcount': slot.itemcount,\n                'uri': slot.uri}\n    d = defer.maybeDeferred(slot.storage.store, slot.file)\n    d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n                                        extra={'spider': spider}))\n    d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                        extra={'spider': spider, 'failure': f}))\n    return d",
                "def item_scraped(self, item, spider):\n    slot = self.slot\n    slot.exporter.export_item(item)\n    slot.itemcount += 1\n    return item",
                "def _load_components(self, setting_prefix):\n    conf = dict(self.settings['%s_BASE' % setting_prefix])\n    conf.update(self.settings[setting_prefix])\n    d = {}\n    for k, v in conf.items():\n        try:\n            d[k] = load_object(v)\n        except NotConfigured:\n            pass\n    return d",
                "def _exporter_supported(self, format):\n    if format in self.exporters:\n        return True\n    logger.error(\"Unknown feed format: %(format)s\", {'format': format})",
                "def _storage_supported(self, uri):\n    scheme = urlparse(uri).scheme\n    if scheme in self.storages:\n        try:\n            self._get_storage(uri)\n            return True\n        except NotConfigured:\n            logger.error(\"Disabled feed storage scheme: %(scheme)s\",\n                         {'scheme': scheme})\n    else:\n        logger.error(\"Unknown feed storage scheme: %(scheme)s\",\n                     {'scheme': scheme})",
                "def _get_exporter(self, *args, **kwargs):\n    return self.exporters[self.format](*args, **kwargs)",
                "def _get_storage(self, uri):\n    return self.storages[urlparse(uri).scheme](uri)",
                "def _get_uri_params(self, spider):\n    params = {}\n    for k in dir(spider):\n        params[k] = getattr(spider, k)\n    ts = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n    params['time'] = ts\n    self._uripar(params, spider)\n    return params"
            ],
            "inscope_function_signatures": [
                "__init__(uri)",
                "open(spider)",
                "store(file)",
                "open(self, spider)",
                "store(self, file)",
                "_store_in_thread(self, file)",
                "__init__(self, uri, _stdout=sys.stdout)",
                "open(self, spider)",
                "store(self, file)",
                "__init__(self, uri)",
                "open(self, spider)",
                "store(self, file)",
                "__init__(self, uri)",
                "_store_in_thread(self, file)",
                "__init__(self, uri)",
                "_store_in_thread(self, file)",
                "__init__(self, file, exporter, storage, uri)",
                "__init__(self, settings)",
                "from_crawler(cls, crawler)",
                "open_spider(self, spider)",
                "close_spider(self, spider)",
                "item_scraped(self, item, spider)",
                "_load_components(self, setting_prefix)",
                "_exporter_supported(self, format)",
                "_storage_supported(self, uri)",
                "_get_exporter(self, *args, **kwargs)",
                "_get_storage(self, uri)",
                "_get_uri_params(self, spider)"
            ],
            "variables_in_file": {
                "logger": [
                    26,
                    210,
                    184,
                    186,
                    219,
                    222
                ],
                "logging.getLogger": [
                    26
                ],
                "logging": [
                    26
                ],
                "__name__": [
                    26
                ],
                "Interface": [
                    29
                ],
                "object": [
                    130,
                    70,
                    139,
                    44,
                    57
                ],
                "TemporaryFile": [
                    47
                ],
                "threads.deferToThread": [
                    50
                ],
                "threads": [
                    50
                ],
                "self._store_in_thread": [
                    50
                ],
                "self": [
                    132,
                    133,
                    134,
                    135,
                    136,
                    142,
                    143,
                    144,
                    146,
                    147,
                    148,
                    149,
                    151,
                    153,
                    154,
                    156,
                    167,
                    168,
                    170,
                    172,
                    175,
                    176,
                    50,
                    180,
                    60,
                    63,
                    191,
                    197,
                    198,
                    73,
                    76,
                    79,
                    208,
                    214,
                    216,
                    93,
                    95,
                    96,
                    97,
                    98,
                    226,
                    229,
                    102,
                    103,
                    104,
                    237,
                    113,
                    114,
                    115,
                    116,
                    117,
                    122,
                    123,
                    124
                ],
                "file": [
                    132,
                    101,
                    105,
                    169,
                    170,
                    172,
                    50,
                    82,
                    120,
                    126
                ],
                "NotImplementedError": [
                    53
                ],
                "implementer": [
                    56,
                    43,
                    69
                ],
                "IFeedStorage": [
                    56,
                    43,
                    69
                ],
                "sys.stdout": [
                    59
                ],
                "sys": [
                    59
                ],
                "self._stdout": [
                    60,
                    63
                ],
                "_stdout": [
                    60
                ],
                "self.path": [
                    73,
                    76,
                    79,
                    117,
                    124
                ],
                "file_uri_to_path": [
                    73
                ],
                "uri": [
                    229,
                    167,
                    135,
                    73,
                    168,
                    172,
                    112,
                    213,
                    216,
                    94
                ],
                "dirname": [
                    76,
                    77,
                    78,
                    124,
                    125
                ],
                "os.path.dirname": [
                    76
                ],
                "os.path": [
                    76,
                    77
                ],
                "os": [
                    76,
                    77,
                    78
                ],
                "os.path.exists": [
                    77
                ],
                "os.makedirs": [
                    78
                ],
                "open": [
                    79
                ],
                "file.close": [
                    82
                ],
                "BlockingFeedStorage": [
                    109,
                    85
                ],
                "ImportError": [
                    91
                ],
                "NotConfigured": [
                    203,
                    145,
                    150,
                    152,
                    218,
                    92
                ],
                "self.connect_s3": [
                    93,
                    102
                ],
                "boto.connect_s3": [
                    93
                ],
                "boto": [
                    93
                ],
                "u": [
                    96,
                    97,
                    98,
                    112,
                    113,
                    114,
                    115,
                    116,
                    117,
                    94,
                    95
                ],
                "urlparse": [
                    112,
                    229,
                    213,
                    94
                ],
                "self.bucketname": [
                    103,
                    95
                ],
                "u.hostname": [
                    113,
                    95
                ],
                "self.access_key": [
                    96,
                    102
                ],
                "u.username": [
                    96,
                    115
                ],
                "settings": [
                    96,
                    97,
                    142,
                    143,
                    146,
                    153,
                    154,
                    155
                ],
                "self.secret_key": [
                    97,
                    102
                ],
                "u.password": [
                    97,
                    116
                ],
                "self.keyname": [
                    104,
                    98
                ],
                "u.path": [
                    98,
                    117
                ],
                "file.seek": [
                    120,
                    101
                ],
                "conn": [
                    102,
                    103
                ],
                "bucket": [
                    104,
                    103
                ],
                "conn.get_bucket": [
                    103
                ],
                "key": [
                    104,
                    105,
                    106
                ],
                "bucket.new_key": [
                    104
                ],
                "key.set_contents_from_file": [
                    105
                ],
                "key.close": [
                    106
                ],
                "self.host": [
                    113,
                    122
                ],
                "self.port": [
                    114,
                    122
                ],
                "int": [
                    114
                ],
                "u.port": [
                    114
                ],
                "self.username": [
                    123,
                    115
                ],
                "self.password": [
                    123,
                    116
                ],
                "ftp": [
                    121,
                    122,
                    123,
                    125,
                    126,
                    127
                ],
                "FTP": [
                    121
                ],
                "ftp.connect": [
                    122
                ],
                "ftp.login": [
                    123
                ],
                "filename": [
                    124,
                    126
                ],
                "posixpath.split": [
                    124
                ],
                "posixpath": [
                    124
                ],
                "ftp_makedirs_cwd": [
                    125
                ],
                "ftp.storbinary": [
                    126
                ],
                "ftp.quit": [
                    127
                ],
                "self.file": [
                    132
                ],
                "self.exporter": [
                    133
                ],
                "exporter": [
                    170,
                    171,
                    172,
                    133
                ],
                "self.storage": [
                    134
                ],
                "storage": [
                    168,
                    169,
                    172,
                    134
                ],
                "self.uri": [
                    135
                ],
                "self.itemcount": [
                    136
                ],
                "self.settings": [
                    198,
                    197,
                    142
                ],
                "self.urifmt": [
                    144,
                    167,
                    149,
                    143
                ],
                "self.format": [
                    146,
                    226,
                    180,
                    151
                ],
                "lower": [
                    146
                ],
                "self.storages": [
                    147,
                    229,
                    214
                ],
                "self._load_components": [
                    147,
                    148
                ],
                "self.exporters": [
                    208,
                    226,
                    148
                ],
                "self._storage_supported": [
                    149
                ],
                "self._exporter_supported": [
                    151
                ],
                "self.store_empty": [
                    176,
                    153
                ],
                "settings.getbool": [
                    153
                ],
                "self.export_fields": [
                    170,
                    154
                ],
                "settings.getlist": [
                    154
                ],
                "uripar": [
                    155,
                    156
                ],
                "self._uripar": [
                    156,
                    237
                ],
                "load_object": [
                    202,
                    156
                ],
                "o": [
                    160,
                    161,
                    162,
                    163,
                    164
                ],
                "cls": [
                    160
                ],
                "crawler.settings": [
                    160
                ],
                "crawler": [
                    160,
                    161,
                    162,
                    163
                ],
                "crawler.signals.connect": [
                    161,
                    162,
                    163
                ],
                "crawler.signals": [
                    161,
                    162,
                    163
                ],
                "o.open_spider": [
                    161
                ],
                "signals.spider_opened": [
                    161
                ],
                "signals": [
                    161,
                    162,
                    163
                ],
                "o.close_spider": [
                    162
                ],
                "signals.spider_closed": [
                    162
                ],
                "o.item_scraped": [
                    163
                ],
                "signals.item_scraped": [
                    163
                ],
                "classmethod": [
                    158
                ],
                "self._get_uri_params": [
                    167
                ],
                "spider": [
                    167,
                    169,
                    233,
                    234,
                    237,
                    185,
                    187
                ],
                "self._get_storage": [
                    168,
                    216
                ],
                "storage.open": [
                    169
                ],
                "self._get_exporter": [
                    170
                ],
                "exporter.start_exporting": [
                    171
                ],
                "self.slot": [
                    191,
                    172,
                    175
                ],
                "SpiderSlot": [
                    172
                ],
                "slot": [
                    192,
                    193,
                    175,
                    176,
                    178,
                    181,
                    182,
                    183,
                    191
                ],
                "slot.itemcount": [
                    176,
                    193,
                    181
                ],
                "slot.exporter.finish_exporting": [
                    178
                ],
                "slot.exporter": [
                    192,
                    178
                ],
                "logfmt": [
                    184,
                    186,
                    179
                ],
                "log_args": [
                    184,
                    186,
                    180
                ],
                "slot.uri": [
                    182
                ],
                "d": [
                    199,
                    202,
                    205,
                    183,
                    184,
                    186,
                    188
                ],
                "defer.maybeDeferred": [
                    183
                ],
                "defer": [
                    183
                ],
                "slot.storage.store": [
                    183
                ],
                "slot.storage": [
                    183
                ],
                "slot.file": [
                    183
                ],
                "d.addCallback": [
                    184
                ],
                "logger.info": [
                    184
                ],
                "d.addErrback": [
                    186
                ],
                "logger.error": [
                    210,
                    186,
                    219,
                    222
                ],
                "f": [
                    187
                ],
                "slot.exporter.export_item": [
                    192
                ],
                "item": [
                    192,
                    194
                ],
                "conf": [
                    200,
                    197,
                    198
                ],
                "dict": [
                    197
                ],
                "setting_prefix": [
                    197,
                    198
                ],
                "conf.update": [
                    198
                ],
                "k": [
                    200,
                    233,
                    202,
                    234
                ],
                "v": [
                    200,
                    202
                ],
                "conf.items": [
                    200
                ],
                "format": [
                    208,
                    210
                ],
                "scheme": [
                    229,
                    213,
                    214,
                    220,
                    223
                ],
                "args": [
                    226
                ],
                "kwargs": [
                    226
                ],
                "params": [
                    232,
                    234,
                    236,
                    237,
                    238
                ],
                "dir": [
                    233
                ],
                "getattr": [
                    234
                ],
                "ts": [
                    235,
                    236
                ],
                "replace": [
                    235
                ],
                "isoformat": [
                    235
                ],
                "datetime.utcnow": [
                    235
                ],
                "datetime": [
                    235
                ]
            },
            "filtered_variables_in_file": {
                "logger": [
                    26,
                    210,
                    184,
                    186,
                    219,
                    222
                ],
                "logging.getLogger": [
                    26
                ],
                "logging": [
                    26
                ],
                "Interface": [
                    29
                ],
                "TemporaryFile": [
                    47
                ],
                "threads.deferToThread": [
                    50
                ],
                "threads": [
                    50
                ],
                "self._store_in_thread": [
                    50
                ],
                "self": [
                    132,
                    133,
                    134,
                    135,
                    136,
                    142,
                    143,
                    144,
                    146,
                    147,
                    148,
                    149,
                    151,
                    153,
                    154,
                    156,
                    167,
                    168,
                    170,
                    172,
                    175,
                    176,
                    50,
                    180,
                    60,
                    63,
                    191,
                    197,
                    198,
                    73,
                    76,
                    79,
                    208,
                    214,
                    216,
                    93,
                    95,
                    96,
                    97,
                    98,
                    226,
                    229,
                    102,
                    103,
                    104,
                    237,
                    113,
                    114,
                    115,
                    116,
                    117,
                    122,
                    123,
                    124
                ],
                "file": [
                    132,
                    101,
                    105,
                    169,
                    170,
                    172,
                    50,
                    82,
                    120,
                    126
                ],
                "implementer": [
                    56,
                    43,
                    69
                ],
                "IFeedStorage": [
                    56,
                    43,
                    69
                ],
                "sys.stdout": [
                    59
                ],
                "sys": [
                    59
                ],
                "self._stdout": [
                    60,
                    63
                ],
                "_stdout": [
                    60
                ],
                "self.path": [
                    73,
                    76,
                    79,
                    117,
                    124
                ],
                "file_uri_to_path": [
                    73
                ],
                "uri": [
                    229,
                    167,
                    135,
                    73,
                    168,
                    172,
                    112,
                    213,
                    216,
                    94
                ],
                "dirname": [
                    76,
                    77,
                    78,
                    124,
                    125
                ],
                "os.path.dirname": [
                    76
                ],
                "os.path": [
                    76,
                    77
                ],
                "os": [
                    76,
                    77,
                    78
                ],
                "os.path.exists": [
                    77
                ],
                "os.makedirs": [
                    78
                ],
                "file.close": [
                    82
                ],
                "BlockingFeedStorage": [
                    109,
                    85
                ],
                "NotConfigured": [
                    203,
                    145,
                    150,
                    152,
                    218,
                    92
                ],
                "self.connect_s3": [
                    93,
                    102
                ],
                "boto.connect_s3": [
                    93
                ],
                "boto": [
                    93
                ],
                "u": [
                    96,
                    97,
                    98,
                    112,
                    113,
                    114,
                    115,
                    116,
                    117,
                    94,
                    95
                ],
                "urlparse": [
                    112,
                    229,
                    213,
                    94
                ],
                "self.bucketname": [
                    103,
                    95
                ],
                "u.hostname": [
                    113,
                    95
                ],
                "self.access_key": [
                    96,
                    102
                ],
                "u.username": [
                    96,
                    115
                ],
                "settings": [
                    96,
                    97,
                    142,
                    143,
                    146,
                    153,
                    154,
                    155
                ],
                "self.secret_key": [
                    97,
                    102
                ],
                "u.password": [
                    97,
                    116
                ],
                "self.keyname": [
                    104,
                    98
                ],
                "u.path": [
                    98,
                    117
                ],
                "file.seek": [
                    120,
                    101
                ],
                "conn": [
                    102,
                    103
                ],
                "bucket": [
                    104,
                    103
                ],
                "conn.get_bucket": [
                    103
                ],
                "key": [
                    104,
                    105,
                    106
                ],
                "bucket.new_key": [
                    104
                ],
                "key.set_contents_from_file": [
                    105
                ],
                "key.close": [
                    106
                ],
                "self.host": [
                    113,
                    122
                ],
                "self.port": [
                    114,
                    122
                ],
                "u.port": [
                    114
                ],
                "self.username": [
                    123,
                    115
                ],
                "self.password": [
                    123,
                    116
                ],
                "ftp": [
                    121,
                    122,
                    123,
                    125,
                    126,
                    127
                ],
                "FTP": [
                    121
                ],
                "ftp.connect": [
                    122
                ],
                "ftp.login": [
                    123
                ],
                "filename": [
                    124,
                    126
                ],
                "posixpath.split": [
                    124
                ],
                "posixpath": [
                    124
                ],
                "ftp_makedirs_cwd": [
                    125
                ],
                "ftp.storbinary": [
                    126
                ],
                "ftp.quit": [
                    127
                ],
                "self.file": [
                    132
                ],
                "self.exporter": [
                    133
                ],
                "exporter": [
                    170,
                    171,
                    172,
                    133
                ],
                "self.storage": [
                    134
                ],
                "storage": [
                    168,
                    169,
                    172,
                    134
                ],
                "self.uri": [
                    135
                ],
                "self.itemcount": [
                    136
                ],
                "self.settings": [
                    198,
                    197,
                    142
                ],
                "self.urifmt": [
                    144,
                    167,
                    149,
                    143
                ],
                "self.format": [
                    146,
                    226,
                    180,
                    151
                ],
                "lower": [
                    146
                ],
                "self.storages": [
                    147,
                    229,
                    214
                ],
                "self._load_components": [
                    147,
                    148
                ],
                "self.exporters": [
                    208,
                    226,
                    148
                ],
                "self._storage_supported": [
                    149
                ],
                "self._exporter_supported": [
                    151
                ],
                "self.store_empty": [
                    176,
                    153
                ],
                "settings.getbool": [
                    153
                ],
                "self.export_fields": [
                    170,
                    154
                ],
                "settings.getlist": [
                    154
                ],
                "uripar": [
                    155,
                    156
                ],
                "self._uripar": [
                    156,
                    237
                ],
                "load_object": [
                    202,
                    156
                ],
                "o": [
                    160,
                    161,
                    162,
                    163,
                    164
                ],
                "cls": [
                    160
                ],
                "crawler.settings": [
                    160
                ],
                "crawler": [
                    160,
                    161,
                    162,
                    163
                ],
                "crawler.signals.connect": [
                    161,
                    162,
                    163
                ],
                "crawler.signals": [
                    161,
                    162,
                    163
                ],
                "o.open_spider": [
                    161
                ],
                "signals.spider_opened": [
                    161
                ],
                "signals": [
                    161,
                    162,
                    163
                ],
                "o.close_spider": [
                    162
                ],
                "signals.spider_closed": [
                    162
                ],
                "o.item_scraped": [
                    163
                ],
                "signals.item_scraped": [
                    163
                ],
                "self._get_uri_params": [
                    167
                ],
                "spider": [
                    167,
                    169,
                    233,
                    234,
                    237,
                    185,
                    187
                ],
                "self._get_storage": [
                    168,
                    216
                ],
                "storage.open": [
                    169
                ],
                "self._get_exporter": [
                    170
                ],
                "exporter.start_exporting": [
                    171
                ],
                "self.slot": [
                    191,
                    172,
                    175
                ],
                "SpiderSlot": [
                    172
                ],
                "slot": [
                    192,
                    193,
                    175,
                    176,
                    178,
                    181,
                    182,
                    183,
                    191
                ],
                "slot.itemcount": [
                    176,
                    193,
                    181
                ],
                "slot.exporter.finish_exporting": [
                    178
                ],
                "slot.exporter": [
                    192,
                    178
                ],
                "logfmt": [
                    184,
                    186,
                    179
                ],
                "log_args": [
                    184,
                    186,
                    180
                ],
                "slot.uri": [
                    182
                ],
                "d": [
                    199,
                    202,
                    205,
                    183,
                    184,
                    186,
                    188
                ],
                "defer.maybeDeferred": [
                    183
                ],
                "defer": [
                    183
                ],
                "slot.storage.store": [
                    183
                ],
                "slot.storage": [
                    183
                ],
                "slot.file": [
                    183
                ],
                "d.addCallback": [
                    184
                ],
                "logger.info": [
                    184
                ],
                "d.addErrback": [
                    186
                ],
                "logger.error": [
                    210,
                    186,
                    219,
                    222
                ],
                "f": [
                    187
                ],
                "slot.exporter.export_item": [
                    192
                ],
                "item": [
                    192,
                    194
                ],
                "conf": [
                    200,
                    197,
                    198
                ],
                "setting_prefix": [
                    197,
                    198
                ],
                "conf.update": [
                    198
                ],
                "k": [
                    200,
                    233,
                    202,
                    234
                ],
                "v": [
                    200,
                    202
                ],
                "conf.items": [
                    200
                ],
                "scheme": [
                    229,
                    213,
                    214,
                    220,
                    223
                ],
                "args": [
                    226
                ],
                "kwargs": [
                    226
                ],
                "params": [
                    232,
                    234,
                    236,
                    237,
                    238
                ],
                "ts": [
                    235,
                    236
                ],
                "replace": [
                    235
                ],
                "isoformat": [
                    235
                ],
                "datetime.utcnow": [
                    235
                ],
                "datetime": [
                    235
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs/repos/scrapy_33/scrapy/log.py": {
            "buggy_functions": [
                {
                    "function_name": "msg",
                    "function_code": "def msg(message=None, _level=logging.INFO, **kw):\n    warnings.warn('log.msg has been deprecated, create a python logger and '\n                  'log through it instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n\n    level = kw.pop('level', _level)\n    message = kw.pop('format', message)\n    # NOTE: logger.log doesn't handle well passing empty dictionaries with format\n    # arguments because of some weird use-case:\n    # https://hg.python.org/cpython/file/648dcafa7e5f/Lib/logging/__init__.py#l269\n    logger.log(level, message, *[kw] if kw else [])\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 30,
                    "end_line": 40,
                    "variables": {
                        "logging.INFO": [
                            30
                        ],
                        "logging": [
                            30
                        ],
                        "warnings.warn": [
                            31
                        ],
                        "warnings": [
                            31
                        ],
                        "ScrapyDeprecationWarning": [
                            33
                        ],
                        "level": [
                            40,
                            35
                        ],
                        "kw.pop": [
                            35,
                            36
                        ],
                        "kw": [
                            40,
                            35,
                            36
                        ],
                        "_level": [
                            35
                        ],
                        "message": [
                            40,
                            36
                        ],
                        "logger.log": [
                            40
                        ],
                        "logger": [
                            40
                        ]
                    },
                    "filtered_variables": {
                        "logging.INFO": [
                            30
                        ],
                        "logging": [
                            30
                        ],
                        "warnings.warn": [
                            31
                        ],
                        "warnings": [
                            31
                        ],
                        "ScrapyDeprecationWarning": [
                            33
                        ],
                        "level": [
                            40,
                            35
                        ],
                        "kw.pop": [
                            35,
                            36
                        ],
                        "kw": [
                            40,
                            35,
                            36
                        ],
                        "_level": [
                            35
                        ],
                        "message": [
                            40,
                            36
                        ],
                        "logger.log": [
                            40
                        ],
                        "logger": [
                            40
                        ]
                    },
                    "diff_line_number": 30,
                    "class_data": null,
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "err",
                    "function_code": "def err(_stuff=None, _why=None, **kw):\n    warnings.warn('log.err has been deprecated, create a python logger and '\n                  'use its error method instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n\n    level = kw.pop('level', logging.ERROR)\n    failure = kw.pop('failure', _stuff) or Failure()\n    message = kw.pop('why', _why) or failure.value\n    logger.log(level, message, *[kw] if kw else [], extra={'failure': failure})\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 43,
                    "end_line": 51,
                    "variables": {
                        "warnings.warn": [
                            44
                        ],
                        "warnings": [
                            44
                        ],
                        "ScrapyDeprecationWarning": [
                            46
                        ],
                        "level": [
                            48,
                            51
                        ],
                        "kw.pop": [
                            48,
                            49,
                            50
                        ],
                        "kw": [
                            48,
                            49,
                            50,
                            51
                        ],
                        "logging.ERROR": [
                            48
                        ],
                        "logging": [
                            48
                        ],
                        "failure": [
                            49,
                            50,
                            51
                        ],
                        "_stuff": [
                            49
                        ],
                        "Failure": [
                            49
                        ],
                        "message": [
                            50,
                            51
                        ],
                        "_why": [
                            50
                        ],
                        "failure.value": [
                            50
                        ],
                        "logger.log": [
                            51
                        ],
                        "logger": [
                            51
                        ]
                    },
                    "filtered_variables": {
                        "warnings.warn": [
                            44
                        ],
                        "warnings": [
                            44
                        ],
                        "ScrapyDeprecationWarning": [
                            46
                        ],
                        "level": [
                            48,
                            51
                        ],
                        "kw.pop": [
                            48,
                            49,
                            50
                        ],
                        "kw": [
                            48,
                            49,
                            50,
                            51
                        ],
                        "logging.ERROR": [
                            48
                        ],
                        "logging": [
                            48
                        ],
                        "failure": [
                            49,
                            50,
                            51
                        ],
                        "_stuff": [
                            49
                        ],
                        "Failure": [
                            49
                        ],
                        "message": [
                            50,
                            51
                        ],
                        "_why": [
                            50
                        ],
                        "failure.value": [
                            50
                        ],
                        "logger.log": [
                            51
                        ],
                        "logger": [
                            51
                        ]
                    },
                    "diff_line_number": 43,
                    "class_data": null,
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "\"\"\"\nThis module is kept to provide a helpful warning about its removal.\n\"\"\"\n\nimport logging\nimport warnings\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nlogger = logging.getLogger(__name__)\n\nwarnings.warn(\"Module `scrapy.log` has been deprecated, Scrapy now relies on \"\n              \"the builtin Python library for logging. Read the updated \"\n              \"logging entry in the documentation to learn more.\",\n              ScrapyDeprecationWarning, stacklevel=2)\n\n\n# Imports kept for backwards-compatibility\n\nDEBUG = logging.DEBUG\nINFO = logging.INFO\nWARNING = logging.WARNING\nERROR = logging.ERROR\nCRITICAL = logging.CRITICAL\nSILENT = CRITICAL + 1",
                    "start_line": 1,
                    "end_line": 29
                }
            ],
            "inscope_functions": [
                "def msg(message=None, _level=logging.INFO, **kw):\n    warnings.warn('log.msg has been deprecated, create a python logger and '\n                  'log through it instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n\n    level = kw.pop('level', _level)\n    message = kw.pop('format', message)\n    # NOTE: logger.log doesn't handle well passing empty dictionaries with format\n    # arguments because of some weird use-case:\n    # https://hg.python.org/cpython/file/648dcafa7e5f/Lib/logging/__init__.py#l269\n    logger.log(level, message, *[kw] if kw else [])",
                "def err(_stuff=None, _why=None, **kw):\n    warnings.warn('log.err has been deprecated, create a python logger and '\n                  'use its error method instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n\n    level = kw.pop('level', logging.ERROR)\n    failure = kw.pop('failure', _stuff) or Failure()\n    message = kw.pop('why', _why) or failure.value\n    logger.log(level, message, *[kw] if kw else [], extra={'failure': failure})"
            ],
            "inscope_function_signatures": [
                "msg(message=None, _level=logging.INFO, **kw)",
                "err(_stuff=None, _why=None, **kw)"
            ],
            "variables_in_file": {
                "logger": [
                    40,
                    51,
                    12
                ],
                "logging.getLogger": [
                    12
                ],
                "logging": [
                    12,
                    48,
                    22,
                    23,
                    24,
                    25,
                    26,
                    30
                ],
                "__name__": [
                    12
                ],
                "warnings.warn": [
                    44,
                    14,
                    31
                ],
                "warnings": [
                    44,
                    14,
                    31
                ],
                "ScrapyDeprecationWarning": [
                    17,
                    46,
                    33
                ],
                "DEBUG": [
                    22
                ],
                "logging.DEBUG": [
                    22
                ],
                "INFO": [
                    23
                ],
                "logging.INFO": [
                    30,
                    23
                ],
                "WARNING": [
                    24
                ],
                "logging.WARNING": [
                    24
                ],
                "ERROR": [
                    25
                ],
                "logging.ERROR": [
                    48,
                    25
                ],
                "CRITICAL": [
                    26,
                    27
                ],
                "logging.CRITICAL": [
                    26
                ],
                "SILENT": [
                    27
                ],
                "level": [
                    40,
                    51,
                    48,
                    35
                ],
                "kw.pop": [
                    35,
                    36,
                    48,
                    49,
                    50
                ],
                "kw": [
                    35,
                    36,
                    40,
                    48,
                    49,
                    50,
                    51
                ],
                "_level": [
                    35
                ],
                "message": [
                    40,
                    50,
                    51,
                    36
                ],
                "logger.log": [
                    40,
                    51
                ],
                "failure": [
                    49,
                    50,
                    51
                ],
                "_stuff": [
                    49
                ],
                "Failure": [
                    49
                ],
                "_why": [
                    50
                ],
                "failure.value": [
                    50
                ]
            },
            "filtered_variables_in_file": {
                "logger": [
                    40,
                    51,
                    12
                ],
                "logging.getLogger": [
                    12
                ],
                "logging": [
                    12,
                    48,
                    22,
                    23,
                    24,
                    25,
                    26,
                    30
                ],
                "warnings.warn": [
                    44,
                    14,
                    31
                ],
                "warnings": [
                    44,
                    14,
                    31
                ],
                "ScrapyDeprecationWarning": [
                    17,
                    46,
                    33
                ],
                "DEBUG": [
                    22
                ],
                "logging.DEBUG": [
                    22
                ],
                "INFO": [
                    23
                ],
                "logging.INFO": [
                    30,
                    23
                ],
                "WARNING": [
                    24
                ],
                "logging.WARNING": [
                    24
                ],
                "ERROR": [
                    25
                ],
                "logging.ERROR": [
                    48,
                    25
                ],
                "CRITICAL": [
                    26,
                    27
                ],
                "logging.CRITICAL": [
                    26
                ],
                "SILENT": [
                    27
                ],
                "level": [
                    40,
                    51,
                    48,
                    35
                ],
                "kw.pop": [
                    35,
                    36,
                    48,
                    49,
                    50
                ],
                "kw": [
                    35,
                    36,
                    40,
                    48,
                    49,
                    50,
                    51
                ],
                "_level": [
                    35
                ],
                "message": [
                    40,
                    50,
                    51,
                    36
                ],
                "logger.log": [
                    40,
                    51
                ],
                "failure": [
                    49,
                    50,
                    51
                ],
                "_stuff": [
                    49
                ],
                "Failure": [
                    49
                ],
                "_why": [
                    50
                ],
                "failure.value": [
                    50
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs/repos/scrapy_33/scrapy/pipelines/files.py": {
            "buggy_functions": [
                {
                    "function_name": "media_to_download",
                    "function_code": "def media_to_download(self, request, info):\n    def _onsuccess(result):\n        if not result:\n            return  # returning None force download\n\n        last_modified = result.get('last_modified', None)\n        if not last_modified:\n            return  # returning None force download\n\n        age_seconds = time.time() - last_modified\n        age_days = age_seconds / 60 / 60 / 24\n        if age_days > self.EXPIRES:\n            return  # returning None force download\n\n        referer = request.headers.get('Referer')\n        logger.debug(\n            'File (uptodate): Downloaded %(medianame)s from %(request)s '\n            'referred in <%(referer)s>',\n            {'medianame': self.MEDIA_NAME, 'request': request,\n             'referer': referer},\n            extra={'spider': info.spider}\n        )\n        self.inc_stats(info.spider, 'uptodate')\n\n        checksum = result.get('checksum', None)\n        return {'url': request.url, 'path': path, 'checksum': checksum}\n\n    path = self.file_path(request, info=info)\n    dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n    dfd.addCallbacks(_onsuccess, lambda _: None)\n    dfd.addErrback(\n        lambda f:\n        logger.error(self.__class__.__name__ + '.store.stat_file',\n                     extra={'spider': info.spider, 'failure': f})\n    )\n    return dfd\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 182,
                    "end_line": 217,
                    "variables": {
                        "result": [
                            184,
                            187,
                            206
                        ],
                        "last_modified": [
                            187,
                            188,
                            191
                        ],
                        "result.get": [
                            187,
                            206
                        ],
                        "age_seconds": [
                            192,
                            191
                        ],
                        "time.time": [
                            191
                        ],
                        "time": [
                            191
                        ],
                        "age_days": [
                            192,
                            193
                        ],
                        "self.EXPIRES": [
                            193
                        ],
                        "self": [
                            193,
                            200,
                            204,
                            209,
                            210,
                            214
                        ],
                        "referer": [
                            201,
                            196
                        ],
                        "request.headers.get": [
                            196
                        ],
                        "request.headers": [
                            196
                        ],
                        "request": [
                            200,
                            209,
                            196,
                            207
                        ],
                        "logger.debug": [
                            197
                        ],
                        "logger": [
                            197,
                            214
                        ],
                        "self.MEDIA_NAME": [
                            200
                        ],
                        "info.spider": [
                            202,
                            204,
                            215
                        ],
                        "info": [
                            202,
                            204,
                            209,
                            210,
                            215
                        ],
                        "self.inc_stats": [
                            204
                        ],
                        "checksum": [
                            206,
                            207
                        ],
                        "request.url": [
                            207
                        ],
                        "path": [
                            209,
                            210,
                            207
                        ],
                        "self.file_path": [
                            209
                        ],
                        "dfd": [
                            217,
                            210,
                            211,
                            212
                        ],
                        "defer.maybeDeferred": [
                            210
                        ],
                        "defer": [
                            210
                        ],
                        "self.store.stat_file": [
                            210
                        ],
                        "self.store": [
                            210
                        ],
                        "dfd.addCallbacks": [
                            211
                        ],
                        "_onsuccess": [
                            211
                        ],
                        "dfd.addErrback": [
                            212
                        ],
                        "logger.error": [
                            214
                        ],
                        "self.__class__.__name__": [
                            214
                        ],
                        "self.__class__": [
                            214
                        ],
                        "f": [
                            215
                        ]
                    },
                    "filtered_variables": {
                        "result": [
                            184,
                            187,
                            206
                        ],
                        "last_modified": [
                            187,
                            188,
                            191
                        ],
                        "result.get": [
                            187,
                            206
                        ],
                        "age_seconds": [
                            192,
                            191
                        ],
                        "time.time": [
                            191
                        ],
                        "time": [
                            191
                        ],
                        "age_days": [
                            192,
                            193
                        ],
                        "self.EXPIRES": [
                            193
                        ],
                        "self": [
                            193,
                            200,
                            204,
                            209,
                            210,
                            214
                        ],
                        "referer": [
                            201,
                            196
                        ],
                        "request.headers.get": [
                            196
                        ],
                        "request.headers": [
                            196
                        ],
                        "request": [
                            200,
                            209,
                            196,
                            207
                        ],
                        "logger.debug": [
                            197
                        ],
                        "logger": [
                            197,
                            214
                        ],
                        "self.MEDIA_NAME": [
                            200
                        ],
                        "info.spider": [
                            202,
                            204,
                            215
                        ],
                        "info": [
                            202,
                            204,
                            209,
                            210,
                            215
                        ],
                        "self.inc_stats": [
                            204
                        ],
                        "checksum": [
                            206,
                            207
                        ],
                        "request.url": [
                            207
                        ],
                        "path": [
                            209,
                            210,
                            207
                        ],
                        "self.file_path": [
                            209
                        ],
                        "dfd": [
                            217,
                            210,
                            211,
                            212
                        ],
                        "defer.maybeDeferred": [
                            210
                        ],
                        "defer": [
                            210
                        ],
                        "self.store.stat_file": [
                            210
                        ],
                        "self.store": [
                            210
                        ],
                        "dfd.addCallbacks": [
                            211
                        ],
                        "_onsuccess": [
                            211
                        ],
                        "dfd.addErrback": [
                            212
                        ],
                        "logger.error": [
                            214
                        ],
                        "self.__class__.__name__": [
                            214
                        ],
                        "self.__class__": [
                            214
                        ],
                        "f": [
                            215
                        ]
                    },
                    "diff_line_number": 215,
                    "class_data": {
                        "signature": "class FilesPipeline(MediaPipeline)",
                        "docstring": "Abstract pipeline that implement the file downloading\n\nThis pipeline tries to minimize network transfers and file processing,\ndoing stat of the files and determining if file is new, uptodate or\nexpired.\n\n`new` files are those that pipeline never processed and needs to be\n    downloaded from supplier site the first time.\n\n`uptodate` files are the ones that the pipeline processed and are still\n    valid files.\n\n`expired` files are those that pipeline already processed but the last\n    modification was made long time ago, so a reprocessing is recommended to\n    refresh it in case of change.",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, store_uri, download_func=None):\n    if not store_uri:\n        raise NotConfigured\n    self.store = self._get_store(store_uri)\n    super(FilesPipeline, self).__init__(download_func=download_func)",
                            "@classmethod\ndef from_settings(cls, settings):\n    s3store = cls.STORE_SCHEMES['s3']\n    s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n    s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n    cls.FILES_URLS_FIELD = settings.get('FILES_URLS_FIELD', cls.DEFAULT_FILES_URLS_FIELD)\n    cls.FILES_RESULT_FIELD = settings.get('FILES_RESULT_FIELD', cls.DEFAULT_FILES_RESULT_FIELD)\n    cls.EXPIRES = settings.getint('FILES_EXPIRES', 90)\n    store_uri = settings['FILES_STORE']\n    return cls(store_uri)",
                            "def _get_store(self, uri):\n    if os.path.isabs(uri):\n        scheme = 'file'\n    else:\n        scheme = urlparse(uri).scheme\n    store_cls = self.STORE_SCHEMES[scheme]\n    return store_cls(uri)",
                            "def media_to_download(self, request, info):\n\n    def _onsuccess(result):\n        if not result:\n            return\n        last_modified = result.get('last_modified', None)\n        if not last_modified:\n            return\n        age_seconds = time.time() - last_modified\n        age_days = age_seconds / 60 / 60 / 24\n        if age_days > self.EXPIRES:\n            return\n        referer = request.headers.get('Referer')\n        logger.debug('File (uptodate): Downloaded %(medianame)s from %(request)s referred in <%(referer)s>', {'medianame': self.MEDIA_NAME, 'request': request, 'referer': referer}, extra={'spider': info.spider})\n        self.inc_stats(info.spider, 'uptodate')\n        checksum = result.get('checksum', None)\n        return {'url': request.url, 'path': path, 'checksum': checksum}\n    path = self.file_path(request, info=info)\n    dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n    dfd.addCallbacks(_onsuccess, lambda _: None)\n    dfd.addErrback(lambda f: logger.error(self.__class__.__name__ + '.store.stat_file', extra={'spider': info.spider, 'failure': f}))\n    return dfd",
                            "def media_failed(self, failure, request, info):\n    if not isinstance(failure.value, IgnoreRequest):\n        referer = request.headers.get('Referer')\n        logger.warning('File (unknown-error): Error downloading %(medianame)s from %(request)s referred in <%(referer)s>: %(exception)s', {'medianame': self.MEDIA_NAME, 'request': request, 'referer': referer, 'exception': failure.value}, extra={'spider': info.spider})\n    raise FileException",
                            "def media_downloaded(self, response, request, info):\n    referer = request.headers.get('Referer')\n    if response.status != 200:\n        logger.warning('File (code: %(status)s): Error downloading file from %(request)s referred in <%(referer)s>', {'status': response.status, 'request': request, 'referer': referer}, extra={'spider': info.spider})\n        raise FileException('download-error')\n    if not response.body:\n        logger.warning('File (empty-content): Empty file from %(request)s referred in <%(referer)s>: no-content', {'request': request, 'referer': referer}, extra={'spider': info.spider})\n        raise FileException('empty-content')\n    status = 'cached' if 'cached' in response.flags else 'downloaded'\n    logger.debug('File (%(status)s): Downloaded file from %(request)s referred in <%(referer)s>', {'status': status, 'request': request, 'referer': referer}, extra={'spider': info.spider})\n    self.inc_stats(info.spider, status)\n    try:\n        path = self.file_path(request, response=response, info=info)\n        checksum = self.file_downloaded(response, request, info)\n    except FileException as exc:\n        logger.warning('File (error): Error processing file from %(request)s referred in <%(referer)s>: %(errormsg)s', {'request': request, 'referer': referer, 'errormsg': str(exc)}, extra={'spider': info.spider}, exc_info=True)\n        raise\n    except Exception as exc:\n        logger.error('File (unknown-error): Error processing file from %(request)s referred in <%(referer)s>', {'request': request, 'referer': referer}, exc_info=True, extra={'spider': info.spider})\n        raise FileException(str(exc))\n    return {'url': request.url, 'path': path, 'checksum': checksum}",
                            "def inc_stats(self, spider, status):\n    spider.crawler.stats.inc_value('file_count', spider=spider)\n    spider.crawler.stats.inc_value('file_status_count/%s' % status, spider=spider)",
                            "def get_media_requests(self, item, info):\n    return [Request(x) for x in item.get(self.FILES_URLS_FIELD, [])]",
                            "def file_downloaded(self, response, request, info):\n    path = self.file_path(request, response=response, info=info)\n    buf = BytesIO(response.body)\n    self.store.persist_file(path, buf, info)\n    checksum = md5sum(buf)\n    return checksum",
                            "def item_completed(self, results, item, info):\n    if isinstance(item, dict) or self.FILES_RESULT_FIELD in item.fields:\n        item[self.FILES_RESULT_FIELD] = [x for ok, x in results if ok]\n    return item",
                            "def file_path(self, request, response=None, info=None):\n\n    def _warn():\n        from scrapy.exceptions import ScrapyDeprecationWarning\n        import warnings\n        warnings.warn('FilesPipeline.file_key(url) method is deprecated, please use file_path(request, response=None, info=None) instead', category=ScrapyDeprecationWarning, stacklevel=1)\n    if not isinstance(request, Request):\n        _warn()\n        url = request\n    else:\n        url = request.url\n    if not hasattr(self.file_key, '_base'):\n        _warn()\n        return self.file_key(url)\n    media_guid = hashlib.sha1(url).hexdigest()\n    media_ext = os.path.splitext(url)[1]\n    return 'full/%s%s' % (media_guid, media_ext)",
                            "def file_key(self, url):\n    return self.file_path(url)",
                            "def _onsuccess(result):\n    if not result:\n        return\n    last_modified = result.get('last_modified', None)\n    if not last_modified:\n        return\n    age_seconds = time.time() - last_modified\n    age_days = age_seconds / 60 / 60 / 24\n    if age_days > self.EXPIRES:\n        return\n    referer = request.headers.get('Referer')\n    logger.debug('File (uptodate): Downloaded %(medianame)s from %(request)s referred in <%(referer)s>', {'medianame': self.MEDIA_NAME, 'request': request, 'referer': referer}, extra={'spider': info.spider})\n    self.inc_stats(info.spider, 'uptodate')\n    checksum = result.get('checksum', None)\n    return {'url': request.url, 'path': path, 'checksum': checksum}",
                            "def _warn():\n    from scrapy.exceptions import ScrapyDeprecationWarning\n    import warnings\n    warnings.warn('FilesPipeline.file_key(url) method is deprecated, please use file_path(request, response=None, info=None) instead', category=ScrapyDeprecationWarning, stacklevel=1)"
                        ],
                        "constructor_variables": [
                            "store"
                        ],
                        "class_level_variables": [
                            "MEDIA_NAME",
                            "EXPIRES",
                            "STORE_SCHEMES",
                            "DEFAULT_FILES_URLS_FIELD",
                            "DEFAULT_FILES_RESULT_FIELD"
                        ],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, store_uri, download_func=None)",
                            "from_settings(cls, settings)",
                            "_get_store(self, uri)",
                            "media_to_download(self, request, info)",
                            "media_failed(self, failure, request, info)",
                            "media_downloaded(self, response, request, info)",
                            "inc_stats(self, spider, status)",
                            "get_media_requests(self, item, info)",
                            "file_downloaded(self, response, request, info)",
                            "item_completed(self, results, item, info)",
                            "file_path(self, request, response=None, info=None)",
                            "file_key(self, url)",
                            "_onsuccess(result)",
                            "_warn()"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "from scrapy.utils.misc import md5sum",
                    "start_line": 27,
                    "end_line": 28
                }
            ],
            "inscope_functions": [
                "def __init__(self, basedir):\n    if '://' in basedir:\n        basedir = basedir.split('://', 1)[1]\n    self.basedir = basedir\n    self._mkdir(self.basedir)\n    self.created_directories = defaultdict(set)",
                "def persist_file(self, path, buf, info, meta=None, headers=None):\n    absolute_path = self._get_filesystem_path(path)\n    self._mkdir(os.path.dirname(absolute_path), info)\n    with open(absolute_path, 'wb') as f:\n        f.write(buf.getvalue())",
                "def stat_file(self, path, info):\n    absolute_path = self._get_filesystem_path(path)\n    try:\n        last_modified = os.path.getmtime(absolute_path)\n    except:  # FIXME: catching everything!\n        return {}\n\n    with open(absolute_path, 'rb') as f:\n        checksum = md5sum(f)\n\n    return {'last_modified': last_modified, 'checksum': checksum}",
                "def _get_filesystem_path(self, path):\n    path_comps = path.split('/')\n    return os.path.join(self.basedir, *path_comps)",
                "def _mkdir(self, dirname, domain=None):\n    seen = self.created_directories[domain] if domain else set()\n    if dirname not in seen:\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n        seen.add(dirname)",
                "def __init__(self, uri):\n    assert uri.startswith('s3://')\n    self.bucket, self.prefix = uri[5:].split('/', 1)",
                "def stat_file(self, path, info):\n    def _onsuccess(boto_key):\n        checksum = boto_key.etag.strip('\"')\n        last_modified = boto_key.last_modified\n        modified_tuple = rfc822.parsedate_tz(last_modified)\n        modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n        return {'checksum': checksum, 'last_modified': modified_stamp}\n\n    return self._get_boto_key(path).addCallback(_onsuccess)",
                "def _get_boto_bucket(self):\n    from boto.s3.connection import S3Connection\n    # disable ssl (is_secure=False) because of this python bug:\n    # http://bugs.python.org/issue5103\n    c = S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)\n    return c.get_bucket(self.bucket, validate=False)",
                "def _get_boto_key(self, path):\n    b = self._get_boto_bucket()\n    key_name = '%s%s' % (self.prefix, path)\n    return threads.deferToThread(b.get_key, key_name)",
                "def persist_file(self, path, buf, info, meta=None, headers=None):\n    \"\"\"Upload file to S3 storage\"\"\"\n    b = self._get_boto_bucket()\n    key_name = '%s%s' % (self.prefix, path)\n    k = b.new_key(key_name)\n    if meta:\n        for metakey, metavalue in six.iteritems(meta):\n            k.set_metadata(metakey, str(metavalue))\n    h = self.HEADERS.copy()\n    if headers:\n        h.update(headers)\n    buf.seek(0)\n    return threads.deferToThread(k.set_contents_from_string, buf.getvalue(),\n                                 headers=h, policy=self.POLICY)",
                "def __init__(self, store_uri, download_func=None):\n    if not store_uri:\n        raise NotConfigured\n    self.store = self._get_store(store_uri)\n    super(FilesPipeline, self).__init__(download_func=download_func)",
                "@classmethod\ndef from_settings(cls, settings):\n    s3store = cls.STORE_SCHEMES['s3']\n    s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n    s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n\n    cls.FILES_URLS_FIELD = settings.get('FILES_URLS_FIELD', cls.DEFAULT_FILES_URLS_FIELD)\n    cls.FILES_RESULT_FIELD = settings.get('FILES_RESULT_FIELD', cls.DEFAULT_FILES_RESULT_FIELD)\n    cls.EXPIRES = settings.getint('FILES_EXPIRES', 90)\n    store_uri = settings['FILES_STORE']\n    return cls(store_uri)",
                "def _get_store(self, uri):\n    if os.path.isabs(uri):  # to support win32 paths like: C:\\\\some\\dir\n        scheme = 'file'\n    else:\n        scheme = urlparse(uri).scheme\n    store_cls = self.STORE_SCHEMES[scheme]\n    return store_cls(uri)",
                "def media_to_download(self, request, info):\n    def _onsuccess(result):\n        if not result:\n            return  # returning None force download\n\n        last_modified = result.get('last_modified', None)\n        if not last_modified:\n            return  # returning None force download\n\n        age_seconds = time.time() - last_modified\n        age_days = age_seconds / 60 / 60 / 24\n        if age_days > self.EXPIRES:\n            return  # returning None force download\n\n        referer = request.headers.get('Referer')\n        logger.debug(\n            'File (uptodate): Downloaded %(medianame)s from %(request)s '\n            'referred in <%(referer)s>',\n            {'medianame': self.MEDIA_NAME, 'request': request,\n             'referer': referer},\n            extra={'spider': info.spider}\n        )\n        self.inc_stats(info.spider, 'uptodate')\n\n        checksum = result.get('checksum', None)\n        return {'url': request.url, 'path': path, 'checksum': checksum}\n\n    path = self.file_path(request, info=info)\n    dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n    dfd.addCallbacks(_onsuccess, lambda _: None)\n    dfd.addErrback(\n        lambda f:\n        logger.error(self.__class__.__name__ + '.store.stat_file',\n                     extra={'spider': info.spider, 'failure': f})\n    )\n    return dfd",
                "def media_failed(self, failure, request, info):\n    if not isinstance(failure.value, IgnoreRequest):\n        referer = request.headers.get('Referer')\n        logger.warning(\n            'File (unknown-error): Error downloading %(medianame)s from '\n            '%(request)s referred in <%(referer)s>: %(exception)s',\n            {'medianame': self.MEDIA_NAME, 'request': request,\n             'referer': referer, 'exception': failure.value},\n            extra={'spider': info.spider}\n        )\n\n    raise FileException",
                "def media_downloaded(self, response, request, info):\n    referer = request.headers.get('Referer')\n\n    if response.status != 200:\n        logger.warning(\n            'File (code: %(status)s): Error downloading file from '\n            '%(request)s referred in <%(referer)s>',\n            {'status': response.status,\n             'request': request, 'referer': referer},\n            extra={'spider': info.spider}\n        )\n        raise FileException('download-error')\n\n    if not response.body:\n        logger.warning(\n            'File (empty-content): Empty file from %(request)s referred '\n            'in <%(referer)s>: no-content',\n            {'request': request, 'referer': referer},\n            extra={'spider': info.spider}\n        )\n        raise FileException('empty-content')\n\n    status = 'cached' if 'cached' in response.flags else 'downloaded'\n    logger.debug(\n        'File (%(status)s): Downloaded file from %(request)s referred in '\n        '<%(referer)s>',\n        {'status': status, 'request': request, 'referer': referer},\n        extra={'spider': info.spider}\n    )\n    self.inc_stats(info.spider, status)\n\n    try:\n        path = self.file_path(request, response=response, info=info)\n        checksum = self.file_downloaded(response, request, info)\n    except FileException as exc:\n        logger.warning(\n            'File (error): Error processing file from %(request)s '\n            'referred in <%(referer)s>: %(errormsg)s',\n            {'request': request, 'referer': referer, 'errormsg': str(exc)},\n            extra={'spider': info.spider}, exc_info=True\n        )\n        raise\n    except Exception as exc:\n        logger.error(\n            'File (unknown-error): Error processing file from %(request)s '\n            'referred in <%(referer)s>',\n            {'request': request, 'referer': referer},\n            exc_info=True, extra={'spider': info.spider}\n        )\n        raise FileException(str(exc))\n\n    return {'url': request.url, 'path': path, 'checksum': checksum}",
                "def inc_stats(self, spider, status):\n    spider.crawler.stats.inc_value('file_count', spider=spider)\n    spider.crawler.stats.inc_value('file_status_count/%s' % status, spider=spider)",
                "def get_media_requests(self, item, info):\n    return [Request(x) for x in item.get(self.FILES_URLS_FIELD, [])]",
                "def file_downloaded(self, response, request, info):\n    path = self.file_path(request, response=response, info=info)\n    buf = BytesIO(response.body)\n    self.store.persist_file(path, buf, info)\n    checksum = md5sum(buf)\n    return checksum",
                "def item_completed(self, results, item, info):\n    if isinstance(item, dict) or self.FILES_RESULT_FIELD in item.fields:\n        item[self.FILES_RESULT_FIELD] = [x for ok, x in results if ok]\n    return item",
                "def file_path(self, request, response=None, info=None):\n    ## start of deprecation warning block (can be removed in the future)\n    def _warn():\n        from scrapy.exceptions import ScrapyDeprecationWarning\n        import warnings\n        warnings.warn('FilesPipeline.file_key(url) method is deprecated, please use '\n                      'file_path(request, response=None, info=None) instead',\n                      category=ScrapyDeprecationWarning, stacklevel=1)\n\n    # check if called from file_key with url as first argument\n    if not isinstance(request, Request):\n        _warn()\n        url = request\n    else:\n        url = request.url\n\n    # detect if file_key() method has been overridden\n    if not hasattr(self.file_key, '_base'):\n        _warn()\n        return self.file_key(url)\n    ## end of deprecation warning block\n\n    media_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation\n    media_ext = os.path.splitext(url)[1]  # change to request.url after deprecation\n    return 'full/%s%s' % (media_guid, media_ext)",
                "def file_key(self, url):\n    return self.file_path(url)",
                "def _onsuccess(boto_key):\n    checksum = boto_key.etag.strip('\"')\n    last_modified = boto_key.last_modified\n    modified_tuple = rfc822.parsedate_tz(last_modified)\n    modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n    return {'checksum': checksum, 'last_modified': modified_stamp}",
                "def _onsuccess(result):\n    if not result:\n        return  # returning None force download\n\n    last_modified = result.get('last_modified', None)\n    if not last_modified:\n        return  # returning None force download\n\n    age_seconds = time.time() - last_modified\n    age_days = age_seconds / 60 / 60 / 24\n    if age_days > self.EXPIRES:\n        return  # returning None force download\n\n    referer = request.headers.get('Referer')\n    logger.debug(\n        'File (uptodate): Downloaded %(medianame)s from %(request)s '\n        'referred in <%(referer)s>',\n        {'medianame': self.MEDIA_NAME, 'request': request,\n         'referer': referer},\n        extra={'spider': info.spider}\n    )\n    self.inc_stats(info.spider, 'uptodate')\n\n    checksum = result.get('checksum', None)\n    return {'url': request.url, 'path': path, 'checksum': checksum}",
                "def _warn():\n    from scrapy.exceptions import ScrapyDeprecationWarning\n    import warnings\n    warnings.warn('FilesPipeline.file_key(url) method is deprecated, please use '\n                  'file_path(request, response=None, info=None) instead',\n                  category=ScrapyDeprecationWarning, stacklevel=1)"
            ],
            "inscope_function_signatures": [
                "__init__(self, basedir)",
                "persist_file(self, path, buf, info, meta=None, headers=None)",
                "stat_file(self, path, info)",
                "_get_filesystem_path(self, path)",
                "_mkdir(self, dirname, domain=None)",
                "__init__(self, uri)",
                "stat_file(self, path, info)",
                "_get_boto_bucket(self)",
                "_get_boto_key(self, path)",
                "persist_file(self, path, buf, info, meta=None, headers=None)",
                "__init__(self, store_uri, download_func=None)",
                "from_settings(cls, settings)",
                "_get_store(self, uri)",
                "media_to_download(self, request, info)",
                "media_failed(self, failure, request, info)",
                "media_downloaded(self, response, request, info)",
                "inc_stats(self, spider, status)",
                "get_media_requests(self, item, info)",
                "file_downloaded(self, response, request, info)",
                "item_completed(self, results, item, info)",
                "file_path(self, request, response=None, info=None)",
                "file_key(self, url)",
                "_onsuccess(boto_key)",
                "_onsuccess(result)",
                "_warn()"
            ],
            "variables_in_file": {
                "ImportError": [
                    19
                ],
                "logger": [
                    197,
                    267,
                    236,
                    275,
                    214,
                    246,
                    29,
                    222,
                    255
                ],
                "logging.getLogger": [
                    29
                ],
                "logging": [
                    29
                ],
                "__name__": [
                    29
                ],
                "Exception": [
                    32,
                    274
                ],
                "object": [
                    75,
                    36
                ],
                "basedir": [
                    40,
                    41,
                    39
                ],
                "basedir.split": [
                    40
                ],
                "self.basedir": [
                    65,
                    41,
                    42
                ],
                "self": [
                    261,
                    264,
                    265,
                    159,
                    160,
                    291,
                    294,
                    296,
                    41,
                    42,
                    43,
                    301,
                    46,
                    47,
                    302,
                    179,
                    52,
                    65,
                    193,
                    322,
                    68,
                    324,
                    200,
                    204,
                    333,
                    209,
                    210,
                    214,
                    87,
                    97,
                    225,
                    103,
                    104,
                    107,
                    108,
                    113,
                    114,
                    119,
                    124
                ],
                "self._mkdir": [
                    42,
                    47
                ],
                "self.created_directories": [
                    43,
                    68
                ],
                "defaultdict": [
                    43
                ],
                "set": [
                    43,
                    68
                ],
                "absolute_path": [
                    46,
                    47,
                    48,
                    52,
                    54,
                    58
                ],
                "self._get_filesystem_path": [
                    52,
                    46
                ],
                "path": [
                    64,
                    97,
                    294,
                    264,
                    296,
                    108,
                    46,
                    207,
                    209,
                    114,
                    210,
                    52,
                    283
                ],
                "os.path.dirname": [
                    47
                ],
                "os.path": [
                    65,
                    70,
                    328,
                    175,
                    47,
                    54
                ],
                "os": [
                    65,
                    70,
                    71,
                    328,
                    47,
                    175,
                    54
                ],
                "info": [
                    227,
                    259,
                    261,
                    294,
                    264,
                    265,
                    202,
                    296,
                    204,
                    47,
                    271,
                    209,
                    210,
                    241,
                    215,
                    250,
                    279
                ],
                "open": [
                    48,
                    58
                ],
                "f": [
                    48,
                    49,
                    215,
                    58,
                    59
                ],
                "f.write": [
                    49
                ],
                "buf.getvalue": [
                    49,
                    123
                ],
                "buf": [
                    295,
                    296,
                    297,
                    49,
                    122,
                    123
                ],
                "last_modified": [
                    188,
                    61,
                    54,
                    187,
                    92,
                    93,
                    191
                ],
                "os.path.getmtime": [
                    54
                ],
                "checksum": [
                    59,
                    265,
                    297,
                    298,
                    206,
                    207,
                    283,
                    91,
                    61,
                    95
                ],
                "md5sum": [
                    297,
                    59
                ],
                "path_comps": [
                    64,
                    65
                ],
                "path.split": [
                    64
                ],
                "os.path.join": [
                    65
                ],
                "seen": [
                    72,
                    68,
                    69
                ],
                "domain": [
                    68
                ],
                "dirname": [
                    72,
                    69,
                    70,
                    71
                ],
                "os.path.exists": [
                    70
                ],
                "os.makedirs": [
                    71
                ],
                "seen.add": [
                    72
                ],
                "AWS_ACCESS_KEY_ID": [
                    77
                ],
                "AWS_SECRET_ACCESS_KEY": [
                    78
                ],
                "POLICY": [
                    80
                ],
                "HEADERS": [
                    81
                ],
                "uri.startswith": [
                    86
                ],
                "uri": [
                    175,
                    178,
                    180,
                    86,
                    87
                ],
                "self.bucket": [
                    104,
                    87
                ],
                "self.prefix": [
                    114,
                    108,
                    87
                ],
                "split": [
                    87
                ],
                "boto_key.etag.strip": [
                    91
                ],
                "boto_key.etag": [
                    91
                ],
                "boto_key": [
                    91,
                    92
                ],
                "boto_key.last_modified": [
                    92
                ],
                "modified_tuple": [
                    93,
                    94
                ],
                "rfc822.parsedate_tz": [
                    93
                ],
                "rfc822": [
                    93,
                    94
                ],
                "modified_stamp": [
                    94,
                    95
                ],
                "int": [
                    94
                ],
                "rfc822.mktime_tz": [
                    94
                ],
                "addCallback": [
                    97
                ],
                "self._get_boto_key": [
                    97
                ],
                "_onsuccess": [
                    97,
                    211
                ],
                "c": [
                    104,
                    103
                ],
                "S3Connection": [
                    103
                ],
                "self.AWS_ACCESS_KEY_ID": [
                    103
                ],
                "self.AWS_SECRET_ACCESS_KEY": [
                    103
                ],
                "c.get_bucket": [
                    104
                ],
                "b": [
                    115,
                    113,
                    107,
                    109
                ],
                "self._get_boto_bucket": [
                    113,
                    107
                ],
                "key_name": [
                    114,
                    115,
                    108,
                    109
                ],
                "threads.deferToThread": [
                    123,
                    109
                ],
                "threads": [
                    123,
                    109
                ],
                "b.get_key": [
                    109
                ],
                "k": [
                    123,
                    115,
                    118
                ],
                "b.new_key": [
                    115
                ],
                "meta": [
                    116,
                    117
                ],
                "metakey": [
                    117,
                    118
                ],
                "metavalue": [
                    117,
                    118
                ],
                "six.iteritems": [
                    117
                ],
                "six": [
                    117
                ],
                "k.set_metadata": [
                    118
                ],
                "str": [
                    281,
                    118,
                    270
                ],
                "h": [
                    121,
                    124,
                    119
                ],
                "self.HEADERS.copy": [
                    119
                ],
                "self.HEADERS": [
                    119
                ],
                "headers": [
                    120,
                    121
                ],
                "h.update": [
                    121
                ],
                "buf.seek": [
                    122
                ],
                "k.set_contents_from_string": [
                    123
                ],
                "self.POLICY": [
                    124
                ],
                "MediaPipeline": [
                    127
                ],
                "MEDIA_NAME": [
                    146
                ],
                "EXPIRES": [
                    147
                ],
                "STORE_SCHEMES": [
                    148
                ],
                "FSFilesStore": [
                    149,
                    150
                ],
                "S3FilesStore": [
                    151
                ],
                "DEFAULT_FILES_URLS_FIELD": [
                    153
                ],
                "DEFAULT_FILES_RESULT_FIELD": [
                    154
                ],
                "store_uri": [
                    171,
                    172,
                    157,
                    159
                ],
                "NotConfigured": [
                    158
                ],
                "self.store": [
                    296,
                    210,
                    159
                ],
                "self._get_store": [
                    159
                ],
                "__init__": [
                    160
                ],
                "super": [
                    160
                ],
                "FilesPipeline": [
                    160
                ],
                "download_func": [
                    160
                ],
                "s3store": [
                    164,
                    165,
                    166
                ],
                "cls.STORE_SCHEMES": [
                    164
                ],
                "cls": [
                    164,
                    168,
                    169,
                    170,
                    172
                ],
                "s3store.AWS_ACCESS_KEY_ID": [
                    165
                ],
                "settings": [
                    165,
                    166,
                    168,
                    169,
                    170,
                    171
                ],
                "s3store.AWS_SECRET_ACCESS_KEY": [
                    166
                ],
                "cls.FILES_URLS_FIELD": [
                    168
                ],
                "settings.get": [
                    168,
                    169
                ],
                "cls.DEFAULT_FILES_URLS_FIELD": [
                    168
                ],
                "cls.FILES_RESULT_FIELD": [
                    169
                ],
                "cls.DEFAULT_FILES_RESULT_FIELD": [
                    169
                ],
                "cls.EXPIRES": [
                    170
                ],
                "settings.getint": [
                    170
                ],
                "classmethod": [
                    162
                ],
                "os.path.isabs": [
                    175
                ],
                "scheme": [
                    176,
                    178,
                    179
                ],
                "urlparse": [
                    178
                ],
                "store_cls": [
                    179,
                    180
                ],
                "self.STORE_SCHEMES": [
                    179
                ],
                "result": [
                    184,
                    187,
                    206
                ],
                "result.get": [
                    187,
                    206
                ],
                "age_seconds": [
                    192,
                    191
                ],
                "time.time": [
                    191
                ],
                "time": [
                    191
                ],
                "age_days": [
                    192,
                    193
                ],
                "self.EXPIRES": [
                    193
                ],
                "referer": [
                    226,
                    258,
                    196,
                    201,
                    233,
                    270,
                    240,
                    278,
                    249,
                    221
                ],
                "request.headers.get": [
                    233,
                    196,
                    221
                ],
                "request.headers": [
                    233,
                    196,
                    221
                ],
                "request": [
                    258,
                    264,
                    265,
                    270,
                    278,
                    283,
                    294,
                    315,
                    317,
                    319,
                    196,
                    200,
                    207,
                    209,
                    221,
                    225,
                    233,
                    240,
                    249
                ],
                "logger.debug": [
                    197,
                    255
                ],
                "self.MEDIA_NAME": [
                    200,
                    225
                ],
                "info.spider": [
                    227,
                    259,
                    261,
                    202,
                    204,
                    271,
                    241,
                    215,
                    250,
                    279
                ],
                "self.inc_stats": [
                    204,
                    261
                ],
                "request.url": [
                    283,
                    319,
                    207
                ],
                "self.file_path": [
                    264,
                    209,
                    333,
                    294
                ],
                "dfd": [
                    217,
                    210,
                    211,
                    212
                ],
                "defer.maybeDeferred": [
                    210
                ],
                "defer": [
                    210
                ],
                "self.store.stat_file": [
                    210
                ],
                "dfd.addCallbacks": [
                    211
                ],
                "dfd.addErrback": [
                    212
                ],
                "logger.error": [
                    275,
                    214
                ],
                "self.__class__.__name__": [
                    214
                ],
                "self.__class__": [
                    214
                ],
                "isinstance": [
                    315,
                    220,
                    301
                ],
                "failure.value": [
                    226,
                    220
                ],
                "failure": [
                    226,
                    220
                ],
                "IgnoreRequest": [
                    220
                ],
                "logger.warning": [
                    267,
                    236,
                    222,
                    246
                ],
                "FileException": [
                    230,
                    266,
                    243,
                    281,
                    252
                ],
                "response.status": [
                    235,
                    239
                ],
                "response": [
                    294,
                    295,
                    264,
                    265,
                    235,
                    239,
                    245,
                    254
                ],
                "response.body": [
                    245,
                    295
                ],
                "status": [
                    258,
                    261,
                    254,
                    287
                ],
                "response.flags": [
                    254
                ],
                "self.file_downloaded": [
                    265
                ],
                "exc": [
                    281,
                    270
                ],
                "spider.crawler.stats.inc_value": [
                    286,
                    287
                ],
                "spider.crawler.stats": [
                    286,
                    287
                ],
                "spider.crawler": [
                    286,
                    287
                ],
                "spider": [
                    286,
                    287
                ],
                "Request": [
                    315,
                    291
                ],
                "x": [
                    291,
                    302
                ],
                "item.get": [
                    291
                ],
                "item": [
                    291,
                    301,
                    302,
                    303
                ],
                "self.FILES_URLS_FIELD": [
                    291
                ],
                "BytesIO": [
                    295
                ],
                "self.store.persist_file": [
                    296
                ],
                "dict": [
                    301
                ],
                "self.FILES_RESULT_FIELD": [
                    301,
                    302
                ],
                "item.fields": [
                    301
                ],
                "ok": [
                    302
                ],
                "results": [
                    302
                ],
                "warnings.warn": [
                    310
                ],
                "warnings": [
                    310
                ],
                "ScrapyDeprecationWarning": [
                    312
                ],
                "_warn": [
                    323,
                    316
                ],
                "url": [
                    324,
                    327,
                    328,
                    333,
                    317,
                    319
                ],
                "hasattr": [
                    322
                ],
                "self.file_key": [
                    322,
                    324
                ],
                "media_guid": [
                    329,
                    327
                ],
                "hexdigest": [
                    327
                ],
                "hashlib.sha1": [
                    327
                ],
                "hashlib": [
                    327
                ],
                "media_ext": [
                    328,
                    329
                ],
                "os.path.splitext": [
                    328
                ],
                "file_key._base": [
                    334
                ],
                "file_key": [
                    334
                ]
            },
            "filtered_variables_in_file": {
                "logger": [
                    197,
                    267,
                    236,
                    275,
                    214,
                    246,
                    29,
                    222,
                    255
                ],
                "logging.getLogger": [
                    29
                ],
                "logging": [
                    29
                ],
                "basedir": [
                    40,
                    41,
                    39
                ],
                "basedir.split": [
                    40
                ],
                "self.basedir": [
                    65,
                    41,
                    42
                ],
                "self": [
                    261,
                    264,
                    265,
                    159,
                    160,
                    291,
                    294,
                    296,
                    41,
                    42,
                    43,
                    301,
                    46,
                    47,
                    302,
                    179,
                    52,
                    65,
                    193,
                    322,
                    68,
                    324,
                    200,
                    204,
                    333,
                    209,
                    210,
                    214,
                    87,
                    97,
                    225,
                    103,
                    104,
                    107,
                    108,
                    113,
                    114,
                    119,
                    124
                ],
                "self._mkdir": [
                    42,
                    47
                ],
                "self.created_directories": [
                    43,
                    68
                ],
                "defaultdict": [
                    43
                ],
                "absolute_path": [
                    46,
                    47,
                    48,
                    52,
                    54,
                    58
                ],
                "self._get_filesystem_path": [
                    52,
                    46
                ],
                "path": [
                    64,
                    97,
                    294,
                    264,
                    296,
                    108,
                    46,
                    207,
                    209,
                    114,
                    210,
                    52,
                    283
                ],
                "os.path.dirname": [
                    47
                ],
                "os.path": [
                    65,
                    70,
                    328,
                    175,
                    47,
                    54
                ],
                "os": [
                    65,
                    70,
                    71,
                    328,
                    47,
                    175,
                    54
                ],
                "info": [
                    227,
                    259,
                    261,
                    294,
                    264,
                    265,
                    202,
                    296,
                    204,
                    47,
                    271,
                    209,
                    210,
                    241,
                    215,
                    250,
                    279
                ],
                "f": [
                    48,
                    49,
                    215,
                    58,
                    59
                ],
                "f.write": [
                    49
                ],
                "buf.getvalue": [
                    49,
                    123
                ],
                "buf": [
                    295,
                    296,
                    297,
                    49,
                    122,
                    123
                ],
                "last_modified": [
                    188,
                    61,
                    54,
                    187,
                    92,
                    93,
                    191
                ],
                "os.path.getmtime": [
                    54
                ],
                "checksum": [
                    59,
                    265,
                    297,
                    298,
                    206,
                    207,
                    283,
                    91,
                    61,
                    95
                ],
                "md5sum": [
                    297,
                    59
                ],
                "path_comps": [
                    64,
                    65
                ],
                "path.split": [
                    64
                ],
                "os.path.join": [
                    65
                ],
                "seen": [
                    72,
                    68,
                    69
                ],
                "domain": [
                    68
                ],
                "dirname": [
                    72,
                    69,
                    70,
                    71
                ],
                "os.path.exists": [
                    70
                ],
                "os.makedirs": [
                    71
                ],
                "seen.add": [
                    72
                ],
                "AWS_ACCESS_KEY_ID": [
                    77
                ],
                "AWS_SECRET_ACCESS_KEY": [
                    78
                ],
                "POLICY": [
                    80
                ],
                "HEADERS": [
                    81
                ],
                "uri.startswith": [
                    86
                ],
                "uri": [
                    175,
                    178,
                    180,
                    86,
                    87
                ],
                "self.bucket": [
                    104,
                    87
                ],
                "self.prefix": [
                    114,
                    108,
                    87
                ],
                "split": [
                    87
                ],
                "boto_key.etag.strip": [
                    91
                ],
                "boto_key.etag": [
                    91
                ],
                "boto_key": [
                    91,
                    92
                ],
                "boto_key.last_modified": [
                    92
                ],
                "modified_tuple": [
                    93,
                    94
                ],
                "rfc822.parsedate_tz": [
                    93
                ],
                "rfc822": [
                    93,
                    94
                ],
                "modified_stamp": [
                    94,
                    95
                ],
                "rfc822.mktime_tz": [
                    94
                ],
                "addCallback": [
                    97
                ],
                "self._get_boto_key": [
                    97
                ],
                "_onsuccess": [
                    97,
                    211
                ],
                "c": [
                    104,
                    103
                ],
                "S3Connection": [
                    103
                ],
                "self.AWS_ACCESS_KEY_ID": [
                    103
                ],
                "self.AWS_SECRET_ACCESS_KEY": [
                    103
                ],
                "c.get_bucket": [
                    104
                ],
                "b": [
                    115,
                    113,
                    107,
                    109
                ],
                "self._get_boto_bucket": [
                    113,
                    107
                ],
                "key_name": [
                    114,
                    115,
                    108,
                    109
                ],
                "threads.deferToThread": [
                    123,
                    109
                ],
                "threads": [
                    123,
                    109
                ],
                "b.get_key": [
                    109
                ],
                "k": [
                    123,
                    115,
                    118
                ],
                "b.new_key": [
                    115
                ],
                "meta": [
                    116,
                    117
                ],
                "metakey": [
                    117,
                    118
                ],
                "metavalue": [
                    117,
                    118
                ],
                "six.iteritems": [
                    117
                ],
                "six": [
                    117
                ],
                "k.set_metadata": [
                    118
                ],
                "h": [
                    121,
                    124,
                    119
                ],
                "self.HEADERS.copy": [
                    119
                ],
                "self.HEADERS": [
                    119
                ],
                "headers": [
                    120,
                    121
                ],
                "h.update": [
                    121
                ],
                "buf.seek": [
                    122
                ],
                "k.set_contents_from_string": [
                    123
                ],
                "self.POLICY": [
                    124
                ],
                "MediaPipeline": [
                    127
                ],
                "MEDIA_NAME": [
                    146
                ],
                "EXPIRES": [
                    147
                ],
                "STORE_SCHEMES": [
                    148
                ],
                "FSFilesStore": [
                    149,
                    150
                ],
                "S3FilesStore": [
                    151
                ],
                "DEFAULT_FILES_URLS_FIELD": [
                    153
                ],
                "DEFAULT_FILES_RESULT_FIELD": [
                    154
                ],
                "store_uri": [
                    171,
                    172,
                    157,
                    159
                ],
                "NotConfigured": [
                    158
                ],
                "self.store": [
                    296,
                    210,
                    159
                ],
                "self._get_store": [
                    159
                ],
                "__init__": [
                    160
                ],
                "FilesPipeline": [
                    160
                ],
                "download_func": [
                    160
                ],
                "s3store": [
                    164,
                    165,
                    166
                ],
                "cls.STORE_SCHEMES": [
                    164
                ],
                "cls": [
                    164,
                    168,
                    169,
                    170,
                    172
                ],
                "s3store.AWS_ACCESS_KEY_ID": [
                    165
                ],
                "settings": [
                    165,
                    166,
                    168,
                    169,
                    170,
                    171
                ],
                "s3store.AWS_SECRET_ACCESS_KEY": [
                    166
                ],
                "cls.FILES_URLS_FIELD": [
                    168
                ],
                "settings.get": [
                    168,
                    169
                ],
                "cls.DEFAULT_FILES_URLS_FIELD": [
                    168
                ],
                "cls.FILES_RESULT_FIELD": [
                    169
                ],
                "cls.DEFAULT_FILES_RESULT_FIELD": [
                    169
                ],
                "cls.EXPIRES": [
                    170
                ],
                "settings.getint": [
                    170
                ],
                "os.path.isabs": [
                    175
                ],
                "scheme": [
                    176,
                    178,
                    179
                ],
                "urlparse": [
                    178
                ],
                "store_cls": [
                    179,
                    180
                ],
                "self.STORE_SCHEMES": [
                    179
                ],
                "result": [
                    184,
                    187,
                    206
                ],
                "result.get": [
                    187,
                    206
                ],
                "age_seconds": [
                    192,
                    191
                ],
                "time.time": [
                    191
                ],
                "time": [
                    191
                ],
                "age_days": [
                    192,
                    193
                ],
                "self.EXPIRES": [
                    193
                ],
                "referer": [
                    226,
                    258,
                    196,
                    201,
                    233,
                    270,
                    240,
                    278,
                    249,
                    221
                ],
                "request.headers.get": [
                    233,
                    196,
                    221
                ],
                "request.headers": [
                    233,
                    196,
                    221
                ],
                "request": [
                    258,
                    264,
                    265,
                    270,
                    278,
                    283,
                    294,
                    315,
                    317,
                    319,
                    196,
                    200,
                    207,
                    209,
                    221,
                    225,
                    233,
                    240,
                    249
                ],
                "logger.debug": [
                    197,
                    255
                ],
                "self.MEDIA_NAME": [
                    200,
                    225
                ],
                "info.spider": [
                    227,
                    259,
                    261,
                    202,
                    204,
                    271,
                    241,
                    215,
                    250,
                    279
                ],
                "self.inc_stats": [
                    204,
                    261
                ],
                "request.url": [
                    283,
                    319,
                    207
                ],
                "self.file_path": [
                    264,
                    209,
                    333,
                    294
                ],
                "dfd": [
                    217,
                    210,
                    211,
                    212
                ],
                "defer.maybeDeferred": [
                    210
                ],
                "defer": [
                    210
                ],
                "self.store.stat_file": [
                    210
                ],
                "dfd.addCallbacks": [
                    211
                ],
                "dfd.addErrback": [
                    212
                ],
                "logger.error": [
                    275,
                    214
                ],
                "self.__class__.__name__": [
                    214
                ],
                "self.__class__": [
                    214
                ],
                "failure.value": [
                    226,
                    220
                ],
                "failure": [
                    226,
                    220
                ],
                "IgnoreRequest": [
                    220
                ],
                "logger.warning": [
                    267,
                    236,
                    222,
                    246
                ],
                "FileException": [
                    230,
                    266,
                    243,
                    281,
                    252
                ],
                "response.status": [
                    235,
                    239
                ],
                "response": [
                    294,
                    295,
                    264,
                    265,
                    235,
                    239,
                    245,
                    254
                ],
                "response.body": [
                    245,
                    295
                ],
                "status": [
                    258,
                    261,
                    254,
                    287
                ],
                "response.flags": [
                    254
                ],
                "self.file_downloaded": [
                    265
                ],
                "exc": [
                    281,
                    270
                ],
                "spider.crawler.stats.inc_value": [
                    286,
                    287
                ],
                "spider.crawler.stats": [
                    286,
                    287
                ],
                "spider.crawler": [
                    286,
                    287
                ],
                "spider": [
                    286,
                    287
                ],
                "Request": [
                    315,
                    291
                ],
                "x": [
                    291,
                    302
                ],
                "item.get": [
                    291
                ],
                "item": [
                    291,
                    301,
                    302,
                    303
                ],
                "self.FILES_URLS_FIELD": [
                    291
                ],
                "BytesIO": [
                    295
                ],
                "self.store.persist_file": [
                    296
                ],
                "self.FILES_RESULT_FIELD": [
                    301,
                    302
                ],
                "item.fields": [
                    301
                ],
                "ok": [
                    302
                ],
                "results": [
                    302
                ],
                "warnings.warn": [
                    310
                ],
                "warnings": [
                    310
                ],
                "ScrapyDeprecationWarning": [
                    312
                ],
                "_warn": [
                    323,
                    316
                ],
                "url": [
                    324,
                    327,
                    328,
                    333,
                    317,
                    319
                ],
                "self.file_key": [
                    322,
                    324
                ],
                "media_guid": [
                    329,
                    327
                ],
                "hexdigest": [
                    327
                ],
                "hashlib.sha1": [
                    327
                ],
                "hashlib": [
                    327
                ],
                "media_ext": [
                    328,
                    329
                ],
                "os.path.splitext": [
                    328
                ],
                "file_key._base": [
                    334
                ],
                "file_key": [
                    334
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs/repos/scrapy_33/scrapy/pipelines/media.py": {
            "buggy_functions": [
                {
                    "function_name": "_process_request",
                    "function_code": "def _process_request(self, request, info):\n    fp = request_fingerprint(request)\n    cb = request.callback or (lambda _: _)\n    eb = request.errback\n    request.callback = None\n    request.errback = None\n\n    # Return cached result if request was already seen\n    if fp in info.downloaded:\n        return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n\n    # Otherwise, wait for result\n    wad = Deferred().addCallbacks(cb, eb)\n    info.waiting[fp].append(wad)\n\n    # Check if request is downloading right now to avoid doing it twice\n    if fp in info.downloading:\n        return wad\n\n    # Download request checking media_to_download hook output first\n    info.downloading.add(fp)\n    dfd = mustbe_deferred(self.media_to_download, request, info)\n    dfd.addCallback(self._check_media_to_download, request, info)\n    dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n    dfd.addErrback(lambda f: logger.error(\n        f.value, extra={'spider': info.spider, 'failure': f})\n    )\n    return dfd.addBoth(lambda _: wad)  # it must return wad at last\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 48,
                    "end_line": 75,
                    "variables": {
                        "fp": [
                            64,
                            68,
                            71,
                            49,
                            56,
                            57,
                            61
                        ],
                        "request_fingerprint": [
                            49
                        ],
                        "request": [
                            69,
                            70,
                            49,
                            50,
                            51,
                            52,
                            53
                        ],
                        "cb": [
                            57,
                            50,
                            60
                        ],
                        "request.callback": [
                            50,
                            52
                        ],
                        "_": [
                            50
                        ],
                        "eb": [
                            57,
                            51,
                            60
                        ],
                        "request.errback": [
                            51,
                            53
                        ],
                        "info.downloaded": [
                            56,
                            57
                        ],
                        "info": [
                            64,
                            68,
                            69,
                            70,
                            71,
                            73,
                            56,
                            57,
                            61
                        ],
                        "addCallbacks": [
                            57,
                            60
                        ],
                        "defer_result": [
                            57
                        ],
                        "wad": [
                            65,
                            75,
                            60,
                            61
                        ],
                        "Deferred": [
                            60
                        ],
                        "append": [
                            61
                        ],
                        "info.waiting": [
                            61
                        ],
                        "info.downloading": [
                            64,
                            68
                        ],
                        "info.downloading.add": [
                            68
                        ],
                        "dfd": [
                            69,
                            70,
                            71,
                            72,
                            75
                        ],
                        "mustbe_deferred": [
                            69
                        ],
                        "self.media_to_download": [
                            69
                        ],
                        "self": [
                            69,
                            70,
                            71
                        ],
                        "dfd.addCallback": [
                            70
                        ],
                        "self._check_media_to_download": [
                            70
                        ],
                        "dfd.addBoth": [
                            75,
                            71
                        ],
                        "self._cache_result_and_execute_waiters": [
                            71
                        ],
                        "dfd.addErrback": [
                            72
                        ],
                        "logger.error": [
                            72
                        ],
                        "logger": [
                            72
                        ],
                        "f.value": [
                            73
                        ],
                        "f": [
                            73
                        ],
                        "info.spider": [
                            73
                        ]
                    },
                    "filtered_variables": {
                        "fp": [
                            64,
                            68,
                            71,
                            49,
                            56,
                            57,
                            61
                        ],
                        "request_fingerprint": [
                            49
                        ],
                        "request": [
                            69,
                            70,
                            49,
                            50,
                            51,
                            52,
                            53
                        ],
                        "cb": [
                            57,
                            50,
                            60
                        ],
                        "request.callback": [
                            50,
                            52
                        ],
                        "_": [
                            50
                        ],
                        "eb": [
                            57,
                            51,
                            60
                        ],
                        "request.errback": [
                            51,
                            53
                        ],
                        "info.downloaded": [
                            56,
                            57
                        ],
                        "info": [
                            64,
                            68,
                            69,
                            70,
                            71,
                            73,
                            56,
                            57,
                            61
                        ],
                        "addCallbacks": [
                            57,
                            60
                        ],
                        "defer_result": [
                            57
                        ],
                        "wad": [
                            65,
                            75,
                            60,
                            61
                        ],
                        "Deferred": [
                            60
                        ],
                        "append": [
                            61
                        ],
                        "info.waiting": [
                            61
                        ],
                        "info.downloading": [
                            64,
                            68
                        ],
                        "info.downloading.add": [
                            68
                        ],
                        "dfd": [
                            69,
                            70,
                            71,
                            72,
                            75
                        ],
                        "mustbe_deferred": [
                            69
                        ],
                        "self.media_to_download": [
                            69
                        ],
                        "self": [
                            69,
                            70,
                            71
                        ],
                        "dfd.addCallback": [
                            70
                        ],
                        "self._check_media_to_download": [
                            70
                        ],
                        "dfd.addBoth": [
                            75,
                            71
                        ],
                        "self._cache_result_and_execute_waiters": [
                            71
                        ],
                        "dfd.addErrback": [
                            72
                        ],
                        "logger.error": [
                            72
                        ],
                        "logger": [
                            72
                        ],
                        "f.value": [
                            73
                        ],
                        "f": [
                            73
                        ],
                        "info.spider": [
                            73
                        ]
                    },
                    "diff_line_number": 73,
                    "class_data": {
                        "signature": "class MediaPipeline(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, download_func=None):\n    self.download_func = download_func",
                            "@classmethod\ndef from_crawler(cls, crawler):\n    try:\n        pipe = cls.from_settings(crawler.settings)\n    except AttributeError:\n        pipe = cls()\n    pipe.crawler = crawler\n    return pipe",
                            "def open_spider(self, spider):\n    self.spiderinfo = self.SpiderInfo(spider)",
                            "def process_item(self, item, spider):\n    info = self.spiderinfo\n    requests = arg_to_iter(self.get_media_requests(item, info))\n    dlist = [self._process_request(r, info) for r in requests]\n    dfd = DeferredList(dlist, consumeErrors=1)\n    return dfd.addCallback(self.item_completed, item, info)",
                            "def _process_request(self, request, info):\n    fp = request_fingerprint(request)\n    cb = request.callback or (lambda _: _)\n    eb = request.errback\n    request.callback = None\n    request.errback = None\n    if fp in info.downloaded:\n        return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n    wad = Deferred().addCallbacks(cb, eb)\n    info.waiting[fp].append(wad)\n    if fp in info.downloading:\n        return wad\n    info.downloading.add(fp)\n    dfd = mustbe_deferred(self.media_to_download, request, info)\n    dfd.addCallback(self._check_media_to_download, request, info)\n    dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n    dfd.addErrback(lambda f: logger.error(f.value, extra={'spider': info.spider, 'failure': f}))\n    return dfd.addBoth(lambda _: wad)",
                            "def _check_media_to_download(self, result, request, info):\n    if result is not None:\n        return result\n    if self.download_func:\n        dfd = mustbe_deferred(self.download_func, request, info.spider)\n        dfd.addCallbacks(callback=self.media_downloaded, callbackArgs=(request, info), errback=self.media_failed, errbackArgs=(request, info))\n    else:\n        request.meta['handle_httpstatus_all'] = True\n        dfd = self.crawler.engine.download(request, info.spider)\n        dfd.addCallbacks(callback=self.media_downloaded, callbackArgs=(request, info), errback=self.media_failed, errbackArgs=(request, info))\n    return dfd",
                            "def _cache_result_and_execute_waiters(self, result, fp, info):\n    if isinstance(result, Failure):\n        result.cleanFailure()\n        result.frames = []\n        result.stack = None\n    info.downloading.remove(fp)\n    info.downloaded[fp] = result\n    for wad in info.waiting.pop(fp):\n        defer_result(result).chainDeferred(wad)",
                            "def media_to_download(self, request, info):\n    \"\"\"Check request before starting download\"\"\"\n    pass",
                            "def get_media_requests(self, item, info):\n    \"\"\"Returns the media requests to download\"\"\"\n    pass",
                            "def media_downloaded(self, response, request, info):\n    \"\"\"Handler for success downloads\"\"\"\n    return response",
                            "def media_failed(self, failure, request, info):\n    \"\"\"Handler for failed downloads\"\"\"\n    return failure",
                            "def item_completed(self, results, item, info):\n    \"\"\"Called per item when all media requests has been processed\"\"\"\n    if self.LOG_FAILED_RESULTS:\n        for ok, value in results:\n            if not ok:\n                logger.error('%(class)s found errors processing %(item)s', {'class': self.__class__.__name__, 'item': item}, extra={'spider': info.spider, 'failure': value})\n    return item",
                            "def __init__(self, spider):\n    self.spider = spider\n    self.downloading = set()\n    self.downloaded = {}\n    self.waiting = defaultdict(list)"
                        ],
                        "constructor_variables": [
                            "download_func"
                        ],
                        "class_level_variables": [
                            "LOG_FAILED_RESULTS"
                        ],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, download_func=None)",
                            "from_crawler(cls, crawler)",
                            "open_spider(self, spider)",
                            "process_item(self, item, spider)",
                            "_process_request(self, request, info)",
                            "_check_media_to_download(self, result, request, info)",
                            "_cache_result_and_execute_waiters(self, result, fp, info)",
                            "media_to_download(self, request, info)",
                            "get_media_requests(self, item, info)",
                            "media_downloaded(self, response, request, info)",
                            "media_failed(self, failure, request, info)",
                            "item_completed(self, results, item, info)",
                            "__init__(self, spider)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "item_completed",
                    "function_code": "def item_completed(self, results, item, info):\n    \"\"\"Called per item when all media requests has been processed\"\"\"\n    if self.LOG_FAILED_RESULTS:\n        for ok, value in results:\n            if not ok:\n                logger.error(\n                    '%(class)s found errors processing %(item)s',\n                    {'class': self.__class__.__name__, 'item': item},\n                    extra={'spider': info.spider, 'failure': value}\n                )\n    return item\n",
                    "decorators": [],
                    "docstring": "Called per item when all media requests has been processed",
                    "start_line": 122,
                    "end_line": 132,
                    "variables": {
                        "self.LOG_FAILED_RESULTS": [
                            124
                        ],
                        "self": [
                            129,
                            124
                        ],
                        "ok": [
                            125,
                            126
                        ],
                        "value": [
                            130,
                            125
                        ],
                        "results": [
                            125
                        ],
                        "logger.error": [
                            127
                        ],
                        "logger": [
                            127
                        ],
                        "self.__class__.__name__": [
                            129
                        ],
                        "self.__class__": [
                            129
                        ],
                        "item": [
                            129,
                            132
                        ],
                        "info.spider": [
                            130
                        ],
                        "info": [
                            130
                        ]
                    },
                    "filtered_variables": {
                        "self.LOG_FAILED_RESULTS": [
                            124
                        ],
                        "self": [
                            129,
                            124
                        ],
                        "ok": [
                            125,
                            126
                        ],
                        "value": [
                            130,
                            125
                        ],
                        "results": [
                            125
                        ],
                        "logger.error": [
                            127
                        ],
                        "logger": [
                            127
                        ],
                        "self.__class__.__name__": [
                            129
                        ],
                        "self.__class__": [
                            129
                        ],
                        "item": [
                            129,
                            132
                        ],
                        "info.spider": [
                            130
                        ],
                        "info": [
                            130
                        ]
                    },
                    "diff_line_number": 130,
                    "class_data": {
                        "signature": "class MediaPipeline(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, download_func=None):\n    self.download_func = download_func",
                            "@classmethod\ndef from_crawler(cls, crawler):\n    try:\n        pipe = cls.from_settings(crawler.settings)\n    except AttributeError:\n        pipe = cls()\n    pipe.crawler = crawler\n    return pipe",
                            "def open_spider(self, spider):\n    self.spiderinfo = self.SpiderInfo(spider)",
                            "def process_item(self, item, spider):\n    info = self.spiderinfo\n    requests = arg_to_iter(self.get_media_requests(item, info))\n    dlist = [self._process_request(r, info) for r in requests]\n    dfd = DeferredList(dlist, consumeErrors=1)\n    return dfd.addCallback(self.item_completed, item, info)",
                            "def _process_request(self, request, info):\n    fp = request_fingerprint(request)\n    cb = request.callback or (lambda _: _)\n    eb = request.errback\n    request.callback = None\n    request.errback = None\n    if fp in info.downloaded:\n        return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n    wad = Deferred().addCallbacks(cb, eb)\n    info.waiting[fp].append(wad)\n    if fp in info.downloading:\n        return wad\n    info.downloading.add(fp)\n    dfd = mustbe_deferred(self.media_to_download, request, info)\n    dfd.addCallback(self._check_media_to_download, request, info)\n    dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n    dfd.addErrback(lambda f: logger.error(f.value, extra={'spider': info.spider, 'failure': f}))\n    return dfd.addBoth(lambda _: wad)",
                            "def _check_media_to_download(self, result, request, info):\n    if result is not None:\n        return result\n    if self.download_func:\n        dfd = mustbe_deferred(self.download_func, request, info.spider)\n        dfd.addCallbacks(callback=self.media_downloaded, callbackArgs=(request, info), errback=self.media_failed, errbackArgs=(request, info))\n    else:\n        request.meta['handle_httpstatus_all'] = True\n        dfd = self.crawler.engine.download(request, info.spider)\n        dfd.addCallbacks(callback=self.media_downloaded, callbackArgs=(request, info), errback=self.media_failed, errbackArgs=(request, info))\n    return dfd",
                            "def _cache_result_and_execute_waiters(self, result, fp, info):\n    if isinstance(result, Failure):\n        result.cleanFailure()\n        result.frames = []\n        result.stack = None\n    info.downloading.remove(fp)\n    info.downloaded[fp] = result\n    for wad in info.waiting.pop(fp):\n        defer_result(result).chainDeferred(wad)",
                            "def media_to_download(self, request, info):\n    \"\"\"Check request before starting download\"\"\"\n    pass",
                            "def get_media_requests(self, item, info):\n    \"\"\"Returns the media requests to download\"\"\"\n    pass",
                            "def media_downloaded(self, response, request, info):\n    \"\"\"Handler for success downloads\"\"\"\n    return response",
                            "def media_failed(self, failure, request, info):\n    \"\"\"Handler for failed downloads\"\"\"\n    return failure",
                            "def item_completed(self, results, item, info):\n    \"\"\"Called per item when all media requests has been processed\"\"\"\n    if self.LOG_FAILED_RESULTS:\n        for ok, value in results:\n            if not ok:\n                logger.error('%(class)s found errors processing %(item)s', {'class': self.__class__.__name__, 'item': item}, extra={'spider': info.spider, 'failure': value})\n    return item",
                            "def __init__(self, spider):\n    self.spider = spider\n    self.downloading = set()\n    self.downloaded = {}\n    self.waiting = defaultdict(list)"
                        ],
                        "constructor_variables": [
                            "download_func"
                        ],
                        "class_level_variables": [
                            "LOG_FAILED_RESULTS"
                        ],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, download_func=None)",
                            "from_crawler(cls, crawler)",
                            "open_spider(self, spider)",
                            "process_item(self, item, spider)",
                            "_process_request(self, request, info)",
                            "_check_media_to_download(self, result, request, info)",
                            "_cache_result_and_execute_waiters(self, result, fp, info)",
                            "media_to_download(self, request, info)",
                            "get_media_requests(self, item, info)",
                            "media_downloaded(self, response, request, info)",
                            "media_failed(self, failure, request, info)",
                            "item_completed(self, results, item, info)",
                            "__init__(self, spider)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "from scrapy.utils.misc import arg_to_iter",
                    "start_line": 10,
                    "end_line": 11
                }
            ],
            "inscope_functions": [
                "def __init__(self, download_func=None):\n    self.download_func = download_func",
                "@classmethod\ndef from_crawler(cls, crawler):\n    try:\n        pipe = cls.from_settings(crawler.settings)\n    except AttributeError:\n        pipe = cls()\n    pipe.crawler = crawler\n    return pipe",
                "def open_spider(self, spider):\n    self.spiderinfo = self.SpiderInfo(spider)",
                "def process_item(self, item, spider):\n    info = self.spiderinfo\n    requests = arg_to_iter(self.get_media_requests(item, info))\n    dlist = [self._process_request(r, info) for r in requests]\n    dfd = DeferredList(dlist, consumeErrors=1)\n    return dfd.addCallback(self.item_completed, item, info)",
                "def _process_request(self, request, info):\n    fp = request_fingerprint(request)\n    cb = request.callback or (lambda _: _)\n    eb = request.errback\n    request.callback = None\n    request.errback = None\n\n    # Return cached result if request was already seen\n    if fp in info.downloaded:\n        return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n\n    # Otherwise, wait for result\n    wad = Deferred().addCallbacks(cb, eb)\n    info.waiting[fp].append(wad)\n\n    # Check if request is downloading right now to avoid doing it twice\n    if fp in info.downloading:\n        return wad\n\n    # Download request checking media_to_download hook output first\n    info.downloading.add(fp)\n    dfd = mustbe_deferred(self.media_to_download, request, info)\n    dfd.addCallback(self._check_media_to_download, request, info)\n    dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n    dfd.addErrback(lambda f: logger.error(\n        f.value, extra={'spider': info.spider, 'failure': f})\n    )\n    return dfd.addBoth(lambda _: wad)  # it must return wad at last",
                "def _check_media_to_download(self, result, request, info):\n    if result is not None:\n        return result\n    if self.download_func:\n        # this ugly code was left only to support tests. TODO: remove\n        dfd = mustbe_deferred(self.download_func, request, info.spider)\n        dfd.addCallbacks(\n            callback=self.media_downloaded, callbackArgs=(request, info),\n            errback=self.media_failed, errbackArgs=(request, info))\n    else:\n        request.meta['handle_httpstatus_all'] = True\n        dfd = self.crawler.engine.download(request, info.spider)\n        dfd.addCallbacks(\n            callback=self.media_downloaded, callbackArgs=(request, info),\n            errback=self.media_failed, errbackArgs=(request, info))\n    return dfd",
                "def _cache_result_and_execute_waiters(self, result, fp, info):\n    if isinstance(result, Failure):\n        # minimize cached information for failure\n        result.cleanFailure()\n        result.frames = []\n        result.stack = None\n    info.downloading.remove(fp)\n    info.downloaded[fp] = result  # cache result\n    for wad in info.waiting.pop(fp):\n        defer_result(result).chainDeferred(wad)",
                "def media_to_download(self, request, info):\n    \"\"\"Check request before starting download\"\"\"\n    pass",
                "def get_media_requests(self, item, info):\n    \"\"\"Returns the media requests to download\"\"\"\n    pass",
                "def media_downloaded(self, response, request, info):\n    \"\"\"Handler for success downloads\"\"\"\n    return response",
                "def media_failed(self, failure, request, info):\n    \"\"\"Handler for failed downloads\"\"\"\n    return failure",
                "def item_completed(self, results, item, info):\n    \"\"\"Called per item when all media requests has been processed\"\"\"\n    if self.LOG_FAILED_RESULTS:\n        for ok, value in results:\n            if not ok:\n                logger.error(\n                    '%(class)s found errors processing %(item)s',\n                    {'class': self.__class__.__name__, 'item': item},\n                    extra={'spider': info.spider, 'failure': value}\n                )\n    return item",
                "def __init__(self, spider):\n    self.spider = spider\n    self.downloading = set()\n    self.downloaded = {}\n    self.waiting = defaultdict(list)"
            ],
            "inscope_function_signatures": [
                "__init__(self, download_func=None)",
                "from_crawler(cls, crawler)",
                "open_spider(self, spider)",
                "process_item(self, item, spider)",
                "_process_request(self, request, info)",
                "_check_media_to_download(self, result, request, info)",
                "_cache_result_and_execute_waiters(self, result, fp, info)",
                "media_to_download(self, request, info)",
                "get_media_requests(self, item, info)",
                "media_downloaded(self, response, request, info)",
                "media_failed(self, failure, request, info)",
                "item_completed(self, results, item, info)",
                "__init__(self, spider)"
            ],
            "variables_in_file": {
                "logger": [
                    72,
                    12,
                    127
                ],
                "logging.getLogger": [
                    12
                ],
                "logging": [
                    12
                ],
                "__name__": [
                    12
                ],
                "object": [
                    19,
                    15
                ],
                "LOG_FAILED_RESULTS": [
                    17
                ],
                "self.spider": [
                    21
                ],
                "self": [
                    129,
                    21,
                    22,
                    23,
                    24,
                    27,
                    39,
                    42,
                    43,
                    44,
                    46,
                    69,
                    70,
                    71,
                    80,
                    82,
                    84,
                    85,
                    88,
                    90,
                    91,
                    124
                ],
                "spider": [
                    21,
                    39
                ],
                "self.downloading": [
                    22
                ],
                "set": [
                    22
                ],
                "self.downloaded": [
                    23
                ],
                "self.waiting": [
                    24
                ],
                "defaultdict": [
                    24
                ],
                "list": [
                    24
                ],
                "self.download_func": [
                    80,
                    82,
                    27
                ],
                "download_func": [
                    27
                ],
                "pipe": [
                    32,
                    34,
                    35,
                    36
                ],
                "cls.from_settings": [
                    32
                ],
                "cls": [
                    32,
                    34
                ],
                "crawler.settings": [
                    32
                ],
                "crawler": [
                    32,
                    35
                ],
                "AttributeError": [
                    33
                ],
                "pipe.crawler": [
                    35
                ],
                "classmethod": [
                    29
                ],
                "self.spiderinfo": [
                    42,
                    39
                ],
                "self.SpiderInfo": [
                    39
                ],
                "info": [
                    130,
                    42,
                    43,
                    44,
                    46,
                    56,
                    57,
                    61,
                    64,
                    68,
                    69,
                    70,
                    71,
                    73,
                    82,
                    84,
                    85,
                    88,
                    90,
                    91,
                    100,
                    101,
                    102
                ],
                "requests": [
                    43,
                    44
                ],
                "arg_to_iter": [
                    43
                ],
                "self.get_media_requests": [
                    43
                ],
                "item": [
                    129,
                    43,
                    132,
                    46
                ],
                "dlist": [
                    44,
                    45
                ],
                "self._process_request": [
                    44
                ],
                "r": [
                    44
                ],
                "dfd": [
                    69,
                    70,
                    71,
                    72,
                    75,
                    45,
                    46,
                    82,
                    83,
                    88,
                    89,
                    92
                ],
                "DeferredList": [
                    45
                ],
                "dfd.addCallback": [
                    70,
                    46
                ],
                "self.item_completed": [
                    46
                ],
                "fp": [
                    64,
                    68,
                    100,
                    101,
                    71,
                    102,
                    49,
                    56,
                    57,
                    61
                ],
                "request_fingerprint": [
                    49
                ],
                "request": [
                    69,
                    70,
                    91,
                    49,
                    50,
                    51,
                    52,
                    53,
                    82,
                    84,
                    85,
                    87,
                    90,
                    88
                ],
                "cb": [
                    57,
                    50,
                    60
                ],
                "request.callback": [
                    50,
                    52
                ],
                "_": [
                    50
                ],
                "eb": [
                    57,
                    51,
                    60
                ],
                "request.errback": [
                    51,
                    53
                ],
                "info.downloaded": [
                    56,
                    57,
                    101
                ],
                "addCallbacks": [
                    57,
                    60
                ],
                "defer_result": [
                    57,
                    103
                ],
                "wad": [
                    65,
                    102,
                    103,
                    75,
                    60,
                    61
                ],
                "Deferred": [
                    60
                ],
                "append": [
                    61
                ],
                "info.waiting": [
                    61,
                    102
                ],
                "info.downloading": [
                    64,
                    100,
                    68
                ],
                "info.downloading.add": [
                    68
                ],
                "mustbe_deferred": [
                    82,
                    69
                ],
                "self.media_to_download": [
                    69
                ],
                "self._check_media_to_download": [
                    70
                ],
                "dfd.addBoth": [
                    75,
                    71
                ],
                "self._cache_result_and_execute_waiters": [
                    71
                ],
                "dfd.addErrback": [
                    72
                ],
                "logger.error": [
                    72,
                    127
                ],
                "f.value": [
                    73
                ],
                "f": [
                    73
                ],
                "info.spider": [
                    88,
                    73,
                    82,
                    130
                ],
                "result": [
                    97,
                    98,
                    99,
                    101,
                    103,
                    78,
                    79,
                    95
                ],
                "dfd.addCallbacks": [
                    89,
                    83
                ],
                "self.media_downloaded": [
                    90,
                    84
                ],
                "self.media_failed": [
                    91,
                    85
                ],
                "request.meta": [
                    87
                ],
                "self.crawler.engine.download": [
                    88
                ],
                "self.crawler.engine": [
                    88
                ],
                "self.crawler": [
                    88
                ],
                "isinstance": [
                    95
                ],
                "Failure": [
                    95
                ],
                "result.cleanFailure": [
                    97
                ],
                "result.frames": [
                    98
                ],
                "result.stack": [
                    99
                ],
                "info.downloading.remove": [
                    100
                ],
                "info.waiting.pop": [
                    102
                ],
                "chainDeferred": [
                    103
                ],
                "response": [
                    116
                ],
                "failure": [
                    120
                ],
                "self.LOG_FAILED_RESULTS": [
                    124
                ],
                "ok": [
                    125,
                    126
                ],
                "value": [
                    130,
                    125
                ],
                "results": [
                    125
                ],
                "self.__class__.__name__": [
                    129
                ],
                "self.__class__": [
                    129
                ]
            },
            "filtered_variables_in_file": {
                "logger": [
                    72,
                    12,
                    127
                ],
                "logging.getLogger": [
                    12
                ],
                "logging": [
                    12
                ],
                "LOG_FAILED_RESULTS": [
                    17
                ],
                "self.spider": [
                    21
                ],
                "self": [
                    129,
                    21,
                    22,
                    23,
                    24,
                    27,
                    39,
                    42,
                    43,
                    44,
                    46,
                    69,
                    70,
                    71,
                    80,
                    82,
                    84,
                    85,
                    88,
                    90,
                    91,
                    124
                ],
                "spider": [
                    21,
                    39
                ],
                "self.downloading": [
                    22
                ],
                "self.downloaded": [
                    23
                ],
                "self.waiting": [
                    24
                ],
                "defaultdict": [
                    24
                ],
                "self.download_func": [
                    80,
                    82,
                    27
                ],
                "download_func": [
                    27
                ],
                "pipe": [
                    32,
                    34,
                    35,
                    36
                ],
                "cls.from_settings": [
                    32
                ],
                "cls": [
                    32,
                    34
                ],
                "crawler.settings": [
                    32
                ],
                "crawler": [
                    32,
                    35
                ],
                "pipe.crawler": [
                    35
                ],
                "self.spiderinfo": [
                    42,
                    39
                ],
                "self.SpiderInfo": [
                    39
                ],
                "info": [
                    130,
                    42,
                    43,
                    44,
                    46,
                    56,
                    57,
                    61,
                    64,
                    68,
                    69,
                    70,
                    71,
                    73,
                    82,
                    84,
                    85,
                    88,
                    90,
                    91,
                    100,
                    101,
                    102
                ],
                "requests": [
                    43,
                    44
                ],
                "arg_to_iter": [
                    43
                ],
                "self.get_media_requests": [
                    43
                ],
                "item": [
                    129,
                    43,
                    132,
                    46
                ],
                "dlist": [
                    44,
                    45
                ],
                "self._process_request": [
                    44
                ],
                "r": [
                    44
                ],
                "dfd": [
                    69,
                    70,
                    71,
                    72,
                    75,
                    45,
                    46,
                    82,
                    83,
                    88,
                    89,
                    92
                ],
                "DeferredList": [
                    45
                ],
                "dfd.addCallback": [
                    70,
                    46
                ],
                "self.item_completed": [
                    46
                ],
                "fp": [
                    64,
                    68,
                    100,
                    101,
                    71,
                    102,
                    49,
                    56,
                    57,
                    61
                ],
                "request_fingerprint": [
                    49
                ],
                "request": [
                    69,
                    70,
                    91,
                    49,
                    50,
                    51,
                    52,
                    53,
                    82,
                    84,
                    85,
                    87,
                    90,
                    88
                ],
                "cb": [
                    57,
                    50,
                    60
                ],
                "request.callback": [
                    50,
                    52
                ],
                "_": [
                    50
                ],
                "eb": [
                    57,
                    51,
                    60
                ],
                "request.errback": [
                    51,
                    53
                ],
                "info.downloaded": [
                    56,
                    57,
                    101
                ],
                "addCallbacks": [
                    57,
                    60
                ],
                "defer_result": [
                    57,
                    103
                ],
                "wad": [
                    65,
                    102,
                    103,
                    75,
                    60,
                    61
                ],
                "Deferred": [
                    60
                ],
                "append": [
                    61
                ],
                "info.waiting": [
                    61,
                    102
                ],
                "info.downloading": [
                    64,
                    100,
                    68
                ],
                "info.downloading.add": [
                    68
                ],
                "mustbe_deferred": [
                    82,
                    69
                ],
                "self.media_to_download": [
                    69
                ],
                "self._check_media_to_download": [
                    70
                ],
                "dfd.addBoth": [
                    75,
                    71
                ],
                "self._cache_result_and_execute_waiters": [
                    71
                ],
                "dfd.addErrback": [
                    72
                ],
                "logger.error": [
                    72,
                    127
                ],
                "f.value": [
                    73
                ],
                "f": [
                    73
                ],
                "info.spider": [
                    88,
                    73,
                    82,
                    130
                ],
                "result": [
                    97,
                    98,
                    99,
                    101,
                    103,
                    78,
                    79,
                    95
                ],
                "dfd.addCallbacks": [
                    89,
                    83
                ],
                "self.media_downloaded": [
                    90,
                    84
                ],
                "self.media_failed": [
                    91,
                    85
                ],
                "request.meta": [
                    87
                ],
                "self.crawler.engine.download": [
                    88
                ],
                "self.crawler.engine": [
                    88
                ],
                "self.crawler": [
                    88
                ],
                "Failure": [
                    95
                ],
                "result.cleanFailure": [
                    97
                ],
                "result.frames": [
                    98
                ],
                "result.stack": [
                    99
                ],
                "info.downloading.remove": [
                    100
                ],
                "info.waiting.pop": [
                    102
                ],
                "chainDeferred": [
                    103
                ],
                "response": [
                    116
                ],
                "failure": [
                    120
                ],
                "self.LOG_FAILED_RESULTS": [
                    124
                ],
                "ok": [
                    125,
                    126
                ],
                "value": [
                    130,
                    125
                ],
                "results": [
                    125
                ],
                "self.__class__.__name__": [
                    129
                ],
                "self.__class__": [
                    129
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs/repos/scrapy_33/scrapy/utils/signal.py": {
            "buggy_functions": [
                {
                    "function_name": "send_catch_log_deferred",
                    "function_code": "def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n    Returns a deferred that gets fired once all signal handlers deferreds were\n    fired.\n    \"\"\"\n    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         extra={'spider': spider, 'failure': failure})\n        return failure\n\n    dont_log = named.pop('dont_log', None)\n    spider = named.get('spider', None)\n    dfds = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d = maybeDeferred(robustApply, receiver, signal=signal, sender=sender,\n                *arguments, **named)\n        d.addErrback(logerror, receiver)\n        d.addBoth(lambda result: (receiver, result))\n        dfds.append(d)\n    d = DeferredList(dfds)\n    d.addCallback(lambda out: [x[1] for x in out])\n    return d\n",
                    "decorators": [],
                    "docstring": "Like send_catch_log but supports returning deferreds on signal handlers.\nReturns a deferred that gets fired once all signal handlers deferreds were\nfired.",
                    "start_line": 41,
                    "end_line": 64,
                    "variables": {
                        "Any": [
                            41
                        ],
                        "Anonymous": [
                            41
                        ],
                        "dont_log": [
                            53,
                            47
                        ],
                        "isinstance": [
                            47
                        ],
                        "failure.value": [
                            47
                        ],
                        "failure": [
                            50,
                            51,
                            47
                        ],
                        "logger.error": [
                            48
                        ],
                        "logger": [
                            48
                        ],
                        "recv": [
                            49
                        ],
                        "spider": [
                            50,
                            54
                        ],
                        "named.pop": [
                            53
                        ],
                        "named": [
                            58,
                            53,
                            54
                        ],
                        "named.get": [
                            54
                        ],
                        "dfds": [
                            61,
                            62,
                            55
                        ],
                        "receiver": [
                            56,
                            57,
                            59,
                            60
                        ],
                        "liveReceivers": [
                            56
                        ],
                        "getAllReceivers": [
                            56
                        ],
                        "sender": [
                            56,
                            57
                        ],
                        "signal": [
                            56,
                            57
                        ],
                        "d": [
                            64,
                            57,
                            59,
                            60,
                            61,
                            62,
                            63
                        ],
                        "maybeDeferred": [
                            57
                        ],
                        "robustApply": [
                            57
                        ],
                        "arguments": [
                            58
                        ],
                        "d.addErrback": [
                            59
                        ],
                        "logerror": [
                            59
                        ],
                        "d.addBoth": [
                            60
                        ],
                        "result": [
                            60
                        ],
                        "dfds.append": [
                            61
                        ],
                        "DeferredList": [
                            62
                        ],
                        "d.addCallback": [
                            63
                        ],
                        "x": [
                            63
                        ],
                        "out": [
                            63
                        ]
                    },
                    "filtered_variables": {
                        "Any": [
                            41
                        ],
                        "Anonymous": [
                            41
                        ],
                        "dont_log": [
                            53,
                            47
                        ],
                        "failure.value": [
                            47
                        ],
                        "failure": [
                            50,
                            51,
                            47
                        ],
                        "logger.error": [
                            48
                        ],
                        "logger": [
                            48
                        ],
                        "recv": [
                            49
                        ],
                        "spider": [
                            50,
                            54
                        ],
                        "named.pop": [
                            53
                        ],
                        "named": [
                            58,
                            53,
                            54
                        ],
                        "named.get": [
                            54
                        ],
                        "dfds": [
                            61,
                            62,
                            55
                        ],
                        "receiver": [
                            56,
                            57,
                            59,
                            60
                        ],
                        "liveReceivers": [
                            56
                        ],
                        "getAllReceivers": [
                            56
                        ],
                        "sender": [
                            56,
                            57
                        ],
                        "signal": [
                            56,
                            57
                        ],
                        "d": [
                            64,
                            57,
                            59,
                            60,
                            61,
                            62,
                            63
                        ],
                        "maybeDeferred": [
                            57
                        ],
                        "robustApply": [
                            57
                        ],
                        "arguments": [
                            58
                        ],
                        "d.addErrback": [
                            59
                        ],
                        "logerror": [
                            59
                        ],
                        "d.addBoth": [
                            60
                        ],
                        "result": [
                            60
                        ],
                        "dfds.append": [
                            61
                        ],
                        "DeferredList": [
                            62
                        ],
                        "d.addCallback": [
                            63
                        ],
                        "x": [
                            63
                        ],
                        "out": [
                            63
                        ]
                    },
                    "diff_line_number": 50,
                    "class_data": null,
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "from scrapy.xlib.pydispatch.robustapply import robustApply",
                    "start_line": 10,
                    "end_line": 11
                }
            ],
            "inscope_functions": [
                "def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n    Failures instead of exceptions.\n    \"\"\"\n    dont_log = named.pop('dont_log', None)\n    spider = named.get('spider', None)\n    responses = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        try:\n            response = robustApply(receiver, signal=signal, sender=sender,\n                *arguments, **named)\n            if isinstance(response, Deferred):\n                logger.error(\"Cannot return deferreds from signal handler: %(receiver)s\",\n                             {'receiver': receiver}, extra={'spider': spider})\n        except dont_log:\n            result = Failure()\n        except Exception:\n            result = Failure()\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': receiver},\n                         exc_info=True, extra={'spider': spider})\n        else:\n            result = response\n        responses.append((receiver, result))\n    return responses",
                "def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n    Returns a deferred that gets fired once all signal handlers deferreds were\n    fired.\n    \"\"\"\n    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         extra={'spider': spider, 'failure': failure})\n        return failure\n\n    dont_log = named.pop('dont_log', None)\n    spider = named.get('spider', None)\n    dfds = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d = maybeDeferred(robustApply, receiver, signal=signal, sender=sender,\n                *arguments, **named)\n        d.addErrback(logerror, receiver)\n        d.addBoth(lambda result: (receiver, result))\n        dfds.append(d)\n    d = DeferredList(dfds)\n    d.addCallback(lambda out: [x[1] for x in out])\n    return d",
                "def disconnect_all(signal=Any, sender=Any):\n    \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n    tests\n    \"\"\"\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        disconnect(receiver, signal=signal, sender=sender)",
                "def logerror(failure, recv):\n    if dont_log is None or not isinstance(failure.value, dont_log):\n        logger.error(\"Error caught on signal handler: %(receiver)s\",\n                     {'receiver': recv},\n                     extra={'spider': spider, 'failure': failure})\n    return failure"
            ],
            "inscope_function_signatures": [
                "send_catch_log(signal=Any, sender=Anonymous, *arguments, **named)",
                "send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named)",
                "disconnect_all(signal=Any, sender=Any)",
                "logerror(failure, recv)"
            ],
            "variables_in_file": {
                "logger": [
                    48,
                    33,
                    27,
                    12
                ],
                "logging.getLogger": [
                    12
                ],
                "logging": [
                    12
                ],
                "__name__": [
                    12
                ],
                "Any": [
                    41,
                    66,
                    15
                ],
                "Anonymous": [
                    41,
                    15
                ],
                "dont_log": [
                    53,
                    19,
                    29,
                    47
                ],
                "named.pop": [
                    19,
                    53
                ],
                "named": [
                    19,
                    20,
                    53,
                    54,
                    25,
                    58
                ],
                "spider": [
                    35,
                    50,
                    20,
                    54,
                    28
                ],
                "named.get": [
                    20,
                    54
                ],
                "responses": [
                    21,
                    38,
                    39
                ],
                "receiver": [
                    34,
                    38,
                    70,
                    71,
                    60,
                    22,
                    24,
                    57,
                    56,
                    59,
                    28
                ],
                "liveReceivers": [
                    56,
                    70,
                    22
                ],
                "getAllReceivers": [
                    56,
                    70,
                    22
                ],
                "sender": [
                    70,
                    71,
                    22,
                    24,
                    57,
                    56
                ],
                "signal": [
                    70,
                    71,
                    22,
                    24,
                    57,
                    56
                ],
                "response": [
                    24,
                    26,
                    37
                ],
                "robustApply": [
                    24,
                    57
                ],
                "arguments": [
                    25,
                    58
                ],
                "isinstance": [
                    26,
                    47
                ],
                "Deferred": [
                    26
                ],
                "logger.error": [
                    48,
                    33,
                    27
                ],
                "result": [
                    32,
                    37,
                    38,
                    60,
                    30
                ],
                "Failure": [
                    32,
                    30
                ],
                "Exception": [
                    31
                ],
                "responses.append": [
                    38
                ],
                "failure.value": [
                    47
                ],
                "failure": [
                    50,
                    51,
                    47
                ],
                "recv": [
                    49
                ],
                "dfds": [
                    61,
                    62,
                    55
                ],
                "d": [
                    64,
                    57,
                    59,
                    60,
                    61,
                    62,
                    63
                ],
                "maybeDeferred": [
                    57
                ],
                "d.addErrback": [
                    59
                ],
                "logerror": [
                    59
                ],
                "d.addBoth": [
                    60
                ],
                "dfds.append": [
                    61
                ],
                "DeferredList": [
                    62
                ],
                "d.addCallback": [
                    63
                ],
                "x": [
                    63
                ],
                "out": [
                    63
                ],
                "disconnect": [
                    71
                ]
            },
            "filtered_variables_in_file": {
                "logger": [
                    48,
                    33,
                    27,
                    12
                ],
                "logging.getLogger": [
                    12
                ],
                "logging": [
                    12
                ],
                "Any": [
                    41,
                    66,
                    15
                ],
                "Anonymous": [
                    41,
                    15
                ],
                "dont_log": [
                    53,
                    19,
                    29,
                    47
                ],
                "named.pop": [
                    19,
                    53
                ],
                "named": [
                    19,
                    20,
                    53,
                    54,
                    25,
                    58
                ],
                "spider": [
                    35,
                    50,
                    20,
                    54,
                    28
                ],
                "named.get": [
                    20,
                    54
                ],
                "responses": [
                    21,
                    38,
                    39
                ],
                "receiver": [
                    34,
                    38,
                    70,
                    71,
                    60,
                    22,
                    24,
                    57,
                    56,
                    59,
                    28
                ],
                "liveReceivers": [
                    56,
                    70,
                    22
                ],
                "getAllReceivers": [
                    56,
                    70,
                    22
                ],
                "sender": [
                    70,
                    71,
                    22,
                    24,
                    57,
                    56
                ],
                "signal": [
                    70,
                    71,
                    22,
                    24,
                    57,
                    56
                ],
                "response": [
                    24,
                    26,
                    37
                ],
                "robustApply": [
                    24,
                    57
                ],
                "arguments": [
                    25,
                    58
                ],
                "Deferred": [
                    26
                ],
                "logger.error": [
                    48,
                    33,
                    27
                ],
                "result": [
                    32,
                    37,
                    38,
                    60,
                    30
                ],
                "Failure": [
                    32,
                    30
                ],
                "responses.append": [
                    38
                ],
                "failure.value": [
                    47
                ],
                "failure": [
                    50,
                    51,
                    47
                ],
                "recv": [
                    49
                ],
                "dfds": [
                    61,
                    62,
                    55
                ],
                "d": [
                    64,
                    57,
                    59,
                    60,
                    61,
                    62,
                    63
                ],
                "maybeDeferred": [
                    57
                ],
                "d.addErrback": [
                    59
                ],
                "logerror": [
                    59
                ],
                "d.addBoth": [
                    60
                ],
                "dfds.append": [
                    61
                ],
                "DeferredList": [
                    62
                ],
                "d.addCallback": [
                    63
                ],
                "x": [
                    63
                ],
                "out": [
                    63
                ],
                "disconnect": [
                    71
                ]
            }
        },
        "test_data": []
    }
}