{
    "1.1.1": "def fit_generator(model,\n                  generator,\n                  steps_per_epoch=None,\n                  epochs=1,\n                  verbose=1,\n                  callbacks=None,\n                  validation_data=None,\n                  validation_steps=None,\n                  class_weight=None,\n                  max_queue_size=10,\n                  workers=1,\n                  use_multiprocessing=False,\n                  shuffle=True,\n                  initial_epoch=0):\n    \n    wait_time = 0.01  # in seconds\n    epoch = initial_epoch\n\n    do_validation = bool(validation_data)\n    model._make_train_function()\n    if do_validation:\n        model._make_test_function()\n\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n        warnings.warn(\n            UserWarning('Using a generator with `use_multiprocessing=True`'\n                        ' and multiple workers may duplicate your data.'\n                        ' Please consider using the`keras.utils.Sequence'\n                        ' class.'))\n    if steps_per_epoch is None:\n        if is_sequence:\n            steps_per_epoch = len(generator)\n        else:\n            raise ValueError('`steps_per_epoch=None` is only valid for a'\n                             ' generator based on the '\n                             '`keras.utils.Sequence`'\n                             ' class. Please specify `steps_per_epoch` '\n                             'or use the `keras.utils.Sequence` class.')\n\n    # python 2 has 'next', 3 has '__next__'\n    # avoid any explicit version checks\n    val_gen = (hasattr(validation_data, 'next') or\n               hasattr(validation_data, '__next__') or\n               isinstance(validation_data, Sequence))\n    if (val_gen and not isinstance(validation_data, Sequence) and\n            not validation_steps):\n        raise ValueError('`validation_steps=None` is only valid for a'\n                         ' generator based on the `keras.utils.Sequence`'\n                         ' class. Please specify `validation_steps` or use'\n                         ' the `keras.utils.Sequence` class.')\n\n    # Prepare display labels.\n    out_labels = model.metrics_names\n    callback_metrics = out_labels + ['val_' + n for n in out_labels]\n\n    # prepare callbacks\n    model.history = cbks.History()\n    _callbacks = [cbks.BaseLogger(\n        stateful_metrics=model.stateful_metric_names)]\n    if verbose:\n        _callbacks.append(\n            cbks.ProgbarLogger(\n                count_mode='steps',\n                stateful_metrics=model.stateful_metric_names))\n    _callbacks += (callbacks or []) + [model.history]\n    callbacks = cbks.CallbackList(_callbacks)\n\n    # it's possible to callback a different model than self:\n    if hasattr(model, 'callback_model') and model.callback_model:\n        callback_model = model.callback_model\n    else:\n        callback_model = model\n    callbacks.set_model(callback_model)\n    callbacks.set_params({\n        'epochs': epochs,\n        'steps': steps_per_epoch,\n        'verbose': verbose,\n        'do_validation': do_validation,\n        'metrics': callback_metrics,\n    })\n    callbacks.on_train_begin()\n\n    enqueuer = None\n    val_enqueuer = None\n\n    try:\n        if do_validation:\n            if val_gen and workers > 0:\n                # Create an Enqueuer that can be reused\n                val_data = validation_data\n                if isinstance(val_data, Sequence):\n                    val_enqueuer = OrderedEnqueuer(\n                        val_data,\n                        use_multiprocessing=use_multiprocessing)\n                    validation_steps = validation_steps or len(val_data)\n                else:\n                    val_enqueuer = GeneratorEnqueuer(\n                        val_data,\n                        use_multiprocessing=use_multiprocessing)\n                val_enqueuer.start(workers=workers,\n                                   max_queue_size=max_queue_size)\n                val_enqueuer_gen = val_enqueuer.get()\n            elif val_gen:\n                val_data = validation_data\n                if isinstance(val_data, Sequence):\n                    val_enqueuer_gen = iter_sequence_infinite(generator)\n                else:\n                    val_enqueuer_gen = val_data\n            else:\n                # Prepare data for validation\n                if len(validation_data) == 2:\n                    val_x, val_y = validation_data\n                    val_sample_weight = None\n                elif len(validation_data) == 3:\n                    val_x, val_y, val_sample_weight = validation_data\n                else:\n                    raise ValueError('`validation_data` should be a tuple '\n                                     '`(val_x, val_y, val_sample_weight)` '\n                                     'or `(val_x, val_y)`. Found: ' +\n                                     str(validation_data))\n                val_x, val_y, val_sample_weights = model._standardize_user_data(\n                    val_x, val_y, val_sample_weight)\n                val_data = val_x + val_y + val_sample_weights\n                if model.uses_learning_phase and not isinstance(K.learning_phase(),\n                                                                int):\n                    val_data += [0.]\n                for cbk in callbacks:\n                    cbk.validation_data = val_data\n\n        if workers > 0:\n            if is_sequence:\n                enqueuer = OrderedEnqueuer(\n                    generator,\n                    use_multiprocessing=use_multiprocessing,\n                    shuffle=shuffle)\n            else:\n                enqueuer = GeneratorEnqueuer(\n                    generator,\n                    use_multiprocessing=use_multiprocessing,\n                    wait_time=wait_time)\n            enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n            output_generator = enqueuer.get()\n        else:\n            if is_sequence:\n                output_generator = iter_sequence_infinite(generator)\n            else:\n                output_generator = generator\n\n        callback_model.stop_training = False\n        # Construct epoch logs.\n        epoch_logs = {}\n        while epoch < epochs:\n            for m in model.stateful_metric_functions:\n                m.reset_states()\n            callbacks.on_epoch_begin(epoch)\n            steps_done = 0\n            batch_index = 0\n            while steps_done < steps_per_epoch:\n                generator_output = next(output_generator)\n\n                if not hasattr(generator_output, '__len__'):\n                    raise ValueError('Output of generator should be '\n                                     'a tuple `(x, y, sample_weight)` '\n                                     'or `(x, y)`. Found: ' +\n                                     str(generator_output))\n\n                if len(generator_output) == 2:\n                    x, y = generator_output\n                    sample_weight = None\n                elif len(generator_output) == 3:\n                    x, y, sample_weight = generator_output\n                else:\n                    raise ValueError('Output of generator should be '\n                                     'a tuple `(x, y, sample_weight)` '\n                                     'or `(x, y)`. Found: ' +\n                                     str(generator_output))\n                # build batch logs\n                batch_logs = {}\n                if x is None or len(x) == 0:\n                    # Handle data tensors support when no input given\n                    # step-size = 1 for data tensors\n                    batch_size = 1\n                elif isinstance(x, list):\n                    batch_size = x[0].shape[0]\n                elif isinstance(x, dict):\n                    batch_size = list(x.values())[0].shape[0]\n                else:\n                    batch_size = x.shape[0]\n                batch_logs['batch'] = batch_index\n                batch_logs['size'] = batch_size\n                callbacks.on_batch_begin(batch_index, batch_logs)\n\n                outs = model.train_on_batch(x, y,\n                                            sample_weight=sample_weight,\n                                            class_weight=class_weight)\n\n                outs = to_list(outs)\n                for l, o in zip(out_labels, outs):\n                    batch_logs[l] = o\n\n                callbacks.on_batch_end(batch_index, batch_logs)\n\n                batch_index += 1\n                steps_done += 1\n\n                # Epoch finished.\n                if steps_done >= steps_per_epoch and do_validation:\n                    if val_gen:\n                        val_outs = model.evaluate_generator(\n                            val_enqueuer_gen,\n                            validation_steps,\n                            workers=0)\n                    else:\n                        # No need for try/except because\n                        # data has already been validated.\n                        val_outs = model.evaluate(\n                            val_x, val_y,\n                            batch_size=batch_size,\n                            sample_weight=val_sample_weights,\n                            verbose=0)\n                    val_outs = to_list(val_outs)\n                    # Same labels assumed.\n                    for l, o in zip(out_labels, val_outs):\n                        epoch_logs['val_' + l] = o\n\n                if callback_model.stop_training:\n                    break\n\n            callbacks.on_epoch_end(epoch, epoch_logs)\n            epoch += 1\n            if callback_model.stop_training:\n                break\n\n    finally:\n        try:\n            if enqueuer is not None:\n                enqueuer.stop()\n        finally:\n            if val_enqueuer is not None:\n                val_enqueuer.stop()\n\n    callbacks.on_train_end()\n    return model.history\n",
    "1.1.2": "See docstring for `Model.fit_generator`.",
    "1.2.1": null,
    "1.2.2": null,
    "1.2.3": null,
    "1.3.1": "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_13/keras/engine/training_generator.py",
    "1.3.2": [
        "evaluate_generator(model, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)"
    ],
    "1.4.1": [
        "def test_model_methods():\n    a = Input(shape=(3,), name='input_a')\n    b = Input(shape=(3,), name='input_b')\n\n    a_2 = Dense(4, name='dense_1')(a)\n    dp = Dropout(0.5, name='dropout')\n    b_2 = dp(b)\n\n    model = Model([a, b], [a_2, b_2])\n\n    optimizer = 'rmsprop'\n    loss = 'mse'\n    loss_weights = [1., 0.5]\n\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n\n    output_a_np = np.random.random((10, 4))\n    output_b_np = np.random.random((10, 3))\n\n    # training/testing doesn't work before compiling.\n    with pytest.raises(RuntimeError):\n        model.train_on_batch([input_a_np, input_b_np],\n                             [output_a_np, output_b_np])\n\n    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                  sample_weight_mode=None)\n\n    # test train_on_batch\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n    out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                               [output_a_np, output_b_np])\n    out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                               {'dense_1': output_a_np, 'dropout': output_b_np})\n\n    # test fit\n    out = model.fit([input_a_np, input_b_np],\n                    [output_a_np, output_b_np], epochs=1, batch_size=4)\n    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                    [output_a_np, output_b_np], epochs=1, batch_size=4)\n    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                    {'dense_1': output_a_np, 'dropout': output_b_np},\n                    epochs=1, batch_size=4)\n\n    # test validation_split\n    out = model.fit([input_a_np, input_b_np],\n                    [output_a_np, output_b_np],\n                    epochs=1, batch_size=4, validation_split=0.5)\n    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                    [output_a_np, output_b_np],\n                    epochs=1, batch_size=4, validation_split=0.5)\n\n    # test validation data\n    out = model.fit([input_a_np, input_b_np],\n                    [output_a_np, output_b_np],\n                    epochs=1, batch_size=4,\n                    validation_data=([input_a_np, input_b_np],\n                                     [output_a_np, output_b_np]))\n    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                    [output_a_np, output_b_np],\n                    epochs=1, batch_size=4, validation_split=0.5,\n                    validation_data=({'input_a': input_a_np,\n                                      'input_b': input_b_np},\n                                     [output_a_np, output_b_np]))\n    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                    {'dense_1': output_a_np, 'dropout': output_b_np},\n                    epochs=1, batch_size=4, validation_split=0.5,\n                    validation_data=(\n                        {'input_a': input_a_np, 'input_b': input_b_np},\n                        {'dense_1': output_a_np, 'dropout': output_b_np}))\n\n    # test_on_batch\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np])\n    out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                              [output_a_np, output_b_np])\n    out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                              {'dense_1': output_a_np, 'dropout': output_b_np})\n\n    # predict_on_batch\n    out = model.predict_on_batch([input_a_np, input_b_np])\n    out = model.predict_on_batch({'input_a': input_a_np,\n                                  'input_b': input_b_np})\n\n    # predict, evaluate\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n\n    output_a_np = np.random.random((10, 4))\n    output_b_np = np.random.random((10, 3))\n\n    out = model.evaluate([input_a_np, input_b_np],\n                         [output_a_np, output_b_np],\n                         batch_size=4)\n    out = model.predict([input_a_np, input_b_np], batch_size=4)\n\n    # with sample_weight\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n\n    output_a_np = np.random.random((10, 4))\n    output_b_np = np.random.random((10, 3))\n\n    sample_weight = [None, np.random.random((10,))]\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np],\n                               sample_weight=sample_weight)\n\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np],\n                              sample_weight=sample_weight)\n\n    # test accuracy metric\n    model.compile(optimizer, loss, metrics=['acc'],\n                  sample_weight_mode=None)\n\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n    assert len(out) == 5\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np])\n    assert len(out) == 5\n\n    # this should also work\n    model.compile(optimizer, loss, metrics={'dense_1': 'acc'},\n                  sample_weight_mode=None)\n\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n    assert len(out) == 4\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np])\n    assert len(out) == 4\n\n    # and this as well\n    model.compile(optimizer, loss, metrics={'dense_1': ['acc']},\n                  sample_weight_mode=None)\n\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n    assert len(out) == 4\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np])\n    assert len(out) == 4\n\n    # test starting from non-zero initial epoch\n    trained_epochs = []\n    trained_batches = []\n\n    # define tracer callback\n    def on_epoch_begin(epoch, logs):\n        trained_epochs.append(epoch)\n\n    def on_batch_begin(batch, logs):\n        trained_batches.append(batch)\n\n    tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin,\n                                on_batch_begin=on_batch_begin)\n\n    out = model.fit([input_a_np, input_b_np],\n                    [output_a_np, output_b_np], epochs=5, batch_size=4,\n                    initial_epoch=2, callbacks=[tracker_cb])\n    assert trained_epochs == [2, 3, 4]\n\n    # test starting from non-zero initial epoch for generator too\n    trained_epochs = []\n\n    @threadsafe_generator\n    def gen_data(batch_sz):\n        while True:\n            yield ([np.random.random((batch_sz, 3)),\n                    np.random.random((batch_sz, 3))],\n                   [np.random.random((batch_sz, 4)),\n                    np.random.random((batch_sz, 3))])\n\n    out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,\n                              initial_epoch=2, callbacks=[tracker_cb])\n    assert trained_epochs == [2, 3, 4]\n\n    # test with a custom metric function\n    def mse(y_true, y_pred):\n        return K.mean(K.pow(y_true - y_pred, 2))\n\n    model.compile(optimizer, loss, metrics=[mse],\n                  sample_weight_mode=None)\n\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n    out_len = 1 + 2 * (1 + 1)  # total loss + 2 outputs * (loss + metric)\n    assert len(out) == out_len\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np])\n    assert len(out) == out_len\n\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n\n    output_a_np = np.random.random((10, 4))\n    output_b_np = np.random.random((10, 3))\n\n    out = model.fit([input_a_np, input_b_np],\n                    [output_a_np, output_b_np],\n                    batch_size=4, epochs=1)\n    out = model.evaluate([input_a_np, input_b_np],\n                         [output_a_np, output_b_np],\n                         batch_size=4)\n    out = model.predict([input_a_np, input_b_np], batch_size=4)\n\n    # enable verbose for evaluate_generator\n    out = model.evaluate_generator(gen_data(4), steps=3, verbose=1)\n\n    # empty batch\n    with pytest.raises(ValueError):\n        @threadsafe_generator\n        def gen_data():\n            while True:\n                yield (np.asarray([]), np.asarray([]))\n\n        out = model.evaluate_generator(gen_data(), steps=1)\n\n    # x is not a list of numpy arrays.\n    with pytest.raises(ValueError):\n        out = model.predict([None])\n\n    # x does not match _feed_input_names.\n    with pytest.raises(ValueError):\n        out = model.predict([input_a_np, None, input_b_np])\n    with pytest.raises(ValueError):\n        out = model.predict([None, input_a_np, input_b_np])\n\n    # all input/output/weight arrays should have the same number of samples.\n    with pytest.raises(ValueError):\n        out = model.train_on_batch([input_a_np, input_b_np[:2]],\n                                   [output_a_np, output_b_np],\n                                   sample_weight=sample_weight)\n    with pytest.raises(ValueError):\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np[:2]],\n                                   sample_weight=sample_weight)\n    with pytest.raises(ValueError):\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np],\n                                   sample_weight=[sample_weight[1],\n                                                  sample_weight[1][:2]])\n\n    # `sample_weight` is neither a dict nor a list.\n    with pytest.raises(TypeError):\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np],\n                                   sample_weight=tuple(sample_weight))\n\n    # `validation_data` is neither a tuple nor a triple.\n    with pytest.raises(ValueError):\n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np],\n                        epochs=1, batch_size=4,\n                        validation_data=([input_a_np, input_b_np],))\n\n    # `loss` does not match outputs.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss=['mse', 'mae', 'mape'])\n\n    # `loss_weights` does not match output_names.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss='mse', loss_weights={'lstm': 0.5})\n\n    # `loss_weights` does not match outputs.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss='mse', loss_weights=[0.5])\n\n    # `loss_weights` is invalid type.\n    with pytest.raises(TypeError):\n        model.compile(optimizer, loss='mse', loss_weights=(0.5, 0.5))\n\n    # `sample_weight_mode` does not match output_names.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss='mse',\n                      sample_weight_mode={'lstm': 'temporal'})\n\n    # `sample_weight_mode` does not match output_names.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss='mse', sample_weight_mode=['temporal'])\n\n    # `sample_weight_mode` matches output_names partially.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss='mse',\n                      sample_weight_mode={'dense_1': 'temporal'})\n\n    # `loss` does not exist.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss=[])\n\n    model.compile(optimizer, loss=['mse', 'mae'])\n    model.compile(optimizer, loss='mse', loss_weights={'dense_1': 0.2,\n                                                       'dropout': 0.8})\n    model.compile(optimizer, loss='mse', loss_weights=[0.2, 0.8])\n\n    # the rank of weight arrays should be 1.\n    with pytest.raises(ValueError):\n        out = model.train_on_batch(\n            [input_a_np, input_b_np],\n            [output_a_np, output_b_np],\n            sample_weight=[None, np.random.random((10, 20, 30))])\n\n    model.compile(optimizer, loss='mse',\n                  sample_weight_mode={'dense_1': None, 'dropout': 'temporal'})\n    model.compile(optimizer, loss='mse', sample_weight_mode=[None, 'temporal'])\n\n    # the rank of output arrays should be at least 3D.\n    with pytest.raises(ValueError):\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np],\n                                   sample_weight=sample_weight)\n\n    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                  sample_weight_mode=None)\n    trained_epochs = []\n    trained_batches = []\n    val_seq = RandomSequence(4)\n    out = model.fit_generator(generator=RandomSequence(3),\n                              steps_per_epoch=3,\n                              epochs=5,\n                              initial_epoch=0,\n                              validation_data=val_seq,\n                              validation_steps=3,\n                              max_queue_size=1,\n                              callbacks=[tracker_cb])\n    assert trained_epochs == [0, 1, 2, 3, 4]\n    assert trained_batches == list(range(3)) * 5\n    assert len(val_seq.logs) <= 4 * 5\n\n    # steps_per_epoch will be equal to len of sequence if it's unspecified\n    trained_epochs = []\n    trained_batches = []\n    val_seq = RandomSequence(4)\n    out = model.fit_generator(generator=RandomSequence(3),\n                              epochs=5,\n                              initial_epoch=0,\n                              validation_data=val_seq,\n                              callbacks=[tracker_cb])\n    assert trained_epochs == [0, 1, 2, 3, 4]\n    assert trained_batches == list(range(12)) * 5\n    assert len(val_seq.logs) == 12 * 5\n\n    # test for workers = 0\n    trained_epochs = []\n    trained_batches = []\n    val_seq = RandomSequence(4)\n    out = model.fit_generator(generator=RandomSequence(3),\n                              epochs=5,\n                              validation_data=val_seq,\n                              callbacks=[tracker_cb],\n                              workers=0)\n    assert trained_epochs == [0, 1, 2, 3, 4]\n    assert trained_batches == list(range(12)) * 5\n    assert len(val_seq.logs) == 12 * 5\n\n    # fit_generator will throw an exception\n    # if steps is unspecified for regular generator\n    with pytest.raises(ValueError):\n        @threadsafe_generator\n        def gen_data():\n            while True:\n                yield (np.asarray([]), np.asarray([]))\n\n        out = model.fit_generator(generator=gen_data(), epochs=5,\n                                  initial_epoch=0, validation_data=gen_data(),\n                                  callbacks=[tracker_cb])\n\n    # Check if generator is only accessed an expected number of times\n    gen_counters = [0, 0]\n\n    @threadsafe_generator\n    def gen_data(i):\n        while True:\n            gen_counters[i] += 1\n            yield ([np.random.random((1, 3)), np.random.random((1, 3))],\n                   [np.random.random((1, 4)), np.random.random((1, 3))])\n    out = model.fit_generator(generator=gen_data(0), epochs=3,\n                              steps_per_epoch=2,\n                              validation_data=gen_data(1),\n                              validation_steps=1,\n                              max_queue_size=2,\n                              workers=2)\n\n    # Need range check here as filling\n    # of the queue depends on sleep in the enqueuers\n    max_train = 3 * 2 + 2 * 2\n    min_train = 2 * 3\n    assert min_train <= gen_counters[0] <= max_train\n    # 12 = (epoch * workers * validation steps * max_queue_size)\n    assert 3 <= gen_counters[1] <= 12\n\n    gen_counters = [0]\n    out = model.fit_generator(generator=RandomSequence(3), epochs=3,\n                              validation_data=gen_data(0),\n                              validation_steps=1,\n                              max_queue_size=2,\n                              workers=2)\n\n    # 12 = (epoch * workers * validation steps * max_queue_size)\n    # Need range check here as filling\n    # of the queue depends on sleep in the enqueuers\n    assert 3 <= gen_counters[0] <= 12\n\n    # predict_generator output shape behavior should be consistent\n    def expected_shape(batch_size, n_batches):\n        return (batch_size * n_batches, 4), (batch_size * n_batches, 3)\n\n    # Multiple outputs and one step.\n    batch_size = 5\n    sequence_length = 1\n    shape_0, shape_1 = expected_shape(batch_size, sequence_length)\n    out = model.predict_generator(\n        RandomSequence(batch_size, sequence_length=sequence_length))\n    assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n\n    # Multiple outputs and multiple steps.\n    batch_size = 5\n    sequence_length = 2\n    shape_0, shape_1 = expected_shape(batch_size, sequence_length)\n    out = model.predict_generator(\n        RandomSequence(batch_size, sequence_length=sequence_length))\n    assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n\n    # Create a model with a single output.\n    single_output_model = Model([a, b], a_2)\n    single_output_model.compile(optimizer, loss,\n                                metrics=[], sample_weight_mode=None)\n\n    # Single output and one step.\n    batch_size = 5\n    sequence_length = 1\n    shape_0, _ = expected_shape(batch_size, sequence_length)\n    out = single_output_model.predict_generator(\n        RandomSequence(batch_size, sequence_length=sequence_length))\n    assert np.shape(out) == shape_0\n\n    # Single output and multiple steps.\n    batch_size = 5\n    sequence_length = 2\n    shape_0, _ = expected_shape(batch_size, sequence_length)\n    out = single_output_model.predict_generator(\n        RandomSequence(batch_size, sequence_length=sequence_length))\n    assert np.shape(out) == shape_0"
    ],
    "1.4.2": [
        "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_13/tests/keras/engine/test_training.py"
    ],
    "2.1.1": [
        [
            "E               ValueError: `steps=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps` or use the `keras.utils.Sequence` class."
        ]
    ],
    "2.1.2": [
        [
            "def test_model_methods():\n        a = Input(shape=(3,), name='input_a')\n        b = Input(shape=(3,), name='input_b')\n    \n        a_2 = Dense(4, name='dense_1')(a)\n        dp = Dropout(0.5, name='dropout')\n        b_2 = dp(b)\n    \n        model = Model([a, b], [a_2, b_2])\n    \n        optimizer = 'rmsprop'\n        loss = 'mse'\n        loss_weights = [1., 0.5]\n    \n        input_a_np = np.random.random((10, 3))\n        input_b_np = np.random.random((10, 3))\n    \n        output_a_np = np.random.random((10, 4))\n        output_b_np = np.random.random((10, 3))\n    \n        # training/testing doesn't work before compiling.\n        with pytest.raises(RuntimeError):\n            model.train_on_batch([input_a_np, input_b_np],\n                                 [output_a_np, output_b_np])\n    \n        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                      sample_weight_mode=None)\n    \n        # test train_on_batch\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np])\n        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                                   [output_a_np, output_b_np])\n        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                                   {'dense_1': output_a_np, 'dropout': output_b_np})\n    \n        # test fit\n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np], epochs=1, batch_size=4)\n        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                        [output_a_np, output_b_np], epochs=1, batch_size=4)\n        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                        {'dense_1': output_a_np, 'dropout': output_b_np},\n                        epochs=1, batch_size=4)\n    \n        # test validation_split\n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np],\n                        epochs=1, batch_size=4, validation_split=0.5)\n        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                        [output_a_np, output_b_np],\n                        epochs=1, batch_size=4, validation_split=0.5)\n    \n        # test validation data\n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np],\n                        epochs=1, batch_size=4,\n                        validation_data=([input_a_np, input_b_np],\n                                         [output_a_np, output_b_np]))\n        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                        [output_a_np, output_b_np],\n                        epochs=1, batch_size=4, validation_split=0.5,\n                        validation_data=({'input_a': input_a_np,\n                                          'input_b': input_b_np},\n                                         [output_a_np, output_b_np]))\n        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                        {'dense_1': output_a_np, 'dropout': output_b_np},\n                        epochs=1, batch_size=4, validation_split=0.5,\n                        validation_data=(\n                            {'input_a': input_a_np, 'input_b': input_b_np},\n                            {'dense_1': output_a_np, 'dropout': output_b_np}))\n    \n        # test_on_batch\n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np])\n        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                                  [output_a_np, output_b_np])\n        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                                  {'dense_1': output_a_np, 'dropout': output_b_np})\n    \n        # predict_on_batch\n        out = model.predict_on_batch([input_a_np, input_b_np])\n        out = model.predict_on_batch({'input_a': input_a_np,\n                                      'input_b': input_b_np})\n    \n        # predict, evaluate\n        input_a_np = np.random.random((10, 3))\n        input_b_np = np.random.random((10, 3))\n    \n        output_a_np = np.random.random((10, 4))\n        output_b_np = np.random.random((10, 3))\n    \n        out = model.evaluate([input_a_np, input_b_np],\n                             [output_a_np, output_b_np],\n                             batch_size=4)\n        out = model.predict([input_a_np, input_b_np], batch_size=4)\n    \n        # with sample_weight\n        input_a_np = np.random.random((10, 3))\n        input_b_np = np.random.random((10, 3))\n    \n        output_a_np = np.random.random((10, 4))\n        output_b_np = np.random.random((10, 3))\n    \n        sample_weight = [None, np.random.random((10,))]\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np],\n                                   sample_weight=sample_weight)\n    \n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np],\n                                  sample_weight=sample_weight)\n    \n        # test accuracy metric\n        model.compile(optimizer, loss, metrics=['acc'],\n                      sample_weight_mode=None)\n    \n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np])\n        assert len(out) == 5\n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np])\n        assert len(out) == 5\n    \n        # this should also work\n        model.compile(optimizer, loss, metrics={'dense_1': 'acc'},\n                      sample_weight_mode=None)\n    \n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np])\n        assert len(out) == 4\n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np])\n        assert len(out) == 4\n    \n        # and this as well\n        model.compile(optimizer, loss, metrics={'dense_1': ['acc']},\n                      sample_weight_mode=None)\n    \n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np])\n        assert len(out) == 4\n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np])\n        assert len(out) == 4\n    \n        # test starting from non-zero initial epoch\n        trained_epochs = []\n        trained_batches = []\n    \n        # define tracer callback\n        def on_epoch_begin(epoch, logs):\n            trained_epochs.append(epoch)\n    \n        def on_batch_begin(batch, logs):\n            trained_batches.append(batch)\n    \n        tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin,\n                                    on_batch_begin=on_batch_begin)\n    \n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np], epochs=5, batch_size=4,\n                        initial_epoch=2, callbacks=[tracker_cb])\n        assert trained_epochs == [2, 3, 4]\n    \n        # test starting from non-zero initial epoch for generator too\n        trained_epochs = []\n    \n        @threadsafe_generator\n        def gen_data(batch_sz):\n            while True:\n                yield ([np.random.random((batch_sz, 3)),\n                        np.random.random((batch_sz, 3))],\n                       [np.random.random((batch_sz, 4)),\n                        np.random.random((batch_sz, 3))])\n    \n        out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,\n                                  initial_epoch=2, callbacks=[tracker_cb])\n        assert trained_epochs == [2, 3, 4]\n    \n        # test with a custom metric function\n        def mse(y_true, y_pred):\n            return K.mean(K.pow(y_true - y_pred, 2))\n    \n        model.compile(optimizer, loss, metrics=[mse],\n                      sample_weight_mode=None)\n    \n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np])\n        out_len = 1 + 2 * (1 + 1)  # total loss + 2 outputs * (loss + metric)\n        assert len(out) == out_len\n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np])\n        assert len(out) == out_len\n    \n        input_a_np = np.random.random((10, 3))\n        input_b_np = np.random.random((10, 3))\n    \n        output_a_np = np.random.random((10, 4))\n        output_b_np = np.random.random((10, 3))\n    \n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np],\n                        batch_size=4, epochs=1)\n        out = model.evaluate([input_a_np, input_b_np],\n                             [output_a_np, output_b_np],\n                             batch_size=4)\n        out = model.predict([input_a_np, input_b_np], batch_size=4)\n    \n        # enable verbose for evaluate_generator\n        out = model.evaluate_generator(gen_data(4), steps=3, verbose=1)\n    \n        # empty batch\n        with pytest.raises(ValueError):\n            @threadsafe_generator\n            def gen_data():\n                while True:\n                    yield (np.asarray([]), np.asarray([]))\n    \n            out = model.evaluate_generator(gen_data(), steps=1)\n    \n        # x is not a list of numpy arrays.\n        with pytest.raises(ValueError):\n            out = model.predict([None])\n    \n        # x does not match _feed_input_names.\n        with pytest.raises(ValueError):\n            out = model.predict([input_a_np, None, input_b_np])\n        with pytest.raises(ValueError):\n            out = model.predict([None, input_a_np, input_b_np])\n    \n        # all input/output/weight arrays should have the same number of samples.\n        with pytest.raises(ValueError):\n            out = model.train_on_batch([input_a_np, input_b_np[:2]],\n                                       [output_a_np, output_b_np],\n                                       sample_weight=sample_weight)\n        with pytest.raises(ValueError):\n            out = model.train_on_batch([input_a_np, input_b_np],\n                                       [output_a_np, output_b_np[:2]],\n                                       sample_weight=sample_weight)\n        with pytest.raises(ValueError):\n            out = model.train_on_batch([input_a_np, input_b_np],\n                                       [output_a_np, output_b_np],\n                                       sample_weight=[sample_weight[1],\n                                                      sample_weight[1][:2]])\n    \n        # `sample_weight` is neither a dict nor a list.\n        with pytest.raises(TypeError):\n            out = model.train_on_batch([input_a_np, input_b_np],\n                                       [output_a_np, output_b_np],\n                                       sample_weight=tuple(sample_weight))\n    \n        # `validation_data` is neither a tuple nor a triple.\n        with pytest.raises(ValueError):\n            out = model.fit([input_a_np, input_b_np],\n                            [output_a_np, output_b_np],\n                            epochs=1, batch_size=4,\n                            validation_data=([input_a_np, input_b_np],))\n    \n        # `loss` does not match outputs.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss=['mse', 'mae', 'mape'])\n    \n        # `loss_weights` does not match output_names.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss='mse', loss_weights={'lstm': 0.5})\n    \n        # `loss_weights` does not match outputs.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss='mse', loss_weights=[0.5])\n    \n        # `loss_weights` is invalid type.\n        with pytest.raises(TypeError):\n            model.compile(optimizer, loss='mse', loss_weights=(0.5, 0.5))\n    \n        # `sample_weight_mode` does not match output_names.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss='mse',\n                          sample_weight_mode={'lstm': 'temporal'})\n    \n        # `sample_weight_mode` does not match output_names.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss='mse', sample_weight_mode=['temporal'])\n    \n        # `sample_weight_mode` matches output_names partially.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss='mse',\n                          sample_weight_mode={'dense_1': 'temporal'})\n    \n        # `loss` does not exist.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss=[])\n    \n        model.compile(optimizer, loss=['mse', 'mae'])\n        model.compile(optimizer, loss='mse', loss_weights={'dense_1': 0.2,\n                                                           'dropout': 0.8})\n        model.compile(optimizer, loss='mse', loss_weights=[0.2, 0.8])\n    \n        # the rank of weight arrays should be 1.\n        with pytest.raises(ValueError):\n            out = model.train_on_batch(\n                [input_a_np, input_b_np],\n                [output_a_np, output_b_np],\n                sample_weight=[None, np.random.random((10, 20, 30))])\n    \n        model.compile(optimizer, loss='mse',\n                      sample_weight_mode={'dense_1': None, 'dropout': 'temporal'})\n        model.compile(optimizer, loss='mse', sample_weight_mode=[None, 'temporal'])\n    \n        # the rank of output arrays should be at least 3D.\n        with pytest.raises(ValueError):\n            out = model.train_on_batch([input_a_np, input_b_np],\n                                       [output_a_np, output_b_np],\n                                       sample_weight=sample_weight)\n    \n        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                      sample_weight_mode=None)\n        trained_epochs = []\n        trained_batches = []\n        val_seq = RandomSequence(4)\n        out = model.fit_generator(generator=RandomSequence(3),\n                                  steps_per_epoch=3,\n                                  epochs=5,\n                                  initial_epoch=0,\n                                  validation_data=val_seq,\n                                  validation_steps=3,\n                                  max_queue_size=1,\n                                  callbacks=[tracker_cb])\n        assert trained_epochs == [0, 1, 2, 3, 4]\n        assert trained_batches == list(range(3)) * 5\n        assert len(val_seq.logs) <= 4 * 5\n    \n        # steps_per_epoch will be equal to len of sequence if it's unspecified\n        trained_epochs = []\n        trained_batches = []\n        val_seq = RandomSequence(4)\n        out = model.fit_generator(generator=RandomSequence(3),\n                                  epochs=5,\n                                  initial_epoch=0,\n                                  validation_data=val_seq,\n                                  callbacks=[tracker_cb])\n        assert trained_epochs == [0, 1, 2, 3, 4]\n        assert trained_batches == list(range(12)) * 5\n        assert len(val_seq.logs) == 12 * 5\n    \n        # test for workers = 0\n        trained_epochs = []\n        trained_batches = []\n        val_seq = RandomSequence(4)\n        out = model.fit_generator(generator=RandomSequence(3),\n                                  epochs=5,\n                                  validation_data=val_seq,\n                                  callbacks=[tracker_cb],\n>                                 workers=0)\n\ntests/keras/engine/test_training.py:479: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nkeras/legacy/interfaces.py:91: in wrapper\n    return func(*args, **kwargs)\nkeras/engine/training.py:1418: in fit_generator\n    initial_epoch=initial_epoch)\nkeras/engine/training_generator.py:233: in fit_generator\n    workers=0)\nkeras/legacy/interfaces.py:91: in wrapper\n    return func(*args, **kwargs)\nkeras/engine/training.py:1472: in evaluate_generator\n    verbose=verbose)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmodel = <keras.engine.training.Model object at 0x128c14b90>\ngenerator = <generator object iter_sequence_infinite at 0x128b4bcd0>\nsteps = None, max_queue_size = 10, workers = 0, use_multiprocessing = False\nverbose = 0\n\n    def evaluate_generator(model, generator,\n                           steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,\n                           verbose=0):\n        \"\"\"See docstring for `Model.evaluate_generator`.\"\"\"\n        model._make_test_function()\n    \n        if hasattr(model, 'metrics'):\n            for m in model.stateful_metric_functions:\n                m.reset_states()\n            stateful_metric_indices = [\n                i for i, name in enumerate(model.metrics_names)\n                if str(name) in model.stateful_metric_names]\n        else:\n            stateful_metric_indices = []\n    \n        steps_done = 0\n        wait_time = 0.01\n        outs_per_batch = []\n        batch_sizes = []\n        is_sequence = isinstance(generator, Sequence)\n        if not is_sequence and use_multiprocessing and workers > 1:\n            warnings.warn(\n                UserWarning('Using a generator with `use_multiprocessing=True`'\n                            ' and multiple workers may duplicate your data.'\n                            ' Please consider using the`keras.utils.Sequence'\n                            ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n>               raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')",
            "\nkeras/engine/training_generator.py:300: ValueError"
        ]
    ],
    "2.1.3": [
        [
            {
                "initial_epoch": "2",
                "model": "<keras.engine.training.Model object at 0x128f1f250>",
                "generator": "<test_training.threadsafe_iter object at 0x128fffc10>",
                "use_multiprocessing": "False",
                "workers": "1",
                "steps_per_epoch": "3",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc']",
                "model.history": "<keras.callbacks.History object at 0x1296f34d0>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x128f7fb50>]",
                "epochs": "5",
                "max_queue_size": "10",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "False",
                "is_sequence": "False",
                "val_gen": "False",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss', 'val_dense_1_acc']",
                "model.history": "<keras.callbacks.History object at 0x1296ffe50>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x1296ffe90>, <keras.callbacks.ProgbarLogger object at 0x1296fa710>, <keras.callbacks.LambdaCallback object at 0x128f7fb50>, <keras.callbacks.History object at 0x1296ffe50>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x1296ffed0>",
                "callback_model": "<keras.engine.training.Model object at 0x128f1f250>",
                "enqueuer": "<keras.utils.data_utils.GeneratorEnqueuer object at 0x1296fff10>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{}",
                "steps_done": "3",
                "batch_index": "3",
                "generator_output": "array of shape 2",
                "x": "[array([[0.25044506, 0.62332589, 0.47443591],\n       [0.49610704, 0.74256793, 0.44802082],\n       [0.02375696, 0.62255307, 0.21782964],\n       [0.32558103, 0.56665586, 0.74258776]]), array([[0.54507593, 0.53415948, 0.05475881],\n       [0.44757854, 0.84188869, 0.75464354],\n       [0.62677039, 0.35324681, 0.41535339],\n       [0.58189602, 0.39116838, 0.69000318]])]",
                "y": "array of shape 2",
                "batch_logs": "{'batch': 2, 'size': 4, 'loss': 0.6783549, 'dense_1_loss': 0.29872873, 'dropout_loss': 0.37962618, 'dense_1_acc': 0.25}",
                "batch_size": "4",
                "outs": "[0.6783549, 0.29872873, 0.37962618, 0.25]",
                "l": "'dense_1_acc'",
                "o": "0.25"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.RandomSequence object at 0x1297b9bd0>",
                "model": "<keras.engine.training.Model object at 0x128f1f250>",
                "generator": "<test_training.RandomSequence object at 0x1296f3c50>",
                "use_multiprocessing": "False",
                "workers": "1",
                "steps_per_epoch": "3",
                "validation_steps": "3",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x1297b9b10>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x128f7fb50>]",
                "epochs": "5",
                "max_queue_size": "1",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "True",
                "is_sequence": "True",
                "val_gen": "True",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x1298d4990>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x1297b9b10>, <keras.callbacks.ProgbarLogger object at 0x12974db90>, <keras.callbacks.LambdaCallback object at 0x128f7fb50>, <keras.callbacks.History object at 0x1298d4990>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x12974d290>",
                "callback_model": "<keras.engine.training.Model object at 0x128f1f250>",
                "enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x12993d290>",
                "val_enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x129713810>",
                "val_data": "<test_training.RandomSequence object at 0x1297b9bd0>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.4858063260714213, 'val_dense_1_loss': 0.3751429816087087, 'val_dropout_loss': 0.22132670631011328, 'loss': 0.6482590039571127, 'dense_1_loss': 0.44087253014246625, 'dropout_loss': 0.4147729476292928}",
                "steps_done": "3",
                "batch_index": "3",
                "generator_output": "array of shape 2",
                "x": "[array([[0.03062245, 0.56584317, 0.48460057],\n       [0.45432965, 0.15047951, 0.22551511],\n       [0.7545759 , 0.65916348, 0.88976629]]), array([[0.91033414, 0.3189712 , 0.36913342],\n       [0.91223001, 0.03159107, 0.00244064],\n       [0.64946492, 0.68212623, 0.8553552 ]])]",
                "y": "[array([[0.26303693, 0.70809539, 0.86316495, 0.57920413],\n       [0.44938037, 0.5517148 , 0.96236581, 0.98841051],\n       [0.77449851, 0.28415968, 0.86597657, 0.11402565]]), array([[0.36367182, 0.93335299, 0.05988899],\n       [0.39147021, 0.88036966, 0.00913316],\n       [0.1104466 , 0.37903425, 0.096669  ]])]",
                "batch_logs": "{'batch': 2, 'size': 3, 'loss': 0.8361666, 'dense_1_loss': 0.37248722, 'dropout_loss': 0.9273588}",
                "batch_size": "3",
                "outs": "[0.8361666, 0.37248722, 0.9273588]",
                "l": "'dropout_loss'",
                "o": "0.22132670631011328",
                "val_outs": "[0.4858063260714213, 0.3751429816087087, 0.22132670631011328]"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.RandomSequence object at 0x1297c5b90>",
                "model": "<keras.engine.training.Model object at 0x128f1f250>",
                "generator": "<test_training.RandomSequence object at 0x1296f3c50>",
                "use_multiprocessing": "False",
                "workers": "1",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x1298d4990>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x128f7fb50>]",
                "epochs": "5",
                "max_queue_size": "10",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "True",
                "is_sequence": "True",
                "steps_per_epoch": "12",
                "val_gen": "True",
                "validation_steps": "12",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x1298d4210>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x1298d4150>, <keras.callbacks.ProgbarLogger object at 0x129965c90>, <keras.callbacks.LambdaCallback object at 0x128f7fb50>, <keras.callbacks.History object at 0x1298d4210>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x1298d4310>",
                "callback_model": "<keras.engine.training.Model object at 0x128f1f250>",
                "enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x129976e50>",
                "val_enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x1298d4290>",
                "val_data": "<test_training.RandomSequence object at 0x1297c5b90>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.3951319803794225, 'val_dense_1_loss': 0.30565445125102997, 'val_dropout_loss': 0.17895505453149477, 'loss': 0.5493177274862925, 'dense_1_loss': 0.2664797517160575, 'dropout_loss': 0.5656759589910507}",
                "steps_done": "12",
                "batch_index": "12",
                "generator_output": "array of shape 2",
                "x": "[array([[0.94250718, 0.04816398, 0.34800717],\n       [0.83742205, 0.01431056, 0.69837378],\n       [0.31121159, 0.10758953, 0.98956245]]), array([[0.86353655, 0.30005288, 0.5057982 ],\n       [0.49764291, 0.93803462, 0.39691721],\n       [0.96015106, 0.1871427 , 0.25467481]])]",
                "y": "[array([[0.30597472, 0.18303112, 0.80258088, 0.58091967],\n       [0.04206835, 0.57246962, 0.01791959, 0.79492024],\n       [0.85702618, 0.82779109, 0.49911759, 0.23253451]]), array([[0.45923311, 0.61978156, 0.67093754],\n       [0.62152501, 0.1579871 , 0.80955539],\n       [0.10552379, 0.55313282, 0.93999014]])]",
                "batch_logs": "{'batch': 11, 'size': 3, 'loss': 0.62174714, 'dense_1_loss': 0.48520198, 'dropout_loss': 0.27309033}",
                "batch_size": "3",
                "outs": "[0.62174714, 0.48520198, 0.27309033]",
                "l": "'dropout_loss'",
                "o": "0.17895505453149477",
                "val_outs": "[0.3951319803794225, 0.30565445125102997, 0.17895505453149477]"
            }
        ]
    ],
    "2.1.4": [
        [
            {
                "initial_epoch": "int",
                "model": "Model",
                "generator": "threadsafe_iter",
                "use_multiprocessing": "bool",
                "workers": "int",
                "steps_per_epoch": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "GeneratorEnqueuer",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float32"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "RandomSequence",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "steps_per_epoch": "int",
                "validation_steps": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "OrderedEnqueuer",
                "val_enqueuer": "OrderedEnqueuer",
                "val_data": "RandomSequence",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "RandomSequence",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "steps_per_epoch": "int",
                "val_gen": "bool",
                "validation_steps": "int",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "OrderedEnqueuer",
                "val_enqueuer": "OrderedEnqueuer",
                "val_data": "RandomSequence",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ]
    ],
    "2.1.5": [
        [
            {
                "initial_epoch": "2",
                "model": "<keras.engine.training.Model object at 0x120c17a50>",
                "generator": "<test_training.threadsafe_iter object at 0x120db86d0>",
                "use_multiprocessing": "False",
                "workers": "1",
                "steps_per_epoch": "3",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc']",
                "model.history": "<keras.callbacks.History object at 0x12138f590>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x120c24bd0>]",
                "epochs": "5",
                "max_queue_size": "10",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "False",
                "is_sequence": "False",
                "val_gen": "False",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss', 'val_dense_1_acc']",
                "model.history": "<keras.callbacks.History object at 0x12139dfd0>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x12139de90>, <keras.callbacks.ProgbarLogger object at 0x12139ded0>, <keras.callbacks.LambdaCallback object at 0x120c24bd0>, <keras.callbacks.History object at 0x12139dfd0>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x1213a3050>",
                "callback_model": "<keras.engine.training.Model object at 0x120c17a50>",
                "enqueuer": "<keras.utils.data_utils.GeneratorEnqueuer object at 0x1213a3090>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{}",
                "steps_done": "3",
                "batch_index": "3",
                "generator_output": "array of shape 2",
                "x": "[array([[0.9899192 , 0.88210012, 0.18766169],\n       [0.5231623 , 0.48253197, 0.86609352],\n       [0.07217275, 0.01049042, 0.55759622],\n       [0.52417512, 0.73017325, 0.46854299]]), array([[0.82563314, 0.01289165, 0.96675638],\n       [0.24457216, 0.30862315, 0.20929656],\n       [0.73916556, 0.82678804, 0.73482776],\n       [0.54532442, 0.79989604, 0.69061218]])]",
                "y": "[array([[0.83301052, 0.48736863, 0.47470295, 0.64108848],\n       [0.96167937, 0.73287114, 0.11000773, 0.06531697],\n       [0.62174048, 0.72956737, 0.15953769, 0.14772536],\n       [0.65505686, 0.74605194, 0.163456  , 0.15978132]]), array([[0.989968  , 0.93152002, 0.95073512],\n       [0.449425  , 0.85032046, 0.71002708],\n       [0.98645707, 0.49621296, 0.20119898],\n       [0.33747508, 0.45442038, 0.97527181]])]",
                "batch_logs": "{'batch': 2, 'size': 4, 'loss': 0.5255562, 'dense_1_loss': 0.18845831, 'dropout_loss': 0.3370979, 'dense_1_acc': 0.25}",
                "batch_size": "4",
                "outs": "[0.5255562, 0.18845831, 0.3370979, 0.25]",
                "l": "'dense_1_acc'",
                "o": "0.25"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.RandomSequence object at 0x121486cd0>",
                "model": "<keras.engine.training.Model object at 0x120c17a50>",
                "generator": "<test_training.RandomSequence object at 0x120bd7fd0>",
                "use_multiprocessing": "False",
                "workers": "1",
                "steps_per_epoch": "3",
                "validation_steps": "3",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12144ee10>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x120c24bd0>]",
                "epochs": "5",
                "max_queue_size": "1",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "True",
                "is_sequence": "True",
                "val_gen": "True",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12156f6d0>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x12144ee10>, <keras.callbacks.ProgbarLogger object at 0x1213e9790>, <keras.callbacks.LambdaCallback object at 0x120c24bd0>, <keras.callbacks.History object at 0x12156f6d0>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x1213e9510>",
                "callback_model": "<keras.engine.training.Model object at 0x120c17a50>",
                "enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x1215d79d0>",
                "val_enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x1213d6690>",
                "val_data": "<test_training.RandomSequence object at 0x121486cd0>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.33746684590975445, 'val_dense_1_loss': 0.22730581959088644, 'val_dropout_loss': 0.220322052637736, 'loss': 0.5917022029558817, 'dense_1_loss': 0.3276292582352956, 'dropout_loss': 0.5281459291776022}",
                "steps_done": "3",
                "batch_index": "3",
                "generator_output": "array of shape 2",
                "x": "[array([[0.86506443, 0.26323313, 0.99470638],\n       [0.5187822 , 0.11088037, 0.14293887],\n       [0.79282808, 0.99490766, 0.7989046 ]]), array([[0.50993892, 0.76551026, 0.87844999],\n       [0.97923453, 0.03383577, 0.96976663],\n       [0.65763227, 0.31687151, 0.05942337]])]",
                "y": "[array([[0.22354791, 0.02560432, 0.25126498, 0.92226467],\n       [0.86307544, 0.83413628, 0.37039889, 0.93820043],\n       [0.25092566, 0.53282421, 0.9459383 , 0.02224539]]), array([[0.78175908, 0.90123628, 0.41571799],\n       [0.85835311, 0.77698703, 0.24229004],\n       [0.49136702, 0.41330885, 0.54875243]])]",
                "batch_logs": "{'batch': 2, 'size': 3, 'loss': 0.701772, 'dense_1_loss': 0.41423216, 'dropout_loss': 0.5750797}",
                "batch_size": "3",
                "outs": "[0.701772, 0.41423216, 0.5750797]",
                "l": "'dropout_loss'",
                "o": "0.220322052637736",
                "val_outs": "[0.33746684590975445, 0.22730581959088644, 0.220322052637736]"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.RandomSequence object at 0x121461bd0>",
                "model": "<keras.engine.training.Model object at 0x120c17a50>",
                "generator": "<test_training.RandomSequence object at 0x121486cd0>",
                "use_multiprocessing": "False",
                "workers": "1",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12156f6d0>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x120c24bd0>]",
                "epochs": "5",
                "max_queue_size": "10",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "True",
                "is_sequence": "True",
                "steps_per_epoch": "12",
                "val_gen": "True",
                "validation_steps": "12",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x1215fa650>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x1215d79d0>, <keras.callbacks.ProgbarLogger object at 0x12156ff10>, <keras.callbacks.LambdaCallback object at 0x120c24bd0>, <keras.callbacks.History object at 0x1215fa650>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x12156fe90>",
                "callback_model": "<keras.engine.training.Model object at 0x120c17a50>",
                "enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x1216098d0>",
                "val_enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x12156fd50>",
                "val_data": "<test_training.RandomSequence object at 0x121461bd0>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.2532428229848544, 'val_dense_1_loss': 0.17959954217076302, 'val_dropout_loss': 0.14728656162818274, 'loss': 0.4830641771356265, 'dense_1_loss': 0.18903294454018274, 'dropout_loss': 0.5880624564985434}",
                "steps_done": "12",
                "batch_index": "12",
                "generator_output": "array of shape 2",
                "x": "[array([[0.55632962, 0.88888302, 0.00448913],\n       [0.2277785 , 0.71322347, 0.16749158],\n       [0.02572308, 0.88537316, 0.45577961]]), array([[0.54687029, 0.7800896 , 0.61530782],\n       [0.26554572, 0.77724813, 0.44842596],\n       [0.22670124, 0.0096392 , 0.00193563]])]",
                "y": "[array([[0.02474219, 0.02905777, 0.506869  , 0.95897107],\n       [0.87177873, 0.25296333, 0.09496267, 0.03648539],\n       [0.1982393 , 0.2939088 , 0.73634354, 0.28293335]]), array([[0.1302791 , 0.02319584, 0.63960603],\n       [0.15235142, 0.59231131, 0.8448126 ],\n       [0.72111226, 0.51230656, 0.02636658]])]",
                "batch_logs": "{'batch': 11, 'size': 3, 'loss': 0.5831582, 'dense_1_loss': 0.31060424, 'dropout_loss': 0.5451079}",
                "batch_size": "3",
                "outs": "[0.5831582, 0.31060424, 0.5451079]",
                "l": "'dropout_loss'",
                "o": "0.14728656162818274",
                "val_outs": "[0.2532428229848544, 0.17959954217076302, 0.14728656162818274]"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.RandomSequence object at 0x121486cd0>",
                "model": "<keras.engine.training.Model object at 0x120c17a50>",
                "generator": "<test_training.RandomSequence object at 0x121461bd0>",
                "use_multiprocessing": "False",
                "workers": "0",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x1215fa650>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x120c24bd0>]",
                "epochs": "5",
                "max_queue_size": "10",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "True",
                "is_sequence": "True",
                "steps_per_epoch": "12",
                "val_gen": "True",
                "validation_steps": "12",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x1215d7550>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x1215d7110>, <keras.callbacks.ProgbarLogger object at 0x1215dcfd0>, <keras.callbacks.LambdaCallback object at 0x120c24bd0>, <keras.callbacks.History object at 0x1215d7550>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x1215dcc10>",
                "callback_model": "<keras.engine.training.Model object at 0x120c17a50>",
                "val_data": "<test_training.RandomSequence object at 0x121486cd0>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.22735685544709364, 'val_dense_1_loss': 0.15065917186439037, 'val_dropout_loss': 0.15339536840716997, 'loss': 0.4336046452323596, 'dense_1_loss': 0.1693198907499512, 'dropout_loss': 0.5285695170362791}",
                "steps_done": "12",
                "batch_index": "12",
                "generator_output": "array of shape 2",
                "x": "[array([[0.62907869, 0.08504984, 0.97171258],\n       [0.50176958, 0.38648688, 0.00469271],\n       [0.70592272, 0.45519223, 0.73037414]]), array([[0.0596335 , 0.81294033, 0.87127789],\n       [0.15459483, 0.84654799, 0.11224677],\n       [0.84951371, 0.8918483 , 0.82180991]])]",
                "y": "[array([[0.90091591, 0.53612588, 0.79034579, 0.09940707],\n       [0.57571409, 0.58226737, 0.42111425, 0.12979596],\n       [0.74934329, 0.21081899, 0.34066603, 0.25450838]]), array([[0.44846624, 0.63910692, 0.51187143],\n       [0.02810765, 0.9297192 , 0.82592522],\n       [0.34429743, 0.47098552, 0.63702425]])]",
                "batch_logs": "{'batch': 11, 'size': 3, 'loss': 0.4031917, 'dense_1_loss': 0.051911008, 'dropout_loss': 0.7025614}",
                "batch_size": "3",
                "outs": "[0.4031917, 0.051911008, 0.7025614]",
                "l": "'dropout_loss'",
                "o": "0.15339536840716997",
                "val_outs": "[0.22735685544709364, 0.15065917186439037, 0.15339536840716997]"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.threadsafe_iter object at 0x121609650>",
                "model": "<keras.engine.training.Model object at 0x120c17a50>",
                "generator": "<test_training.threadsafe_iter object at 0x121609e10>",
                "use_multiprocessing": "False",
                "workers": "2",
                "steps_per_epoch": "2",
                "validation_steps": "1",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x1215d7550>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "epochs": "3",
                "max_queue_size": "2",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "3",
                "do_validation": "True",
                "is_sequence": "False",
                "val_gen": "True",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x1215e1650>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x1215e1350>, <keras.callbacks.ProgbarLogger object at 0x1215e16d0>, <keras.callbacks.History object at 0x1215e1650>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x1215e1810>",
                "callback_model": "<keras.engine.training.Model object at 0x120c17a50>",
                "enqueuer": "<keras.utils.data_utils.GeneratorEnqueuer object at 0x12161ad10>",
                "val_enqueuer": "<keras.utils.data_utils.GeneratorEnqueuer object at 0x1215e1290>",
                "val_data": "<test_training.threadsafe_iter object at 0x121609650>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.4355315566062927, 'val_dense_1_loss': 0.24573855102062225, 'val_dropout_loss': 0.37958601117134094, 'loss': 0.5422470569610596, 'dense_1_loss': 0.38084442913532257, 'dropout_loss': 0.3228052258491516}",
                "steps_done": "2",
                "batch_index": "2",
                "generator_output": "([array([[0.18644246, 0.99301684, 0.96090383]]), array([[0.11259909, 0.53179801, 0.13452683]])], [array([[0.8736046 , 0.68220968, 0.80044106, 0.26180542]]), array([[0.1232678 , 0.90758218, 0.89060294]])])",
                "x": "[array([[0.18644246, 0.99301684, 0.96090383]]), array([[0.11259909, 0.53179801, 0.13452683]])]",
                "y": "[array([[0.8736046 , 0.68220968, 0.80044106, 0.26180542]]), array([[0.1232678 , 0.90758218, 0.89060294]])]",
                "batch_logs": "{'batch': 1, 'size': 1, 'loss': 0.6206976, 'dense_1_loss': 0.5497211, 'dropout_loss': 0.14195293}",
                "batch_size": "1",
                "outs": "[0.6206976, 0.5497211, 0.14195293]",
                "l": "'dropout_loss'",
                "o": "0.37958601117134094",
                "val_outs": "[0.4355315566062927, 0.24573855102062225, 0.37958601117134094]"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.threadsafe_iter object at 0x120b6d9d0>",
                "model": "<keras.engine.training.Model object at 0x120c17a50>",
                "generator": "<test_training.RandomSequence object at 0x121503d50>",
                "use_multiprocessing": "False",
                "workers": "2",
                "validation_steps": "1",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x1215e1650>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "epochs": "3",
                "max_queue_size": "2",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "3",
                "do_validation": "True",
                "is_sequence": "True",
                "steps_per_epoch": "12",
                "val_gen": "True",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x120adfc50>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x12156f6d0>, <keras.callbacks.ProgbarLogger object at 0x12156fcd0>, <keras.callbacks.History object at 0x120adfc50>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x12156f250>",
                "callback_model": "<keras.engine.training.Model object at 0x120c17a50>",
                "enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x121628e90>",
                "val_enqueuer": "<keras.utils.data_utils.GeneratorEnqueuer object at 0x12156f550>",
                "val_data": "<test_training.threadsafe_iter object at 0x120b6d9d0>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.09774186462163925, 'val_dense_1_loss': 0.06338896602392197, 'val_dropout_loss': 0.06870579719543457, 'loss': 0.3739760493238767, 'dense_1_loss': 0.13541209449370703, 'dropout_loss': 0.47712790966033936}",
                "steps_done": "12",
                "batch_index": "12",
                "generator_output": "array of shape 2",
                "x": "[array([[0.31648128, 0.27849409, 0.433869  ],\n       [0.31606368, 0.88321103, 0.89427302],\n       [0.29173009, 0.68276058, 0.96481968]]), array([[0.49394374, 0.79591134, 0.78383527],\n       [0.09249908, 0.25496035, 0.51156233],\n       [0.2169283 , 0.62852028, 0.13148572]])]",
                "y": "[array([[0.67495663, 0.3257444 , 0.50514461, 0.35936243],\n       [0.11195883, 0.85518948, 0.38423711, 0.69645542],\n       [0.50478954, 0.1177758 , 0.9318879 , 0.58166792]]), array([[0.86414693, 0.34009808, 0.57752193],\n       [0.90341327, 0.42712033, 0.85685262],\n       [0.79778095, 0.25827596, 0.44062524]])]",
                "batch_logs": "{'batch': 11, 'size': 3, 'loss': 0.34625465, 'dense_1_loss': 0.084198825, 'dropout_loss': 0.5241116}",
                "batch_size": "3",
                "outs": "[0.34625465, 0.084198825, 0.5241116]",
                "l": "'dropout_loss'",
                "o": "0.06870579719543457",
                "val_outs": "[0.09774186462163925, 0.06338896602392197, 0.06870579719543457]"
            }
        ]
    ],
    "2.1.6": [
        [
            {
                "initial_epoch": "int",
                "model": "Model",
                "generator": "threadsafe_iter",
                "use_multiprocessing": "bool",
                "workers": "int",
                "steps_per_epoch": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "GeneratorEnqueuer",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float32"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "RandomSequence",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "steps_per_epoch": "int",
                "validation_steps": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "OrderedEnqueuer",
                "val_enqueuer": "OrderedEnqueuer",
                "val_data": "RandomSequence",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "RandomSequence",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "steps_per_epoch": "int",
                "val_gen": "bool",
                "validation_steps": "int",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "OrderedEnqueuer",
                "val_enqueuer": "OrderedEnqueuer",
                "val_data": "RandomSequence",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "RandomSequence",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "steps_per_epoch": "int",
                "val_gen": "bool",
                "validation_steps": "int",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "val_data": "RandomSequence",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "threadsafe_iter",
                "model": "Model",
                "generator": "threadsafe_iter",
                "use_multiprocessing": "bool",
                "workers": "int",
                "steps_per_epoch": "int",
                "validation_steps": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "GeneratorEnqueuer",
                "val_enqueuer": "GeneratorEnqueuer",
                "val_data": "threadsafe_iter",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "threadsafe_iter",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "validation_steps": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "steps_per_epoch": "int",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "OrderedEnqueuer",
                "val_enqueuer": "GeneratorEnqueuer",
                "val_data": "threadsafe_iter",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ]
    ],
    "3.1.1": [
        "\n"
    ],
    "3.1.2": [
        "\n"
    ]
}