{
    "keras:37": {
        "/Volumes/SSD2T/bgp_envs/repos/keras_37/keras/layers/recurrent.py": {
            "buggy_functions": [
                {
                    "function_name": "__call__",
                    "function_code": "def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n    inputs, initial_state, constants = self._standardize_args(\n        inputs, initial_state, constants)\n\n    if initial_state is None and constants is None:\n        return super(RNN, self).__call__(inputs, **kwargs)\n\n    # If any of `initial_state` or `constants` are specified and are Keras\n    # tensors, then add them to the inputs and temporarily modify the\n    # input_spec to include them.\n\n    additional_inputs = []\n    additional_specs = []\n    if initial_state is not None:\n        kwargs['initial_state'] = initial_state\n        additional_inputs += initial_state\n        self.state_spec = [InputSpec(shape=K.int_shape(state))\n                           for state in initial_state]\n        additional_specs += self.state_spec\n    if constants is not None:\n        kwargs['constants'] = constants\n        additional_inputs += constants\n        self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n                               for constant in constants]\n        self._num_constants = len(constants)\n        additional_specs += self.constants_spec\n    # at this point additional_inputs cannot be empty\n    is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')\n    for tensor in additional_inputs:\n        if hasattr(tensor, '_keras_history') != is_keras_tensor:\n            raise ValueError('The initial state or constants of an RNN'\n                             ' layer cannot be specified with a mix of'\n                             ' Keras tensors and non-Keras tensors')\n\n    if is_keras_tensor:\n        # Compute the full input spec, including state and constants\n        full_input = [inputs] + additional_inputs\n        full_input_spec = self.input_spec + additional_specs\n        # Perform the call with temporarily replaced input_spec\n        original_input_spec = self.input_spec\n        self.input_spec = full_input_spec\n        output = super(RNN, self).__call__(full_input, **kwargs)\n        self.input_spec = original_input_spec\n        return output\n    else:\n        return super(RNN, self).__call__(inputs, **kwargs)\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 494,
                    "end_line": 539,
                    "variables": {
                        "inputs": [
                            495,
                            496,
                            530,
                            499,
                            539
                        ],
                        "initial_state": [
                            495,
                            496,
                            498,
                            507,
                            508,
                            509,
                            511
                        ],
                        "constants": [
                            513,
                            514,
                            515,
                            517,
                            518,
                            495,
                            496,
                            498
                        ],
                        "self._standardize_args": [
                            495
                        ],
                        "self": [
                            512,
                            516,
                            518,
                            519,
                            495,
                            499,
                            531,
                            533,
                            534,
                            535,
                            536,
                            539,
                            510
                        ],
                        "__call__": [
                            539,
                            499,
                            535
                        ],
                        "super": [
                            539,
                            499,
                            535
                        ],
                        "RNN": [
                            539,
                            499,
                            535
                        ],
                        "kwargs": [
                            514,
                            499,
                            535,
                            539,
                            508
                        ],
                        "additional_inputs": [
                            515,
                            521,
                            522,
                            530,
                            505,
                            509
                        ],
                        "additional_specs": [
                            512,
                            506,
                            531,
                            519
                        ],
                        "self.state_spec": [
                            512,
                            510
                        ],
                        "InputSpec": [
                            516,
                            510
                        ],
                        "K.int_shape": [
                            516,
                            510
                        ],
                        "K": [
                            516,
                            510
                        ],
                        "state": [
                            510,
                            511
                        ],
                        "self.constants_spec": [
                            516,
                            519
                        ],
                        "constant": [
                            516,
                            517
                        ],
                        "self._num_constants": [
                            518
                        ],
                        "len": [
                            518
                        ],
                        "is_keras_tensor": [
                            528,
                            521,
                            523
                        ],
                        "hasattr": [
                            521,
                            523
                        ],
                        "tensor": [
                            522,
                            523
                        ],
                        "ValueError": [
                            524
                        ],
                        "full_input": [
                            530,
                            535
                        ],
                        "full_input_spec": [
                            531,
                            534
                        ],
                        "self.input_spec": [
                            536,
                            531,
                            533,
                            534
                        ],
                        "original_input_spec": [
                            536,
                            533
                        ],
                        "output": [
                            537,
                            535
                        ]
                    },
                    "filtered_variables": {
                        "inputs": [
                            495,
                            496,
                            530,
                            499,
                            539
                        ],
                        "initial_state": [
                            495,
                            496,
                            498,
                            507,
                            508,
                            509,
                            511
                        ],
                        "constants": [
                            513,
                            514,
                            515,
                            517,
                            518,
                            495,
                            496,
                            498
                        ],
                        "self._standardize_args": [
                            495
                        ],
                        "self": [
                            512,
                            516,
                            518,
                            519,
                            495,
                            499,
                            531,
                            533,
                            534,
                            535,
                            536,
                            539,
                            510
                        ],
                        "__call__": [
                            539,
                            499,
                            535
                        ],
                        "RNN": [
                            539,
                            499,
                            535
                        ],
                        "kwargs": [
                            514,
                            499,
                            535,
                            539,
                            508
                        ],
                        "additional_inputs": [
                            515,
                            521,
                            522,
                            530,
                            505,
                            509
                        ],
                        "additional_specs": [
                            512,
                            506,
                            531,
                            519
                        ],
                        "self.state_spec": [
                            512,
                            510
                        ],
                        "InputSpec": [
                            516,
                            510
                        ],
                        "K.int_shape": [
                            516,
                            510
                        ],
                        "K": [
                            516,
                            510
                        ],
                        "state": [
                            510,
                            511
                        ],
                        "self.constants_spec": [
                            516,
                            519
                        ],
                        "constant": [
                            516,
                            517
                        ],
                        "self._num_constants": [
                            518
                        ],
                        "is_keras_tensor": [
                            528,
                            521,
                            523
                        ],
                        "tensor": [
                            522,
                            523
                        ],
                        "full_input": [
                            530,
                            535
                        ],
                        "full_input_spec": [
                            531,
                            534
                        ],
                        "self.input_spec": [
                            536,
                            531,
                            533,
                            534
                        ],
                        "original_input_spec": [
                            536,
                            533
                        ],
                        "output": [
                            537,
                            535
                        ]
                    },
                    "diff_line_number": 521,
                    "class_data": {
                        "signature": "class RNN(Layer)",
                        "docstring": "Base class for recurrent layers.\n\n# Arguments\n    cell: A RNN cell instance. A RNN cell is a class that has:\n        - a `call(input_at_t, states_at_t)` method, returning\n            `(output_at_t, states_at_t_plus_1)`. The call method of the\n            cell can also take the optional argument `constants`, see\n            section \"Note on passing external constants\" below.\n        - a `state_size` attribute. This can be a single integer\n            (single state) in which case it is\n            the size of the recurrent state\n            (which should be the same as the size of the cell output).\n            This can also be a list/tuple of integers\n            (one size per state). In this case, the first entry\n            (`state_size[0]`) should be the same as\n            the size of the cell output.\n        It is also possible for `cell` to be a list of RNN cell instances,\n        in which cases the cells get stacked on after the other in the RNN,\n        implementing an efficient stacked RNN.\n    return_sequences: Boolean. Whether to return the last output.\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    unroll: Boolean (default False).\n        If True, the network will be unrolled,\n        else a symbolic loop will be used.\n        Unrolling can speed-up a RNN,\n        although it tends to be more memory-intensive.\n        Unrolling is only suitable for short sequences.\n    input_dim: dimensionality of the input (integer).\n        This argument (or alternatively,\n        the keyword argument `input_shape`)\n        is required when using this layer as the first layer in a model.\n    input_length: Length of input sequences, to be specified\n        when it is constant.\n        This argument is required if you are going to connect\n        `Flatten` then `Dense` layers upstream\n        (without it, the shape of the dense outputs cannot be computed).\n        Note that if the recurrent layer is not the first layer\n        in your model, you would need to specify the input length\n        at the level of the first layer\n        (e.g. via the `input_shape` argument)\n\n# Input shape\n    3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n# Output shape\n    - if `return_state`: a list of tensors. The first tensor is\n        the output. The remaining tensors are the last states,\n        each with shape `(batch_size, units)`.\n    - if `return_sequences`: 3D tensor with shape\n        `(batch_size, timesteps, units)`.\n    - else, 2D tensor with shape `(batch_size, units)`.\n\n# Masking\n    This layer supports masking for input data with a variable number\n    of timesteps. To introduce masks to your data,\n    use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n    set to `True`.\n\n# Note on using statefulness in RNNs\n    You can set RNN layers to be 'stateful', which means that the states\n    computed for the samples in one batch will be reused as initial states\n    for the samples in the next batch. This assumes a one-to-one mapping\n    between samples in different successive batches.\n\n    To enable statefulness:\n        - specify `stateful=True` in the layer constructor.\n        - specify a fixed batch size for your model, by passing\n            if sequential model:\n              `batch_input_shape=(...)` to the first layer in your model.\n            else for functional model with 1 or more Input layers:\n              `batch_shape=(...)` to all the first layers in your model.\n            This is the expected shape of your inputs\n            *including the batch size*.\n            It should be a tuple of integers, e.g. `(32, 10, 100)`.\n        - specify `shuffle=False` when calling fit().\n\n    To reset the states of your model, call `.reset_states()` on either\n    a specific layer, or on your entire model.\n\n# Note on specifying the initial state of RNNs\n    You can specify the initial state of RNN layers symbolically by\n    calling them with the keyword argument `initial_state`. The value of\n    `initial_state` should be a tensor or list of tensors representing\n    the initial state of the RNN layer.\n\n    You can specify the initial state of RNN layers numerically by\n    calling `reset_states` with the keyword argument `states`. The value of\n    `states` should be a numpy array or list of numpy arrays representing\n    the initial state of the RNN layer.\n\n# Note on passing external constants to RNNs\n    You can pass \"external\" constants to the cell using the `constants`\n    keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n    requires that the `cell.call` method accepts the same keyword argument\n    `constants`. Such constants can be used to condition the cell\n    transformation on additional static inputs (not changing over time),\n    a.k.a. an attention mechanism.\n\n# Examples\n\n```python\n    # First, let's define a RNN Cell, as a layer subclass.\n\n    class MinimalRNNCell(keras.layers.Layer):\n\n        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(MinimalRNNCell, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                          initializer='uniform',\n                                          name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.built = True\n\n        def call(self, inputs, states):\n            prev_output = states[0]\n            h = K.dot(inputs, self.kernel)\n            output = h + K.dot(prev_output, self.recurrent_kernel)\n            return output, [output]\n\n    # Let's use this cell in a RNN layer:\n\n    cell = MinimalRNNCell(32)\n    x = keras.Input((None, 5))\n    layer = RNN(cell)\n    y = layer(x)\n\n    # Here's how to use the cell to build a stacked RNN:\n\n    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n    x = keras.Input((None, 5))\n    layer = RNN(cells)\n    y = layer(x)\n```",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs):\n    if isinstance(cell, (list, tuple)):\n        cell = StackedRNNCells(cell)\n    if not hasattr(cell, 'call'):\n        raise ValueError('`cell` should have a `call` method. The RNN was passed:', cell)\n    if not hasattr(cell, 'state_size'):\n        raise ValueError('The RNN cell should have an attribute `state_size` (tuple of integers, one integer per RNN state).')\n    super(RNN, self).__init__(**kwargs)\n    self.cell = cell\n    self.return_sequences = return_sequences\n    self.return_state = return_state\n    self.go_backwards = go_backwards\n    self.stateful = stateful\n    self.unroll = unroll\n    self.supports_masking = True\n    self.input_spec = [InputSpec(ndim=3)]\n    self.state_spec = None\n    self._states = None\n    self.constants_spec = None\n    self._num_constants = None",
                            "@property\ndef states(self):\n    if self._states is None:\n        if isinstance(self.cell.state_size, int):\n            num_states = 1\n        else:\n            num_states = len(self.cell.state_size)\n        return [None for _ in range(num_states)]\n    return self._states",
                            "@states.setter\ndef states(self, states):\n    self._states = states",
                            "def compute_output_shape(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = self.cell.state_size\n    else:\n        state_size = [self.cell.state_size]\n    output_dim = state_size[0]\n    if self.return_sequences:\n        output_shape = (input_shape[0], input_shape[1], output_dim)\n    else:\n        output_shape = (input_shape[0], output_dim)\n    if self.return_state:\n        state_shape = [(input_shape[0], dim) for dim in state_size]\n        return [output_shape] + state_shape\n    else:\n        return output_shape",
                            "def compute_mask(self, inputs, mask):\n    if isinstance(mask, list):\n        mask = mask[0]\n    output_mask = mask if self.return_sequences else None\n    if self.return_state:\n        state_mask = [None for _ in self.states]\n        return [output_mask] + state_mask\n    else:\n        return output_mask",
                            "def build(self, input_shape):\n    if self._num_constants is not None:\n        constants_shape = input_shape[-self._num_constants:]\n    else:\n        constants_shape = None\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    batch_size = input_shape[0] if self.stateful else None\n    input_dim = input_shape[-1]\n    self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))\n    if isinstance(self.cell, Layer):\n        step_input_shape = (input_shape[0],) + input_shape[2:]\n        if constants_shape is not None:\n            self.cell.build([step_input_shape] + constants_shape)\n        else:\n            self.cell.build(step_input_shape)\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = list(self.cell.state_size)\n    else:\n        state_size = [self.cell.state_size]\n    if self.state_spec is not None:\n        if [spec.shape[-1] for spec in self.state_spec] != state_size:\n            raise ValueError('An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`={}; however `cell.state_size` is {}'.format(self.state_spec, self.cell.state_size))\n    else:\n        self.state_spec = [InputSpec(shape=(None, dim)) for dim in state_size]\n    if self.stateful:\n        self.reset_states()",
                            "def get_initial_state(self, inputs):\n    initial_state = K.zeros_like(inputs)\n    initial_state = K.sum(initial_state, axis=(1, 2))\n    initial_state = K.expand_dims(initial_state)\n    if hasattr(self.cell.state_size, '__len__'):\n        return [K.tile(initial_state, [1, dim]) for dim in self.cell.state_size]\n    else:\n        return [K.tile(initial_state, [1, self.cell.state_size])]",
                            "def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n    inputs, initial_state, constants = self._standardize_args(inputs, initial_state, constants)\n    if initial_state is None and constants is None:\n        return super(RNN, self).__call__(inputs, **kwargs)\n    additional_inputs = []\n    additional_specs = []\n    if initial_state is not None:\n        kwargs['initial_state'] = initial_state\n        additional_inputs += initial_state\n        self.state_spec = [InputSpec(shape=K.int_shape(state)) for state in initial_state]\n        additional_specs += self.state_spec\n    if constants is not None:\n        kwargs['constants'] = constants\n        additional_inputs += constants\n        self.constants_spec = [InputSpec(shape=K.int_shape(constant)) for constant in constants]\n        self._num_constants = len(constants)\n        additional_specs += self.constants_spec\n    is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')\n    for tensor in additional_inputs:\n        if hasattr(tensor, '_keras_history') != is_keras_tensor:\n            raise ValueError('The initial state or constants of an RNN layer cannot be specified with a mix of Keras tensors and non-Keras tensors')\n    if is_keras_tensor:\n        full_input = [inputs] + additional_inputs\n        full_input_spec = self.input_spec + additional_specs\n        original_input_spec = self.input_spec\n        self.input_spec = full_input_spec\n        output = super(RNN, self).__call__(full_input, **kwargs)\n        self.input_spec = original_input_spec\n        return output\n    else:\n        return super(RNN, self).__call__(inputs, **kwargs)",
                            "def call(self, inputs, mask=None, training=None, initial_state=None, constants=None):\n    if isinstance(inputs, list):\n        inputs = inputs[0]\n    if initial_state is not None:\n        pass\n    elif self.stateful:\n        initial_state = self.states\n    else:\n        initial_state = self.get_initial_state(inputs)\n    if isinstance(mask, list):\n        mask = mask[0]\n    if len(initial_state) != len(self.states):\n        raise ValueError('Layer has ' + str(len(self.states)) + ' states but was passed ' + str(len(initial_state)) + ' initial states.')\n    input_shape = K.int_shape(inputs)\n    timesteps = input_shape[1]\n    if self.unroll and timesteps in [None, 1]:\n        raise ValueError('Cannot unroll a RNN if the time dimension is undefined or equal to 1. \\n- If using a Sequential model, specify the time dimension by passing an `input_shape` or `batch_input_shape` argument to your first layer. If your first layer is an Embedding, you can also use the `input_length` argument.\\n- If using the functional API, specify the time dimension by passing a `shape` or `batch_shape` argument to your Input layer.')\n    kwargs = {}\n    if has_arg(self.cell.call, 'training'):\n        kwargs['training'] = training\n    if constants:\n        if not has_arg(self.cell.call, 'constants'):\n            raise ValueError('RNN cell does not support constants')\n\n        def step(inputs, states):\n            constants = states[-self._num_constants:]\n            states = states[:-self._num_constants]\n            return self.cell.call(inputs, states, constants=constants, **kwargs)\n    else:\n\n        def step(inputs, states):\n            return self.cell.call(inputs, states, **kwargs)\n    last_output, outputs, states = K.rnn(step, inputs, initial_state, constants=constants, go_backwards=self.go_backwards, mask=mask, unroll=self.unroll, input_length=timesteps)\n    if self.stateful:\n        updates = []\n        for i in range(len(states)):\n            updates.append((self.states[i], states[i]))\n        self.add_update(updates, inputs)\n    if self.return_sequences:\n        output = outputs\n    else:\n        output = last_output\n    if getattr(last_output, '_uses_learning_phase', False):\n        output._uses_learning_phase = True\n        for state in states:\n            state._uses_learning_phase = True\n    if self.return_state:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        else:\n            states = list(states)\n        return [output] + states\n    else:\n        return output",
                            "def _standardize_args(self, inputs, initial_state, constants):\n    \"\"\"Standardize `__call__` to a single list of tensor inputs.\n\n    When running a model loaded from file, the input tensors\n    `initial_state` and `constants` can be passed to `RNN.__call__` as part\n    of `inputs` instead of by the dedicated keyword arguments. This method\n    makes sure the arguments are separated and that `initial_state` and\n    `constants` are lists of tensors (or None).\n\n    # Arguments\n        inputs: tensor or list/tuple of tensors\n        initial_state: tensor or list of tensors or None\n        constants: tensor or list of tensors or None\n\n    # Returns\n        inputs: tensor\n        initial_state: list of tensors or None\n        constants: list of tensors or None\n    \"\"\"\n    if isinstance(inputs, list):\n        assert initial_state is None and constants is None\n        if self._num_constants is not None:\n            constants = inputs[-self._num_constants:]\n            inputs = inputs[:-self._num_constants]\n        if len(inputs) > 1:\n            initial_state = inputs[1:]\n        inputs = inputs[0]\n\n    def to_list_or_none(x):\n        if x is None or isinstance(x, list):\n            return x\n        if isinstance(x, tuple):\n            return list(x)\n        return [x]\n    initial_state = to_list_or_none(initial_state)\n    constants = to_list_or_none(constants)\n    return (inputs, initial_state, constants)",
                            "def reset_states(self, states=None):\n    if not self.stateful:\n        raise AttributeError('Layer must be stateful.')\n    batch_size = self.input_spec[0].shape[0]\n    if not batch_size:\n        raise ValueError('If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \\n- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.\\n- If using the functional API, specify the batch size by passing a `batch_shape` argument to your Input layer.')\n    if self.states[0] is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            self.states = [K.zeros((batch_size, dim)) for dim in self.cell.state_size]\n        else:\n            self.states = [K.zeros((batch_size, self.cell.state_size))]\n    elif states is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            for state, dim in zip(self.states, self.cell.state_size):\n                K.set_value(state, np.zeros((batch_size, dim)))\n        else:\n            K.set_value(self.states[0], np.zeros((batch_size, self.cell.state_size)))\n    else:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        if len(states) != len(self.states):\n            raise ValueError('Layer ' + self.name + ' expects ' + str(len(self.states)) + ' states, but it received ' + str(len(states)) + ' state values. Input received: ' + str(states))\n        for index, (value, state) in enumerate(zip(states, self.states)):\n            if hasattr(self.cell.state_size, '__len__'):\n                dim = self.cell.state_size[index]\n            else:\n                dim = self.cell.state_size\n            if value.shape != (batch_size, dim):\n                raise ValueError('State ' + str(index) + ' is incompatible with layer ' + self.name + ': expected shape=' + str((batch_size, dim)) + ', found shape=' + str(value.shape))\n            K.set_value(state, value)",
                            "def get_config(self):\n    config = {'return_sequences': self.return_sequences, 'return_state': self.return_state, 'go_backwards': self.go_backwards, 'stateful': self.stateful, 'unroll': self.unroll}\n    if self._num_constants is not None:\n        config['num_constants'] = self._num_constants\n    cell_config = self.cell.get_config()\n    config['cell'] = {'class_name': self.cell.__class__.__name__, 'config': cell_config}\n    base_config = super(RNN, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                            "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cell = deserialize_layer(config.pop('cell'), custom_objects=custom_objects)\n    num_constants = config.pop('num_constants', None)\n    layer = cls(cell, **config)\n    layer._num_constants = num_constants\n    return layer",
                            "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    if isinstance(self.cell, Layer):\n        return self.cell.trainable_weights\n    return []",
                            "@property\ndef non_trainable_weights(self):\n    if isinstance(self.cell, Layer):\n        if not self.trainable:\n            return self.cell.weights\n        return self.cell.non_trainable_weights\n    return []",
                            "@property\ndef losses(self):\n    if isinstance(self.cell, Layer):\n        return self.cell.losses\n    return []",
                            "def get_losses_for(self, inputs=None):\n    if isinstance(self.cell, Layer):\n        cell_losses = self.cell.get_losses_for(inputs)\n        return cell_losses + super(RNN, self).get_losses_for(inputs)\n    return super(RNN, self).get_losses_for(inputs)",
                            "def to_list_or_none(x):\n    if x is None or isinstance(x, list):\n        return x\n    if isinstance(x, tuple):\n        return list(x)\n    return [x]",
                            "def step(inputs, states):\n    constants = states[-self._num_constants:]\n    states = states[:-self._num_constants]\n    return self.cell.call(inputs, states, constants=constants, **kwargs)",
                            "def step(inputs, states):\n    return self.cell.call(inputs, states, **kwargs)"
                        ],
                        "constructor_variables": [
                            "_num_constants",
                            "supports_masking",
                            "state_spec",
                            "input_spec",
                            "constants_spec",
                            "unroll",
                            "cell",
                            "stateful",
                            "return_sequences",
                            "go_backwards",
                            "return_state",
                            "_states"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
                            "states(self)",
                            "states(self, states)",
                            "compute_output_shape(self, input_shape)",
                            "compute_mask(self, inputs, mask)",
                            "build(self, input_shape)",
                            "get_initial_state(self, inputs)",
                            "__call__(self, inputs, initial_state=None, constants=None, **kwargs)",
                            "call(self, inputs, mask=None, training=None, initial_state=None, constants=None)",
                            "_standardize_args(self, inputs, initial_state, constants)",
                            "reset_states(self, states=None)",
                            "get_config(self)",
                            "from_config(cls, config, custom_objects=None)",
                            "trainable_weights(self)",
                            "non_trainable_weights(self)",
                            "losses(self)",
                            "get_losses_for(self, inputs=None)",
                            "to_list_or_none(x)",
                            "step(inputs, states)",
                            "step(inputs, states)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "inscope_functions": [
                "def _generate_dropout_ones(inputs, dims):\n    # Currently, CNTK can't instantiate `ones` with symbolic shapes.\n    # Will update workaround once CNTK supports it.\n    if K.backend() == 'cntk':\n        ones = K.ones_like(K.reshape(inputs[:, 0], (-1, 1)))\n        return K.tile(ones, (1, dims))\n    else:\n        return K.ones((K.shape(inputs)[0], dims))",
                "def _generate_dropout_mask(ones, rate, training=None, count=1):\n    def dropped_inputs():\n        return K.dropout(ones, rate)\n\n    if count > 1:\n        return [K.in_train_phase(\n            dropped_inputs,\n            ones,\n            training=training) for _ in range(count)]\n    return K.in_train_phase(\n        dropped_inputs,\n        ones,\n        training=training)",
                "def __init__(self, cells, **kwargs):\n    for cell in cells:\n        if not hasattr(cell, 'call'):\n            raise ValueError('All cells must have a `call` method. '\n                             'received cells:', cells)\n        if not hasattr(cell, 'state_size'):\n            raise ValueError('All cells must have a '\n                             '`state_size` attribute. '\n                             'received cells:', cells)\n    self.cells = cells\n    super(StackedRNNCells, self).__init__(**kwargs)",
                "@property\ndef state_size(self):\n    # States are a flat list\n    # in reverse order of the cell stack.\n    # This allows to preserve the requirement\n    # `stack.state_size[0] == output_dim`.\n    # e.g. states of a 2-layer LSTM would be\n    # `[h2, c2, h1, c1]`\n    # (assuming one LSTM has states [h, c])\n    state_size = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            state_size += list(cell.state_size)\n        else:\n            state_size.append(cell.state_size)\n    return tuple(state_size)",
                "def call(self, inputs, states, constants=None, **kwargs):\n    # Recover per-cell states.\n    nested_states = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            nested_states.append(states[:len(cell.state_size)])\n            states = states[len(cell.state_size):]\n        else:\n            nested_states.append([states[0]])\n            states = states[1:]\n    nested_states = nested_states[::-1]\n\n    # Call the cells in order and store the returned states.\n    new_nested_states = []\n    for cell, states in zip(self.cells, nested_states):\n        if has_arg(cell.call, 'constants'):\n            inputs, states = cell.call(inputs, states,\n                                       constants=constants,\n                                       **kwargs)\n        else:\n            inputs, states = cell.call(inputs, states, **kwargs)\n        new_nested_states.append(states)\n\n    # Format the new states as a flat list\n    # in reverse cell order.\n    states = []\n    for cell_states in new_nested_states[::-1]:\n        states += cell_states\n    return inputs, states",
                "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        constants_shape = input_shape[1:]\n        input_shape = input_shape[0]\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            if has_arg(cell.call, 'constants'):\n                cell.build([input_shape] + constants_shape)\n            else:\n                cell.build(input_shape)\n        if hasattr(cell.state_size, '__len__'):\n            output_dim = cell.state_size[0]\n        else:\n            output_dim = cell.state_size\n        input_shape = (input_shape[0], output_dim)\n    self.built = True",
                "def get_config(self):\n    cells = []\n    for cell in self.cells:\n        cells.append({'class_name': cell.__class__.__name__,\n                      'config': cell.get_config()})\n    config = {'cells': cells}\n    base_config = super(StackedRNNCells, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cells = []\n    for cell_config in config.pop('cells'):\n        cells.append(deserialize_layer(cell_config,\n                                       custom_objects=custom_objects))\n    return cls(cells, **config)",
                "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.trainable_weights\n    return weights",
                "@property\ndef non_trainable_weights(self):\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.non_trainable_weights\n    if not self.trainable:\n        trainable_weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                trainable_weights += cell.trainable_weights\n        return trainable_weights + weights\n    return weights",
                "def get_weights(self):\n    \"\"\"Retrieves the weights of the model.\n\n    # Returns\n        A flat list of Numpy arrays.\n    \"\"\"\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.weights\n    return K.batch_get_value(weights)",
                "def set_weights(self, weights):\n    \"\"\"Sets the weights of the model.\n\n    # Arguments\n        weights: A list of Numpy arrays with shapes and types matching\n            the output of `model.get_weights()`.\n    \"\"\"\n    tuples = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            num_param = len(cell.weights)\n            weights = weights[:num_param]\n            for sw, w in zip(cell.weights, weights):\n                tuples.append((sw, w))\n            weights = weights[num_param:]\n    K.batch_set_value(tuples)",
                "@property\ndef losses(self):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.losses\n            losses += cell_losses\n    return losses",
                "def get_losses_for(self, inputs=None):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.get_losses_for(inputs)\n            losses += cell_losses\n    return losses",
                "def __init__(self, cell,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if isinstance(cell, (list, tuple)):\n        cell = StackedRNNCells(cell)\n    if not hasattr(cell, 'call'):\n        raise ValueError('`cell` should have a `call` method. '\n                         'The RNN was passed:', cell)\n    if not hasattr(cell, 'state_size'):\n        raise ValueError('The RNN cell should have '\n                         'an attribute `state_size` '\n                         '(tuple of integers, '\n                         'one integer per RNN state).')\n    super(RNN, self).__init__(**kwargs)\n    self.cell = cell\n    self.return_sequences = return_sequences\n    self.return_state = return_state\n    self.go_backwards = go_backwards\n    self.stateful = stateful\n    self.unroll = unroll\n\n    self.supports_masking = True\n    self.input_spec = [InputSpec(ndim=3)]\n    self.state_spec = None\n    self._states = None\n    self.constants_spec = None\n    self._num_constants = None",
                "@property\ndef states(self):\n    if self._states is None:\n        if isinstance(self.cell.state_size, int):\n            num_states = 1\n        else:\n            num_states = len(self.cell.state_size)\n        return [None for _ in range(num_states)]\n    return self._states",
                "@states.setter\ndef states(self, states):\n    self._states = states",
                "def compute_output_shape(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = self.cell.state_size\n    else:\n        state_size = [self.cell.state_size]\n    output_dim = state_size[0]\n\n    if self.return_sequences:\n        output_shape = (input_shape[0], input_shape[1], output_dim)\n    else:\n        output_shape = (input_shape[0], output_dim)\n\n    if self.return_state:\n        state_shape = [(input_shape[0], dim) for dim in state_size]\n        return [output_shape] + state_shape\n    else:\n        return output_shape",
                "def compute_mask(self, inputs, mask):\n    if isinstance(mask, list):\n        mask = mask[0]\n    output_mask = mask if self.return_sequences else None\n    if self.return_state:\n        state_mask = [None for _ in self.states]\n        return [output_mask] + state_mask\n    else:\n        return output_mask",
                "def build(self, input_shape):\n    # Note input_shape will be list of shapes of initial states and\n    # constants if these are passed in __call__.\n    if self._num_constants is not None:\n        constants_shape = input_shape[-self._num_constants:]\n    else:\n        constants_shape = None\n\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n\n    batch_size = input_shape[0] if self.stateful else None\n    input_dim = input_shape[-1]\n    self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))\n\n    # allow cell (if layer) to build before we set or validate state_spec\n    if isinstance(self.cell, Layer):\n        step_input_shape = (input_shape[0],) + input_shape[2:]\n        if constants_shape is not None:\n            self.cell.build([step_input_shape] + constants_shape)\n        else:\n            self.cell.build(step_input_shape)\n\n    # set or validate state_spec\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = list(self.cell.state_size)\n    else:\n        state_size = [self.cell.state_size]\n\n    if self.state_spec is not None:\n        # initial_state was passed in call, check compatibility\n        if [spec.shape[-1] for spec in self.state_spec] != state_size:\n            raise ValueError(\n                'An `initial_state` was passed that is not compatible with '\n                '`cell.state_size`. Received `state_spec`={}; '\n                'however `cell.state_size` is '\n                '{}'.format(self.state_spec, self.cell.state_size))\n    else:\n        self.state_spec = [InputSpec(shape=(None, dim))\n                           for dim in state_size]\n    if self.stateful:\n        self.reset_states()",
                "def get_initial_state(self, inputs):\n    # build an all-zero tensor of shape (samples, output_dim)\n    initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n    initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n    initial_state = K.expand_dims(initial_state)  # (samples, 1)\n    if hasattr(self.cell.state_size, '__len__'):\n        return [K.tile(initial_state, [1, dim])\n                for dim in self.cell.state_size]\n    else:\n        return [K.tile(initial_state, [1, self.cell.state_size])]",
                "def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n    inputs, initial_state, constants = self._standardize_args(\n        inputs, initial_state, constants)\n\n    if initial_state is None and constants is None:\n        return super(RNN, self).__call__(inputs, **kwargs)\n\n    # If any of `initial_state` or `constants` are specified and are Keras\n    # tensors, then add them to the inputs and temporarily modify the\n    # input_spec to include them.\n\n    additional_inputs = []\n    additional_specs = []\n    if initial_state is not None:\n        kwargs['initial_state'] = initial_state\n        additional_inputs += initial_state\n        self.state_spec = [InputSpec(shape=K.int_shape(state))\n                           for state in initial_state]\n        additional_specs += self.state_spec\n    if constants is not None:\n        kwargs['constants'] = constants\n        additional_inputs += constants\n        self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n                               for constant in constants]\n        self._num_constants = len(constants)\n        additional_specs += self.constants_spec\n    # at this point additional_inputs cannot be empty\n    is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')\n    for tensor in additional_inputs:\n        if hasattr(tensor, '_keras_history') != is_keras_tensor:\n            raise ValueError('The initial state or constants of an RNN'\n                             ' layer cannot be specified with a mix of'\n                             ' Keras tensors and non-Keras tensors')\n\n    if is_keras_tensor:\n        # Compute the full input spec, including state and constants\n        full_input = [inputs] + additional_inputs\n        full_input_spec = self.input_spec + additional_specs\n        # Perform the call with temporarily replaced input_spec\n        original_input_spec = self.input_spec\n        self.input_spec = full_input_spec\n        output = super(RNN, self).__call__(full_input, **kwargs)\n        self.input_spec = original_input_spec\n        return output\n    else:\n        return super(RNN, self).__call__(inputs, **kwargs)",
                "def call(self,\n         inputs,\n         mask=None,\n         training=None,\n         initial_state=None,\n         constants=None):\n    # input shape: `(samples, time (padded with zeros), input_dim)`\n    # note that the .build() method of subclasses MUST define\n    # self.input_spec and self.state_spec with complete input shapes.\n    if isinstance(inputs, list):\n        inputs = inputs[0]\n    if initial_state is not None:\n        pass\n    elif self.stateful:\n        initial_state = self.states\n    else:\n        initial_state = self.get_initial_state(inputs)\n\n    if isinstance(mask, list):\n        mask = mask[0]\n\n    if len(initial_state) != len(self.states):\n        raise ValueError('Layer has ' + str(len(self.states)) +\n                         ' states but was passed ' +\n                         str(len(initial_state)) +\n                         ' initial states.')\n    input_shape = K.int_shape(inputs)\n    timesteps = input_shape[1]\n    if self.unroll and timesteps in [None, 1]:\n        raise ValueError('Cannot unroll a RNN if the '\n                         'time dimension is undefined or equal to 1. \\n'\n                         '- If using a Sequential model, '\n                         'specify the time dimension by passing '\n                         'an `input_shape` or `batch_input_shape` '\n                         'argument to your first layer. If your '\n                         'first layer is an Embedding, you can '\n                         'also use the `input_length` argument.\\n'\n                         '- If using the functional API, specify '\n                         'the time dimension by passing a `shape` '\n                         'or `batch_shape` argument to your Input layer.')\n\n    kwargs = {}\n    if has_arg(self.cell.call, 'training'):\n        kwargs['training'] = training\n\n    if constants:\n        if not has_arg(self.cell.call, 'constants'):\n            raise ValueError('RNN cell does not support constants')\n\n        def step(inputs, states):\n            constants = states[-self._num_constants:]\n            states = states[:-self._num_constants]\n            return self.cell.call(inputs, states, constants=constants,\n                                  **kwargs)\n    else:\n        def step(inputs, states):\n            return self.cell.call(inputs, states, **kwargs)\n\n    last_output, outputs, states = K.rnn(step,\n                                         inputs,\n                                         initial_state,\n                                         constants=constants,\n                                         go_backwards=self.go_backwards,\n                                         mask=mask,\n                                         unroll=self.unroll,\n                                         input_length=timesteps)\n    if self.stateful:\n        updates = []\n        for i in range(len(states)):\n            updates.append((self.states[i], states[i]))\n        self.add_update(updates, inputs)\n\n    if self.return_sequences:\n        output = outputs\n    else:\n        output = last_output\n\n    # Properly set learning phase\n    if getattr(last_output, '_uses_learning_phase', False):\n        output._uses_learning_phase = True\n        for state in states:\n            state._uses_learning_phase = True\n\n    if self.return_state:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        else:\n            states = list(states)\n        return [output] + states\n    else:\n        return output",
                "def _standardize_args(self, inputs, initial_state, constants):\n    \"\"\"Standardize `__call__` to a single list of tensor inputs.\n\n    When running a model loaded from file, the input tensors\n    `initial_state` and `constants` can be passed to `RNN.__call__` as part\n    of `inputs` instead of by the dedicated keyword arguments. This method\n    makes sure the arguments are separated and that `initial_state` and\n    `constants` are lists of tensors (or None).\n\n    # Arguments\n        inputs: tensor or list/tuple of tensors\n        initial_state: tensor or list of tensors or None\n        constants: tensor or list of tensors or None\n\n    # Returns\n        inputs: tensor\n        initial_state: list of tensors or None\n        constants: list of tensors or None\n    \"\"\"\n    if isinstance(inputs, list):\n        assert initial_state is None and constants is None\n        if self._num_constants is not None:\n            constants = inputs[-self._num_constants:]\n            inputs = inputs[:-self._num_constants]\n        if len(inputs) > 1:\n            initial_state = inputs[1:]\n        inputs = inputs[0]\n\n    def to_list_or_none(x):\n        if x is None or isinstance(x, list):\n            return x\n        if isinstance(x, tuple):\n            return list(x)\n        return [x]\n\n    initial_state = to_list_or_none(initial_state)\n    constants = to_list_or_none(constants)\n\n    return inputs, initial_state, constants",
                "def reset_states(self, states=None):\n    if not self.stateful:\n        raise AttributeError('Layer must be stateful.')\n    batch_size = self.input_spec[0].shape[0]\n    if not batch_size:\n        raise ValueError('If a RNN is stateful, it needs to know '\n                         'its batch size. Specify the batch size '\n                         'of your input tensors: \\n'\n                         '- If using a Sequential model, '\n                         'specify the batch size by passing '\n                         'a `batch_input_shape` '\n                         'argument to your first layer.\\n'\n                         '- If using the functional API, specify '\n                         'the batch size by passing a '\n                         '`batch_shape` argument to your Input layer.')\n    # initialize state if None\n    if self.states[0] is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            self.states = [K.zeros((batch_size, dim))\n                           for dim in self.cell.state_size]\n        else:\n            self.states = [K.zeros((batch_size, self.cell.state_size))]\n    elif states is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            for state, dim in zip(self.states, self.cell.state_size):\n                K.set_value(state, np.zeros((batch_size, dim)))\n        else:\n            K.set_value(self.states[0],\n                        np.zeros((batch_size, self.cell.state_size)))\n    else:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        if len(states) != len(self.states):\n            raise ValueError('Layer ' + self.name + ' expects ' +\n                             str(len(self.states)) + ' states, '\n                             'but it received ' + str(len(states)) +\n                             ' state values. Input received: ' +\n                             str(states))\n        for index, (value, state) in enumerate(zip(states, self.states)):\n            if hasattr(self.cell.state_size, '__len__'):\n                dim = self.cell.state_size[index]\n            else:\n                dim = self.cell.state_size\n            if value.shape != (batch_size, dim):\n                raise ValueError('State ' + str(index) +\n                                 ' is incompatible with layer ' +\n                                 self.name + ': expected shape=' +\n                                 str((batch_size, dim)) +\n                                 ', found shape=' + str(value.shape))\n            # TODO: consider batch calls to `set_value`.\n            K.set_value(state, value)",
                "def get_config(self):\n    config = {'return_sequences': self.return_sequences,\n              'return_state': self.return_state,\n              'go_backwards': self.go_backwards,\n              'stateful': self.stateful,\n              'unroll': self.unroll}\n    if self._num_constants is not None:\n        config['num_constants'] = self._num_constants\n\n    cell_config = self.cell.get_config()\n    config['cell'] = {'class_name': self.cell.__class__.__name__,\n                      'config': cell_config}\n    base_config = super(RNN, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cell = deserialize_layer(config.pop('cell'),\n                             custom_objects=custom_objects)\n    num_constants = config.pop('num_constants', None)\n    layer = cls(cell, **config)\n    layer._num_constants = num_constants\n    return layer",
                "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    if isinstance(self.cell, Layer):\n        return self.cell.trainable_weights\n    return []",
                "@property\ndef non_trainable_weights(self):\n    if isinstance(self.cell, Layer):\n        if not self.trainable:\n            return self.cell.weights\n        return self.cell.non_trainable_weights\n    return []",
                "@property\ndef losses(self):\n    if isinstance(self.cell, Layer):\n        return self.cell.losses\n    return []",
                "def get_losses_for(self, inputs=None):\n    if isinstance(self.cell, Layer):\n        cell_losses = self.cell.get_losses_for(inputs)\n        return cell_losses + super(RNN, self).get_losses_for(inputs)\n    return super(RNN, self).get_losses_for(inputs)",
                "def __init__(self, units,\n             activation='tanh',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             **kwargs):\n    super(SimpleRNNCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.state_size = self.units\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                "def build(self, input_shape):\n    self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                  name='kernel',\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units),\n        name='recurrent_kernel',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight(shape=(self.units,),\n                                    name='bias',\n                                    initializer=self.bias_initializer,\n                                    regularizer=self.bias_regularizer,\n                                    constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
                "def call(self, inputs, states, training=None):\n    prev_output = states[0]\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n            self.dropout,\n            training=training)\n    if (0 < self.recurrent_dropout < 1 and\n            self._recurrent_dropout_mask is None):\n        self._recurrent_dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, self.units),\n            self.recurrent_dropout,\n            training=training)\n\n    dp_mask = self._dropout_mask\n    rec_dp_mask = self._recurrent_dropout_mask\n\n    if dp_mask is not None:\n        h = K.dot(inputs * dp_mask, self.kernel)\n    else:\n        h = K.dot(inputs, self.kernel)\n    if self.bias is not None:\n        h = K.bias_add(h, self.bias)\n\n    if rec_dp_mask is not None:\n        prev_output *= rec_dp_mask\n    output = h + K.dot(prev_output, self.recurrent_kernel)\n    if self.activation is not None:\n        output = self.activation(output)\n\n    # Properly set learning phase on output tensor.\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            output._uses_learning_phase = True\n    return output, [output]",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout}\n    base_config = super(SimpleRNNCell, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@interfaces.legacy_recurrent_support\ndef __init__(self, units,\n             activation='tanh',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             activity_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if 'implementation' in kwargs:\n        kwargs.pop('implementation')\n        warnings.warn('The `implementation` argument '\n                      'in `SimpleRNN` has been deprecated. '\n                      'Please remove it from your layer call.')\n    if K.backend() == 'theano':\n        warnings.warn(\n            'RNN dropout is no longer supported with the Theano backend '\n            'due to technical limitations. '\n            'You can either set `dropout` and `recurrent_dropout` to 0, '\n            'or use the TensorFlow backend.')\n        dropout = 0.\n        recurrent_dropout = 0.\n\n    cell = SimpleRNNCell(units,\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         recurrent_initializer=recurrent_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         recurrent_regularizer=recurrent_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         recurrent_constraint=recurrent_constraint,\n                         bias_constraint=bias_constraint,\n                         dropout=dropout,\n                         recurrent_dropout=recurrent_dropout)\n    super(SimpleRNN, self).__init__(cell,\n                                    return_sequences=return_sequences,\n                                    return_state=return_state,\n                                    go_backwards=go_backwards,\n                                    stateful=stateful,\n                                    unroll=unroll,\n                                    **kwargs)\n    self.activity_regularizer = regularizers.get(activity_regularizer)",
                "def call(self, inputs, mask=None, training=None, initial_state=None):\n    self.cell._dropout_mask = None\n    self.cell._recurrent_dropout_mask = None\n    return super(SimpleRNN, self).call(inputs,\n                                       mask=mask,\n                                       training=training,\n                                       initial_state=initial_state)",
                "@property\ndef units(self):\n    return self.cell.units",
                "@property\ndef activation(self):\n    return self.cell.activation",
                "@property\ndef use_bias(self):\n    return self.cell.use_bias",
                "@property\ndef kernel_initializer(self):\n    return self.cell.kernel_initializer",
                "@property\ndef recurrent_initializer(self):\n    return self.cell.recurrent_initializer",
                "@property\ndef bias_initializer(self):\n    return self.cell.bias_initializer",
                "@property\ndef kernel_regularizer(self):\n    return self.cell.kernel_regularizer",
                "@property\ndef recurrent_regularizer(self):\n    return self.cell.recurrent_regularizer",
                "@property\ndef bias_regularizer(self):\n    return self.cell.bias_regularizer",
                "@property\ndef kernel_constraint(self):\n    return self.cell.kernel_constraint",
                "@property\ndef recurrent_constraint(self):\n    return self.cell.recurrent_constraint",
                "@property\ndef bias_constraint(self):\n    return self.cell.bias_constraint",
                "@property\ndef dropout(self):\n    return self.cell.dropout",
                "@property\ndef recurrent_dropout(self):\n    return self.cell.recurrent_dropout",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout}\n    base_config = super(SimpleRNN, self).get_config()\n    del base_config['cell']\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config):\n    if 'implementation' in config:\n        config.pop('implementation')\n    return cls(**config)",
                "def __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             **kwargs):\n    super(GRUCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.recurrent_activation = activations.get(recurrent_activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.implementation = implementation\n    self.state_size = self.units\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                "def build(self, input_shape):\n    input_dim = input_shape[-1]\n    self.kernel = self.add_weight(shape=(input_dim, self.units * 3),\n                                  name='kernel',\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units * 3),\n        name='recurrent_kernel',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n\n    if self.use_bias:\n        self.bias = self.add_weight(shape=(self.units * 3,),\n                                    name='bias',\n                                    initializer=self.bias_initializer,\n                                    regularizer=self.bias_regularizer,\n                                    constraint=self.bias_constraint)\n    else:\n        self.bias = None\n\n    self.kernel_z = self.kernel[:, :self.units]\n    self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n    self.kernel_r = self.kernel[:, self.units: self.units * 2]\n    self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                    self.units:\n                                                    self.units * 2]\n    self.kernel_h = self.kernel[:, self.units * 2:]\n    self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n    if self.use_bias:\n        self.bias_z = self.bias[:self.units]\n        self.bias_r = self.bias[self.units: self.units * 2]\n        self.bias_h = self.bias[self.units * 2:]\n    else:\n        self.bias_z = None\n        self.bias_r = None\n        self.bias_h = None\n    self.built = True",
                "def call(self, inputs, states, training=None):\n    h_tm1 = states[0]  # previous memory\n\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n            self.dropout,\n            training=training,\n            count=3)\n    if (0 < self.recurrent_dropout < 1 and\n            self._recurrent_dropout_mask is None):\n        self._recurrent_dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, self.units),\n            self.recurrent_dropout,\n            training=training,\n            count=3)\n\n    # dropout matrices for input units\n    dp_mask = self._dropout_mask\n    # dropout matrices for recurrent units\n    rec_dp_mask = self._recurrent_dropout_mask\n\n    if self.implementation == 1:\n        if 0. < self.dropout < 1.:\n            inputs_z = inputs * dp_mask[0]\n            inputs_r = inputs * dp_mask[1]\n            inputs_h = inputs * dp_mask[2]\n        else:\n            inputs_z = inputs\n            inputs_r = inputs\n            inputs_h = inputs\n        x_z = K.dot(inputs_z, self.kernel_z)\n        x_r = K.dot(inputs_r, self.kernel_r)\n        x_h = K.dot(inputs_h, self.kernel_h)\n        if self.use_bias:\n            x_z = K.bias_add(x_z, self.bias_z)\n            x_r = K.bias_add(x_r, self.bias_r)\n            x_h = K.bias_add(x_h, self.bias_h)\n\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1_z = h_tm1 * rec_dp_mask[0]\n            h_tm1_r = h_tm1 * rec_dp_mask[1]\n            h_tm1_h = h_tm1 * rec_dp_mask[2]\n        else:\n            h_tm1_z = h_tm1\n            h_tm1_r = h_tm1\n            h_tm1_h = h_tm1\n        z = self.recurrent_activation(x_z + K.dot(h_tm1_z,\n                                                  self.recurrent_kernel_z))\n        r = self.recurrent_activation(x_r + K.dot(h_tm1_r,\n                                                  self.recurrent_kernel_r))\n\n        hh = self.activation(x_h + K.dot(r * h_tm1_h,\n                                         self.recurrent_kernel_h))\n    else:\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n        matrix_x = K.dot(inputs, self.kernel)\n        if self.use_bias:\n            matrix_x = K.bias_add(matrix_x, self.bias)\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1 *= rec_dp_mask[0]\n        matrix_inner = K.dot(h_tm1,\n                             self.recurrent_kernel[:, :2 * self.units])\n\n        x_z = matrix_x[:, :self.units]\n        x_r = matrix_x[:, self.units: 2 * self.units]\n        recurrent_z = matrix_inner[:, :self.units]\n        recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n        z = self.recurrent_activation(x_z + recurrent_z)\n        r = self.recurrent_activation(x_r + recurrent_r)\n\n        x_h = matrix_x[:, 2 * self.units:]\n        recurrent_h = K.dot(r * h_tm1,\n                            self.recurrent_kernel[:, 2 * self.units:])\n        hh = self.activation(x_h + recurrent_h)\n    h = z * h_tm1 + (1 - z) * hh\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            h._uses_learning_phase = True\n    return h, [h]",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'recurrent_activation': activations.serialize(self.recurrent_activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout,\n              'implementation': self.implementation}\n    base_config = super(GRUCell, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@interfaces.legacy_recurrent_support\ndef __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             activity_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if implementation == 0:\n        warnings.warn('`implementation=0` has been deprecated, '\n                      'and now defaults to `implementation=1`.'\n                      'Please update your layer call.')\n    if K.backend() == 'theano':\n        warnings.warn(\n            'RNN dropout is no longer supported with the Theano backend '\n            'due to technical limitations. '\n            'You can either set `dropout` and `recurrent_dropout` to 0, '\n            'or use the TensorFlow backend.')\n        dropout = 0.\n        recurrent_dropout = 0.\n\n    cell = GRUCell(units,\n                   activation=activation,\n                   recurrent_activation=recurrent_activation,\n                   use_bias=use_bias,\n                   kernel_initializer=kernel_initializer,\n                   recurrent_initializer=recurrent_initializer,\n                   bias_initializer=bias_initializer,\n                   kernel_regularizer=kernel_regularizer,\n                   recurrent_regularizer=recurrent_regularizer,\n                   bias_regularizer=bias_regularizer,\n                   kernel_constraint=kernel_constraint,\n                   recurrent_constraint=recurrent_constraint,\n                   bias_constraint=bias_constraint,\n                   dropout=dropout,\n                   recurrent_dropout=recurrent_dropout,\n                   implementation=implementation)\n    super(GRU, self).__init__(cell,\n                              return_sequences=return_sequences,\n                              return_state=return_state,\n                              go_backwards=go_backwards,\n                              stateful=stateful,\n                              unroll=unroll,\n                              **kwargs)\n    self.activity_regularizer = regularizers.get(activity_regularizer)",
                "def call(self, inputs, mask=None, training=None, initial_state=None):\n    self.cell._dropout_mask = None\n    self.cell._recurrent_dropout_mask = None\n    return super(GRU, self).call(inputs,\n                                 mask=mask,\n                                 training=training,\n                                 initial_state=initial_state)",
                "@property\ndef units(self):\n    return self.cell.units",
                "@property\ndef activation(self):\n    return self.cell.activation",
                "@property\ndef recurrent_activation(self):\n    return self.cell.recurrent_activation",
                "@property\ndef use_bias(self):\n    return self.cell.use_bias",
                "@property\ndef kernel_initializer(self):\n    return self.cell.kernel_initializer",
                "@property\ndef recurrent_initializer(self):\n    return self.cell.recurrent_initializer",
                "@property\ndef bias_initializer(self):\n    return self.cell.bias_initializer",
                "@property\ndef kernel_regularizer(self):\n    return self.cell.kernel_regularizer",
                "@property\ndef recurrent_regularizer(self):\n    return self.cell.recurrent_regularizer",
                "@property\ndef bias_regularizer(self):\n    return self.cell.bias_regularizer",
                "@property\ndef kernel_constraint(self):\n    return self.cell.kernel_constraint",
                "@property\ndef recurrent_constraint(self):\n    return self.cell.recurrent_constraint",
                "@property\ndef bias_constraint(self):\n    return self.cell.bias_constraint",
                "@property\ndef dropout(self):\n    return self.cell.dropout",
                "@property\ndef recurrent_dropout(self):\n    return self.cell.recurrent_dropout",
                "@property\ndef implementation(self):\n    return self.cell.implementation",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'recurrent_activation': activations.serialize(self.recurrent_activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout,\n              'implementation': self.implementation}\n    base_config = super(GRU, self).get_config()\n    del base_config['cell']\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config):\n    if 'implementation' in config and config['implementation'] == 0:\n        config['implementation'] = 1\n    return cls(**config)",
                "def __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             unit_forget_bias=True,\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             **kwargs):\n    super(LSTMCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.recurrent_activation = activations.get(recurrent_activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.unit_forget_bias = unit_forget_bias\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.implementation = implementation\n    self.state_size = (self.units, self.units)\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                "def build(self, input_shape):\n    input_dim = input_shape[-1]\n    self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n                                  name='kernel',\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units * 4),\n        name='recurrent_kernel',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n\n    if self.use_bias:\n        if self.unit_forget_bias:\n            def bias_initializer(_, *args, **kwargs):\n                return K.concatenate([\n                    self.bias_initializer((self.units,), *args, **kwargs),\n                    initializers.Ones()((self.units,), *args, **kwargs),\n                    self.bias_initializer((self.units * 2,), *args, **kwargs),\n                ])\n        else:\n            bias_initializer = self.bias_initializer\n        self.bias = self.add_weight(shape=(self.units * 4,),\n                                    name='bias',\n                                    initializer=bias_initializer,\n                                    regularizer=self.bias_regularizer,\n                                    constraint=self.bias_constraint)\n    else:\n        self.bias = None\n\n    self.kernel_i = self.kernel[:, :self.units]\n    self.kernel_f = self.kernel[:, self.units: self.units * 2]\n    self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n    self.kernel_o = self.kernel[:, self.units * 3:]\n\n    self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n    self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n    self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n    self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n    if self.use_bias:\n        self.bias_i = self.bias[:self.units]\n        self.bias_f = self.bias[self.units: self.units * 2]\n        self.bias_c = self.bias[self.units * 2: self.units * 3]\n        self.bias_o = self.bias[self.units * 3:]\n    else:\n        self.bias_i = None\n        self.bias_f = None\n        self.bias_c = None\n        self.bias_o = None\n    self.built = True",
                "def call(self, inputs, states, training=None):\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n            self.dropout,\n            training=training,\n            count=4)\n    if (0 < self.recurrent_dropout < 1 and\n            self._recurrent_dropout_mask is None):\n        self._recurrent_dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, self.units),\n            self.recurrent_dropout,\n            training=training,\n            count=4)\n\n    # dropout matrices for input units\n    dp_mask = self._dropout_mask\n    # dropout matrices for recurrent units\n    rec_dp_mask = self._recurrent_dropout_mask\n\n    h_tm1 = states[0]  # previous memory state\n    c_tm1 = states[1]  # previous carry state\n\n    if self.implementation == 1:\n        if 0 < self.dropout < 1.:\n            inputs_i = inputs * dp_mask[0]\n            inputs_f = inputs * dp_mask[1]\n            inputs_c = inputs * dp_mask[2]\n            inputs_o = inputs * dp_mask[3]\n        else:\n            inputs_i = inputs\n            inputs_f = inputs\n            inputs_c = inputs\n            inputs_o = inputs\n        x_i = K.dot(inputs_i, self.kernel_i)\n        x_f = K.dot(inputs_f, self.kernel_f)\n        x_c = K.dot(inputs_c, self.kernel_c)\n        x_o = K.dot(inputs_o, self.kernel_o)\n        if self.use_bias:\n            x_i = K.bias_add(x_i, self.bias_i)\n            x_f = K.bias_add(x_f, self.bias_f)\n            x_c = K.bias_add(x_c, self.bias_c)\n            x_o = K.bias_add(x_o, self.bias_o)\n\n        if 0 < self.recurrent_dropout < 1.:\n            h_tm1_i = h_tm1 * rec_dp_mask[0]\n            h_tm1_f = h_tm1 * rec_dp_mask[1]\n            h_tm1_c = h_tm1 * rec_dp_mask[2]\n            h_tm1_o = h_tm1 * rec_dp_mask[3]\n        else:\n            h_tm1_i = h_tm1\n            h_tm1_f = h_tm1\n            h_tm1_c = h_tm1\n            h_tm1_o = h_tm1\n        i = self.recurrent_activation(x_i + K.dot(h_tm1_i,\n                                                  self.recurrent_kernel_i))\n        f = self.recurrent_activation(x_f + K.dot(h_tm1_f,\n                                                  self.recurrent_kernel_f))\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,\n                                                        self.recurrent_kernel_c))\n        o = self.recurrent_activation(x_o + K.dot(h_tm1_o,\n                                                  self.recurrent_kernel_o))\n    else:\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n        z = K.dot(inputs, self.kernel)\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1 *= rec_dp_mask[0]\n        z += K.dot(h_tm1, self.recurrent_kernel)\n        if self.use_bias:\n            z = K.bias_add(z, self.bias)\n\n        z0 = z[:, :self.units]\n        z1 = z[:, self.units: 2 * self.units]\n        z2 = z[:, 2 * self.units: 3 * self.units]\n        z3 = z[:, 3 * self.units:]\n\n        i = self.recurrent_activation(z0)\n        f = self.recurrent_activation(z1)\n        c = f * c_tm1 + i * self.activation(z2)\n        o = self.recurrent_activation(z3)\n\n    h = o * self.activation(c)\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            h._uses_learning_phase = True\n    return h, [h, c]",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'recurrent_activation': activations.serialize(self.recurrent_activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'unit_forget_bias': self.unit_forget_bias,\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout,\n              'implementation': self.implementation}\n    base_config = super(LSTMCell, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@interfaces.legacy_recurrent_support\ndef __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             unit_forget_bias=True,\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             activity_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if implementation == 0:\n        warnings.warn('`implementation=0` has been deprecated, '\n                      'and now defaults to `implementation=1`.'\n                      'Please update your layer call.')\n    if K.backend() == 'theano':\n        warnings.warn(\n            'RNN dropout is no longer supported with the Theano backend '\n            'due to technical limitations. '\n            'You can either set `dropout` and `recurrent_dropout` to 0, '\n            'or use the TensorFlow backend.')\n        dropout = 0.\n        recurrent_dropout = 0.\n\n    cell = LSTMCell(units,\n                    activation=activation,\n                    recurrent_activation=recurrent_activation,\n                    use_bias=use_bias,\n                    kernel_initializer=kernel_initializer,\n                    recurrent_initializer=recurrent_initializer,\n                    unit_forget_bias=unit_forget_bias,\n                    bias_initializer=bias_initializer,\n                    kernel_regularizer=kernel_regularizer,\n                    recurrent_regularizer=recurrent_regularizer,\n                    bias_regularizer=bias_regularizer,\n                    kernel_constraint=kernel_constraint,\n                    recurrent_constraint=recurrent_constraint,\n                    bias_constraint=bias_constraint,\n                    dropout=dropout,\n                    recurrent_dropout=recurrent_dropout,\n                    implementation=implementation)\n    super(LSTM, self).__init__(cell,\n                               return_sequences=return_sequences,\n                               return_state=return_state,\n                               go_backwards=go_backwards,\n                               stateful=stateful,\n                               unroll=unroll,\n                               **kwargs)\n    self.activity_regularizer = regularizers.get(activity_regularizer)",
                "def call(self, inputs, mask=None, training=None, initial_state=None):\n    self.cell._dropout_mask = None\n    self.cell._recurrent_dropout_mask = None\n    return super(LSTM, self).call(inputs,\n                                  mask=mask,\n                                  training=training,\n                                  initial_state=initial_state)",
                "@property\ndef units(self):\n    return self.cell.units",
                "@property\ndef activation(self):\n    return self.cell.activation",
                "@property\ndef recurrent_activation(self):\n    return self.cell.recurrent_activation",
                "@property\ndef use_bias(self):\n    return self.cell.use_bias",
                "@property\ndef kernel_initializer(self):\n    return self.cell.kernel_initializer",
                "@property\ndef recurrent_initializer(self):\n    return self.cell.recurrent_initializer",
                "@property\ndef bias_initializer(self):\n    return self.cell.bias_initializer",
                "@property\ndef unit_forget_bias(self):\n    return self.cell.unit_forget_bias",
                "@property\ndef kernel_regularizer(self):\n    return self.cell.kernel_regularizer",
                "@property\ndef recurrent_regularizer(self):\n    return self.cell.recurrent_regularizer",
                "@property\ndef bias_regularizer(self):\n    return self.cell.bias_regularizer",
                "@property\ndef kernel_constraint(self):\n    return self.cell.kernel_constraint",
                "@property\ndef recurrent_constraint(self):\n    return self.cell.recurrent_constraint",
                "@property\ndef bias_constraint(self):\n    return self.cell.bias_constraint",
                "@property\ndef dropout(self):\n    return self.cell.dropout",
                "@property\ndef recurrent_dropout(self):\n    return self.cell.recurrent_dropout",
                "@property\ndef implementation(self):\n    return self.cell.implementation",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'recurrent_activation': activations.serialize(self.recurrent_activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'unit_forget_bias': self.unit_forget_bias,\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout,\n              'implementation': self.implementation}\n    base_config = super(LSTM, self).get_config()\n    del base_config['cell']\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config):\n    if 'implementation' in config and config['implementation'] == 0:\n        config['implementation'] = 1\n    return cls(**config)",
                "def dropped_inputs():\n    return K.dropout(ones, rate)",
                "def to_list_or_none(x):\n    if x is None or isinstance(x, list):\n        return x\n    if isinstance(x, tuple):\n        return list(x)\n    return [x]",
                "def step(inputs, states):\n    constants = states[-self._num_constants:]\n    states = states[:-self._num_constants]\n    return self.cell.call(inputs, states, constants=constants,\n                          **kwargs)",
                "def step(inputs, states):\n    return self.cell.call(inputs, states, **kwargs)",
                "def bias_initializer(_, *args, **kwargs):\n    return K.concatenate([\n        self.bias_initializer((self.units,), *args, **kwargs),\n        initializers.Ones()((self.units,), *args, **kwargs),\n        self.bias_initializer((self.units * 2,), *args, **kwargs),\n    ])"
            ],
            "inscope_function_signatures": [
                "_generate_dropout_ones(inputs, dims)",
                "_generate_dropout_mask(ones, rate, training=None, count=1)",
                "__init__(self, cells, **kwargs)",
                "state_size(self)",
                "call(self, inputs, states, constants=None, **kwargs)",
                "build(self, input_shape)",
                "get_config(self)",
                "from_config(cls, config, custom_objects=None)",
                "trainable_weights(self)",
                "non_trainable_weights(self)",
                "get_weights(self)",
                "set_weights(self, weights)",
                "losses(self)",
                "get_losses_for(self, inputs=None)",
                "__init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
                "states(self)",
                "states(self, states)",
                "compute_output_shape(self, input_shape)",
                "compute_mask(self, inputs, mask)",
                "build(self, input_shape)",
                "get_initial_state(self, inputs)",
                "__call__(self, inputs, initial_state=None, constants=None, **kwargs)",
                "call(self, inputs, mask=None, training=None, initial_state=None, constants=None)",
                "_standardize_args(self, inputs, initial_state, constants)",
                "reset_states(self, states=None)",
                "get_config(self)",
                "from_config(cls, config, custom_objects=None)",
                "trainable_weights(self)",
                "non_trainable_weights(self)",
                "losses(self)",
                "get_losses_for(self, inputs=None)",
                "__init__(self, units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, **kwargs)",
                "build(self, input_shape)",
                "call(self, inputs, states, training=None)",
                "get_config(self)",
                "__init__(self, units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
                "call(self, inputs, mask=None, training=None, initial_state=None)",
                "units(self)",
                "activation(self)",
                "use_bias(self)",
                "kernel_initializer(self)",
                "recurrent_initializer(self)",
                "bias_initializer(self)",
                "kernel_regularizer(self)",
                "recurrent_regularizer(self)",
                "bias_regularizer(self)",
                "kernel_constraint(self)",
                "recurrent_constraint(self)",
                "bias_constraint(self)",
                "dropout(self)",
                "recurrent_dropout(self)",
                "get_config(self)",
                "from_config(cls, config)",
                "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, **kwargs)",
                "build(self, input_shape)",
                "call(self, inputs, states, training=None)",
                "get_config(self)",
                "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
                "call(self, inputs, mask=None, training=None, initial_state=None)",
                "units(self)",
                "activation(self)",
                "recurrent_activation(self)",
                "use_bias(self)",
                "kernel_initializer(self)",
                "recurrent_initializer(self)",
                "bias_initializer(self)",
                "kernel_regularizer(self)",
                "recurrent_regularizer(self)",
                "bias_regularizer(self)",
                "kernel_constraint(self)",
                "recurrent_constraint(self)",
                "bias_constraint(self)",
                "dropout(self)",
                "recurrent_dropout(self)",
                "implementation(self)",
                "get_config(self)",
                "from_config(cls, config)",
                "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, **kwargs)",
                "build(self, input_shape)",
                "call(self, inputs, states, training=None)",
                "get_config(self)",
                "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
                "call(self, inputs, mask=None, training=None, initial_state=None)",
                "units(self)",
                "activation(self)",
                "recurrent_activation(self)",
                "use_bias(self)",
                "kernel_initializer(self)",
                "recurrent_initializer(self)",
                "bias_initializer(self)",
                "unit_forget_bias(self)",
                "kernel_regularizer(self)",
                "recurrent_regularizer(self)",
                "bias_regularizer(self)",
                "kernel_constraint(self)",
                "recurrent_constraint(self)",
                "bias_constraint(self)",
                "dropout(self)",
                "recurrent_dropout(self)",
                "implementation(self)",
                "get_config(self)",
                "from_config(cls, config)",
                "dropped_inputs()",
                "to_list_or_none(x)",
                "step(inputs, states)",
                "step(inputs, states)",
                "bias_initializer(_, *args, **kwargs)"
            ],
            "variables_in_file": {
                "Layer": [
                    160,
                    768,
                    773,
                    198,
                    1638,
                    456,
                    779,
                    173,
                    206,
                    111,
                    754,
                    147,
                    212,
                    760,
                    25,
                    186,
                    155,
                    1151
                ],
                "cell": [
                    1528,
                    1037,
                    146,
                    147,
                    148,
                    154,
                    155,
                    156,
                    1051,
                    159,
                    160,
                    161,
                    172,
                    173,
                    174,
                    48,
                    49,
                    52,
                    2046,
                    185,
                    186,
                    187,
                    189,
                    69,
                    70,
                    71,
                    197,
                    73,
                    198,
                    199,
                    205,
                    206,
                    79,
                    80,
                    81,
                    82,
                    207,
                    90,
                    91,
                    92,
                    375,
                    96,
                    743,
                    1512,
                    746,
                    2029,
                    110,
                    111,
                    112,
                    113,
                    370,
                    115,
                    116,
                    117,
                    371,
                    119,
                    372,
                    374,
                    381,
                    125,
                    126,
                    127
                ],
                "cells": [
                    128,
                    135,
                    137,
                    139,
                    48,
                    51,
                    55,
                    56,
                    124,
                    126
                ],
                "hasattr": [
                    70,
                    488,
                    521,
                    712,
                    523,
                    80,
                    49,
                    464,
                    690,
                    52,
                    116,
                    372,
                    375,
                    696,
                    413
                ],
                "ValueError": [
                    706,
                    678,
                    472,
                    524,
                    588,
                    717,
                    50,
                    563,
                    373,
                    53,
                    376,
                    570
                ],
                "self.cells": [
                    69,
                    197,
                    154,
                    172,
                    205,
                    110,
                    79,
                    146,
                    56,
                    185,
                    90,
                    125,
                    159
                ],
                "self": [
                    2053,
                    2056,
                    2057,
                    2058,
                    2065,
                    2069,
                    2073,
                    2077,
                    2081,
                    2085,
                    2089,
                    2093,
                    2097,
                    2101,
                    56,
                    57,
                    2105,
                    2109,
                    2113,
                    69,
                    2117,
                    2121,
                    2125,
                    79,
                    2129,
                    2132,
                    2133,
                    2134,
                    2135,
                    2136,
                    2137,
                    90,
                    2138,
                    2139,
                    2140,
                    2141,
                    2142,
                    2143,
                    2144,
                    2145,
                    2146,
                    2147,
                    2148,
                    2149,
                    2150,
                    110,
                    121,
                    125,
                    129,
                    143,
                    146,
                    154,
                    157,
                    159,
                    172,
                    185,
                    197,
                    205,
                    380,
                    381,
                    382,
                    383,
                    384,
                    385,
                    386,
                    388,
                    389,
                    390,
                    391,
                    392,
                    393,
                    397,
                    398,
                    401,
                    403,
                    407,
                    413,
                    414,
                    416,
                    419,
                    424,
                    433,
                    434,
                    435,
                    443,
                    444,
                    451,
                    453,
                    456,
                    459,
                    461,
                    464,
                    465,
                    467,
                    469,
                    471,
                    476,
                    478,
                    480,
                    481,
                    488,
                    490,
                    492,
                    495,
                    499,
                    510,
                    512,
                    516,
                    518,
                    519,
                    531,
                    533,
                    534,
                    535,
                    536,
                    539,
                    554,
                    555,
                    557,
                    562,
                    563,
                    569,
                    583,
                    587,
                    591,
                    592,
                    593,
                    597,
                    603,
                    605,
                    607,
                    610,
                    611,
                    613,
                    624,
                    654,
                    655,
                    656,
                    674,
                    676,
                    689,
                    690,
                    691,
                    692,
                    694,
                    696,
                    697,
                    700,
                    701,
                    705,
                    706,
                    707,
                    711,
                    712,
                    713,
                    715,
                    719,
                    726,
                    727,
                    728,
                    729,
                    730,
                    731,
                    732,
                    734,
                    735,
                    737,
                    752,
                    754,
                    755,
                    760,
                    761,
                    762,
                    763,
                    768,
                    769,
                    773,
                    774,
                    775,
                    776,
                    838,
                    839,
                    840,
                    841,
                    843,
                    844,
                    845,
                    847,
                    848,
                    849,
                    851,
                    852,
                    853,
                    855,
                    856,
                    857,
                    858,
                    859,
                    862,
                    864,
                    865,
                    866,
                    867,
                    868,
                    870,
                    871,
                    872,
                    873,
                    874,
                    876,
                    877,
                    878,
                    880,
                    881,
                    885,
                    886,
                    888,
                    890,
                    891,
                    892,
                    893,
                    894,
                    897,
                    898,
                    901,
                    903,
                    904,
                    905,
                    909,
                    910,
                    911,
                    914,
                    920,
                    921,
                    922,
                    923,
                    924,
                    925,
                    926,
                    927,
                    928,
                    929,
                    930,
                    931,
                    932,
                    933,
                    934,
                    1051,
                    1058,
                    1061,
                    1062,
                    1063,
                    1070,
                    1074,
                    1078,
                    1082,
                    1086,
                    1090,
                    1094,
                    1098,
                    1102,
                    1106,
                    1110,
                    1114,
                    1118,
                    1122,
                    1125,
                    1126,
                    1127,
                    1128,
                    1129,
                    1130,
                    1131,
                    1132,
                    1133,
                    1134,
                    1135,
                    1136,
                    1137,
                    1138,
                    1139,
                    1140,
                    1224,
                    1225,
                    1226,
                    1227,
                    1228,
                    1230,
                    1231,
                    1232,
                    1234,
                    1235,
                    1236,
                    1238,
                    1239,
                    1240,
                    1242,
                    1243,
                    1244,
                    1245,
                    1246,
                    1247,
                    1251,
                    1253,
                    1254,
                    1255,
                    1256,
                    1257,
                    1259,
                    1260,
                    1261,
                    1263,
                    1264,
                    1266,
                    1267,
                    1268,
                    1270,
                    1272,
                    1273,
                    1274,
                    1275,
                    1276,
                    1277,
                    1278,
                    1279,
                    1281,
                    1282,
                    1283,
                    1284,
                    1286,
                    1287,
                    1288,
                    1289,
                    1294,
                    1295,
                    1297,
                    1300,
                    1301,
                    1302,
                    1303,
                    1304,
                    1309,
                    1311,
                    1313,
                    1314,
                    1322,
                    1323,
                    1324,
                    1325,
                    1326,
                    1327,
                    1328,
                    1330,
                    1338,
                    1339,
                    1340,
                    1341,
                    1343,
                    1344,
                    1346,
                    1348,
                    1349,
                    1350,
                    1351,
                    1354,
                    1356,
                    1357,
                    1358,
                    1359,
                    1361,
                    1362,
                    1364,
                    1366,
                    1367,
                    1369,
                    1375,
                    1376,
                    1377,
                    1378,
                    1379,
                    1380,
                    1381,
                    1382,
                    1383,
                    1384,
                    1385,
                    1386,
                    1387,
                    1388,
                    1389,
                    1390,
                    1391,
                    1528,
                    1535,
                    1538,
                    1539,
                    1540,
                    1547,
                    1551,
                    1555,
                    1559,
                    1563,
                    1567,
                    1571,
                    1575,
                    1579,
                    1583,
                    1587,
                    1591,
                    1595,
                    1599,
                    1603,
                    1607,
                    1610,
                    1611,
                    1612,
                    1613,
                    1614,
                    1615,
                    1616,
                    1617,
                    1618,
                    1619,
                    1620,
                    1621,
                    1622,
                    1623,
                    1624,
                    1625,
                    1626,
                    1627,
                    1716,
                    1717,
                    1718,
                    1719,
                    1720,
                    1722,
                    1723,
                    1724,
                    1725,
                    1727,
                    1728,
                    1729,
                    1731,
                    1732,
                    1733,
                    1735,
                    1736,
                    1737,
                    1738,
                    1739,
                    1740,
                    1744,
                    1746,
                    1747,
                    1748,
                    1749,
                    1750,
                    1752,
                    1753,
                    1754,
                    1756,
                    1757,
                    1760,
                    1761,
                    1762,
                    1765,
                    1766,
                    1769,
                    1770,
                    1772,
                    1774,
                    1775,
                    1776,
                    1777,
                    1779,
                    1780,
                    1781,
                    1782,
                    1784,
                    1785,
                    1786,
                    1787,
                    1788,
                    1790,
                    1791,
                    1792,
                    1793,
                    1794,
                    1797,
                    1798,
                    1800,
                    1803,
                    1804,
                    1805,
                    1806,
                    1807,
                    1812,
                    1814,
                    1819,
                    1820,
                    1830,
                    1831,
                    1832,
                    1833,
                    1834,
                    1835,
                    1836,
                    1837,
                    1838,
                    1840,
                    1850,
                    1851,
                    1852,
                    1853,
                    1854,
                    1855,
                    1856,
                    1857,
                    1859,
                    1861,
                    1862,
                    1864,
                    1865,
                    1866,
                    1868,
                    1869,
                    1870,
                    1871,
                    1873,
                    1874,
                    1875,
                    1876,
                    1878,
                    1879,
                    1885,
                    1886,
                    1887,
                    1888,
                    1889,
                    1890,
                    1891,
                    1892,
                    1893,
                    1894,
                    1895,
                    1896,
                    1897,
                    1898,
                    1899,
                    1900,
                    1901,
                    1902,
                    2046
                ],
                "__init__": [
                    838,
                    1224,
                    1716,
                    1528,
                    57,
                    1051,
                    380,
                    2046
                ],
                "super": [
                    129,
                    1540,
                    775,
                    776,
                    2058,
                    535,
                    1051,
                    539,
                    934,
                    1063,
                    1716,
                    57,
                    838,
                    1224,
                    1627,
                    737,
                    2150,
                    1902,
                    1391,
                    499,
                    1140,
                    1528,
                    380,
                    2046
                ],
                "StackedRNNCells": [
                    57,
                    129,
                    371
                ],
                "kwargs": [
                    1024,
                    514,
                    2052,
                    535,
                    539,
                    1057,
                    1716,
                    57,
                    582,
                    838,
                    584,
                    1224,
                    594,
                    597,
                    94,
                    96,
                    1760,
                    1761,
                    1762,
                    499,
                    380,
                    508,
                    1534,
                    1023
                ],
                "state_size": [
                    416,
                    417,
                    68,
                    71,
                    73,
                    74,
                    425,
                    465,
                    467,
                    471,
                    414,
                    479
                ],
                "cell.state_size": [
                    70,
                    71,
                    73,
                    80,
                    81,
                    82,
                    116,
                    117,
                    119
                ],
                "list": [
                    130,
                    652,
                    662,
                    665,
                    410,
                    550,
                    935,
                    431,
                    559,
                    703,
                    448,
                    71,
                    465,
                    1629,
                    738,
                    2152,
                    107,
                    1903,
                    1392,
                    625,
                    370,
                    628,
                    1142
                ],
                "state_size.append": [
                    73
                ],
                "tuple": [
                    74,
                    625,
                    370,
                    664,
                    703
                ],
                "property": [
                    1545,
                    395,
                    141,
                    1549,
                    2063,
                    1553,
                    2067,
                    1557,
                    151,
                    2071,
                    1561,
                    2075,
                    1565,
                    2079,
                    1569,
                    2083,
                    1573,
                    2087,
                    1577,
                    2091,
                    1068,
                    1581,
                    2095,
                    1072,
                    1585,
                    2099,
                    1076,
                    1589,
                    2103,
                    1080,
                    1593,
                    59,
                    1084,
                    1597,
                    2107,
                    2111,
                    1088,
                    1601,
                    194,
                    2115,
                    1092,
                    1605,
                    2119,
                    1096,
                    2123,
                    1100,
                    2127,
                    1104,
                    1108,
                    1112,
                    1116,
                    1120,
                    750,
                    758,
                    766
                ],
                "nested_states": [
                    78,
                    81,
                    84,
                    86,
                    90
                ],
                "nested_states.append": [
                    81,
                    84
                ],
                "states": [
                    1292,
                    405,
                    407,
                    1816,
                    1817,
                    695,
                    703,
                    704,
                    705,
                    708,
                    710,
                    711,
                    591,
                    592,
                    81,
                    82,
                    593,
                    84,
                    85,
                    597,
                    599,
                    90,
                    92,
                    96,
                    97,
                    609,
                    610,
                    101,
                    103,
                    104,
                    621,
                    625,
                    626,
                    628,
                    629,
                    884
                ],
                "len": [
                    609,
                    705,
                    707,
                    708,
                    518,
                    401,
                    81,
                    82,
                    562,
                    563,
                    565,
                    657,
                    187
                ],
                "new_nested_states": [
                    89,
                    102,
                    97
                ],
                "zip": [
                    697,
                    90,
                    189,
                    711
                ],
                "has_arg": [
                    112,
                    587,
                    91,
                    583
                ],
                "cell.call": [
                    96,
                    91,
                    92,
                    112
                ],
                "inputs": [
                    1540,
                    901,
                    774,
                    775,
                    776,
                    903,
                    1799,
                    2058,
                    652,
                    1806,
                    655,
                    656,
                    657,
                    530,
                    658,
                    659,
                    1296,
                    1303,
                    539,
                    1821,
                    1822,
                    671,
                    1823,
                    1824,
                    1826,
                    1315,
                    1316,
                    1317,
                    550,
                    551,
                    1063,
                    1319,
                    1320,
                    1321,
                    1827,
                    557,
                    1829,
                    567,
                    1347,
                    1348,
                    1860,
                    1861,
                    207,
                    593,
                    597,
                    600,
                    92,
                    96,
                    611,
                    485,
                    104,
                    1828,
                    495,
                    496,
                    499,
                    2165,
                    887,
                    2168,
                    893
                ],
                "constants": [
                    513,
                    514,
                    515,
                    517,
                    518,
                    669,
                    586,
                    653,
                    495,
                    496,
                    591,
                    498,
                    593,
                    655,
                    602,
                    93,
                    671
                ],
                "new_nested_states.append": [
                    97
                ],
                "cell_states": [
                    102,
                    103
                ],
                "isinstance": [
                    768,
                    773,
                    652,
                    398,
                    147,
                    662,
                    664,
                    410,
                    155,
                    160,
                    550,
                    173,
                    431,
                    559,
                    186,
                    703,
                    448,
                    198,
                    456,
                    206,
                    107,
                    111,
                    625,
                    370,
                    754,
                    760
                ],
                "input_shape": [
                    410,
                    411,
                    420,
                    422,
                    425,
                    567,
                    568,
                    444,
                    448,
                    449,
                    451,
                    452,
                    457,
                    1743,
                    862,
                    1250,
                    107,
                    108,
                    109,
                    113,
                    115,
                    120
                ],
                "constants_shape": [
                    458,
                    459,
                    108,
                    113,
                    444,
                    446
                ],
                "cell.build": [
                    113,
                    115
                ],
                "output_dim": [
                    417,
                    420,
                    422,
                    117,
                    119,
                    120
                ],
                "self.built": [
                    1289,
                    121,
                    1794,
                    881
                ],
                "cells.append": [
                    137,
                    126
                ],
                "cell.__class__.__name__": [
                    126
                ],
                "cell.__class__": [
                    126
                ],
                "cell.get_config": [
                    127
                ],
                "config": [
                    128,
                    130,
                    136,
                    139,
                    920,
                    935,
                    1610,
                    2132,
                    726,
                    732,
                    1629,
                    1885,
                    1375,
                    735,
                    1633,
                    738,
                    1634,
                    1635,
                    1125,
                    743,
                    2152,
                    745,
                    746,
                    2156,
                    2157,
                    2158,
                    1903,
                    1392,
                    1142,
                    1146,
                    1147,
                    1148
                ],
                "base_config": [
                    129,
                    130,
                    934,
                    935,
                    1627,
                    1628,
                    1629,
                    737,
                    738,
                    2150,
                    2151,
                    2152,
                    1902,
                    1391,
                    1392,
                    1903,
                    1140,
                    1141,
                    1142
                ],
                "get_config": [
                    129,
                    737,
                    934,
                    2150,
                    1902,
                    1391,
                    1140,
                    1627
                ],
                "dict": [
                    130,
                    738,
                    935,
                    2152,
                    1903,
                    1392,
                    1142,
                    1629
                ],
                "base_config.items": [
                    130,
                    738,
                    935,
                    2152,
                    1903,
                    1392,
                    1142,
                    1629
                ],
                "config.items": [
                    130,
                    738,
                    935,
                    2152,
                    1903,
                    1392,
                    1142,
                    1629
                ],
                "cell_config": [
                    136,
                    137,
                    734,
                    736
                ],
                "config.pop": [
                    136,
                    745,
                    1147,
                    743
                ],
                "deserialize_layer": [
                    137,
                    743
                ],
                "custom_objects": [
                    744,
                    138
                ],
                "cls": [
                    1635,
                    746,
                    139,
                    2158,
                    1148
                ],
                "classmethod": [
                    132,
                    740,
                    2154,
                    1144,
                    1631
                ],
                "self.trainable": [
                    752,
                    761,
                    157,
                    143
                ],
                "weights": [
                    162,
                    163,
                    171,
                    174,
                    175,
                    145,
                    188,
                    148,
                    149,
                    153,
                    156,
                    189,
                    191
                ],
                "cell.trainable_weights": [
                    161,
                    148
                ],
                "cell.non_trainable_weights": [
                    156
                ],
                "trainable_weights": [
                    161,
                    162,
                    158
                ],
                "cell.weights": [
                    187,
                    189,
                    174
                ],
                "K.batch_get_value": [
                    175
                ],
                "K": [
                    2176,
                    516,
                    901,
                    1028,
                    903,
                    1799,
                    905,
                    2180,
                    909,
                    1296,
                    1830,
                    1831,
                    1832,
                    1833,
                    1322,
                    1323,
                    1324,
                    1835,
                    1326,
                    175,
                    1327,
                    1328,
                    1836,
                    691,
                    1837,
                    1838,
                    694,
                    567,
                    698,
                    1338,
                    700,
                    1340,
                    1850,
                    1343,
                    192,
                    1852,
                    1854,
                    1856,
                    1348,
                    1861,
                    1350,
                    1864,
                    1353,
                    1866,
                    723,
                    1365,
                    599,
                    1503,
                    1759,
                    2020,
                    485,
                    486,
                    487,
                    489,
                    492,
                    2164,
                    2165,
                    2166,
                    887,
                    2168,
                    2173,
                    510
                ],
                "tuples": [
                    184,
                    190,
                    192
                ],
                "num_param": [
                    187,
                    188,
                    191
                ],
                "sw": [
                    189,
                    190
                ],
                "w": [
                    189,
                    190
                ],
                "tuples.append": [
                    190
                ],
                "K.batch_set_value": [
                    192
                ],
                "losses": [
                    196,
                    200,
                    201,
                    204,
                    208,
                    209
                ],
                "cell_losses": [
                    774,
                    199,
                    200,
                    775,
                    207,
                    208
                ],
                "cell.losses": [
                    199
                ],
                "cell.get_losses_for": [
                    207
                ],
                "RNN": [
                    737,
                    775,
                    776,
                    938,
                    1906,
                    499,
                    1395,
                    535,
                    539,
                    380
                ],
                "self.cell": [
                    1538,
                    1539,
                    2056,
                    2057,
                    1547,
                    1551,
                    2065,
                    1555,
                    2069,
                    1559,
                    2073,
                    1563,
                    2077,
                    1567,
                    2081,
                    1571,
                    1061,
                    1062,
                    1575,
                    2085,
                    2089,
                    1579,
                    2093,
                    1070,
                    1583,
                    2097,
                    1074,
                    1587,
                    2101,
                    1078,
                    1591,
                    2105,
                    1082,
                    1595,
                    2109,
                    1086,
                    1599,
                    2113,
                    1090,
                    1603,
                    2117,
                    1094,
                    1607,
                    583,
                    2121,
                    1098,
                    587,
                    2125,
                    1102,
                    593,
                    1106,
                    2129,
                    597,
                    1110,
                    1114,
                    1118,
                    1122,
                    690,
                    692,
                    694,
                    696,
                    697,
                    701,
                    712,
                    713,
                    715,
                    734,
                    735,
                    754,
                    755,
                    760,
                    762,
                    763,
                    768,
                    769,
                    773,
                    774,
                    381,
                    398,
                    401,
                    413,
                    414,
                    416,
                    456,
                    459,
                    461,
                    464,
                    465,
                    467,
                    476,
                    488,
                    490,
                    492
                ],
                "self.return_sequences": [
                    419,
                    613,
                    433,
                    726,
                    382
                ],
                "return_sequences": [
                    1529,
                    1052,
                    382,
                    2047
                ],
                "self.return_state": [
                    424,
                    624,
                    434,
                    727,
                    383
                ],
                "return_state": [
                    2048,
                    1530,
                    1053,
                    383
                ],
                "self.go_backwards": [
                    384,
                    603,
                    728
                ],
                "go_backwards": [
                    384,
                    2049,
                    1531,
                    1054
                ],
                "self.stateful": [
                    480,
                    385,
                    674,
                    451,
                    554,
                    729,
                    607
                ],
                "stateful": [
                    385,
                    2050,
                    1532,
                    1055
                ],
                "self.unroll": [
                    569,
                    386,
                    605,
                    730
                ],
                "unroll": [
                    1056,
                    386,
                    2051,
                    1533
                ],
                "self.supports_masking": [
                    388
                ],
                "self.input_spec": [
                    676,
                    453,
                    389,
                    531,
                    533,
                    534,
                    536
                ],
                "InputSpec": [
                    516,
                    453,
                    389,
                    510,
                    478
                ],
                "self.state_spec": [
                    512,
                    390,
                    510,
                    469,
                    471,
                    476,
                    478
                ],
                "self._states": [
                    407,
                    403,
                    397,
                    391
                ],
                "self.constants_spec": [
                    392,
                    516,
                    519
                ],
                "self._num_constants": [
                    732,
                    518,
                    393,
                    654,
                    591,
                    592,
                    655,
                    656,
                    443,
                    444,
                    731
                ],
                "self.cell.state_size": [
                    398,
                    401,
                    413,
                    414,
                    416,
                    690,
                    692,
                    694,
                    696,
                    697,
                    701,
                    712,
                    713,
                    715,
                    464,
                    465,
                    467,
                    476,
                    488,
                    490,
                    492
                ],
                "int": [
                    398
                ],
                "num_states": [
                    401,
                    402,
                    399
                ],
                "_": [
                    402,
                    435,
                    2179
                ],
                "range": [
                    609,
                    402,
                    2179
                ],
                "states.setter": [
                    405
                ],
                "output_shape": [
                    426,
                    428,
                    420,
                    422
                ],
                "state_shape": [
                    425,
                    426
                ],
                "dim": [
                    425,
                    490,
                    489,
                    713,
                    715,
                    716,
                    720,
                    691,
                    692,
                    697,
                    698,
                    478,
                    479
                ],
                "mask": [
                    1541,
                    1064,
                    2059,
                    559,
                    432,
                    433,
                    560,
                    431,
                    604
                ],
                "output_mask": [
                    433,
                    436,
                    438
                ],
                "state_mask": [
                    435,
                    436
                ],
                "self.states": [
                    705,
                    610,
                    707,
                    711,
                    555,
                    689,
                    562,
                    435,
                    563,
                    691,
                    694,
                    697,
                    700
                ],
                "batch_size": [
                    451,
                    676,
                    453,
                    677,
                    716,
                    720,
                    691,
                    694,
                    698,
                    701
                ],
                "input_dim": [
                    1250,
                    1251,
                    452,
                    453,
                    1743,
                    1744
                ],
                "step_input_shape": [
                    457,
                    459,
                    461
                ],
                "self.cell.build": [
                    459,
                    461
                ],
                "spec.shape": [
                    471
                ],
                "spec": [
                    471
                ],
                "format": [
                    473
                ],
                "self.reset_states": [
                    481
                ],
                "initial_state": [
                    1543,
                    653,
                    2061,
                    658,
                    668,
                    671,
                    552,
                    1066,
                    555,
                    557,
                    562,
                    565,
                    601,
                    485,
                    486,
                    487,
                    489,
                    492,
                    495,
                    496,
                    498,
                    507,
                    508,
                    509,
                    511
                ],
                "K.zeros_like": [
                    485
                ],
                "K.sum": [
                    486
                ],
                "K.expand_dims": [
                    487
                ],
                "K.tile": [
                    489,
                    492,
                    2166
                ],
                "self._standardize_args": [
                    495
                ],
                "__call__": [
                    539,
                    499,
                    535
                ],
                "additional_inputs": [
                    515,
                    521,
                    522,
                    530,
                    505,
                    509
                ],
                "additional_specs": [
                    512,
                    506,
                    531,
                    519
                ],
                "K.int_shape": [
                    516,
                    510,
                    567
                ],
                "state": [
                    711,
                    621,
                    622,
                    723,
                    697,
                    698,
                    510,
                    511
                ],
                "constant": [
                    516,
                    517
                ],
                "is_keras_tensor": [
                    528,
                    521,
                    523
                ],
                "tensor": [
                    522,
                    523
                ],
                "full_input": [
                    530,
                    535
                ],
                "full_input_spec": [
                    531,
                    534
                ],
                "original_input_spec": [
                    536,
                    533
                ],
                "output": [
                    614,
                    631,
                    616,
                    620,
                    909,
                    911,
                    916,
                    629,
                    917,
                    535,
                    537
                ],
                "self.get_initial_state": [
                    557
                ],
                "str": [
                    707,
                    708,
                    710,
                    717,
                    720,
                    721,
                    563,
                    565
                ],
                "timesteps": [
                    568,
                    569,
                    606
                ],
                "self.cell.call": [
                    593,
                    587,
                    597,
                    583
                ],
                "training": [
                    2179,
                    1305,
                    1542,
                    584,
                    1065,
                    1801,
                    2183,
                    2060,
                    1808,
                    1298,
                    915,
                    1880,
                    889,
                    1370,
                    895
                ],
                "last_output": [
                    616,
                    619,
                    599
                ],
                "outputs": [
                    614,
                    599
                ],
                "K.rnn": [
                    599
                ],
                "step": [
                    599
                ],
                "updates": [
                    608,
                    610,
                    611
                ],
                "i": [
                    609,
                    610,
                    1873,
                    1875,
                    1850,
                    1854
                ],
                "updates.append": [
                    610
                ],
                "self.add_update": [
                    611
                ],
                "getattr": [
                    619
                ],
                "output._uses_learning_phase": [
                    916,
                    620
                ],
                "state._uses_learning_phase": [
                    622
                ],
                "x": [
                    662,
                    663,
                    664,
                    665,
                    666
                ],
                "to_list_or_none": [
                    668,
                    669
                ],
                "AttributeError": [
                    675
                ],
                "shape": [
                    676
                ],
                "K.zeros": [
                    691,
                    694
                ],
                "K.set_value": [
                    698,
                    723,
                    700
                ],
                "np.zeros": [
                    698,
                    701
                ],
                "np": [
                    698,
                    701
                ],
                "self.name": [
                    706,
                    719
                ],
                "index": [
                    713,
                    717,
                    711
                ],
                "value": [
                    721,
                    723,
                    716,
                    711
                ],
                "enumerate": [
                    711
                ],
                "value.shape": [
                    721,
                    716
                ],
                "self.cell.get_config": [
                    734
                ],
                "self.cell.__class__.__name__": [
                    735
                ],
                "self.cell.__class__": [
                    735
                ],
                "num_constants": [
                    745,
                    747
                ],
                "layer": [
                    746,
                    747,
                    748
                ],
                "layer._num_constants": [
                    747
                ],
                "self.cell.trainable_weights": [
                    755
                ],
                "self.cell.weights": [
                    762
                ],
                "self.cell.non_trainable_weights": [
                    763
                ],
                "self.cell.losses": [
                    769
                ],
                "self.cell.get_losses_for": [
                    774
                ],
                "get_losses_for": [
                    776,
                    775
                ],
                "SimpleRNNCell": [
                    934,
                    1037,
                    838
                ],
                "self.units": [
                    1282,
                    1283,
                    1284,
                    1885,
                    1806,
                    1785,
                    1303,
                    920,
                    1277,
                    1786,
                    1788,
                    1717,
                    839,
                    1225,
                    1354,
                    1610,
                    1356,
                    1357,
                    1358,
                    1359,
                    1738,
                    1744,
                    1868,
                    1869,
                    1364,
                    1870,
                    1366,
                    1750,
                    1871,
                    857,
                    2132,
                    1245,
                    862,
                    1375,
                    1760,
                    1761,
                    1762,
                    1251,
                    868,
                    1125,
                    1766,
                    1257,
                    874,
                    1774,
                    1775,
                    1264,
                    1776,
                    1777,
                    1779,
                    1780,
                    1781,
                    1782,
                    1272,
                    1273,
                    1274,
                    1787,
                    1276,
                    893,
                    1278,
                    1279
                ],
                "units": [
                    839,
                    1512,
                    1225,
                    1037,
                    2029,
                    1717
                ],
                "self.activation": [
                    1376,
                    1126,
                    840,
                    1226,
                    1611,
                    910,
                    911,
                    1886,
                    1875,
                    2133,
                    1718,
                    1367,
                    1878,
                    921,
                    1854,
                    1343
                ],
                "activations.get": [
                    840,
                    1226,
                    1227,
                    1718,
                    1719
                ],
                "activations": [
                    1376,
                    1377,
                    1126,
                    840,
                    1226,
                    1227,
                    1611,
                    1612,
                    2133,
                    1718,
                    1719,
                    2134,
                    921,
                    1886,
                    1887
                ],
                "activation": [
                    840,
                    1513,
                    1226,
                    1038,
                    2030,
                    1718
                ],
                "self.use_bias": [
                    1888,
                    1281,
                    1378,
                    1349,
                    1127,
                    841,
                    873,
                    1834,
                    1228,
                    1325,
                    1613,
                    1263,
                    1784,
                    1865,
                    2135,
                    1720,
                    922,
                    1756
                ],
                "use_bias": [
                    841,
                    1515,
                    1228,
                    1039,
                    2032,
                    1720
                ],
                "self.kernel_initializer": [
                    864,
                    1889,
                    1379,
                    1253,
                    1128,
                    843,
                    1230,
                    1614,
                    1746,
                    2136,
                    1722,
                    923
                ],
                "initializers.get": [
                    843,
                    844,
                    845,
                    1230,
                    1231,
                    1232,
                    1722,
                    1723,
                    1724
                ],
                "initializers": [
                    923,
                    924,
                    925,
                    1722,
                    1723,
                    1724,
                    843,
                    844,
                    845,
                    1230,
                    1231,
                    1232,
                    1614,
                    1615,
                    1616,
                    2136,
                    2137,
                    2138,
                    1761,
                    1889,
                    1379,
                    1380,
                    1381,
                    1890,
                    1891,
                    1128,
                    1129,
                    1130
                ],
                "kernel_initializer": [
                    843,
                    1516,
                    1230,
                    1040,
                    2033,
                    1722
                ],
                "self.recurrent_initializer": [
                    1890,
                    1380,
                    870,
                    1129,
                    1259,
                    844,
                    1231,
                    1615,
                    1752,
                    2137,
                    1723,
                    924
                ],
                "recurrent_initializer": [
                    844,
                    1517,
                    1231,
                    1041,
                    2034,
                    1723
                ],
                "self.bias_initializer": [
                    1760,
                    1762,
                    1891,
                    1381,
                    1765,
                    1130,
                    876,
                    845,
                    1232,
                    1616,
                    1266,
                    2138,
                    1724,
                    925
                ],
                "bias_initializer": [
                    1765,
                    1768,
                    845,
                    1518,
                    1232,
                    1042,
                    2036,
                    1724
                ],
                "self.kernel_regularizer": [
                    865,
                    1893,
                    1254,
                    1382,
                    1131,
                    847,
                    1617,
                    1234,
                    1747,
                    2140,
                    926,
                    1727
                ],
                "regularizers.get": [
                    1728,
                    1729,
                    1058,
                    2053,
                    847,
                    848,
                    849,
                    1234,
                    1235,
                    1236,
                    1727,
                    1535
                ],
                "regularizers": [
                    2053,
                    926,
                    927,
                    928,
                    1058,
                    1727,
                    1728,
                    1729,
                    847,
                    848,
                    849,
                    1234,
                    1235,
                    1236,
                    1617,
                    1618,
                    1619,
                    1620,
                    2140,
                    2141,
                    2142,
                    2143,
                    1893,
                    1382,
                    1383,
                    1384,
                    1894,
                    1895,
                    1131,
                    1132,
                    1133,
                    1134,
                    1535
                ],
                "kernel_regularizer": [
                    1519,
                    847,
                    1234,
                    1043,
                    2037,
                    1727
                ],
                "self.recurrent_regularizer": [
                    1728,
                    1894,
                    871,
                    1383,
                    1132,
                    1260,
                    848,
                    1618,
                    1235,
                    1753,
                    2141,
                    927
                ],
                "recurrent_regularizer": [
                    1728,
                    848,
                    1520,
                    1235,
                    1044,
                    2038
                ],
                "self.bias_regularizer": [
                    928,
                    1729,
                    1895,
                    1384,
                    1769,
                    1133,
                    877,
                    849,
                    1267,
                    1236,
                    1619,
                    2142
                ],
                "bias_regularizer": [
                    1729,
                    849,
                    1521,
                    1236,
                    1045,
                    2039
                ],
                "self.kernel_constraint": [
                    2144,
                    929,
                    866,
                    1731,
                    1255,
                    1896,
                    1385,
                    1135,
                    851,
                    1748,
                    1621,
                    1238
                ],
                "constraints.get": [
                    1731,
                    1732,
                    1733,
                    851,
                    852,
                    853,
                    1238,
                    1239,
                    1240
                ],
                "constraints": [
                    929,
                    930,
                    931,
                    1731,
                    1732,
                    1733,
                    851,
                    852,
                    853,
                    1238,
                    1623,
                    1239,
                    1240,
                    1621,
                    1622,
                    2144,
                    2145,
                    2146,
                    1896,
                    1385,
                    1386,
                    1387,
                    1897,
                    1898,
                    1135,
                    1136,
                    1137
                ],
                "kernel_constraint": [
                    1731,
                    1522,
                    851,
                    1238,
                    1046,
                    2040
                ],
                "self.recurrent_constraint": [
                    2145,
                    930,
                    1732,
                    872,
                    1897,
                    1386,
                    1261,
                    1136,
                    852,
                    1622,
                    1239,
                    1754
                ],
                "recurrent_constraint": [
                    1732,
                    1523,
                    852,
                    1047,
                    1239,
                    2041
                ],
                "self.bias_constraint": [
                    2146,
                    931,
                    1733,
                    1770,
                    1387,
                    1898,
                    878,
                    1137,
                    1268,
                    853,
                    1623,
                    1240
                ],
                "bias_constraint": [
                    1733,
                    1524,
                    853,
                    1048,
                    2042,
                    1240
                ],
                "self.dropout": [
                    1797,
                    1800,
                    1294,
                    1297,
                    914,
                    1820,
                    1314,
                    932,
                    1346,
                    1859,
                    1735,
                    855,
                    1624,
                    1369,
                    1242,
                    1879,
                    2147,
                    1899,
                    1388,
                    1138,
                    885,
                    888
                ],
                "min": [
                    1735,
                    1736,
                    855,
                    856,
                    1242,
                    1243
                ],
                "max": [
                    1735,
                    1736,
                    855,
                    856,
                    1242,
                    1243
                ],
                "dropout": [
                    1509,
                    1735,
                    1034,
                    2026,
                    1525,
                    855,
                    1049,
                    1242,
                    2043
                ],
                "self.recurrent_dropout": [
                    1803,
                    1807,
                    914,
                    1300,
                    1304,
                    933,
                    1840,
                    1330,
                    1862,
                    1351,
                    1736,
                    1879,
                    856,
                    1625,
                    1369,
                    1243,
                    2148,
                    1900,
                    1389,
                    1139,
                    890,
                    894
                ],
                "recurrent_dropout": [
                    1510,
                    1736,
                    1035,
                    2027,
                    1526,
                    856,
                    1050,
                    1243,
                    2044
                ],
                "self.state_size": [
                    857,
                    1738,
                    1245
                ],
                "self._dropout_mask": [
                    897,
                    1797,
                    1798,
                    1739,
                    1294,
                    1295,
                    1812,
                    885,
                    886,
                    858,
                    1309,
                    1246
                ],
                "self._recurrent_dropout_mask": [
                    898,
                    859,
                    1311,
                    1740,
                    1804,
                    1805,
                    1301,
                    1302,
                    1814,
                    891,
                    892,
                    1247
                ],
                "self.kernel": [
                    1251,
                    1348,
                    901,
                    1861,
                    903,
                    1774,
                    1775,
                    1744,
                    1776,
                    1777,
                    1272,
                    1278,
                    1274,
                    862
                ],
                "self.add_weight": [
                    867,
                    1251,
                    1766,
                    1256,
                    874,
                    1264,
                    1744,
                    1749,
                    862
                ],
                "self.recurrent_kernel": [
                    867,
                    1256,
                    1864,
                    1354,
                    909,
                    1779,
                    1780,
                    1749,
                    1366,
                    1781,
                    1782,
                    1273,
                    1275,
                    1279
                ],
                "self.bias": [
                    1282,
                    1283,
                    1284,
                    1350,
                    1766,
                    904,
                    905,
                    874,
                    1866,
                    1772,
                    880,
                    1264,
                    1270,
                    1785,
                    1786,
                    1787,
                    1788
                ],
                "prev_output": [
                    908,
                    884,
                    909
                ],
                "_generate_dropout_mask": [
                    1798,
                    1805,
                    1295,
                    1302,
                    886,
                    892
                ],
                "_generate_dropout_ones": [
                    1799,
                    1806,
                    887,
                    1296,
                    1303,
                    893
                ],
                "K.shape": [
                    1296,
                    1799,
                    2168,
                    887
                ],
                "dp_mask": [
                    1824,
                    897,
                    1315,
                    900,
                    901,
                    1316,
                    1317,
                    1347,
                    1860,
                    1821,
                    1812,
                    1309,
                    1822,
                    1823
                ],
                "rec_dp_mask": [
                    898,
                    1863,
                    1352,
                    907,
                    908,
                    1841,
                    1842,
                    1331,
                    1332,
                    1333,
                    1814,
                    1843,
                    1844,
                    1311
                ],
                "h": [
                    901,
                    903,
                    905,
                    909,
                    1878,
                    1368,
                    1881,
                    1882,
                    1371,
                    1372
                ],
                "K.dot": [
                    901,
                    903,
                    909,
                    1830,
                    1831,
                    1832,
                    1833,
                    1322,
                    1323,
                    1324,
                    1850,
                    1338,
                    1852,
                    1340,
                    1854,
                    1343,
                    1856,
                    1348,
                    1861,
                    1864,
                    1353,
                    1365
                ],
                "K.bias_add": [
                    1350,
                    905,
                    1866,
                    1835,
                    1836,
                    1837,
                    1326,
                    1327,
                    1328,
                    1838
                ],
                "activations.serialize": [
                    1376,
                    1377,
                    1126,
                    1611,
                    1612,
                    2133,
                    2134,
                    921,
                    1886,
                    1887
                ],
                "initializers.serialize": [
                    1889,
                    1890,
                    1379,
                    1380,
                    1381,
                    1891,
                    1128,
                    1129,
                    1130,
                    1614,
                    1615,
                    1616,
                    2136,
                    2137,
                    2138,
                    923,
                    924,
                    925
                ],
                "regularizers.serialize": [
                    926,
                    927,
                    928,
                    1617,
                    1618,
                    1619,
                    1620,
                    2140,
                    2141,
                    2142,
                    2143,
                    1893,
                    1382,
                    1383,
                    1384,
                    1894,
                    1895,
                    1131,
                    1132,
                    1133,
                    1134
                ],
                "constraints.serialize": [
                    2144,
                    929,
                    930,
                    931,
                    2145,
                    2146,
                    1896,
                    1385,
                    1386,
                    1387,
                    1897,
                    1898,
                    1135,
                    1136,
                    1137,
                    1621,
                    1622,
                    1623
                ],
                "kwargs.pop": [
                    1024
                ],
                "warnings.warn": [
                    1504,
                    1025,
                    2017,
                    1029,
                    2021,
                    1500
                ],
                "warnings": [
                    1504,
                    1025,
                    2017,
                    1029,
                    2021,
                    1500
                ],
                "K.backend": [
                    2164,
                    1028,
                    2020,
                    1503
                ],
                "SimpleRNN": [
                    1051,
                    1140,
                    1063
                ],
                "self.activity_regularizer": [
                    1058,
                    2053,
                    1134,
                    1620,
                    2143,
                    1535
                ],
                "activity_regularizer": [
                    1058,
                    2053,
                    1535
                ],
                "interfaces.legacy_recurrent_support": [
                    1001,
                    1475,
                    1991
                ],
                "interfaces": [
                    1001,
                    1475,
                    1991
                ],
                "self.cell._dropout_mask": [
                    2056,
                    1538,
                    1061
                ],
                "self.cell._recurrent_dropout_mask": [
                    2057,
                    1539,
                    1062
                ],
                "call": [
                    2058,
                    1540,
                    1063
                ],
                "self.cell.units": [
                    2065,
                    1547,
                    1070
                ],
                "self.cell.activation": [
                    1074,
                    2069,
                    1551
                ],
                "self.cell.use_bias": [
                    2077,
                    1078,
                    1559
                ],
                "self.cell.kernel_initializer": [
                    2081,
                    1082,
                    1563
                ],
                "self.cell.recurrent_initializer": [
                    2085,
                    1086,
                    1567
                ],
                "self.cell.bias_initializer": [
                    2089,
                    1090,
                    1571
                ],
                "self.cell.kernel_regularizer": [
                    2097,
                    1094,
                    1575
                ],
                "self.cell.recurrent_regularizer": [
                    1098,
                    1579,
                    2101
                ],
                "self.cell.bias_regularizer": [
                    2105,
                    1102,
                    1583
                ],
                "self.cell.kernel_constraint": [
                    1106,
                    1587,
                    2109
                ],
                "self.cell.recurrent_constraint": [
                    2113,
                    1110,
                    1591
                ],
                "self.cell.bias_constraint": [
                    1114,
                    1595,
                    2117
                ],
                "self.cell.dropout": [
                    2121,
                    1118,
                    1599
                ],
                "self.cell.recurrent_dropout": [
                    1122,
                    1603,
                    2125
                ],
                "GRUCell": [
                    1224,
                    1512,
                    1391
                ],
                "self.recurrent_activation": [
                    1856,
                    1377,
                    1852,
                    1227,
                    1612,
                    1850,
                    1361,
                    1362,
                    1873,
                    1874,
                    1876,
                    2134,
                    1719,
                    1338,
                    1340,
                    1887
                ],
                "recurrent_activation": [
                    2031,
                    1514,
                    1227,
                    1719
                ],
                "self.implementation": [
                    1313,
                    2149,
                    1737,
                    1901,
                    1390,
                    1626,
                    1819,
                    1244
                ],
                "implementation": [
                    2016,
                    1737,
                    1527,
                    1499,
                    1244,
                    2045
                ],
                "self.kernel_z": [
                    1272,
                    1322
                ],
                "self.recurrent_kernel_z": [
                    1273,
                    1339
                ],
                "self.kernel_r": [
                    1274,
                    1323
                ],
                "self.recurrent_kernel_r": [
                    1275,
                    1341
                ],
                "self.kernel_h": [
                    1324,
                    1278
                ],
                "self.recurrent_kernel_h": [
                    1344,
                    1279
                ],
                "self.bias_z": [
                    1326,
                    1282,
                    1286
                ],
                "self.bias_r": [
                    1283,
                    1327,
                    1287
                ],
                "self.bias_h": [
                    1288,
                    1328,
                    1284
                ],
                "h_tm1": [
                    1292,
                    1816,
                    1841,
                    1842,
                    1331,
                    1332,
                    1333,
                    1843,
                    1847,
                    1848,
                    1335,
                    1336,
                    1337,
                    1844,
                    1846,
                    1849,
                    1863,
                    1352,
                    1353,
                    1864,
                    1365,
                    1368
                ],
                "inputs_z": [
                    1322,
                    1315,
                    1319
                ],
                "inputs_r": [
                    1320,
                    1323,
                    1316
                ],
                "inputs_h": [
                    1321,
                    1324,
                    1317
                ],
                "x_z": [
                    1322,
                    1356,
                    1326,
                    1361,
                    1338
                ],
                "x_r": [
                    1323,
                    1357,
                    1327,
                    1362,
                    1340
                ],
                "x_h": [
                    1324,
                    1328,
                    1364,
                    1367,
                    1343
                ],
                "h_tm1_z": [
                    1338,
                    1331,
                    1335
                ],
                "h_tm1_r": [
                    1336,
                    1332,
                    1340
                ],
                "h_tm1_h": [
                    1337,
                    1333,
                    1343
                ],
                "z": [
                    1861,
                    1864,
                    1866,
                    1868,
                    1869,
                    1870,
                    1871,
                    1361,
                    1368,
                    1338
                ],
                "r": [
                    1362,
                    1340,
                    1365,
                    1343
                ],
                "hh": [
                    1368,
                    1367,
                    1343
                ],
                "matrix_x": [
                    1348,
                    1350,
                    1356,
                    1357,
                    1364
                ],
                "matrix_inner": [
                    1353,
                    1358,
                    1359
                ],
                "recurrent_z": [
                    1361,
                    1358
                ],
                "recurrent_r": [
                    1362,
                    1359
                ],
                "recurrent_h": [
                    1365,
                    1367
                ],
                "h._uses_learning_phase": [
                    1881,
                    1371
                ],
                "GRU": [
                    1528,
                    1627,
                    1540
                ],
                "self.cell.recurrent_activation": [
                    2073,
                    1555
                ],
                "self.cell.implementation": [
                    2129,
                    1607
                ],
                "LSTMCell": [
                    1716,
                    2029,
                    1902
                ],
                "self.unit_forget_bias": [
                    1757,
                    2139,
                    1892,
                    1725
                ],
                "unit_forget_bias": [
                    2035,
                    1725
                ],
                "K.concatenate": [
                    1759
                ],
                "args": [
                    1760,
                    1761,
                    1762
                ],
                "initializers.Ones": [
                    1761
                ],
                "self.kernel_i": [
                    1830,
                    1774
                ],
                "self.kernel_f": [
                    1831,
                    1775
                ],
                "self.kernel_c": [
                    1776,
                    1832
                ],
                "self.kernel_o": [
                    1777,
                    1833
                ],
                "self.recurrent_kernel_i": [
                    1851,
                    1779
                ],
                "self.recurrent_kernel_f": [
                    1780,
                    1853
                ],
                "self.recurrent_kernel_c": [
                    1781,
                    1855
                ],
                "self.recurrent_kernel_o": [
                    1857,
                    1782
                ],
                "self.bias_i": [
                    1785,
                    1835,
                    1790
                ],
                "self.bias_f": [
                    1786,
                    1836,
                    1791
                ],
                "self.bias_c": [
                    1792,
                    1787,
                    1837
                ],
                "self.bias_o": [
                    1793,
                    1788,
                    1838
                ],
                "c_tm1": [
                    1817,
                    1875,
                    1854
                ],
                "inputs_i": [
                    1826,
                    1821,
                    1830
                ],
                "inputs_f": [
                    1827,
                    1822,
                    1831
                ],
                "inputs_c": [
                    1832,
                    1828,
                    1823
                ],
                "inputs_o": [
                    1824,
                    1833,
                    1829
                ],
                "x_i": [
                    1850,
                    1835,
                    1830
                ],
                "x_f": [
                    1836,
                    1852,
                    1831
                ],
                "x_c": [
                    1832,
                    1837,
                    1854
                ],
                "x_o": [
                    1856,
                    1833,
                    1838
                ],
                "h_tm1_i": [
                    1841,
                    1850,
                    1846
                ],
                "h_tm1_f": [
                    1842,
                    1852,
                    1847
                ],
                "h_tm1_c": [
                    1848,
                    1843,
                    1854
                ],
                "h_tm1_o": [
                    1856,
                    1849,
                    1844
                ],
                "f": [
                    1874,
                    1875,
                    1852,
                    1854
                ],
                "c": [
                    1878,
                    1882,
                    1875,
                    1854
                ],
                "o": [
                    1856,
                    1876,
                    1878
                ],
                "z0": [
                    1873,
                    1868
                ],
                "z1": [
                    1874,
                    1869
                ],
                "z2": [
                    1875,
                    1870
                ],
                "z3": [
                    1876,
                    1871
                ],
                "LSTM": [
                    2058,
                    2150,
                    2046
                ],
                "self.cell.unit_forget_bias": [
                    2093
                ],
                "ones": [
                    2178,
                    2182,
                    2165,
                    2166,
                    2173
                ],
                "K.ones_like": [
                    2165
                ],
                "K.reshape": [
                    2165
                ],
                "dims": [
                    2168,
                    2166
                ],
                "K.ones": [
                    2168
                ],
                "K.dropout": [
                    2173
                ],
                "rate": [
                    2173
                ],
                "count": [
                    2179,
                    2175
                ],
                "K.in_train_phase": [
                    2176,
                    2180
                ],
                "dropped_inputs": [
                    2177,
                    2181
                ]
            },
            "filtered_variables_in_file": {
                "Layer": [
                    160,
                    768,
                    773,
                    198,
                    1638,
                    456,
                    779,
                    173,
                    206,
                    111,
                    754,
                    147,
                    212,
                    760,
                    25,
                    186,
                    155,
                    1151
                ],
                "cell": [
                    1528,
                    1037,
                    146,
                    147,
                    148,
                    154,
                    155,
                    156,
                    1051,
                    159,
                    160,
                    161,
                    172,
                    173,
                    174,
                    48,
                    49,
                    52,
                    2046,
                    185,
                    186,
                    187,
                    189,
                    69,
                    70,
                    71,
                    197,
                    73,
                    198,
                    199,
                    205,
                    206,
                    79,
                    80,
                    81,
                    82,
                    207,
                    90,
                    91,
                    92,
                    375,
                    96,
                    743,
                    1512,
                    746,
                    2029,
                    110,
                    111,
                    112,
                    113,
                    370,
                    115,
                    116,
                    117,
                    371,
                    119,
                    372,
                    374,
                    381,
                    125,
                    126,
                    127
                ],
                "cells": [
                    128,
                    135,
                    137,
                    139,
                    48,
                    51,
                    55,
                    56,
                    124,
                    126
                ],
                "self.cells": [
                    69,
                    197,
                    154,
                    172,
                    205,
                    110,
                    79,
                    146,
                    56,
                    185,
                    90,
                    125,
                    159
                ],
                "self": [
                    2053,
                    2056,
                    2057,
                    2058,
                    2065,
                    2069,
                    2073,
                    2077,
                    2081,
                    2085,
                    2089,
                    2093,
                    2097,
                    2101,
                    56,
                    57,
                    2105,
                    2109,
                    2113,
                    69,
                    2117,
                    2121,
                    2125,
                    79,
                    2129,
                    2132,
                    2133,
                    2134,
                    2135,
                    2136,
                    2137,
                    90,
                    2138,
                    2139,
                    2140,
                    2141,
                    2142,
                    2143,
                    2144,
                    2145,
                    2146,
                    2147,
                    2148,
                    2149,
                    2150,
                    110,
                    121,
                    125,
                    129,
                    143,
                    146,
                    154,
                    157,
                    159,
                    172,
                    185,
                    197,
                    205,
                    380,
                    381,
                    382,
                    383,
                    384,
                    385,
                    386,
                    388,
                    389,
                    390,
                    391,
                    392,
                    393,
                    397,
                    398,
                    401,
                    403,
                    407,
                    413,
                    414,
                    416,
                    419,
                    424,
                    433,
                    434,
                    435,
                    443,
                    444,
                    451,
                    453,
                    456,
                    459,
                    461,
                    464,
                    465,
                    467,
                    469,
                    471,
                    476,
                    478,
                    480,
                    481,
                    488,
                    490,
                    492,
                    495,
                    499,
                    510,
                    512,
                    516,
                    518,
                    519,
                    531,
                    533,
                    534,
                    535,
                    536,
                    539,
                    554,
                    555,
                    557,
                    562,
                    563,
                    569,
                    583,
                    587,
                    591,
                    592,
                    593,
                    597,
                    603,
                    605,
                    607,
                    610,
                    611,
                    613,
                    624,
                    654,
                    655,
                    656,
                    674,
                    676,
                    689,
                    690,
                    691,
                    692,
                    694,
                    696,
                    697,
                    700,
                    701,
                    705,
                    706,
                    707,
                    711,
                    712,
                    713,
                    715,
                    719,
                    726,
                    727,
                    728,
                    729,
                    730,
                    731,
                    732,
                    734,
                    735,
                    737,
                    752,
                    754,
                    755,
                    760,
                    761,
                    762,
                    763,
                    768,
                    769,
                    773,
                    774,
                    775,
                    776,
                    838,
                    839,
                    840,
                    841,
                    843,
                    844,
                    845,
                    847,
                    848,
                    849,
                    851,
                    852,
                    853,
                    855,
                    856,
                    857,
                    858,
                    859,
                    862,
                    864,
                    865,
                    866,
                    867,
                    868,
                    870,
                    871,
                    872,
                    873,
                    874,
                    876,
                    877,
                    878,
                    880,
                    881,
                    885,
                    886,
                    888,
                    890,
                    891,
                    892,
                    893,
                    894,
                    897,
                    898,
                    901,
                    903,
                    904,
                    905,
                    909,
                    910,
                    911,
                    914,
                    920,
                    921,
                    922,
                    923,
                    924,
                    925,
                    926,
                    927,
                    928,
                    929,
                    930,
                    931,
                    932,
                    933,
                    934,
                    1051,
                    1058,
                    1061,
                    1062,
                    1063,
                    1070,
                    1074,
                    1078,
                    1082,
                    1086,
                    1090,
                    1094,
                    1098,
                    1102,
                    1106,
                    1110,
                    1114,
                    1118,
                    1122,
                    1125,
                    1126,
                    1127,
                    1128,
                    1129,
                    1130,
                    1131,
                    1132,
                    1133,
                    1134,
                    1135,
                    1136,
                    1137,
                    1138,
                    1139,
                    1140,
                    1224,
                    1225,
                    1226,
                    1227,
                    1228,
                    1230,
                    1231,
                    1232,
                    1234,
                    1235,
                    1236,
                    1238,
                    1239,
                    1240,
                    1242,
                    1243,
                    1244,
                    1245,
                    1246,
                    1247,
                    1251,
                    1253,
                    1254,
                    1255,
                    1256,
                    1257,
                    1259,
                    1260,
                    1261,
                    1263,
                    1264,
                    1266,
                    1267,
                    1268,
                    1270,
                    1272,
                    1273,
                    1274,
                    1275,
                    1276,
                    1277,
                    1278,
                    1279,
                    1281,
                    1282,
                    1283,
                    1284,
                    1286,
                    1287,
                    1288,
                    1289,
                    1294,
                    1295,
                    1297,
                    1300,
                    1301,
                    1302,
                    1303,
                    1304,
                    1309,
                    1311,
                    1313,
                    1314,
                    1322,
                    1323,
                    1324,
                    1325,
                    1326,
                    1327,
                    1328,
                    1330,
                    1338,
                    1339,
                    1340,
                    1341,
                    1343,
                    1344,
                    1346,
                    1348,
                    1349,
                    1350,
                    1351,
                    1354,
                    1356,
                    1357,
                    1358,
                    1359,
                    1361,
                    1362,
                    1364,
                    1366,
                    1367,
                    1369,
                    1375,
                    1376,
                    1377,
                    1378,
                    1379,
                    1380,
                    1381,
                    1382,
                    1383,
                    1384,
                    1385,
                    1386,
                    1387,
                    1388,
                    1389,
                    1390,
                    1391,
                    1528,
                    1535,
                    1538,
                    1539,
                    1540,
                    1547,
                    1551,
                    1555,
                    1559,
                    1563,
                    1567,
                    1571,
                    1575,
                    1579,
                    1583,
                    1587,
                    1591,
                    1595,
                    1599,
                    1603,
                    1607,
                    1610,
                    1611,
                    1612,
                    1613,
                    1614,
                    1615,
                    1616,
                    1617,
                    1618,
                    1619,
                    1620,
                    1621,
                    1622,
                    1623,
                    1624,
                    1625,
                    1626,
                    1627,
                    1716,
                    1717,
                    1718,
                    1719,
                    1720,
                    1722,
                    1723,
                    1724,
                    1725,
                    1727,
                    1728,
                    1729,
                    1731,
                    1732,
                    1733,
                    1735,
                    1736,
                    1737,
                    1738,
                    1739,
                    1740,
                    1744,
                    1746,
                    1747,
                    1748,
                    1749,
                    1750,
                    1752,
                    1753,
                    1754,
                    1756,
                    1757,
                    1760,
                    1761,
                    1762,
                    1765,
                    1766,
                    1769,
                    1770,
                    1772,
                    1774,
                    1775,
                    1776,
                    1777,
                    1779,
                    1780,
                    1781,
                    1782,
                    1784,
                    1785,
                    1786,
                    1787,
                    1788,
                    1790,
                    1791,
                    1792,
                    1793,
                    1794,
                    1797,
                    1798,
                    1800,
                    1803,
                    1804,
                    1805,
                    1806,
                    1807,
                    1812,
                    1814,
                    1819,
                    1820,
                    1830,
                    1831,
                    1832,
                    1833,
                    1834,
                    1835,
                    1836,
                    1837,
                    1838,
                    1840,
                    1850,
                    1851,
                    1852,
                    1853,
                    1854,
                    1855,
                    1856,
                    1857,
                    1859,
                    1861,
                    1862,
                    1864,
                    1865,
                    1866,
                    1868,
                    1869,
                    1870,
                    1871,
                    1873,
                    1874,
                    1875,
                    1876,
                    1878,
                    1879,
                    1885,
                    1886,
                    1887,
                    1888,
                    1889,
                    1890,
                    1891,
                    1892,
                    1893,
                    1894,
                    1895,
                    1896,
                    1897,
                    1898,
                    1899,
                    1900,
                    1901,
                    1902,
                    2046
                ],
                "__init__": [
                    838,
                    1224,
                    1716,
                    1528,
                    57,
                    1051,
                    380,
                    2046
                ],
                "StackedRNNCells": [
                    57,
                    129,
                    371
                ],
                "kwargs": [
                    1024,
                    514,
                    2052,
                    535,
                    539,
                    1057,
                    1716,
                    57,
                    582,
                    838,
                    584,
                    1224,
                    594,
                    597,
                    94,
                    96,
                    1760,
                    1761,
                    1762,
                    499,
                    380,
                    508,
                    1534,
                    1023
                ],
                "state_size": [
                    416,
                    417,
                    68,
                    71,
                    73,
                    74,
                    425,
                    465,
                    467,
                    471,
                    414,
                    479
                ],
                "cell.state_size": [
                    70,
                    71,
                    73,
                    80,
                    81,
                    82,
                    116,
                    117,
                    119
                ],
                "state_size.append": [
                    73
                ],
                "nested_states": [
                    78,
                    81,
                    84,
                    86,
                    90
                ],
                "nested_states.append": [
                    81,
                    84
                ],
                "states": [
                    1292,
                    405,
                    407,
                    1816,
                    1817,
                    695,
                    703,
                    704,
                    705,
                    708,
                    710,
                    711,
                    591,
                    592,
                    81,
                    82,
                    593,
                    84,
                    85,
                    597,
                    599,
                    90,
                    92,
                    96,
                    97,
                    609,
                    610,
                    101,
                    103,
                    104,
                    621,
                    625,
                    626,
                    628,
                    629,
                    884
                ],
                "new_nested_states": [
                    89,
                    102,
                    97
                ],
                "has_arg": [
                    112,
                    587,
                    91,
                    583
                ],
                "cell.call": [
                    96,
                    91,
                    92,
                    112
                ],
                "inputs": [
                    1540,
                    901,
                    774,
                    775,
                    776,
                    903,
                    1799,
                    2058,
                    652,
                    1806,
                    655,
                    656,
                    657,
                    530,
                    658,
                    659,
                    1296,
                    1303,
                    539,
                    1821,
                    1822,
                    671,
                    1823,
                    1824,
                    1826,
                    1315,
                    1316,
                    1317,
                    550,
                    551,
                    1063,
                    1319,
                    1320,
                    1321,
                    1827,
                    557,
                    1829,
                    567,
                    1347,
                    1348,
                    1860,
                    1861,
                    207,
                    593,
                    597,
                    600,
                    92,
                    96,
                    611,
                    485,
                    104,
                    1828,
                    495,
                    496,
                    499,
                    2165,
                    887,
                    2168,
                    893
                ],
                "constants": [
                    513,
                    514,
                    515,
                    517,
                    518,
                    669,
                    586,
                    653,
                    495,
                    496,
                    591,
                    498,
                    593,
                    655,
                    602,
                    93,
                    671
                ],
                "new_nested_states.append": [
                    97
                ],
                "cell_states": [
                    102,
                    103
                ],
                "input_shape": [
                    410,
                    411,
                    420,
                    422,
                    425,
                    567,
                    568,
                    444,
                    448,
                    449,
                    451,
                    452,
                    457,
                    1743,
                    862,
                    1250,
                    107,
                    108,
                    109,
                    113,
                    115,
                    120
                ],
                "constants_shape": [
                    458,
                    459,
                    108,
                    113,
                    444,
                    446
                ],
                "cell.build": [
                    113,
                    115
                ],
                "output_dim": [
                    417,
                    420,
                    422,
                    117,
                    119,
                    120
                ],
                "self.built": [
                    1289,
                    121,
                    1794,
                    881
                ],
                "cells.append": [
                    137,
                    126
                ],
                "cell.__class__.__name__": [
                    126
                ],
                "cell.__class__": [
                    126
                ],
                "cell.get_config": [
                    127
                ],
                "config": [
                    128,
                    130,
                    136,
                    139,
                    920,
                    935,
                    1610,
                    2132,
                    726,
                    732,
                    1629,
                    1885,
                    1375,
                    735,
                    1633,
                    738,
                    1634,
                    1635,
                    1125,
                    743,
                    2152,
                    745,
                    746,
                    2156,
                    2157,
                    2158,
                    1903,
                    1392,
                    1142,
                    1146,
                    1147,
                    1148
                ],
                "base_config": [
                    129,
                    130,
                    934,
                    935,
                    1627,
                    1628,
                    1629,
                    737,
                    738,
                    2150,
                    2151,
                    2152,
                    1902,
                    1391,
                    1392,
                    1903,
                    1140,
                    1141,
                    1142
                ],
                "get_config": [
                    129,
                    737,
                    934,
                    2150,
                    1902,
                    1391,
                    1140,
                    1627
                ],
                "base_config.items": [
                    130,
                    738,
                    935,
                    2152,
                    1903,
                    1392,
                    1142,
                    1629
                ],
                "config.items": [
                    130,
                    738,
                    935,
                    2152,
                    1903,
                    1392,
                    1142,
                    1629
                ],
                "cell_config": [
                    136,
                    137,
                    734,
                    736
                ],
                "config.pop": [
                    136,
                    745,
                    1147,
                    743
                ],
                "deserialize_layer": [
                    137,
                    743
                ],
                "custom_objects": [
                    744,
                    138
                ],
                "cls": [
                    1635,
                    746,
                    139,
                    2158,
                    1148
                ],
                "self.trainable": [
                    752,
                    761,
                    157,
                    143
                ],
                "weights": [
                    162,
                    163,
                    171,
                    174,
                    175,
                    145,
                    188,
                    148,
                    149,
                    153,
                    156,
                    189,
                    191
                ],
                "cell.trainable_weights": [
                    161,
                    148
                ],
                "cell.non_trainable_weights": [
                    156
                ],
                "trainable_weights": [
                    161,
                    162,
                    158
                ],
                "cell.weights": [
                    187,
                    189,
                    174
                ],
                "K.batch_get_value": [
                    175
                ],
                "K": [
                    2176,
                    516,
                    901,
                    1028,
                    903,
                    1799,
                    905,
                    2180,
                    909,
                    1296,
                    1830,
                    1831,
                    1832,
                    1833,
                    1322,
                    1323,
                    1324,
                    1835,
                    1326,
                    175,
                    1327,
                    1328,
                    1836,
                    691,
                    1837,
                    1838,
                    694,
                    567,
                    698,
                    1338,
                    700,
                    1340,
                    1850,
                    1343,
                    192,
                    1852,
                    1854,
                    1856,
                    1348,
                    1861,
                    1350,
                    1864,
                    1353,
                    1866,
                    723,
                    1365,
                    599,
                    1503,
                    1759,
                    2020,
                    485,
                    486,
                    487,
                    489,
                    492,
                    2164,
                    2165,
                    2166,
                    887,
                    2168,
                    2173,
                    510
                ],
                "tuples": [
                    184,
                    190,
                    192
                ],
                "num_param": [
                    187,
                    188,
                    191
                ],
                "sw": [
                    189,
                    190
                ],
                "w": [
                    189,
                    190
                ],
                "tuples.append": [
                    190
                ],
                "K.batch_set_value": [
                    192
                ],
                "losses": [
                    196,
                    200,
                    201,
                    204,
                    208,
                    209
                ],
                "cell_losses": [
                    774,
                    199,
                    200,
                    775,
                    207,
                    208
                ],
                "cell.losses": [
                    199
                ],
                "cell.get_losses_for": [
                    207
                ],
                "RNN": [
                    737,
                    775,
                    776,
                    938,
                    1906,
                    499,
                    1395,
                    535,
                    539,
                    380
                ],
                "self.cell": [
                    1538,
                    1539,
                    2056,
                    2057,
                    1547,
                    1551,
                    2065,
                    1555,
                    2069,
                    1559,
                    2073,
                    1563,
                    2077,
                    1567,
                    2081,
                    1571,
                    1061,
                    1062,
                    1575,
                    2085,
                    2089,
                    1579,
                    2093,
                    1070,
                    1583,
                    2097,
                    1074,
                    1587,
                    2101,
                    1078,
                    1591,
                    2105,
                    1082,
                    1595,
                    2109,
                    1086,
                    1599,
                    2113,
                    1090,
                    1603,
                    2117,
                    1094,
                    1607,
                    583,
                    2121,
                    1098,
                    587,
                    2125,
                    1102,
                    593,
                    1106,
                    2129,
                    597,
                    1110,
                    1114,
                    1118,
                    1122,
                    690,
                    692,
                    694,
                    696,
                    697,
                    701,
                    712,
                    713,
                    715,
                    734,
                    735,
                    754,
                    755,
                    760,
                    762,
                    763,
                    768,
                    769,
                    773,
                    774,
                    381,
                    398,
                    401,
                    413,
                    414,
                    416,
                    456,
                    459,
                    461,
                    464,
                    465,
                    467,
                    476,
                    488,
                    490,
                    492
                ],
                "self.return_sequences": [
                    419,
                    613,
                    433,
                    726,
                    382
                ],
                "return_sequences": [
                    1529,
                    1052,
                    382,
                    2047
                ],
                "self.return_state": [
                    424,
                    624,
                    434,
                    727,
                    383
                ],
                "return_state": [
                    2048,
                    1530,
                    1053,
                    383
                ],
                "self.go_backwards": [
                    384,
                    603,
                    728
                ],
                "go_backwards": [
                    384,
                    2049,
                    1531,
                    1054
                ],
                "self.stateful": [
                    480,
                    385,
                    674,
                    451,
                    554,
                    729,
                    607
                ],
                "stateful": [
                    385,
                    2050,
                    1532,
                    1055
                ],
                "self.unroll": [
                    569,
                    386,
                    605,
                    730
                ],
                "unroll": [
                    1056,
                    386,
                    2051,
                    1533
                ],
                "self.supports_masking": [
                    388
                ],
                "self.input_spec": [
                    676,
                    453,
                    389,
                    531,
                    533,
                    534,
                    536
                ],
                "InputSpec": [
                    516,
                    453,
                    389,
                    510,
                    478
                ],
                "self.state_spec": [
                    512,
                    390,
                    510,
                    469,
                    471,
                    476,
                    478
                ],
                "self._states": [
                    407,
                    403,
                    397,
                    391
                ],
                "self.constants_spec": [
                    392,
                    516,
                    519
                ],
                "self._num_constants": [
                    732,
                    518,
                    393,
                    654,
                    591,
                    592,
                    655,
                    656,
                    443,
                    444,
                    731
                ],
                "self.cell.state_size": [
                    398,
                    401,
                    413,
                    414,
                    416,
                    690,
                    692,
                    694,
                    696,
                    697,
                    701,
                    712,
                    713,
                    715,
                    464,
                    465,
                    467,
                    476,
                    488,
                    490,
                    492
                ],
                "num_states": [
                    401,
                    402,
                    399
                ],
                "_": [
                    402,
                    435,
                    2179
                ],
                "states.setter": [
                    405
                ],
                "output_shape": [
                    426,
                    428,
                    420,
                    422
                ],
                "state_shape": [
                    425,
                    426
                ],
                "dim": [
                    425,
                    490,
                    489,
                    713,
                    715,
                    716,
                    720,
                    691,
                    692,
                    697,
                    698,
                    478,
                    479
                ],
                "mask": [
                    1541,
                    1064,
                    2059,
                    559,
                    432,
                    433,
                    560,
                    431,
                    604
                ],
                "output_mask": [
                    433,
                    436,
                    438
                ],
                "state_mask": [
                    435,
                    436
                ],
                "self.states": [
                    705,
                    610,
                    707,
                    711,
                    555,
                    689,
                    562,
                    435,
                    563,
                    691,
                    694,
                    697,
                    700
                ],
                "batch_size": [
                    451,
                    676,
                    453,
                    677,
                    716,
                    720,
                    691,
                    694,
                    698,
                    701
                ],
                "input_dim": [
                    1250,
                    1251,
                    452,
                    453,
                    1743,
                    1744
                ],
                "step_input_shape": [
                    457,
                    459,
                    461
                ],
                "self.cell.build": [
                    459,
                    461
                ],
                "spec.shape": [
                    471
                ],
                "spec": [
                    471
                ],
                "self.reset_states": [
                    481
                ],
                "initial_state": [
                    1543,
                    653,
                    2061,
                    658,
                    668,
                    671,
                    552,
                    1066,
                    555,
                    557,
                    562,
                    565,
                    601,
                    485,
                    486,
                    487,
                    489,
                    492,
                    495,
                    496,
                    498,
                    507,
                    508,
                    509,
                    511
                ],
                "K.zeros_like": [
                    485
                ],
                "K.sum": [
                    486
                ],
                "K.expand_dims": [
                    487
                ],
                "K.tile": [
                    489,
                    492,
                    2166
                ],
                "self._standardize_args": [
                    495
                ],
                "__call__": [
                    539,
                    499,
                    535
                ],
                "additional_inputs": [
                    515,
                    521,
                    522,
                    530,
                    505,
                    509
                ],
                "additional_specs": [
                    512,
                    506,
                    531,
                    519
                ],
                "K.int_shape": [
                    516,
                    510,
                    567
                ],
                "state": [
                    711,
                    621,
                    622,
                    723,
                    697,
                    698,
                    510,
                    511
                ],
                "constant": [
                    516,
                    517
                ],
                "is_keras_tensor": [
                    528,
                    521,
                    523
                ],
                "tensor": [
                    522,
                    523
                ],
                "full_input": [
                    530,
                    535
                ],
                "full_input_spec": [
                    531,
                    534
                ],
                "original_input_spec": [
                    536,
                    533
                ],
                "output": [
                    614,
                    631,
                    616,
                    620,
                    909,
                    911,
                    916,
                    629,
                    917,
                    535,
                    537
                ],
                "self.get_initial_state": [
                    557
                ],
                "timesteps": [
                    568,
                    569,
                    606
                ],
                "self.cell.call": [
                    593,
                    587,
                    597,
                    583
                ],
                "training": [
                    2179,
                    1305,
                    1542,
                    584,
                    1065,
                    1801,
                    2183,
                    2060,
                    1808,
                    1298,
                    915,
                    1880,
                    889,
                    1370,
                    895
                ],
                "last_output": [
                    616,
                    619,
                    599
                ],
                "outputs": [
                    614,
                    599
                ],
                "K.rnn": [
                    599
                ],
                "step": [
                    599
                ],
                "updates": [
                    608,
                    610,
                    611
                ],
                "i": [
                    609,
                    610,
                    1873,
                    1875,
                    1850,
                    1854
                ],
                "updates.append": [
                    610
                ],
                "self.add_update": [
                    611
                ],
                "output._uses_learning_phase": [
                    916,
                    620
                ],
                "state._uses_learning_phase": [
                    622
                ],
                "x": [
                    662,
                    663,
                    664,
                    665,
                    666
                ],
                "to_list_or_none": [
                    668,
                    669
                ],
                "shape": [
                    676
                ],
                "K.zeros": [
                    691,
                    694
                ],
                "K.set_value": [
                    698,
                    723,
                    700
                ],
                "np.zeros": [
                    698,
                    701
                ],
                "np": [
                    698,
                    701
                ],
                "self.name": [
                    706,
                    719
                ],
                "index": [
                    713,
                    717,
                    711
                ],
                "value": [
                    721,
                    723,
                    716,
                    711
                ],
                "value.shape": [
                    721,
                    716
                ],
                "self.cell.get_config": [
                    734
                ],
                "self.cell.__class__.__name__": [
                    735
                ],
                "self.cell.__class__": [
                    735
                ],
                "num_constants": [
                    745,
                    747
                ],
                "layer": [
                    746,
                    747,
                    748
                ],
                "layer._num_constants": [
                    747
                ],
                "self.cell.trainable_weights": [
                    755
                ],
                "self.cell.weights": [
                    762
                ],
                "self.cell.non_trainable_weights": [
                    763
                ],
                "self.cell.losses": [
                    769
                ],
                "self.cell.get_losses_for": [
                    774
                ],
                "get_losses_for": [
                    776,
                    775
                ],
                "SimpleRNNCell": [
                    934,
                    1037,
                    838
                ],
                "self.units": [
                    1282,
                    1283,
                    1284,
                    1885,
                    1806,
                    1785,
                    1303,
                    920,
                    1277,
                    1786,
                    1788,
                    1717,
                    839,
                    1225,
                    1354,
                    1610,
                    1356,
                    1357,
                    1358,
                    1359,
                    1738,
                    1744,
                    1868,
                    1869,
                    1364,
                    1870,
                    1366,
                    1750,
                    1871,
                    857,
                    2132,
                    1245,
                    862,
                    1375,
                    1760,
                    1761,
                    1762,
                    1251,
                    868,
                    1125,
                    1766,
                    1257,
                    874,
                    1774,
                    1775,
                    1264,
                    1776,
                    1777,
                    1779,
                    1780,
                    1781,
                    1782,
                    1272,
                    1273,
                    1274,
                    1787,
                    1276,
                    893,
                    1278,
                    1279
                ],
                "units": [
                    839,
                    1512,
                    1225,
                    1037,
                    2029,
                    1717
                ],
                "self.activation": [
                    1376,
                    1126,
                    840,
                    1226,
                    1611,
                    910,
                    911,
                    1886,
                    1875,
                    2133,
                    1718,
                    1367,
                    1878,
                    921,
                    1854,
                    1343
                ],
                "activations.get": [
                    840,
                    1226,
                    1227,
                    1718,
                    1719
                ],
                "activations": [
                    1376,
                    1377,
                    1126,
                    840,
                    1226,
                    1227,
                    1611,
                    1612,
                    2133,
                    1718,
                    1719,
                    2134,
                    921,
                    1886,
                    1887
                ],
                "activation": [
                    840,
                    1513,
                    1226,
                    1038,
                    2030,
                    1718
                ],
                "self.use_bias": [
                    1888,
                    1281,
                    1378,
                    1349,
                    1127,
                    841,
                    873,
                    1834,
                    1228,
                    1325,
                    1613,
                    1263,
                    1784,
                    1865,
                    2135,
                    1720,
                    922,
                    1756
                ],
                "use_bias": [
                    841,
                    1515,
                    1228,
                    1039,
                    2032,
                    1720
                ],
                "self.kernel_initializer": [
                    864,
                    1889,
                    1379,
                    1253,
                    1128,
                    843,
                    1230,
                    1614,
                    1746,
                    2136,
                    1722,
                    923
                ],
                "initializers.get": [
                    843,
                    844,
                    845,
                    1230,
                    1231,
                    1232,
                    1722,
                    1723,
                    1724
                ],
                "initializers": [
                    923,
                    924,
                    925,
                    1722,
                    1723,
                    1724,
                    843,
                    844,
                    845,
                    1230,
                    1231,
                    1232,
                    1614,
                    1615,
                    1616,
                    2136,
                    2137,
                    2138,
                    1761,
                    1889,
                    1379,
                    1380,
                    1381,
                    1890,
                    1891,
                    1128,
                    1129,
                    1130
                ],
                "kernel_initializer": [
                    843,
                    1516,
                    1230,
                    1040,
                    2033,
                    1722
                ],
                "self.recurrent_initializer": [
                    1890,
                    1380,
                    870,
                    1129,
                    1259,
                    844,
                    1231,
                    1615,
                    1752,
                    2137,
                    1723,
                    924
                ],
                "recurrent_initializer": [
                    844,
                    1517,
                    1231,
                    1041,
                    2034,
                    1723
                ],
                "self.bias_initializer": [
                    1760,
                    1762,
                    1891,
                    1381,
                    1765,
                    1130,
                    876,
                    845,
                    1232,
                    1616,
                    1266,
                    2138,
                    1724,
                    925
                ],
                "bias_initializer": [
                    1765,
                    1768,
                    845,
                    1518,
                    1232,
                    1042,
                    2036,
                    1724
                ],
                "self.kernel_regularizer": [
                    865,
                    1893,
                    1254,
                    1382,
                    1131,
                    847,
                    1617,
                    1234,
                    1747,
                    2140,
                    926,
                    1727
                ],
                "regularizers.get": [
                    1728,
                    1729,
                    1058,
                    2053,
                    847,
                    848,
                    849,
                    1234,
                    1235,
                    1236,
                    1727,
                    1535
                ],
                "regularizers": [
                    2053,
                    926,
                    927,
                    928,
                    1058,
                    1727,
                    1728,
                    1729,
                    847,
                    848,
                    849,
                    1234,
                    1235,
                    1236,
                    1617,
                    1618,
                    1619,
                    1620,
                    2140,
                    2141,
                    2142,
                    2143,
                    1893,
                    1382,
                    1383,
                    1384,
                    1894,
                    1895,
                    1131,
                    1132,
                    1133,
                    1134,
                    1535
                ],
                "kernel_regularizer": [
                    1519,
                    847,
                    1234,
                    1043,
                    2037,
                    1727
                ],
                "self.recurrent_regularizer": [
                    1728,
                    1894,
                    871,
                    1383,
                    1132,
                    1260,
                    848,
                    1618,
                    1235,
                    1753,
                    2141,
                    927
                ],
                "recurrent_regularizer": [
                    1728,
                    848,
                    1520,
                    1235,
                    1044,
                    2038
                ],
                "self.bias_regularizer": [
                    928,
                    1729,
                    1895,
                    1384,
                    1769,
                    1133,
                    877,
                    849,
                    1267,
                    1236,
                    1619,
                    2142
                ],
                "bias_regularizer": [
                    1729,
                    849,
                    1521,
                    1236,
                    1045,
                    2039
                ],
                "self.kernel_constraint": [
                    2144,
                    929,
                    866,
                    1731,
                    1255,
                    1896,
                    1385,
                    1135,
                    851,
                    1748,
                    1621,
                    1238
                ],
                "constraints.get": [
                    1731,
                    1732,
                    1733,
                    851,
                    852,
                    853,
                    1238,
                    1239,
                    1240
                ],
                "constraints": [
                    929,
                    930,
                    931,
                    1731,
                    1732,
                    1733,
                    851,
                    852,
                    853,
                    1238,
                    1623,
                    1239,
                    1240,
                    1621,
                    1622,
                    2144,
                    2145,
                    2146,
                    1896,
                    1385,
                    1386,
                    1387,
                    1897,
                    1898,
                    1135,
                    1136,
                    1137
                ],
                "kernel_constraint": [
                    1731,
                    1522,
                    851,
                    1238,
                    1046,
                    2040
                ],
                "self.recurrent_constraint": [
                    2145,
                    930,
                    1732,
                    872,
                    1897,
                    1386,
                    1261,
                    1136,
                    852,
                    1622,
                    1239,
                    1754
                ],
                "recurrent_constraint": [
                    1732,
                    1523,
                    852,
                    1047,
                    1239,
                    2041
                ],
                "self.bias_constraint": [
                    2146,
                    931,
                    1733,
                    1770,
                    1387,
                    1898,
                    878,
                    1137,
                    1268,
                    853,
                    1623,
                    1240
                ],
                "bias_constraint": [
                    1733,
                    1524,
                    853,
                    1048,
                    2042,
                    1240
                ],
                "self.dropout": [
                    1797,
                    1800,
                    1294,
                    1297,
                    914,
                    1820,
                    1314,
                    932,
                    1346,
                    1859,
                    1735,
                    855,
                    1624,
                    1369,
                    1242,
                    1879,
                    2147,
                    1899,
                    1388,
                    1138,
                    885,
                    888
                ],
                "dropout": [
                    1509,
                    1735,
                    1034,
                    2026,
                    1525,
                    855,
                    1049,
                    1242,
                    2043
                ],
                "self.recurrent_dropout": [
                    1803,
                    1807,
                    914,
                    1300,
                    1304,
                    933,
                    1840,
                    1330,
                    1862,
                    1351,
                    1736,
                    1879,
                    856,
                    1625,
                    1369,
                    1243,
                    2148,
                    1900,
                    1389,
                    1139,
                    890,
                    894
                ],
                "recurrent_dropout": [
                    1510,
                    1736,
                    1035,
                    2027,
                    1526,
                    856,
                    1050,
                    1243,
                    2044
                ],
                "self.state_size": [
                    857,
                    1738,
                    1245
                ],
                "self._dropout_mask": [
                    897,
                    1797,
                    1798,
                    1739,
                    1294,
                    1295,
                    1812,
                    885,
                    886,
                    858,
                    1309,
                    1246
                ],
                "self._recurrent_dropout_mask": [
                    898,
                    859,
                    1311,
                    1740,
                    1804,
                    1805,
                    1301,
                    1302,
                    1814,
                    891,
                    892,
                    1247
                ],
                "self.kernel": [
                    1251,
                    1348,
                    901,
                    1861,
                    903,
                    1774,
                    1775,
                    1744,
                    1776,
                    1777,
                    1272,
                    1278,
                    1274,
                    862
                ],
                "self.add_weight": [
                    867,
                    1251,
                    1766,
                    1256,
                    874,
                    1264,
                    1744,
                    1749,
                    862
                ],
                "self.recurrent_kernel": [
                    867,
                    1256,
                    1864,
                    1354,
                    909,
                    1779,
                    1780,
                    1749,
                    1366,
                    1781,
                    1782,
                    1273,
                    1275,
                    1279
                ],
                "self.bias": [
                    1282,
                    1283,
                    1284,
                    1350,
                    1766,
                    904,
                    905,
                    874,
                    1866,
                    1772,
                    880,
                    1264,
                    1270,
                    1785,
                    1786,
                    1787,
                    1788
                ],
                "prev_output": [
                    908,
                    884,
                    909
                ],
                "_generate_dropout_mask": [
                    1798,
                    1805,
                    1295,
                    1302,
                    886,
                    892
                ],
                "_generate_dropout_ones": [
                    1799,
                    1806,
                    887,
                    1296,
                    1303,
                    893
                ],
                "K.shape": [
                    1296,
                    1799,
                    2168,
                    887
                ],
                "dp_mask": [
                    1824,
                    897,
                    1315,
                    900,
                    901,
                    1316,
                    1317,
                    1347,
                    1860,
                    1821,
                    1812,
                    1309,
                    1822,
                    1823
                ],
                "rec_dp_mask": [
                    898,
                    1863,
                    1352,
                    907,
                    908,
                    1841,
                    1842,
                    1331,
                    1332,
                    1333,
                    1814,
                    1843,
                    1844,
                    1311
                ],
                "h": [
                    901,
                    903,
                    905,
                    909,
                    1878,
                    1368,
                    1881,
                    1882,
                    1371,
                    1372
                ],
                "K.dot": [
                    901,
                    903,
                    909,
                    1830,
                    1831,
                    1832,
                    1833,
                    1322,
                    1323,
                    1324,
                    1850,
                    1338,
                    1852,
                    1340,
                    1854,
                    1343,
                    1856,
                    1348,
                    1861,
                    1864,
                    1353,
                    1365
                ],
                "K.bias_add": [
                    1350,
                    905,
                    1866,
                    1835,
                    1836,
                    1837,
                    1326,
                    1327,
                    1328,
                    1838
                ],
                "activations.serialize": [
                    1376,
                    1377,
                    1126,
                    1611,
                    1612,
                    2133,
                    2134,
                    921,
                    1886,
                    1887
                ],
                "initializers.serialize": [
                    1889,
                    1890,
                    1379,
                    1380,
                    1381,
                    1891,
                    1128,
                    1129,
                    1130,
                    1614,
                    1615,
                    1616,
                    2136,
                    2137,
                    2138,
                    923,
                    924,
                    925
                ],
                "regularizers.serialize": [
                    926,
                    927,
                    928,
                    1617,
                    1618,
                    1619,
                    1620,
                    2140,
                    2141,
                    2142,
                    2143,
                    1893,
                    1382,
                    1383,
                    1384,
                    1894,
                    1895,
                    1131,
                    1132,
                    1133,
                    1134
                ],
                "constraints.serialize": [
                    2144,
                    929,
                    930,
                    931,
                    2145,
                    2146,
                    1896,
                    1385,
                    1386,
                    1387,
                    1897,
                    1898,
                    1135,
                    1136,
                    1137,
                    1621,
                    1622,
                    1623
                ],
                "kwargs.pop": [
                    1024
                ],
                "warnings.warn": [
                    1504,
                    1025,
                    2017,
                    1029,
                    2021,
                    1500
                ],
                "warnings": [
                    1504,
                    1025,
                    2017,
                    1029,
                    2021,
                    1500
                ],
                "K.backend": [
                    2164,
                    1028,
                    2020,
                    1503
                ],
                "SimpleRNN": [
                    1051,
                    1140,
                    1063
                ],
                "self.activity_regularizer": [
                    1058,
                    2053,
                    1134,
                    1620,
                    2143,
                    1535
                ],
                "activity_regularizer": [
                    1058,
                    2053,
                    1535
                ],
                "interfaces.legacy_recurrent_support": [
                    1001,
                    1475,
                    1991
                ],
                "interfaces": [
                    1001,
                    1475,
                    1991
                ],
                "self.cell._dropout_mask": [
                    2056,
                    1538,
                    1061
                ],
                "self.cell._recurrent_dropout_mask": [
                    2057,
                    1539,
                    1062
                ],
                "call": [
                    2058,
                    1540,
                    1063
                ],
                "self.cell.units": [
                    2065,
                    1547,
                    1070
                ],
                "self.cell.activation": [
                    1074,
                    2069,
                    1551
                ],
                "self.cell.use_bias": [
                    2077,
                    1078,
                    1559
                ],
                "self.cell.kernel_initializer": [
                    2081,
                    1082,
                    1563
                ],
                "self.cell.recurrent_initializer": [
                    2085,
                    1086,
                    1567
                ],
                "self.cell.bias_initializer": [
                    2089,
                    1090,
                    1571
                ],
                "self.cell.kernel_regularizer": [
                    2097,
                    1094,
                    1575
                ],
                "self.cell.recurrent_regularizer": [
                    1098,
                    1579,
                    2101
                ],
                "self.cell.bias_regularizer": [
                    2105,
                    1102,
                    1583
                ],
                "self.cell.kernel_constraint": [
                    1106,
                    1587,
                    2109
                ],
                "self.cell.recurrent_constraint": [
                    2113,
                    1110,
                    1591
                ],
                "self.cell.bias_constraint": [
                    1114,
                    1595,
                    2117
                ],
                "self.cell.dropout": [
                    2121,
                    1118,
                    1599
                ],
                "self.cell.recurrent_dropout": [
                    1122,
                    1603,
                    2125
                ],
                "GRUCell": [
                    1224,
                    1512,
                    1391
                ],
                "self.recurrent_activation": [
                    1856,
                    1377,
                    1852,
                    1227,
                    1612,
                    1850,
                    1361,
                    1362,
                    1873,
                    1874,
                    1876,
                    2134,
                    1719,
                    1338,
                    1340,
                    1887
                ],
                "recurrent_activation": [
                    2031,
                    1514,
                    1227,
                    1719
                ],
                "self.implementation": [
                    1313,
                    2149,
                    1737,
                    1901,
                    1390,
                    1626,
                    1819,
                    1244
                ],
                "implementation": [
                    2016,
                    1737,
                    1527,
                    1499,
                    1244,
                    2045
                ],
                "self.kernel_z": [
                    1272,
                    1322
                ],
                "self.recurrent_kernel_z": [
                    1273,
                    1339
                ],
                "self.kernel_r": [
                    1274,
                    1323
                ],
                "self.recurrent_kernel_r": [
                    1275,
                    1341
                ],
                "self.kernel_h": [
                    1324,
                    1278
                ],
                "self.recurrent_kernel_h": [
                    1344,
                    1279
                ],
                "self.bias_z": [
                    1326,
                    1282,
                    1286
                ],
                "self.bias_r": [
                    1283,
                    1327,
                    1287
                ],
                "self.bias_h": [
                    1288,
                    1328,
                    1284
                ],
                "h_tm1": [
                    1292,
                    1816,
                    1841,
                    1842,
                    1331,
                    1332,
                    1333,
                    1843,
                    1847,
                    1848,
                    1335,
                    1336,
                    1337,
                    1844,
                    1846,
                    1849,
                    1863,
                    1352,
                    1353,
                    1864,
                    1365,
                    1368
                ],
                "inputs_z": [
                    1322,
                    1315,
                    1319
                ],
                "inputs_r": [
                    1320,
                    1323,
                    1316
                ],
                "inputs_h": [
                    1321,
                    1324,
                    1317
                ],
                "x_z": [
                    1322,
                    1356,
                    1326,
                    1361,
                    1338
                ],
                "x_r": [
                    1323,
                    1357,
                    1327,
                    1362,
                    1340
                ],
                "x_h": [
                    1324,
                    1328,
                    1364,
                    1367,
                    1343
                ],
                "h_tm1_z": [
                    1338,
                    1331,
                    1335
                ],
                "h_tm1_r": [
                    1336,
                    1332,
                    1340
                ],
                "h_tm1_h": [
                    1337,
                    1333,
                    1343
                ],
                "z": [
                    1861,
                    1864,
                    1866,
                    1868,
                    1869,
                    1870,
                    1871,
                    1361,
                    1368,
                    1338
                ],
                "r": [
                    1362,
                    1340,
                    1365,
                    1343
                ],
                "hh": [
                    1368,
                    1367,
                    1343
                ],
                "matrix_x": [
                    1348,
                    1350,
                    1356,
                    1357,
                    1364
                ],
                "matrix_inner": [
                    1353,
                    1358,
                    1359
                ],
                "recurrent_z": [
                    1361,
                    1358
                ],
                "recurrent_r": [
                    1362,
                    1359
                ],
                "recurrent_h": [
                    1365,
                    1367
                ],
                "h._uses_learning_phase": [
                    1881,
                    1371
                ],
                "GRU": [
                    1528,
                    1627,
                    1540
                ],
                "self.cell.recurrent_activation": [
                    2073,
                    1555
                ],
                "self.cell.implementation": [
                    2129,
                    1607
                ],
                "LSTMCell": [
                    1716,
                    2029,
                    1902
                ],
                "self.unit_forget_bias": [
                    1757,
                    2139,
                    1892,
                    1725
                ],
                "unit_forget_bias": [
                    2035,
                    1725
                ],
                "K.concatenate": [
                    1759
                ],
                "args": [
                    1760,
                    1761,
                    1762
                ],
                "initializers.Ones": [
                    1761
                ],
                "self.kernel_i": [
                    1830,
                    1774
                ],
                "self.kernel_f": [
                    1831,
                    1775
                ],
                "self.kernel_c": [
                    1776,
                    1832
                ],
                "self.kernel_o": [
                    1777,
                    1833
                ],
                "self.recurrent_kernel_i": [
                    1851,
                    1779
                ],
                "self.recurrent_kernel_f": [
                    1780,
                    1853
                ],
                "self.recurrent_kernel_c": [
                    1781,
                    1855
                ],
                "self.recurrent_kernel_o": [
                    1857,
                    1782
                ],
                "self.bias_i": [
                    1785,
                    1835,
                    1790
                ],
                "self.bias_f": [
                    1786,
                    1836,
                    1791
                ],
                "self.bias_c": [
                    1792,
                    1787,
                    1837
                ],
                "self.bias_o": [
                    1793,
                    1788,
                    1838
                ],
                "c_tm1": [
                    1817,
                    1875,
                    1854
                ],
                "inputs_i": [
                    1826,
                    1821,
                    1830
                ],
                "inputs_f": [
                    1827,
                    1822,
                    1831
                ],
                "inputs_c": [
                    1832,
                    1828,
                    1823
                ],
                "inputs_o": [
                    1824,
                    1833,
                    1829
                ],
                "x_i": [
                    1850,
                    1835,
                    1830
                ],
                "x_f": [
                    1836,
                    1852,
                    1831
                ],
                "x_c": [
                    1832,
                    1837,
                    1854
                ],
                "x_o": [
                    1856,
                    1833,
                    1838
                ],
                "h_tm1_i": [
                    1841,
                    1850,
                    1846
                ],
                "h_tm1_f": [
                    1842,
                    1852,
                    1847
                ],
                "h_tm1_c": [
                    1848,
                    1843,
                    1854
                ],
                "h_tm1_o": [
                    1856,
                    1849,
                    1844
                ],
                "f": [
                    1874,
                    1875,
                    1852,
                    1854
                ],
                "c": [
                    1878,
                    1882,
                    1875,
                    1854
                ],
                "o": [
                    1856,
                    1876,
                    1878
                ],
                "z0": [
                    1873,
                    1868
                ],
                "z1": [
                    1874,
                    1869
                ],
                "z2": [
                    1875,
                    1870
                ],
                "z3": [
                    1876,
                    1871
                ],
                "LSTM": [
                    2058,
                    2150,
                    2046
                ],
                "self.cell.unit_forget_bias": [
                    2093
                ],
                "ones": [
                    2178,
                    2182,
                    2165,
                    2166,
                    2173
                ],
                "K.ones_like": [
                    2165
                ],
                "K.reshape": [
                    2165
                ],
                "dims": [
                    2168,
                    2166
                ],
                "K.ones": [
                    2168
                ],
                "K.dropout": [
                    2173
                ],
                "rate": [
                    2173
                ],
                "count": [
                    2179,
                    2175
                ],
                "K.in_train_phase": [
                    2176,
                    2180
                ],
                "dropped_inputs": [
                    2177,
                    2181
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs/repos/keras_37/keras/layers/wrappers.py": {
            "buggy_functions": [
                {
                    "function_name": "__init__",
                    "function_code": "def __init__(self, layer, merge_mode='concat', weights=None, **kwargs):\n    if merge_mode not in ['sum', 'mul', 'ave', 'concat', None]:\n        raise ValueError('Invalid merge mode. '\n                         'Merge mode should be one of '\n                         '{\"sum\", \"mul\", \"ave\", \"concat\", None}')\n    self.forward_layer = copy.copy(layer)\n    config = layer.get_config()\n    config['go_backwards'] = not config['go_backwards']\n    self.backward_layer = layer.__class__.from_config(config)\n    self.forward_layer.name = 'forward_' + self.forward_layer.name\n    self.backward_layer.name = 'backward_' + self.backward_layer.name\n    self.merge_mode = merge_mode\n    if weights:\n        nw = len(weights)\n        self.forward_layer.initial_weights = weights[:nw // 2]\n        self.backward_layer.initial_weights = weights[nw // 2:]\n    self.stateful = layer.stateful\n    self.return_sequences = layer.return_sequences\n    self.return_state = layer.return_state\n    self.supports_masking = True\n    self._trainable = True\n    super(Bidirectional, self).__init__(layer, **kwargs)\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 256,
                    "end_line": 277,
                    "variables": {
                        "merge_mode": [
                            257,
                            267
                        ],
                        "ValueError": [
                            258
                        ],
                        "self.forward_layer": [
                            265,
                            261,
                            270
                        ],
                        "self": [
                            261,
                            264,
                            265,
                            266,
                            267,
                            270,
                            271,
                            272,
                            273,
                            274,
                            275,
                            276,
                            277
                        ],
                        "copy.copy": [
                            261
                        ],
                        "copy": [
                            261
                        ],
                        "layer": [
                            261,
                            262,
                            264,
                            272,
                            273,
                            274,
                            277
                        ],
                        "config": [
                            264,
                            262,
                            263
                        ],
                        "layer.get_config": [
                            262
                        ],
                        "self.backward_layer": [
                            264,
                            266,
                            271
                        ],
                        "layer.__class__.from_config": [
                            264
                        ],
                        "layer.__class__": [
                            264
                        ],
                        "self.forward_layer.name": [
                            265
                        ],
                        "self.backward_layer.name": [
                            266
                        ],
                        "self.merge_mode": [
                            267
                        ],
                        "weights": [
                            268,
                            269,
                            270,
                            271
                        ],
                        "nw": [
                            269,
                            270,
                            271
                        ],
                        "len": [
                            269
                        ],
                        "self.forward_layer.initial_weights": [
                            270
                        ],
                        "self.backward_layer.initial_weights": [
                            271
                        ],
                        "self.stateful": [
                            272
                        ],
                        "layer.stateful": [
                            272
                        ],
                        "self.return_sequences": [
                            273
                        ],
                        "layer.return_sequences": [
                            273
                        ],
                        "self.return_state": [
                            274
                        ],
                        "layer.return_state": [
                            274
                        ],
                        "self.supports_masking": [
                            275
                        ],
                        "self._trainable": [
                            276
                        ],
                        "__init__": [
                            277
                        ],
                        "super": [
                            277
                        ],
                        "Bidirectional": [
                            277
                        ],
                        "kwargs": [
                            277
                        ]
                    },
                    "filtered_variables": {
                        "merge_mode": [
                            257,
                            267
                        ],
                        "self.forward_layer": [
                            265,
                            261,
                            270
                        ],
                        "self": [
                            261,
                            264,
                            265,
                            266,
                            267,
                            270,
                            271,
                            272,
                            273,
                            274,
                            275,
                            276,
                            277
                        ],
                        "copy.copy": [
                            261
                        ],
                        "copy": [
                            261
                        ],
                        "layer": [
                            261,
                            262,
                            264,
                            272,
                            273,
                            274,
                            277
                        ],
                        "config": [
                            264,
                            262,
                            263
                        ],
                        "layer.get_config": [
                            262
                        ],
                        "self.backward_layer": [
                            264,
                            266,
                            271
                        ],
                        "layer.__class__.from_config": [
                            264
                        ],
                        "layer.__class__": [
                            264
                        ],
                        "self.forward_layer.name": [
                            265
                        ],
                        "self.backward_layer.name": [
                            266
                        ],
                        "self.merge_mode": [
                            267
                        ],
                        "weights": [
                            268,
                            269,
                            270,
                            271
                        ],
                        "nw": [
                            269,
                            270,
                            271
                        ],
                        "self.forward_layer.initial_weights": [
                            270
                        ],
                        "self.backward_layer.initial_weights": [
                            271
                        ],
                        "self.stateful": [
                            272
                        ],
                        "layer.stateful": [
                            272
                        ],
                        "self.return_sequences": [
                            273
                        ],
                        "layer.return_sequences": [
                            273
                        ],
                        "self.return_state": [
                            274
                        ],
                        "layer.return_state": [
                            274
                        ],
                        "self.supports_masking": [
                            275
                        ],
                        "self._trainable": [
                            276
                        ],
                        "__init__": [
                            277
                        ],
                        "Bidirectional": [
                            277
                        ],
                        "kwargs": [
                            277
                        ]
                    },
                    "diff_line_number": 277,
                    "class_data": {
                        "signature": "class Bidirectional(Wrapper)",
                        "docstring": "Bidirectional wrapper for RNNs.\n\n# Arguments\n    layer: `Recurrent` instance.\n    merge_mode: Mode by which outputs of the\n        forward and backward RNNs will be combined.\n        One of {'sum', 'mul', 'concat', 'ave', None}.\n        If None, the outputs will not be combined,\n        they will be returned as a list.\n\n# Raises\n    ValueError: In case of invalid `merge_mode` argument.\n\n# Examples\n\n```python\n    model = Sequential()\n    model.add(Bidirectional(LSTM(10, return_sequences=True),\n                            input_shape=(5, 10)))\n    model.add(Bidirectional(LSTM(10)))\n    model.add(Dense(5))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n```",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, layer, merge_mode='concat', weights=None, **kwargs):\n    if merge_mode not in ['sum', 'mul', 'ave', 'concat', None]:\n        raise ValueError('Invalid merge mode. Merge mode should be one of {\"sum\", \"mul\", \"ave\", \"concat\", None}')\n    self.forward_layer = copy.copy(layer)\n    config = layer.get_config()\n    config['go_backwards'] = not config['go_backwards']\n    self.backward_layer = layer.__class__.from_config(config)\n    self.forward_layer.name = 'forward_' + self.forward_layer.name\n    self.backward_layer.name = 'backward_' + self.backward_layer.name\n    self.merge_mode = merge_mode\n    if weights:\n        nw = len(weights)\n        self.forward_layer.initial_weights = weights[:nw // 2]\n        self.backward_layer.initial_weights = weights[nw // 2:]\n    self.stateful = layer.stateful\n    self.return_sequences = layer.return_sequences\n    self.return_state = layer.return_state\n    self.supports_masking = True\n    self._trainable = True\n    super(Bidirectional, self).__init__(layer, **kwargs)",
                            "@property\ndef trainable(self):\n    return self._trainable",
                            "@trainable.setter\ndef trainable(self, value):\n    self._trainable = value\n    self.forward_layer.trainable = value\n    self.backward_layer.trainable = value",
                            "def get_weights(self):\n    return self.forward_layer.get_weights() + self.backward_layer.get_weights()",
                            "def set_weights(self, weights):\n    nw = len(weights)\n    self.forward_layer.set_weights(weights[:nw // 2])\n    self.backward_layer.set_weights(weights[nw // 2:])",
                            "def compute_output_shape(self, input_shape):\n    output_shape = self.forward_layer.compute_output_shape(input_shape)\n    if self.return_state:\n        state_shape = output_shape[1:]\n        output_shape = output_shape[0]\n    if self.merge_mode == 'concat':\n        output_shape = list(output_shape)\n        output_shape[-1] *= 2\n        output_shape = tuple(output_shape)\n    elif self.merge_mode is None:\n        output_shape = [output_shape, copy.copy(output_shape)]\n    if self.return_state:\n        if self.merge_mode is None:\n            return output_shape + state_shape + copy.copy(state_shape)\n        return [output_shape] + state_shape + copy.copy(state_shape)\n    return output_shape",
                            "def call(self, inputs, training=None, mask=None, initial_state=None):\n    kwargs = {}\n    if has_arg(self.layer.call, 'training'):\n        kwargs['training'] = training\n    if has_arg(self.layer.call, 'mask'):\n        kwargs['mask'] = mask\n    if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n        if not isinstance(initial_state, list):\n            raise ValueError('When passing `initial_state` to a Bidirectional RNN, the state should be a list containing the states of the underlying RNNs. Found: ' + str(initial_state))\n        forward_state = initial_state[:len(initial_state) // 2]\n        backward_state = initial_state[len(initial_state) // 2:]\n        y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)\n        y_rev = self.backward_layer.call(inputs, initial_state=backward_state, **kwargs)\n    else:\n        y = self.forward_layer.call(inputs, **kwargs)\n        y_rev = self.backward_layer.call(inputs, **kwargs)\n    if self.return_state:\n        states = y[1:] + y_rev[1:]\n        y = y[0]\n        y_rev = y_rev[0]\n    if self.return_sequences:\n        y_rev = K.reverse(y_rev, 1)\n    if self.merge_mode == 'concat':\n        output = K.concatenate([y, y_rev])\n    elif self.merge_mode == 'sum':\n        output = y + y_rev\n    elif self.merge_mode == 'ave':\n        output = (y + y_rev) / 2\n    elif self.merge_mode == 'mul':\n        output = y * y_rev\n    elif self.merge_mode is None:\n        output = [y, y_rev]\n    if getattr(y, '_uses_learning_phase', False) or getattr(y_rev, '_uses_learning_phase', False):\n        if self.merge_mode is None:\n            for out in output:\n                out._uses_learning_phase = True\n        else:\n            output._uses_learning_phase = True\n    if self.return_state:\n        if self.merge_mode is None:\n            return output + states\n        return [output] + states\n    return output",
                            "def reset_states(self):\n    self.forward_layer.reset_states()\n    self.backward_layer.reset_states()",
                            "def build(self, input_shape):\n    with K.name_scope(self.forward_layer.name):\n        self.forward_layer.build(input_shape)\n    with K.name_scope(self.backward_layer.name):\n        self.backward_layer.build(input_shape)\n    self.built = True",
                            "def compute_mask(self, inputs, mask):\n    if self.return_sequences:\n        if not self.merge_mode:\n            return [mask, mask]\n        else:\n            return mask\n    else:\n        return None",
                            "@property\ndef trainable_weights(self):\n    if hasattr(self.forward_layer, 'trainable_weights'):\n        return self.forward_layer.trainable_weights + self.backward_layer.trainable_weights\n    return []",
                            "@property\ndef non_trainable_weights(self):\n    if hasattr(self.forward_layer, 'non_trainable_weights'):\n        return self.forward_layer.non_trainable_weights + self.backward_layer.non_trainable_weights\n    return []",
                            "@property\ndef updates(self):\n    if hasattr(self.forward_layer, 'updates'):\n        return self.forward_layer.updates + self.backward_layer.updates\n    return []",
                            "@property\ndef losses(self):\n    if hasattr(self.forward_layer, 'losses'):\n        return self.forward_layer.losses + self.backward_layer.losses\n    return []",
                            "@property\ndef constraints(self):\n    constraints = {}\n    if hasattr(self.forward_layer, 'constraints'):\n        constraints.update(self.forward_layer.constraints)\n        constraints.update(self.backward_layer.constraints)\n    return constraints",
                            "def get_config(self):\n    config = {'merge_mode': self.merge_mode}\n    base_config = super(Bidirectional, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))"
                        ],
                        "constructor_variables": [
                            "backward_layer",
                            "config",
                            "supports_masking",
                            "_trainable",
                            "forward_layer",
                            "stateful",
                            "return_sequences",
                            "merge_mode",
                            "nw",
                            "return_state"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, layer, merge_mode='concat', weights=None, **kwargs)",
                            "trainable(self)",
                            "trainable(self, value)",
                            "get_weights(self)",
                            "set_weights(self, weights)",
                            "compute_output_shape(self, input_shape)",
                            "call(self, inputs, training=None, mask=None, initial_state=None)",
                            "reset_states(self)",
                            "build(self, input_shape)",
                            "compute_mask(self, inputs, mask)",
                            "trainable_weights(self)",
                            "non_trainable_weights(self)",
                            "updates(self)",
                            "losses(self)",
                            "constraints(self)",
                            "get_config(self)"
                        ]
                    },
                    "variable_values": [
                        [
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x1335e07d0>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x11d0fa990>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x11d0fa990>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                }
                            },
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x1335f7510>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x1335e07d0>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x11d0fa990>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": "{'name': 'lstm_1', 'trainable': True, 'return_sequences': True, 'return_state': True, 'go_backwards': True, 'stateful': False, 'unroll': False, 'units': 3, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 1}",
                                    "variable_type": "dict",
                                    "variable_shape": "25"
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x11d0fa990>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x1335f7550>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                }
                            }
                        ],
                        [
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x135b542d0>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x1335f7f50>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x1335f7f50>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                }
                            },
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x135cb4650>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x135b542d0>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x1335f7f50>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": "{'name': 'lstm_2', 'trainable': True, 'return_sequences': False, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'units': 3, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 1}",
                                    "variable_type": "dict",
                                    "variable_shape": "25"
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x1335f7f50>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x135cb4690>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                }
                            }
                        ],
                        [
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x1335f7f50>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x135b542d0>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x135b542d0>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                }
                            },
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x135e33f90>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x1335f7f50>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x135b542d0>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": "{'name': 'lstm_3', 'trainable': True, 'return_sequences': False, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'units': 3, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 1}",
                                    "variable_type": "dict",
                                    "variable_shape": "25"
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x135b542d0>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x135e33f50>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                }
                            }
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x1387809d0>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x121fc6610>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x121fc6610>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                },
                                "self.input_spec": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.input_spec": {
                                    "variable_value": "[InputSpec(ndim=3)]",
                                    "variable_type": "list",
                                    "variable_shape": "1"
                                }
                            },
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x1387a4810>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x1387809d0>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x121fc6610>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": "{'name': 'lstm_1', 'trainable': True, 'return_sequences': True, 'return_state': True, 'go_backwards': True, 'stateful': False, 'unroll': False, 'units': 3, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 1}",
                                    "variable_type": "dict",
                                    "variable_shape": "25"
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x121fc6610>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x1387807d0>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                },
                                "self.input_spec": {
                                    "variable_value": "[InputSpec(ndim=3)]",
                                    "variable_type": "list",
                                    "variable_shape": "1"
                                },
                                "layer.input_spec": {
                                    "variable_value": "[InputSpec(ndim=3)]",
                                    "variable_type": "list",
                                    "variable_shape": "1"
                                }
                            }
                        ],
                        [
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x13ad2a750>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x1387a4f90>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x1387a4f90>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                },
                                "self.input_spec": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.input_spec": {
                                    "variable_value": "[InputSpec(ndim=3)]",
                                    "variable_type": "list",
                                    "variable_shape": "1"
                                }
                            },
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x13ae73890>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x13ad2a750>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x1387a4f90>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": "{'name': 'lstm_2', 'trainable': True, 'return_sequences': False, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'units': 3, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 1}",
                                    "variable_type": "dict",
                                    "variable_shape": "25"
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x1387a4f90>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x13ae46bd0>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                },
                                "self.input_spec": {
                                    "variable_value": "[InputSpec(ndim=3)]",
                                    "variable_type": "list",
                                    "variable_shape": "1"
                                },
                                "layer.input_spec": {
                                    "variable_value": "[InputSpec(ndim=3)]",
                                    "variable_type": "list",
                                    "variable_shape": "1"
                                }
                            }
                        ],
                        [
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x1387a4f10>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x12937ce90>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x12937ce90>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                },
                                "self.input_spec": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.input_spec": {
                                    "variable_value": "[InputSpec(ndim=3)]",
                                    "variable_type": "list",
                                    "variable_shape": "1"
                                }
                            },
                            {
                                "merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.forward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x13ae76150>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x1387a4f10>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "copy.copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "copy": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x12937ce90>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "config": {
                                    "variable_value": "{'name': 'lstm_3', 'trainable': True, 'return_sequences': False, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'units': 3, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 1}",
                                    "variable_type": "dict",
                                    "variable_shape": "25"
                                },
                                "layer.get_config": {
                                    "variable_value": "<bound method LSTM.get_config of <keras.layers.recurrent.LSTM object at 0x12937ce90>>",
                                    "variable_type": "method",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x13ae64c10>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "layer.__class__.from_config": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "layer.__class__": {
                                    "variable_value": "<class 'keras.layers.recurrent.LSTM'>",
                                    "variable_type": "type",
                                    "variable_shape": null
                                },
                                "self.forward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.name": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "nw": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer.initial_weights": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "layer.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self._trainable": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "Bidirectional": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                },
                                "self.input_spec": {
                                    "variable_value": "[InputSpec(ndim=3)]",
                                    "variable_type": "list",
                                    "variable_shape": "1"
                                },
                                "layer.input_spec": {
                                    "variable_value": "[InputSpec(ndim=3)]",
                                    "variable_type": "list",
                                    "variable_shape": "1"
                                }
                            }
                        ]
                    ]
                },
                {
                    "function_name": "call",
                    "function_code": "def call(self, inputs, training=None, mask=None, initial_state=None):\n    kwargs = {}\n    if has_arg(self.layer.call, 'training'):\n        kwargs['training'] = training\n    if has_arg(self.layer.call, 'mask'):\n        kwargs['mask'] = mask\n\n    if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n        if not isinstance(initial_state, list):\n            raise ValueError(\n                'When passing `initial_state` to a Bidirectional RNN, the state '\n                'should be a list containing the states of the underlying RNNs. '\n                'Found: ' + str(initial_state))\n        forward_state = initial_state[:len(initial_state) // 2]\n        backward_state = initial_state[len(initial_state) // 2:]\n        y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)\n        y_rev = self.backward_layer.call(inputs, initial_state=backward_state, **kwargs)\n    else:\n        y = self.forward_layer.call(inputs, **kwargs)\n        y_rev = self.backward_layer.call(inputs, **kwargs)\n\n    if self.return_state:\n        states = y[1:] + y_rev[1:]\n        y = y[0]\n        y_rev = y_rev[0]\n\n    if self.return_sequences:\n        y_rev = K.reverse(y_rev, 1)\n    if self.merge_mode == 'concat':\n        output = K.concatenate([y, y_rev])\n    elif self.merge_mode == 'sum':\n        output = y + y_rev\n    elif self.merge_mode == 'ave':\n        output = (y + y_rev) / 2\n    elif self.merge_mode == 'mul':\n        output = y * y_rev\n    elif self.merge_mode is None:\n        output = [y, y_rev]\n\n    # Properly set learning phase\n    if (getattr(y, '_uses_learning_phase', False) or\n       getattr(y_rev, '_uses_learning_phase', False)):\n        if self.merge_mode is None:\n            for out in output:\n                out._uses_learning_phase = True\n        else:\n            output._uses_learning_phase = True\n\n    if self.return_state:\n        if self.merge_mode is None:\n            return output + states\n        return [output] + states\n    return output\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 316,
                    "end_line": 368,
                    "variables": {
                        "kwargs": [
                            321,
                            331,
                            332,
                            334,
                            335,
                            317,
                            319
                        ],
                        "has_arg": [
                            320,
                            323,
                            318
                        ],
                        "self.layer.call": [
                            320,
                            323,
                            318
                        ],
                        "self.layer": [
                            320,
                            323,
                            318
                        ],
                        "self": [
                            320,
                            350,
                            352,
                            323,
                            358,
                            331,
                            332,
                            364,
                            334,
                            335,
                            365,
                            337,
                            342,
                            344,
                            346,
                            348,
                            318
                        ],
                        "training": [
                            319
                        ],
                        "mask": [
                            321
                        ],
                        "initial_state": [
                            323,
                            324,
                            328,
                            329,
                            330
                        ],
                        "isinstance": [
                            324
                        ],
                        "list": [
                            324
                        ],
                        "ValueError": [
                            325
                        ],
                        "str": [
                            328
                        ],
                        "forward_state": [
                            329,
                            331
                        ],
                        "len": [
                            329,
                            330
                        ],
                        "backward_state": [
                            330,
                            332
                        ],
                        "y": [
                            353,
                            356,
                            331,
                            334,
                            338,
                            339,
                            345,
                            347,
                            349,
                            351
                        ],
                        "self.forward_layer.call": [
                            331,
                            334
                        ],
                        "self.forward_layer": [
                            331,
                            334
                        ],
                        "inputs": [
                            331,
                            332,
                            334,
                            335
                        ],
                        "y_rev": [
                            353,
                            357,
                            332,
                            335,
                            338,
                            340,
                            343,
                            345,
                            347,
                            349,
                            351
                        ],
                        "self.backward_layer.call": [
                            332,
                            335
                        ],
                        "self.backward_layer": [
                            332,
                            335
                        ],
                        "self.return_state": [
                            337,
                            364
                        ],
                        "states": [
                            338,
                            366,
                            367
                        ],
                        "self.return_sequences": [
                            342
                        ],
                        "K.reverse": [
                            343
                        ],
                        "K": [
                            345,
                            343
                        ],
                        "self.merge_mode": [
                            352,
                            358,
                            365,
                            344,
                            346,
                            348,
                            350
                        ],
                        "output": [
                            353,
                            359,
                            362,
                            366,
                            367,
                            368,
                            345,
                            347,
                            349,
                            351
                        ],
                        "K.concatenate": [
                            345
                        ],
                        "getattr": [
                            356,
                            357
                        ],
                        "out": [
                            360,
                            359
                        ],
                        "out._uses_learning_phase": [
                            360
                        ],
                        "output._uses_learning_phase": [
                            362
                        ]
                    },
                    "filtered_variables": {
                        "kwargs": [
                            321,
                            331,
                            332,
                            334,
                            335,
                            317,
                            319
                        ],
                        "has_arg": [
                            320,
                            323,
                            318
                        ],
                        "self.layer.call": [
                            320,
                            323,
                            318
                        ],
                        "self.layer": [
                            320,
                            323,
                            318
                        ],
                        "self": [
                            320,
                            350,
                            352,
                            323,
                            358,
                            331,
                            332,
                            364,
                            334,
                            335,
                            365,
                            337,
                            342,
                            344,
                            346,
                            348,
                            318
                        ],
                        "training": [
                            319
                        ],
                        "mask": [
                            321
                        ],
                        "initial_state": [
                            323,
                            324,
                            328,
                            329,
                            330
                        ],
                        "forward_state": [
                            329,
                            331
                        ],
                        "backward_state": [
                            330,
                            332
                        ],
                        "y": [
                            353,
                            356,
                            331,
                            334,
                            338,
                            339,
                            345,
                            347,
                            349,
                            351
                        ],
                        "self.forward_layer.call": [
                            331,
                            334
                        ],
                        "self.forward_layer": [
                            331,
                            334
                        ],
                        "inputs": [
                            331,
                            332,
                            334,
                            335
                        ],
                        "y_rev": [
                            353,
                            357,
                            332,
                            335,
                            338,
                            340,
                            343,
                            345,
                            347,
                            349,
                            351
                        ],
                        "self.backward_layer.call": [
                            332,
                            335
                        ],
                        "self.backward_layer": [
                            332,
                            335
                        ],
                        "self.return_state": [
                            337,
                            364
                        ],
                        "states": [
                            338,
                            366,
                            367
                        ],
                        "self.return_sequences": [
                            342
                        ],
                        "K.reverse": [
                            343
                        ],
                        "K": [
                            345,
                            343
                        ],
                        "self.merge_mode": [
                            352,
                            358,
                            365,
                            344,
                            346,
                            348,
                            350
                        ],
                        "output": [
                            353,
                            359,
                            362,
                            366,
                            367,
                            368,
                            345,
                            347,
                            349,
                            351
                        ],
                        "K.concatenate": [
                            345
                        ],
                        "out": [
                            360,
                            359
                        ],
                        "out._uses_learning_phase": [
                            360
                        ],
                        "output._uses_learning_phase": [
                            362
                        ]
                    },
                    "diff_line_number": 316,
                    "class_data": {
                        "signature": "class Bidirectional(Wrapper)",
                        "docstring": "Bidirectional wrapper for RNNs.\n\n# Arguments\n    layer: `Recurrent` instance.\n    merge_mode: Mode by which outputs of the\n        forward and backward RNNs will be combined.\n        One of {'sum', 'mul', 'concat', 'ave', None}.\n        If None, the outputs will not be combined,\n        they will be returned as a list.\n\n# Raises\n    ValueError: In case of invalid `merge_mode` argument.\n\n# Examples\n\n```python\n    model = Sequential()\n    model.add(Bidirectional(LSTM(10, return_sequences=True),\n                            input_shape=(5, 10)))\n    model.add(Bidirectional(LSTM(10)))\n    model.add(Dense(5))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n```",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, layer, merge_mode='concat', weights=None, **kwargs):\n    if merge_mode not in ['sum', 'mul', 'ave', 'concat', None]:\n        raise ValueError('Invalid merge mode. Merge mode should be one of {\"sum\", \"mul\", \"ave\", \"concat\", None}')\n    self.forward_layer = copy.copy(layer)\n    config = layer.get_config()\n    config['go_backwards'] = not config['go_backwards']\n    self.backward_layer = layer.__class__.from_config(config)\n    self.forward_layer.name = 'forward_' + self.forward_layer.name\n    self.backward_layer.name = 'backward_' + self.backward_layer.name\n    self.merge_mode = merge_mode\n    if weights:\n        nw = len(weights)\n        self.forward_layer.initial_weights = weights[:nw // 2]\n        self.backward_layer.initial_weights = weights[nw // 2:]\n    self.stateful = layer.stateful\n    self.return_sequences = layer.return_sequences\n    self.return_state = layer.return_state\n    self.supports_masking = True\n    self._trainable = True\n    super(Bidirectional, self).__init__(layer, **kwargs)",
                            "@property\ndef trainable(self):\n    return self._trainable",
                            "@trainable.setter\ndef trainable(self, value):\n    self._trainable = value\n    self.forward_layer.trainable = value\n    self.backward_layer.trainable = value",
                            "def get_weights(self):\n    return self.forward_layer.get_weights() + self.backward_layer.get_weights()",
                            "def set_weights(self, weights):\n    nw = len(weights)\n    self.forward_layer.set_weights(weights[:nw // 2])\n    self.backward_layer.set_weights(weights[nw // 2:])",
                            "def compute_output_shape(self, input_shape):\n    output_shape = self.forward_layer.compute_output_shape(input_shape)\n    if self.return_state:\n        state_shape = output_shape[1:]\n        output_shape = output_shape[0]\n    if self.merge_mode == 'concat':\n        output_shape = list(output_shape)\n        output_shape[-1] *= 2\n        output_shape = tuple(output_shape)\n    elif self.merge_mode is None:\n        output_shape = [output_shape, copy.copy(output_shape)]\n    if self.return_state:\n        if self.merge_mode is None:\n            return output_shape + state_shape + copy.copy(state_shape)\n        return [output_shape] + state_shape + copy.copy(state_shape)\n    return output_shape",
                            "def call(self, inputs, training=None, mask=None, initial_state=None):\n    kwargs = {}\n    if has_arg(self.layer.call, 'training'):\n        kwargs['training'] = training\n    if has_arg(self.layer.call, 'mask'):\n        kwargs['mask'] = mask\n    if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n        if not isinstance(initial_state, list):\n            raise ValueError('When passing `initial_state` to a Bidirectional RNN, the state should be a list containing the states of the underlying RNNs. Found: ' + str(initial_state))\n        forward_state = initial_state[:len(initial_state) // 2]\n        backward_state = initial_state[len(initial_state) // 2:]\n        y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)\n        y_rev = self.backward_layer.call(inputs, initial_state=backward_state, **kwargs)\n    else:\n        y = self.forward_layer.call(inputs, **kwargs)\n        y_rev = self.backward_layer.call(inputs, **kwargs)\n    if self.return_state:\n        states = y[1:] + y_rev[1:]\n        y = y[0]\n        y_rev = y_rev[0]\n    if self.return_sequences:\n        y_rev = K.reverse(y_rev, 1)\n    if self.merge_mode == 'concat':\n        output = K.concatenate([y, y_rev])\n    elif self.merge_mode == 'sum':\n        output = y + y_rev\n    elif self.merge_mode == 'ave':\n        output = (y + y_rev) / 2\n    elif self.merge_mode == 'mul':\n        output = y * y_rev\n    elif self.merge_mode is None:\n        output = [y, y_rev]\n    if getattr(y, '_uses_learning_phase', False) or getattr(y_rev, '_uses_learning_phase', False):\n        if self.merge_mode is None:\n            for out in output:\n                out._uses_learning_phase = True\n        else:\n            output._uses_learning_phase = True\n    if self.return_state:\n        if self.merge_mode is None:\n            return output + states\n        return [output] + states\n    return output",
                            "def reset_states(self):\n    self.forward_layer.reset_states()\n    self.backward_layer.reset_states()",
                            "def build(self, input_shape):\n    with K.name_scope(self.forward_layer.name):\n        self.forward_layer.build(input_shape)\n    with K.name_scope(self.backward_layer.name):\n        self.backward_layer.build(input_shape)\n    self.built = True",
                            "def compute_mask(self, inputs, mask):\n    if self.return_sequences:\n        if not self.merge_mode:\n            return [mask, mask]\n        else:\n            return mask\n    else:\n        return None",
                            "@property\ndef trainable_weights(self):\n    if hasattr(self.forward_layer, 'trainable_weights'):\n        return self.forward_layer.trainable_weights + self.backward_layer.trainable_weights\n    return []",
                            "@property\ndef non_trainable_weights(self):\n    if hasattr(self.forward_layer, 'non_trainable_weights'):\n        return self.forward_layer.non_trainable_weights + self.backward_layer.non_trainable_weights\n    return []",
                            "@property\ndef updates(self):\n    if hasattr(self.forward_layer, 'updates'):\n        return self.forward_layer.updates + self.backward_layer.updates\n    return []",
                            "@property\ndef losses(self):\n    if hasattr(self.forward_layer, 'losses'):\n        return self.forward_layer.losses + self.backward_layer.losses\n    return []",
                            "@property\ndef constraints(self):\n    constraints = {}\n    if hasattr(self.forward_layer, 'constraints'):\n        constraints.update(self.forward_layer.constraints)\n        constraints.update(self.backward_layer.constraints)\n    return constraints",
                            "def get_config(self):\n    config = {'merge_mode': self.merge_mode}\n    base_config = super(Bidirectional, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))"
                        ],
                        "constructor_variables": [
                            "backward_layer",
                            "config",
                            "supports_masking",
                            "_trainable",
                            "forward_layer",
                            "stateful",
                            "return_sequences",
                            "merge_mode",
                            "nw",
                            "return_state"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, layer, merge_mode='concat', weights=None, **kwargs)",
                            "trainable(self)",
                            "trainable(self, value)",
                            "get_weights(self)",
                            "set_weights(self, weights)",
                            "compute_output_shape(self, input_shape)",
                            "call(self, inputs, training=None, mask=None, initial_state=None)",
                            "reset_states(self)",
                            "build(self, input_shape)",
                            "compute_mask(self, inputs, mask)",
                            "trainable_weights(self)",
                            "non_trainable_weights(self)",
                            "updates(self)",
                            "losses(self)",
                            "constraints(self)",
                            "get_config(self)"
                        ]
                    },
                    "variable_values": [
                        [
                            {
                                "kwargs": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "has_arg": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x130adc110>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x130ba0510>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "training": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "mask": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "initial_state": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "forward_state": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "backward_state": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "y": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.forward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x130ba06d0>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "inputs": {
                                    "variable_value": "<tf.Tensor 'input_1:0' shape=(?, 3, 5) dtype=float32>",
                                    "variable_type": "Tensor",
                                    "variable_shape": "TensorShape([Dimension(None), Dimension(3), Dimension(5)])"
                                },
                                "y_rev": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.backward_layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x130ba0750>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "states": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "K.reverse": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "output": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.concatenate": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "out": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "out._uses_learning_phase": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "output._uses_learning_phase": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            },
                            {
                                "kwargs": {
                                    "variable_value": "{'training': None, 'mask': None}",
                                    "variable_type": "dict",
                                    "variable_shape": "2"
                                },
                                "has_arg": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x130adc110>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x130ba0510>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "training": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "mask": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "initial_state": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "forward_state": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "backward_state": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "y": {
                                    "variable_value": "<tf.Tensor 'bidirectional_1/transpose_1:0' shape=(?, ?, 3) dtype=float32>",
                                    "variable_type": "Tensor",
                                    "variable_shape": "TensorShape([Dimension(None), Dimension(None), Dimension(3)])"
                                },
                                "self.forward_layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.forward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x130ba06d0>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "inputs": {
                                    "variable_value": "<tf.Tensor 'input_1:0' shape=(?, 3, 5) dtype=float32>",
                                    "variable_type": "Tensor",
                                    "variable_shape": "TensorShape([Dimension(None), Dimension(3), Dimension(5)])"
                                },
                                "y_rev": {
                                    "variable_value": "<tf.Tensor 'bidirectional_1/ReverseV2_1:0' shape=(?, ?, 3) dtype=float32>",
                                    "variable_type": "Tensor",
                                    "variable_shape": "TensorShape([Dimension(None), Dimension(None), Dimension(3)])"
                                },
                                "self.backward_layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x130ba0750>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "states": {
                                    "variable_value": "[<tf.Tensor 'bidirectional_1/while/Exit_2:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while/Exit_3:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while_1/Exit_2:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while_1/Exit_3:0' shape=(?, 3) dtype=float32>]",
                                    "variable_type": "list",
                                    "variable_shape": "4"
                                },
                                "self.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "K.reverse": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "output": {
                                    "variable_value": "<tf.Tensor 'bidirectional_1/concat:0' shape=(?, ?, 6) dtype=float32>",
                                    "variable_type": "Tensor",
                                    "variable_shape": "TensorShape([Dimension(None), Dimension(None), Dimension(6)])"
                                },
                                "K.concatenate": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "out": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "out._uses_learning_phase": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "output._uses_learning_phase": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {
                                "kwargs": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "has_arg": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x13316bc90>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x130c82650>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "training": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "mask": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "initial_state": {
                                    "variable_value": "[<tf.Tensor 'bidirectional_1/while/Exit_2:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while/Exit_3:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while_1/Exit_2:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while_1/Exit_3:0' shape=(?, 3) dtype=float32>]",
                                    "variable_type": "list",
                                    "variable_shape": "4"
                                },
                                "forward_state": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "backward_state": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "y": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.forward_layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.forward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x130ba0bd0>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "inputs": {
                                    "variable_value": "<tf.Tensor 'input_2:0' shape=(?, 3, 5) dtype=float32>",
                                    "variable_type": "Tensor",
                                    "variable_shape": "TensorShape([Dimension(None), Dimension(3), Dimension(5)])"
                                },
                                "y_rev": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.backward_layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x130c82ed0>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "states": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "K.reverse": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "output": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.concatenate": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "out": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "out._uses_learning_phase": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "output._uses_learning_phase": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            },
                            {
                                "kwargs": {
                                    "variable_value": "{'training': None, 'mask': None}",
                                    "variable_type": "dict",
                                    "variable_shape": "2"
                                },
                                "has_arg": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x13316bc90>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.wrappers.Bidirectional object at 0x130c82650>",
                                    "variable_type": "Bidirectional",
                                    "variable_shape": null
                                },
                                "training": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "mask": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "initial_state": {
                                    "variable_value": "[<tf.Tensor 'bidirectional_1/while/Exit_2:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while/Exit_3:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while_1/Exit_2:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while_1/Exit_3:0' shape=(?, 3) dtype=float32>]",
                                    "variable_type": "list",
                                    "variable_shape": "4"
                                },
                                "forward_state": {
                                    "variable_value": "[<tf.Tensor 'bidirectional_1/while/Exit_2:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while/Exit_3:0' shape=(?, 3) dtype=float32>]",
                                    "variable_type": "list",
                                    "variable_shape": "2"
                                },
                                "backward_state": {
                                    "variable_value": "[<tf.Tensor 'bidirectional_1/while_1/Exit_2:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'bidirectional_1/while_1/Exit_3:0' shape=(?, 3) dtype=float32>]",
                                    "variable_type": "list",
                                    "variable_shape": "2"
                                },
                                "y": {
                                    "variable_value": "<tf.Tensor 'bidirectional_3/TensorArrayReadV3:0' shape=(?, 3) dtype=float32>",
                                    "variable_type": "Tensor",
                                    "variable_shape": "TensorShape([Dimension(None), Dimension(3)])"
                                },
                                "self.forward_layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.forward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x130ba0bd0>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "inputs": {
                                    "variable_value": "<tf.Tensor 'input_2:0' shape=(?, 3, 5) dtype=float32>",
                                    "variable_type": "Tensor",
                                    "variable_shape": "TensorShape([Dimension(None), Dimension(3), Dimension(5)])"
                                },
                                "y_rev": {
                                    "variable_value": "<tf.Tensor 'bidirectional_3/TensorArrayReadV3_1:0' shape=(?, 3) dtype=float32>",
                                    "variable_type": "Tensor",
                                    "variable_shape": "TensorShape([Dimension(None), Dimension(3)])"
                                },
                                "self.backward_layer.call": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.backward_layer": {
                                    "variable_value": "<keras.layers.recurrent.LSTM object at 0x130c82ed0>",
                                    "variable_type": "LSTM",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "states": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "K.reverse": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.merge_mode": {
                                    "variable_value": "'concat'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "output": {
                                    "variable_value": "<tf.Tensor 'bidirectional_3/concat:0' shape=(?, 6) dtype=float32>",
                                    "variable_type": "Tensor",
                                    "variable_shape": "TensorShape([Dimension(None), Dimension(6)])"
                                },
                                "K.concatenate": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "out": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "out._uses_learning_phase": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "output._uses_learning_phase": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                }
                            }
                        ]
                    ]
                }
            ],
            "inscope_functions": [
                "def __init__(self, layer, **kwargs):\n    self.layer = layer\n    # Tracks mapping of Wrapper inputs to inner layer inputs. Useful when\n    # the inner layer has update ops that depend on its inputs (as opposed\n    # to the inputs to the Wrapper layer).\n    self._input_map = {}\n    super(Wrapper, self).__init__(**kwargs)",
                "def build(self, input_shape=None):\n    self.built = True",
                "@property\ndef activity_regularizer(self):\n    if hasattr(self.layer, 'activity_regularizer'):\n        return self.layer.activity_regularizer\n    else:\n        return None",
                "@property\ndef trainable(self):\n    return self.layer.trainable",
                "@trainable.setter\ndef trainable(self, value):\n    self.layer.trainable = value",
                "@property\ndef trainable_weights(self):\n    return self.layer.trainable_weights",
                "@property\ndef non_trainable_weights(self):\n    return self.layer.non_trainable_weights",
                "@property\ndef updates(self):\n    if hasattr(self.layer, 'updates'):\n        return self.layer.updates\n    return []",
                "def get_updates_for(self, inputs=None):\n    # If the wrapper modifies the inputs, use the modified inputs to\n    # get the updates from the inner layer.\n    inner_inputs = inputs\n    if inputs is not None:\n        uid = _object_list_uid(inputs)\n        if uid in self._input_map:\n            inner_inputs = self._input_map[uid]\n\n    updates = self.layer.get_updates_for(inner_inputs)\n    updates += super(Wrapper, self).get_updates_for(inputs)\n    return updates",
                "@property\ndef losses(self):\n    if hasattr(self.layer, 'losses'):\n        return self.layer.losses\n    return []",
                "def get_losses_for(self, inputs=None):\n    if inputs is None:\n        losses = self.layer.get_losses_for(None)\n        return losses + super(Wrapper, self).get_losses_for(None)\n    return super(Wrapper, self).get_losses_for(inputs)",
                "def get_weights(self):\n    return self.layer.get_weights()",
                "def set_weights(self, weights):\n    self.layer.set_weights(weights)",
                "def get_config(self):\n    config = {'layer': {'class_name': self.layer.__class__.__name__,\n                        'config': self.layer.get_config()}}\n    base_config = super(Wrapper, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    layer = deserialize_layer(config.pop('layer'),\n                              custom_objects=custom_objects)\n    return cls(layer, **config)",
                "def __init__(self, layer, **kwargs):\n    super(TimeDistributed, self).__init__(layer, **kwargs)\n    self.supports_masking = True",
                "def build(self, input_shape):\n    assert len(input_shape) >= 3\n    self.input_spec = InputSpec(shape=input_shape)\n    child_input_shape = (input_shape[0],) + input_shape[2:]\n    if not self.layer.built:\n        self.layer.build(child_input_shape)\n        self.layer.built = True\n    super(TimeDistributed, self).build()",
                "def compute_output_shape(self, input_shape):\n    child_input_shape = (input_shape[0],) + input_shape[2:]\n    child_output_shape = self.layer.compute_output_shape(child_input_shape)\n    timesteps = input_shape[1]\n    return (child_output_shape[0], timesteps) + child_output_shape[1:]",
                "def call(self, inputs, training=None, mask=None):\n    kwargs = {}\n    if has_arg(self.layer.call, 'training'):\n        kwargs['training'] = training\n    uses_learning_phase = False\n\n    input_shape = K.int_shape(inputs)\n    if input_shape[0]:\n        # batch size matters, use rnn-based implementation\n        def step(x, _):\n            global uses_learning_phase\n            output = self.layer.call(x, **kwargs)\n            if hasattr(output, '_uses_learning_phase'):\n                uses_learning_phase = (output._uses_learning_phase or\n                                       uses_learning_phase)\n            return output, []\n\n        _, outputs, _ = K.rnn(step, inputs,\n                              initial_states=[],\n                              input_length=input_shape[1],\n                              unroll=False)\n        y = outputs\n    else:\n        # No batch size specified, therefore the layer will be able\n        # to process batches of any size.\n        # We can go with reshape-based implementation for performance.\n        input_length = input_shape[1]\n        if not input_length:\n            input_length = K.shape(inputs)[1]\n        # Shape: (num_samples * timesteps, ...). And track the\n        # transformation in self._input_map.\n        input_uid = _object_list_uid(inputs)\n        inputs = K.reshape(inputs, (-1,) + input_shape[2:])\n        self._input_map[input_uid] = inputs\n        # (num_samples * timesteps, ...)\n        y = self.layer.call(inputs, **kwargs)\n        if hasattr(y, '_uses_learning_phase'):\n            uses_learning_phase = y._uses_learning_phase\n        # Shape: (num_samples, timesteps, ...)\n        output_shape = self.compute_output_shape(input_shape)\n        y = K.reshape(y, (-1, input_length) + output_shape[2:])\n\n    # Apply activity regularizer if any:\n    if (hasattr(self.layer, 'activity_regularizer') and\n       self.layer.activity_regularizer is not None):\n        regularization_loss = self.layer.activity_regularizer(y)\n        self.add_loss(regularization_loss, inputs)\n\n    if uses_learning_phase:\n        y._uses_learning_phase = True\n    return y",
                "def __init__(self, layer, merge_mode='concat', weights=None, **kwargs):\n    if merge_mode not in ['sum', 'mul', 'ave', 'concat', None]:\n        raise ValueError('Invalid merge mode. '\n                         'Merge mode should be one of '\n                         '{\"sum\", \"mul\", \"ave\", \"concat\", None}')\n    self.forward_layer = copy.copy(layer)\n    config = layer.get_config()\n    config['go_backwards'] = not config['go_backwards']\n    self.backward_layer = layer.__class__.from_config(config)\n    self.forward_layer.name = 'forward_' + self.forward_layer.name\n    self.backward_layer.name = 'backward_' + self.backward_layer.name\n    self.merge_mode = merge_mode\n    if weights:\n        nw = len(weights)\n        self.forward_layer.initial_weights = weights[:nw // 2]\n        self.backward_layer.initial_weights = weights[nw // 2:]\n    self.stateful = layer.stateful\n    self.return_sequences = layer.return_sequences\n    self.return_state = layer.return_state\n    self.supports_masking = True\n    self._trainable = True\n    super(Bidirectional, self).__init__(layer, **kwargs)",
                "@property\ndef trainable(self):\n    return self._trainable",
                "@trainable.setter\ndef trainable(self, value):\n    self._trainable = value\n    self.forward_layer.trainable = value\n    self.backward_layer.trainable = value",
                "def get_weights(self):\n    return self.forward_layer.get_weights() + self.backward_layer.get_weights()",
                "def set_weights(self, weights):\n    nw = len(weights)\n    self.forward_layer.set_weights(weights[:nw // 2])\n    self.backward_layer.set_weights(weights[nw // 2:])",
                "def compute_output_shape(self, input_shape):\n    output_shape = self.forward_layer.compute_output_shape(input_shape)\n    if self.return_state:\n        state_shape = output_shape[1:]\n        output_shape = output_shape[0]\n\n    if self.merge_mode == 'concat':\n        output_shape = list(output_shape)\n        output_shape[-1] *= 2\n        output_shape = tuple(output_shape)\n    elif self.merge_mode is None:\n        output_shape = [output_shape, copy.copy(output_shape)]\n\n    if self.return_state:\n        if self.merge_mode is None:\n            return output_shape + state_shape + copy.copy(state_shape)\n        return [output_shape] + state_shape + copy.copy(state_shape)\n    return output_shape",
                "def call(self, inputs, training=None, mask=None, initial_state=None):\n    kwargs = {}\n    if has_arg(self.layer.call, 'training'):\n        kwargs['training'] = training\n    if has_arg(self.layer.call, 'mask'):\n        kwargs['mask'] = mask\n\n    if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n        if not isinstance(initial_state, list):\n            raise ValueError(\n                'When passing `initial_state` to a Bidirectional RNN, the state '\n                'should be a list containing the states of the underlying RNNs. '\n                'Found: ' + str(initial_state))\n        forward_state = initial_state[:len(initial_state) // 2]\n        backward_state = initial_state[len(initial_state) // 2:]\n        y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)\n        y_rev = self.backward_layer.call(inputs, initial_state=backward_state, **kwargs)\n    else:\n        y = self.forward_layer.call(inputs, **kwargs)\n        y_rev = self.backward_layer.call(inputs, **kwargs)\n\n    if self.return_state:\n        states = y[1:] + y_rev[1:]\n        y = y[0]\n        y_rev = y_rev[0]\n\n    if self.return_sequences:\n        y_rev = K.reverse(y_rev, 1)\n    if self.merge_mode == 'concat':\n        output = K.concatenate([y, y_rev])\n    elif self.merge_mode == 'sum':\n        output = y + y_rev\n    elif self.merge_mode == 'ave':\n        output = (y + y_rev) / 2\n    elif self.merge_mode == 'mul':\n        output = y * y_rev\n    elif self.merge_mode is None:\n        output = [y, y_rev]\n\n    # Properly set learning phase\n    if (getattr(y, '_uses_learning_phase', False) or\n       getattr(y_rev, '_uses_learning_phase', False)):\n        if self.merge_mode is None:\n            for out in output:\n                out._uses_learning_phase = True\n        else:\n            output._uses_learning_phase = True\n\n    if self.return_state:\n        if self.merge_mode is None:\n            return output + states\n        return [output] + states\n    return output",
                "def reset_states(self):\n    self.forward_layer.reset_states()\n    self.backward_layer.reset_states()",
                "def build(self, input_shape):\n    with K.name_scope(self.forward_layer.name):\n        self.forward_layer.build(input_shape)\n    with K.name_scope(self.backward_layer.name):\n        self.backward_layer.build(input_shape)\n    self.built = True",
                "def compute_mask(self, inputs, mask):\n    if self.return_sequences:\n        if not self.merge_mode:\n            return [mask, mask]\n        else:\n            return mask\n    else:\n        return None",
                "@property\ndef trainable_weights(self):\n    if hasattr(self.forward_layer, 'trainable_weights'):\n        return (self.forward_layer.trainable_weights +\n                self.backward_layer.trainable_weights)\n    return []",
                "@property\ndef non_trainable_weights(self):\n    if hasattr(self.forward_layer, 'non_trainable_weights'):\n        return (self.forward_layer.non_trainable_weights +\n                self.backward_layer.non_trainable_weights)\n    return []",
                "@property\ndef updates(self):\n    if hasattr(self.forward_layer, 'updates'):\n        return self.forward_layer.updates + self.backward_layer.updates\n    return []",
                "@property\ndef losses(self):\n    if hasattr(self.forward_layer, 'losses'):\n        return self.forward_layer.losses + self.backward_layer.losses\n    return []",
                "@property\ndef constraints(self):\n    constraints = {}\n    if hasattr(self.forward_layer, 'constraints'):\n        constraints.update(self.forward_layer.constraints)\n        constraints.update(self.backward_layer.constraints)\n    return constraints",
                "def get_config(self):\n    config = {'merge_mode': self.merge_mode}\n    base_config = super(Bidirectional, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "def step(x, _):\n    global uses_learning_phase\n    output = self.layer.call(x, **kwargs)\n    if hasattr(output, '_uses_learning_phase'):\n        uses_learning_phase = (output._uses_learning_phase or\n                               uses_learning_phase)\n    return output, []"
            ],
            "inscope_function_signatures": [
                "__init__(self, layer, **kwargs)",
                "build(self, input_shape=None)",
                "activity_regularizer(self)",
                "trainable(self)",
                "trainable(self, value)",
                "trainable_weights(self)",
                "non_trainable_weights(self)",
                "updates(self)",
                "get_updates_for(self, inputs=None)",
                "losses(self)",
                "get_losses_for(self, inputs=None)",
                "get_weights(self)",
                "set_weights(self, weights)",
                "get_config(self)",
                "from_config(cls, config, custom_objects=None)",
                "__init__(self, layer, **kwargs)",
                "build(self, input_shape)",
                "compute_output_shape(self, input_shape)",
                "call(self, inputs, training=None, mask=None)",
                "__init__(self, layer, merge_mode='concat', weights=None, **kwargs)",
                "trainable(self)",
                "trainable(self, value)",
                "get_weights(self)",
                "set_weights(self, weights)",
                "compute_output_shape(self, input_shape)",
                "call(self, inputs, training=None, mask=None, initial_state=None)",
                "reset_states(self)",
                "build(self, input_shape)",
                "compute_mask(self, inputs, mask)",
                "trainable_weights(self)",
                "non_trainable_weights(self)",
                "updates(self)",
                "losses(self)",
                "constraints(self)",
                "get_config(self)",
                "step(x, _)"
            ],
            "variables_in_file": {
                "Layer": [
                    16
                ],
                "self.layer": [
                    28,
                    165,
                    166,
                    167,
                    40,
                    41,
                    172,
                    47,
                    178,
                    51,
                    55,
                    59,
                    187,
                    318,
                    63,
                    64,
                    320,
                    323,
                    76,
                    82,
                    83,
                    211,
                    88,
                    219,
                    220,
                    93,
                    221,
                    96,
                    99,
                    100
                ],
                "self": [
                    28,
                    32,
                    33,
                    36,
                    40,
                    41,
                    47,
                    51,
                    55,
                    59,
                    63,
                    64,
                    73,
                    74,
                    76,
                    77,
                    82,
                    83,
                    88,
                    89,
                    90,
                    93,
                    96,
                    99,
                    100,
                    101,
                    158,
                    159,
                    163,
                    165,
                    166,
                    167,
                    168,
                    172,
                    178,
                    187,
                    209,
                    211,
                    215,
                    219,
                    220,
                    221,
                    222,
                    261,
                    264,
                    265,
                    266,
                    267,
                    270,
                    271,
                    272,
                    273,
                    274,
                    275,
                    276,
                    277,
                    281,
                    285,
                    286,
                    287,
                    290,
                    294,
                    295,
                    298,
                    299,
                    303,
                    307,
                    310,
                    311,
                    318,
                    320,
                    323,
                    331,
                    332,
                    334,
                    335,
                    337,
                    342,
                    344,
                    346,
                    348,
                    350,
                    352,
                    358,
                    364,
                    365,
                    371,
                    372,
                    375,
                    376,
                    377,
                    378,
                    379,
                    382,
                    383,
                    392,
                    393,
                    394,
                    399,
                    400,
                    401,
                    406,
                    407,
                    412,
                    413,
                    419,
                    420,
                    421,
                    425,
                    426
                ],
                "layer": [
                    261,
                    262,
                    264,
                    107,
                    109,
                    272,
                    273,
                    274,
                    277,
                    28,
                    158
                ],
                "self._input_map": [
                    32,
                    73,
                    74,
                    209
                ],
                "__init__": [
                    33,
                    277,
                    158
                ],
                "super": [
                    33,
                    101,
                    168,
                    426,
                    77,
                    277,
                    89,
                    90,
                    158
                ],
                "Wrapper": [
                    33,
                    101,
                    229,
                    77,
                    112,
                    89,
                    90
                ],
                "kwargs": [
                    33,
                    321,
                    331,
                    332,
                    334,
                    335,
                    177,
                    179,
                    211,
                    277,
                    187,
                    317,
                    158,
                    319
                ],
                "self.built": [
                    379,
                    36
                ],
                "hasattr": [
                    419,
                    40,
                    392,
                    399,
                    82,
                    212,
                    406,
                    412,
                    219,
                    188,
                    63
                ],
                "self.layer.activity_regularizer": [
                    41,
                    220,
                    221
                ],
                "property": [
                    416,
                    38,
                    390,
                    45,
                    397,
                    80,
                    404,
                    53,
                    279,
                    57,
                    410,
                    61
                ],
                "self.layer.trainable": [
                    51,
                    47
                ],
                "value": [
                    51,
                    285,
                    286,
                    287
                ],
                "trainable.setter": [
                    49,
                    283
                ],
                "trainable": [
                    49,
                    283
                ],
                "self.layer.trainable_weights": [
                    55
                ],
                "self.layer.non_trainable_weights": [
                    59
                ],
                "self.layer.updates": [
                    64
                ],
                "inner_inputs": [
                    74,
                    76,
                    70
                ],
                "inputs": [
                    193,
                    70,
                    71,
                    72,
                    331,
                    204,
                    77,
                    332,
                    207,
                    208,
                    209,
                    334,
                    211,
                    335,
                    182,
                    87,
                    90,
                    222
                ],
                "uid": [
                    72,
                    73,
                    74
                ],
                "_object_list_uid": [
                    72,
                    207
                ],
                "updates": [
                    76,
                    77,
                    78
                ],
                "self.layer.get_updates_for": [
                    76
                ],
                "get_updates_for": [
                    77
                ],
                "self.layer.losses": [
                    83
                ],
                "losses": [
                    88,
                    89
                ],
                "self.layer.get_losses_for": [
                    88
                ],
                "get_losses_for": [
                    89,
                    90
                ],
                "self.layer.get_weights": [
                    93
                ],
                "self.layer.set_weights": [
                    96
                ],
                "weights": [
                    96,
                    293,
                    294,
                    295,
                    268,
                    269,
                    270,
                    271
                ],
                "config": [
                    99,
                    102,
                    262,
                    263,
                    264,
                    425,
                    107,
                    427,
                    109
                ],
                "self.layer.__class__.__name__": [
                    99
                ],
                "self.layer.__class__": [
                    99
                ],
                "self.layer.get_config": [
                    100
                ],
                "base_config": [
                    426,
                    427,
                    101,
                    102
                ],
                "get_config": [
                    426,
                    101
                ],
                "dict": [
                    427,
                    102
                ],
                "list": [
                    304,
                    427,
                    324,
                    102
                ],
                "base_config.items": [
                    427,
                    102
                ],
                "config.items": [
                    427,
                    102
                ],
                "deserialize_layer": [
                    107
                ],
                "config.pop": [
                    107
                ],
                "custom_objects": [
                    108
                ],
                "cls": [
                    109
                ],
                "classmethod": [
                    104
                ],
                "TimeDistributed": [
                    168,
                    158
                ],
                "self.supports_masking": [
                    275,
                    159
                ],
                "len": [
                    162,
                    293,
                    329,
                    330,
                    269
                ],
                "input_shape": [
                    162,
                    163,
                    164,
                    195,
                    202,
                    171,
                    298,
                    173,
                    378,
                    208,
                    182,
                    183,
                    376,
                    215
                ],
                "self.input_spec": [
                    163
                ],
                "InputSpec": [
                    163
                ],
                "child_input_shape": [
                    172,
                    171,
                    164,
                    166
                ],
                "self.layer.built": [
                    165,
                    167
                ],
                "self.layer.build": [
                    166
                ],
                "build": [
                    168
                ],
                "child_output_shape": [
                    172,
                    174
                ],
                "self.layer.compute_output_shape": [
                    172
                ],
                "timesteps": [
                    173,
                    174
                ],
                "has_arg": [
                    320,
                    178,
                    323,
                    318
                ],
                "self.layer.call": [
                    320,
                    323,
                    178,
                    211,
                    187,
                    318
                ],
                "training": [
                    179,
                    319
                ],
                "uses_learning_phase": [
                    224,
                    180,
                    213,
                    189,
                    190
                ],
                "K.int_shape": [
                    182
                ],
                "K": [
                    193,
                    377,
                    204,
                    208,
                    182,
                    343,
                    216,
                    345,
                    375
                ],
                "output": [
                    353,
                    351,
                    359,
                    362,
                    349,
                    366,
                    367,
                    368,
                    347,
                    345,
                    187,
                    188,
                    189,
                    191
                ],
                "x": [
                    187
                ],
                "output._uses_learning_phase": [
                    362,
                    189
                ],
                "_": [
                    193
                ],
                "outputs": [
                    193,
                    197
                ],
                "K.rnn": [
                    193
                ],
                "step": [
                    193
                ],
                "y": [
                    225,
                    226,
                    353,
                    356,
                    197,
                    331,
                    334,
                    338,
                    211,
                    212,
                    213,
                    339,
                    216,
                    345,
                    347,
                    349,
                    221,
                    351
                ],
                "input_length": [
                    216,
                    202,
                    203,
                    204
                ],
                "K.shape": [
                    204
                ],
                "input_uid": [
                    209,
                    207
                ],
                "K.reshape": [
                    208,
                    216
                ],
                "y._uses_learning_phase": [
                    225,
                    213
                ],
                "output_shape": [
                    312,
                    298,
                    300,
                    301,
                    304,
                    305,
                    306,
                    308,
                    215,
                    216,
                    313,
                    314
                ],
                "self.compute_output_shape": [
                    215
                ],
                "regularization_loss": [
                    221,
                    222
                ],
                "self.add_loss": [
                    222
                ],
                "merge_mode": [
                    257,
                    267
                ],
                "ValueError": [
                    258,
                    325
                ],
                "self.forward_layer": [
                    261,
                    392,
                    265,
                    393,
                    270,
                    399,
                    400,
                    406,
                    407,
                    412,
                    413,
                    286,
                    290,
                    419,
                    420,
                    294,
                    298,
                    331,
                    334,
                    371,
                    375,
                    376
                ],
                "copy.copy": [
                    312,
                    313,
                    308,
                    261
                ],
                "copy": [
                    312,
                    313,
                    308,
                    261
                ],
                "layer.get_config": [
                    262
                ],
                "self.backward_layer": [
                    290,
                    421,
                    295,
                    264,
                    266,
                    394,
                    332,
                    271,
                    335,
                    401,
                    372,
                    407,
                    377,
                    378,
                    413,
                    287
                ],
                "layer.__class__.from_config": [
                    264
                ],
                "layer.__class__": [
                    264
                ],
                "self.forward_layer.name": [
                    265,
                    375
                ],
                "self.backward_layer.name": [
                    377,
                    266
                ],
                "self.merge_mode": [
                    352,
                    358,
                    425,
                    267,
                    365,
                    303,
                    307,
                    311,
                    344,
                    346,
                    348,
                    350,
                    383
                ],
                "nw": [
                    293,
                    294,
                    295,
                    269,
                    270,
                    271
                ],
                "self.forward_layer.initial_weights": [
                    270
                ],
                "self.backward_layer.initial_weights": [
                    271
                ],
                "self.stateful": [
                    272
                ],
                "layer.stateful": [
                    272
                ],
                "self.return_sequences": [
                    273,
                    382,
                    342
                ],
                "layer.return_sequences": [
                    273
                ],
                "self.return_state": [
                    299,
                    364,
                    337,
                    274,
                    310
                ],
                "layer.return_state": [
                    274
                ],
                "self._trainable": [
                    281,
                    276,
                    285
                ],
                "Bidirectional": [
                    426,
                    277
                ],
                "self.forward_layer.trainable": [
                    286
                ],
                "self.backward_layer.trainable": [
                    287
                ],
                "self.forward_layer.get_weights": [
                    290
                ],
                "self.backward_layer.get_weights": [
                    290
                ],
                "self.forward_layer.set_weights": [
                    294
                ],
                "self.backward_layer.set_weights": [
                    295
                ],
                "self.forward_layer.compute_output_shape": [
                    298
                ],
                "state_shape": [
                    312,
                    313,
                    300
                ],
                "tuple": [
                    306
                ],
                "mask": [
                    384,
                    321,
                    386
                ],
                "initial_state": [
                    323,
                    324,
                    328,
                    329,
                    330
                ],
                "isinstance": [
                    324
                ],
                "str": [
                    328
                ],
                "forward_state": [
                    329,
                    331
                ],
                "backward_state": [
                    330,
                    332
                ],
                "self.forward_layer.call": [
                    331,
                    334
                ],
                "y_rev": [
                    353,
                    357,
                    332,
                    335,
                    338,
                    340,
                    343,
                    345,
                    347,
                    349,
                    351
                ],
                "self.backward_layer.call": [
                    332,
                    335
                ],
                "states": [
                    338,
                    366,
                    367
                ],
                "K.reverse": [
                    343
                ],
                "K.concatenate": [
                    345
                ],
                "getattr": [
                    356,
                    357
                ],
                "out": [
                    360,
                    359
                ],
                "out._uses_learning_phase": [
                    360
                ],
                "self.forward_layer.reset_states": [
                    371
                ],
                "self.backward_layer.reset_states": [
                    372
                ],
                "K.name_scope": [
                    377,
                    375
                ],
                "self.forward_layer.build": [
                    376
                ],
                "self.backward_layer.build": [
                    378
                ],
                "self.forward_layer.trainable_weights": [
                    393
                ],
                "self.backward_layer.trainable_weights": [
                    394
                ],
                "self.forward_layer.non_trainable_weights": [
                    400
                ],
                "self.backward_layer.non_trainable_weights": [
                    401
                ],
                "self.forward_layer.updates": [
                    407
                ],
                "self.backward_layer.updates": [
                    407
                ],
                "self.forward_layer.losses": [
                    413
                ],
                "self.backward_layer.losses": [
                    413
                ],
                "constraints": [
                    418,
                    420,
                    421,
                    422
                ],
                "constraints.update": [
                    420,
                    421
                ],
                "self.forward_layer.constraints": [
                    420
                ],
                "self.backward_layer.constraints": [
                    421
                ]
            },
            "filtered_variables_in_file": {
                "Layer": [
                    16
                ],
                "self.layer": [
                    28,
                    165,
                    166,
                    167,
                    40,
                    41,
                    172,
                    47,
                    178,
                    51,
                    55,
                    59,
                    187,
                    318,
                    63,
                    64,
                    320,
                    323,
                    76,
                    82,
                    83,
                    211,
                    88,
                    219,
                    220,
                    93,
                    221,
                    96,
                    99,
                    100
                ],
                "self": [
                    28,
                    32,
                    33,
                    36,
                    40,
                    41,
                    47,
                    51,
                    55,
                    59,
                    63,
                    64,
                    73,
                    74,
                    76,
                    77,
                    82,
                    83,
                    88,
                    89,
                    90,
                    93,
                    96,
                    99,
                    100,
                    101,
                    158,
                    159,
                    163,
                    165,
                    166,
                    167,
                    168,
                    172,
                    178,
                    187,
                    209,
                    211,
                    215,
                    219,
                    220,
                    221,
                    222,
                    261,
                    264,
                    265,
                    266,
                    267,
                    270,
                    271,
                    272,
                    273,
                    274,
                    275,
                    276,
                    277,
                    281,
                    285,
                    286,
                    287,
                    290,
                    294,
                    295,
                    298,
                    299,
                    303,
                    307,
                    310,
                    311,
                    318,
                    320,
                    323,
                    331,
                    332,
                    334,
                    335,
                    337,
                    342,
                    344,
                    346,
                    348,
                    350,
                    352,
                    358,
                    364,
                    365,
                    371,
                    372,
                    375,
                    376,
                    377,
                    378,
                    379,
                    382,
                    383,
                    392,
                    393,
                    394,
                    399,
                    400,
                    401,
                    406,
                    407,
                    412,
                    413,
                    419,
                    420,
                    421,
                    425,
                    426
                ],
                "layer": [
                    261,
                    262,
                    264,
                    107,
                    109,
                    272,
                    273,
                    274,
                    277,
                    28,
                    158
                ],
                "self._input_map": [
                    32,
                    73,
                    74,
                    209
                ],
                "__init__": [
                    33,
                    277,
                    158
                ],
                "Wrapper": [
                    33,
                    101,
                    229,
                    77,
                    112,
                    89,
                    90
                ],
                "kwargs": [
                    33,
                    321,
                    331,
                    332,
                    334,
                    335,
                    177,
                    179,
                    211,
                    277,
                    187,
                    317,
                    158,
                    319
                ],
                "self.built": [
                    379,
                    36
                ],
                "self.layer.activity_regularizer": [
                    41,
                    220,
                    221
                ],
                "self.layer.trainable": [
                    51,
                    47
                ],
                "value": [
                    51,
                    285,
                    286,
                    287
                ],
                "trainable.setter": [
                    49,
                    283
                ],
                "trainable": [
                    49,
                    283
                ],
                "self.layer.trainable_weights": [
                    55
                ],
                "self.layer.non_trainable_weights": [
                    59
                ],
                "self.layer.updates": [
                    64
                ],
                "inner_inputs": [
                    74,
                    76,
                    70
                ],
                "inputs": [
                    193,
                    70,
                    71,
                    72,
                    331,
                    204,
                    77,
                    332,
                    207,
                    208,
                    209,
                    334,
                    211,
                    335,
                    182,
                    87,
                    90,
                    222
                ],
                "uid": [
                    72,
                    73,
                    74
                ],
                "_object_list_uid": [
                    72,
                    207
                ],
                "updates": [
                    76,
                    77,
                    78
                ],
                "self.layer.get_updates_for": [
                    76
                ],
                "get_updates_for": [
                    77
                ],
                "self.layer.losses": [
                    83
                ],
                "losses": [
                    88,
                    89
                ],
                "self.layer.get_losses_for": [
                    88
                ],
                "get_losses_for": [
                    89,
                    90
                ],
                "self.layer.get_weights": [
                    93
                ],
                "self.layer.set_weights": [
                    96
                ],
                "weights": [
                    96,
                    293,
                    294,
                    295,
                    268,
                    269,
                    270,
                    271
                ],
                "config": [
                    99,
                    102,
                    262,
                    263,
                    264,
                    425,
                    107,
                    427,
                    109
                ],
                "self.layer.__class__.__name__": [
                    99
                ],
                "self.layer.__class__": [
                    99
                ],
                "self.layer.get_config": [
                    100
                ],
                "base_config": [
                    426,
                    427,
                    101,
                    102
                ],
                "get_config": [
                    426,
                    101
                ],
                "base_config.items": [
                    427,
                    102
                ],
                "config.items": [
                    427,
                    102
                ],
                "deserialize_layer": [
                    107
                ],
                "config.pop": [
                    107
                ],
                "custom_objects": [
                    108
                ],
                "cls": [
                    109
                ],
                "TimeDistributed": [
                    168,
                    158
                ],
                "self.supports_masking": [
                    275,
                    159
                ],
                "input_shape": [
                    162,
                    163,
                    164,
                    195,
                    202,
                    171,
                    298,
                    173,
                    378,
                    208,
                    182,
                    183,
                    376,
                    215
                ],
                "self.input_spec": [
                    163
                ],
                "InputSpec": [
                    163
                ],
                "child_input_shape": [
                    172,
                    171,
                    164,
                    166
                ],
                "self.layer.built": [
                    165,
                    167
                ],
                "self.layer.build": [
                    166
                ],
                "build": [
                    168
                ],
                "child_output_shape": [
                    172,
                    174
                ],
                "self.layer.compute_output_shape": [
                    172
                ],
                "timesteps": [
                    173,
                    174
                ],
                "has_arg": [
                    320,
                    178,
                    323,
                    318
                ],
                "self.layer.call": [
                    320,
                    323,
                    178,
                    211,
                    187,
                    318
                ],
                "training": [
                    179,
                    319
                ],
                "uses_learning_phase": [
                    224,
                    180,
                    213,
                    189,
                    190
                ],
                "K.int_shape": [
                    182
                ],
                "K": [
                    193,
                    377,
                    204,
                    208,
                    182,
                    343,
                    216,
                    345,
                    375
                ],
                "output": [
                    353,
                    351,
                    359,
                    362,
                    349,
                    366,
                    367,
                    368,
                    347,
                    345,
                    187,
                    188,
                    189,
                    191
                ],
                "x": [
                    187
                ],
                "output._uses_learning_phase": [
                    362,
                    189
                ],
                "_": [
                    193
                ],
                "outputs": [
                    193,
                    197
                ],
                "K.rnn": [
                    193
                ],
                "step": [
                    193
                ],
                "y": [
                    225,
                    226,
                    353,
                    356,
                    197,
                    331,
                    334,
                    338,
                    211,
                    212,
                    213,
                    339,
                    216,
                    345,
                    347,
                    349,
                    221,
                    351
                ],
                "input_length": [
                    216,
                    202,
                    203,
                    204
                ],
                "K.shape": [
                    204
                ],
                "input_uid": [
                    209,
                    207
                ],
                "K.reshape": [
                    208,
                    216
                ],
                "y._uses_learning_phase": [
                    225,
                    213
                ],
                "output_shape": [
                    312,
                    298,
                    300,
                    301,
                    304,
                    305,
                    306,
                    308,
                    215,
                    216,
                    313,
                    314
                ],
                "self.compute_output_shape": [
                    215
                ],
                "regularization_loss": [
                    221,
                    222
                ],
                "self.add_loss": [
                    222
                ],
                "merge_mode": [
                    257,
                    267
                ],
                "self.forward_layer": [
                    261,
                    392,
                    265,
                    393,
                    270,
                    399,
                    400,
                    406,
                    407,
                    412,
                    413,
                    286,
                    290,
                    419,
                    420,
                    294,
                    298,
                    331,
                    334,
                    371,
                    375,
                    376
                ],
                "copy.copy": [
                    312,
                    313,
                    308,
                    261
                ],
                "copy": [
                    312,
                    313,
                    308,
                    261
                ],
                "layer.get_config": [
                    262
                ],
                "self.backward_layer": [
                    290,
                    421,
                    295,
                    264,
                    266,
                    394,
                    332,
                    271,
                    335,
                    401,
                    372,
                    407,
                    377,
                    378,
                    413,
                    287
                ],
                "layer.__class__.from_config": [
                    264
                ],
                "layer.__class__": [
                    264
                ],
                "self.forward_layer.name": [
                    265,
                    375
                ],
                "self.backward_layer.name": [
                    377,
                    266
                ],
                "self.merge_mode": [
                    352,
                    358,
                    425,
                    267,
                    365,
                    303,
                    307,
                    311,
                    344,
                    346,
                    348,
                    350,
                    383
                ],
                "nw": [
                    293,
                    294,
                    295,
                    269,
                    270,
                    271
                ],
                "self.forward_layer.initial_weights": [
                    270
                ],
                "self.backward_layer.initial_weights": [
                    271
                ],
                "self.stateful": [
                    272
                ],
                "layer.stateful": [
                    272
                ],
                "self.return_sequences": [
                    273,
                    382,
                    342
                ],
                "layer.return_sequences": [
                    273
                ],
                "self.return_state": [
                    299,
                    364,
                    337,
                    274,
                    310
                ],
                "layer.return_state": [
                    274
                ],
                "self._trainable": [
                    281,
                    276,
                    285
                ],
                "Bidirectional": [
                    426,
                    277
                ],
                "self.forward_layer.trainable": [
                    286
                ],
                "self.backward_layer.trainable": [
                    287
                ],
                "self.forward_layer.get_weights": [
                    290
                ],
                "self.backward_layer.get_weights": [
                    290
                ],
                "self.forward_layer.set_weights": [
                    294
                ],
                "self.backward_layer.set_weights": [
                    295
                ],
                "self.forward_layer.compute_output_shape": [
                    298
                ],
                "state_shape": [
                    312,
                    313,
                    300
                ],
                "mask": [
                    384,
                    321,
                    386
                ],
                "initial_state": [
                    323,
                    324,
                    328,
                    329,
                    330
                ],
                "forward_state": [
                    329,
                    331
                ],
                "backward_state": [
                    330,
                    332
                ],
                "self.forward_layer.call": [
                    331,
                    334
                ],
                "y_rev": [
                    353,
                    357,
                    332,
                    335,
                    338,
                    340,
                    343,
                    345,
                    347,
                    349,
                    351
                ],
                "self.backward_layer.call": [
                    332,
                    335
                ],
                "states": [
                    338,
                    366,
                    367
                ],
                "K.reverse": [
                    343
                ],
                "K.concatenate": [
                    345
                ],
                "out": [
                    360,
                    359
                ],
                "out._uses_learning_phase": [
                    360
                ],
                "self.forward_layer.reset_states": [
                    371
                ],
                "self.backward_layer.reset_states": [
                    372
                ],
                "K.name_scope": [
                    377,
                    375
                ],
                "self.forward_layer.build": [
                    376
                ],
                "self.backward_layer.build": [
                    378
                ],
                "self.forward_layer.trainable_weights": [
                    393
                ],
                "self.backward_layer.trainable_weights": [
                    394
                ],
                "self.forward_layer.non_trainable_weights": [
                    400
                ],
                "self.backward_layer.non_trainable_weights": [
                    401
                ],
                "self.forward_layer.updates": [
                    407
                ],
                "self.backward_layer.updates": [
                    407
                ],
                "self.forward_layer.losses": [
                    413
                ],
                "self.backward_layer.losses": [
                    413
                ],
                "constraints": [
                    418,
                    420,
                    421,
                    422
                ],
                "constraints.update": [
                    420,
                    421
                ],
                "self.forward_layer.constraints": [
                    420
                ],
                "self.backward_layer.constraints": [
                    421
                ]
            }
        },
        "test_data": [
            {
                "test_path": "/Volumes/SSD2T/bgp_envs/repos/keras_37/tests/keras/layers/wrappers_test.py",
                "test_function": "test_Bidirectional_state_reuse",
                "test_function_code": "@keras_test\ndef test_Bidirectional_state_reuse():\n    rnn = layers.LSTM\n    samples = 2\n    dim = 5\n    timesteps = 3\n    units = 3\n\n    input1 = Input((timesteps, dim))\n    layer = wrappers.Bidirectional(rnn(units, return_state=True, return_sequences=True))\n    state = layer(input1)[1:]\n\n    # test passing invalid initial_state: passing a tensor\n    input2 = Input((timesteps, dim))\n    with pytest.raises(ValueError):\n        output = wrappers.Bidirectional(rnn(units))(input2, initial_state=state[0])\n\n    # test valid usage: passing a list\n    output = wrappers.Bidirectional(rnn(units))(input2, initial_state=state)\n    model = Model([input1, input2], output)\n    assert len(model.layers) == 4\n    assert isinstance(model.layers[-1].input, list)\n    inputs = [np.random.rand(samples, timesteps, dim),\n              np.random.rand(samples, timesteps, dim)]\n    outputs = model.predict(inputs)",
                "test_error": "assert 2 == 4   +2   -4",
                "full_test_error": "@keras_test\n    def test_Bidirectional_state_reuse():\n        rnn = layers.LSTM\n        samples = 2\n        dim = 5\n        timesteps = 3\n        units = 3\n    \n        input1 = Input((timesteps, dim))\n        layer = wrappers.Bidirectional(rnn(units, return_state=True, return_sequences=True))\n        state = layer(input1)[1:]\n    \n        # test passing invalid initial_state: passing a tensor\n        input2 = Input((timesteps, dim))\n        with pytest.raises(ValueError):\n            output = wrappers.Bidirectional(rnn(units))(input2, initial_state=state[0])\n    \n        # test valid usage: passing a list\n        output = wrappers.Bidirectional(rnn(units))(input2, initial_state=state)\n        model = Model([input1, input2], output)\n>       assert len(model.layers) == 4\nE       assert 2 == 4\nE         +2\nE         -4\n\ntests/keras/layers/wrappers_test.py:340: AssertionError",
                "traceback": null,
                "test_error_location": null,
                "test_function_decorators": [
                    "keras_test"
                ]
            }
        ]
    }
}