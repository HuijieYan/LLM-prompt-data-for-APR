{
    "keras:1": {
        "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_1/keras/backend/tensorflow_backend.py": {
            "buggy_functions": [
                {
                    "function_name": "update",
                    "function_code": "@symbolic\ndef update(x, new_x):\n    \"\"\"Update the value of `x` to `new_x`.\n\n    # Arguments\n        x: A `Variable`.\n        new_x: A tensor of same shape as `x`.\n\n    # Returns\n        The variable `x` updated.\n    \"\"\"\n    return tf_state_ops.assign(x, new_x)\n",
                    "decorators": [
                        "symbolic"
                    ],
                    "docstring": "Update the value of `x` to `new_x`.\n\n# Arguments\n    x: A `Variable`.\n    new_x: A tensor of same shape as `x`.\n\n# Returns\n    The variable `x` updated.",
                    "start_line": 1195,
                    "end_line": 1206,
                    "variables": {
                        "tf_state_ops.assign": [
                            1206
                        ],
                        "tf_state_ops": [
                            1206
                        ],
                        "x": [
                            1206
                        ],
                        "new_x": [
                            1206
                        ],
                        "symbolic": [
                            1195
                        ]
                    },
                    "filtered_variables": {
                        "tf_state_ops.assign": [
                            1206
                        ],
                        "tf_state_ops": [
                            1206
                        ],
                        "x": [
                            1206
                        ],
                        "new_x": [
                            1206
                        ],
                        "symbolic": [
                            1195
                        ]
                    },
                    "diff_line_number": 1206,
                    "class_data": null,
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "update_add",
                    "function_code": "@symbolic\ndef update_add(x, increment):\n    \"\"\"Update the value of `x` by adding `increment`.\n\n    # Arguments\n        x: A `Variable`.\n        increment: A tensor of same shape as `x`.\n\n    # Returns\n        The variable `x` updated.\n    \"\"\"\n    return tf_state_ops.assign_add(x, increment)\n",
                    "decorators": [
                        "symbolic"
                    ],
                    "docstring": "Update the value of `x` by adding `increment`.\n\n# Arguments\n    x: A `Variable`.\n    increment: A tensor of same shape as `x`.\n\n# Returns\n    The variable `x` updated.",
                    "start_line": 1209,
                    "end_line": 1220,
                    "variables": {
                        "tf_state_ops.assign_add": [
                            1220
                        ],
                        "tf_state_ops": [
                            1220
                        ],
                        "x": [
                            1220
                        ],
                        "increment": [
                            1220
                        ],
                        "symbolic": [
                            1209
                        ]
                    },
                    "filtered_variables": {
                        "tf_state_ops.assign_add": [
                            1220
                        ],
                        "tf_state_ops": [
                            1220
                        ],
                        "x": [
                            1220
                        ],
                        "increment": [
                            1220
                        ],
                        "symbolic": [
                            1209
                        ]
                    },
                    "diff_line_number": 1220,
                    "class_data": null,
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "update_sub",
                    "function_code": "@symbolic\ndef update_sub(x, decrement):\n    \"\"\"Update the value of `x` by subtracting `decrement`.\n\n    # Arguments\n        x: A `Variable`.\n        decrement: A tensor of same shape as `x`.\n\n    # Returns\n        The variable `x` updated.\n    \"\"\"\n    return tf_state_ops.assign_sub(x, decrement)\n",
                    "decorators": [
                        "symbolic"
                    ],
                    "docstring": "Update the value of `x` by subtracting `decrement`.\n\n# Arguments\n    x: A `Variable`.\n    decrement: A tensor of same shape as `x`.\n\n# Returns\n    The variable `x` updated.",
                    "start_line": 1223,
                    "end_line": 1234,
                    "variables": {
                        "tf_state_ops.assign_sub": [
                            1234
                        ],
                        "tf_state_ops": [
                            1234
                        ],
                        "x": [
                            1234
                        ],
                        "decrement": [
                            1234
                        ],
                        "symbolic": [
                            1223
                        ]
                    },
                    "filtered_variables": {
                        "tf_state_ops.assign_sub": [
                            1234
                        ],
                        "tf_state_ops": [
                            1234
                        ],
                        "x": [
                            1234
                        ],
                        "decrement": [
                            1234
                        ],
                        "symbolic": [
                            1223
                        ]
                    },
                    "diff_line_number": 1234,
                    "class_data": null,
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "print_tensor",
                    "function_code": "def print_tensor(x, message=''):\n    \"\"\"Prints `message` and the tensor value when evaluated.\n\n     Note that `print_tensor` returns a new tensor identical to `x`\n     which should be used in the following code. Otherwise the\n     print operation is not taken into account during evaluation.\n\n     # Example\n     ```python\n         >>> x = K.print_tensor(x, message=\"x is: \")\n     ```\n\n    # Arguments\n        x: Tensor to print.\n        message: Message to print jointly with the tensor.\n\n    # Returns\n        The same tensor `x`, unchanged.\n    \"\"\"\n    # TODO\n    return tf.Print(x, [x], message)\n",
                    "decorators": [],
                    "docstring": "Prints `message` and the tensor value when evaluated.\n\n Note that `print_tensor` returns a new tensor identical to `x`\n which should be used in the following code. Otherwise the\n print operation is not taken into account during evaluation.\n\n # Example\n ```python\n     >>> x = K.print_tensor(x, message=\"x is: \")\n ```\n\n# Arguments\n    x: Tensor to print.\n    message: Message to print jointly with the tensor.\n\n# Returns\n    The same tensor `x`, unchanged.",
                    "start_line": 2883,
                    "end_line": 2903,
                    "variables": {
                        "tf.Print": [
                            2903
                        ],
                        "tf": [
                            2903
                        ],
                        "x": [
                            2903
                        ],
                        "message": [
                            2903
                        ]
                    },
                    "filtered_variables": {
                        "tf.Print": [
                            2903
                        ],
                        "tf": [
                            2903
                        ],
                        "x": [
                            2903
                        ],
                        "message": [
                            2903
                        ]
                    },
                    "diff_line_number": 2883,
                    "class_data": null,
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "import functools",
                    "start_line": 16,
                    "end_line": 17
                }
            ],
            "inscope_functions": [
                "def _is_tf_1():\n    return tf.__version__.startswith('1.')",
                "def symbolic(func):\n    \"\"\"Decorator used in TensorFlow 2.0 to enter the Keras graph.\n\n    # Arguments\n        func: Function to decorate.\n\n    # Returns\n        Decorated function.\n    \"\"\"\n    if _is_tf_1():\n        return func\n\n    @functools.wraps(func)\n    def symbolic_fn_wrapper(*args, **kwargs):\n        if _SYMBOLIC_SCOPE.value:\n            with get_graph().as_default():\n                return func(*args, **kwargs)\n        else:\n            return func(*args, **kwargs)\n    return symbolic_fn_wrapper",
                "def is_symbolic(x):\n    return isinstance(x, tf.Tensor) and hasattr(x, 'op')",
                "def eager(func):\n    \"\"\"Decorator used in TensorFlow 2.0 to exit the Keras graph.\n\n    # Arguments\n        func: Function to decorate.\n\n    # Returns\n        Decorated function.\n    \"\"\"\n    if _is_tf_1():\n        return func\n\n    global _SYMBOLIC_SCOPE\n\n    @functools.wraps(func)\n    def eager_fn_wrapper(*args, **kwargs):\n        prev_value = _SYMBOLIC_SCOPE.value\n        try:\n            _SYMBOLIC_SCOPE.value = False\n            with context.eager_mode():\n                out = func(*args, **kwargs)\n        finally:\n            _SYMBOLIC_SCOPE.value = prev_value\n        return out\n    return eager_fn_wrapper",
                "def get_uid(prefix=''):\n    \"\"\"Provides a unique UID given a string prefix.\n\n    # Arguments\n        prefix: string.\n\n    # Returns\n        An integer.\n\n    # Example\n    ```python\n        >>> keras.backend.get_uid('dense')\n        1\n        >>> keras.backend.get_uid('dense')\n        2\n    ```\n\n    \"\"\"\n    return tf_keras_backend.get_uid(prefix)",
                "def manual_variable_initialization(value):\n    \"\"\"Sets the manual variable initialization flag.\n\n    This boolean flag determines whether\n    variables should be initialized\n    as they are instantiated (default), or if\n    the user should handle the initialization.\n\n    # Arguments\n        value: Python boolean.\n    \"\"\"\n    tf_keras_backend.manual_variable_initialization(value)",
                "def epsilon():\n    \"\"\"Returns the value of the fuzz factor used in numeric expressions.\n\n    # Returns\n        A float.\n\n    # Example\n    ```python\n        >>> keras.backend.epsilon()\n        1e-07\n    ```\n    \"\"\"\n    return tf_keras_backend.epsilon()",
                "def reset_uids():\n    \"\"\"Resets graph identifiers.\"\"\"\n    tf_keras_backend.reset_uids()",
                "def set_epsilon(e):\n    \"\"\"Sets the value of the fuzz factor used in numeric expressions.\n\n    # Arguments\n        e: float. New value of epsilon.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> K.epsilon()\n        1e-07\n        >>> K.set_epsilon(1e-05)\n        >>> K.epsilon()\n        1e-05\n    ```\n    \"\"\"\n    tf_keras_backend.set_epsilon(e)",
                "def floatx():\n    \"\"\"Returns the default float type, as a string.\n    (e.g. 'float16', 'float32', 'float64').\n\n    # Returns\n        String, the current default float type.\n\n    # Example\n    ```python\n        >>> keras.backend.floatx()\n        'float32'\n    ```\n    \"\"\"\n    return tf_keras_backend.floatx()",
                "def set_floatx(floatx):\n    \"\"\"Sets the default float type.\n\n    # Arguments\n        floatx: String, 'float16', 'float32', or 'float64'.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> K.floatx()\n        'float32'\n        >>> K.set_floatx('float16')\n        >>> K.floatx()\n        'float16'\n    ```\n    \"\"\"\n    tf_keras_backend.set_floatx(floatx)",
                "def cast_to_floatx(x):\n    \"\"\"Cast a Numpy array to the default Keras float type.\n\n    # Arguments\n        x: Numpy array.\n\n    # Returns\n        The same Numpy array, cast to its new type.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> K.floatx()\n        'float32'\n        >>> arr = numpy.array([1.0, 2.0], dtype='float64')\n        >>> arr.dtype\n        dtype('float64')\n        >>> new_arr = K.cast_to_floatx(arr)\n        >>> new_arr\n        array([ 1.,  2.], dtype=float32)\n        >>> new_arr.dtype\n        dtype('float32')\n    ```\n    \"\"\"\n    return tf_keras_backend.cast_to_floatx(x)",
                "def image_data_format():\n    \"\"\"Returns the default image data format convention.\n\n    # Returns\n        A string, either `'channels_first'` or `'channels_last'`\n\n    # Example\n    ```python\n        >>> keras.backend.image_data_format()\n        'channels_first'\n    ```\n    \"\"\"\n    return tf_keras_backend.image_data_format()",
                "def set_image_data_format(data_format):\n    \"\"\"Sets the value of the data format convention.\n\n    # Arguments\n        data_format: string. `'channels_first'` or `'channels_last'`.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> K.image_data_format()\n        'channels_first'\n        >>> K.set_image_data_format('channels_last')\n        >>> K.image_data_format()\n        'channels_last'\n    ```\n    \"\"\"\n    tf_keras_backend.set_image_data_format(data_format)",
                "def normalize_data_format(value):\n    \"\"\"Checks that the value correspond to a valid data format.\n\n    # Arguments\n        value: String or None. `'channels_first'` or `'channels_last'`.\n\n    # Returns\n        A string, either `'channels_first'` or `'channels_last'`\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> K.normalize_data_format(None)\n        'channels_first'\n        >>> K.normalize_data_format('channels_last')\n        'channels_last'\n    ```\n\n    # Raises\n        ValueError: if `value` or the global `data_format` invalid.\n    \"\"\"\n    if value is None:\n        value = image_data_format()\n    data_format = value.lower()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('The `data_format` argument must be one of '\n                         '\"channels_first\", \"channels_last\". Received: ' +\n                         str(value))\n    return data_format",
                "@symbolic\ndef learning_phase():\n    \"\"\"Returns the learning phase flag.\n\n    The learning phase flag is a bool tensor (0 = test, 1 = train)\n    to be passed as input to any Keras function\n    that uses a different behavior at train time and test time.\n\n    # Returns\n        Learning phase (scalar integer tensor or Python integer).\n    \"\"\"\n    lp = tf_keras_backend.learning_phase()\n    if _is_tf_1():\n        return lp\n    else:\n        if isinstance(lp, int):\n            return lp\n        if lp in _LEARNING_PHASE_CACHE:\n            return _LEARNING_PHASE_CACHE[lp]\n        with name_scope(''):\n            int_lp = tf.cast(lp, 'int32', name='learning_phase')\n        _LEARNING_PHASE_CACHE[lp] = int_lp\n        return int_lp",
                "@symbolic\ndef set_learning_phase(value):\n    \"\"\"Sets the learning phase to a fixed value.\n\n    # Arguments\n        value: Learning phase value, either 0 or 1 (integers).\n\n    # Raises\n        ValueError: if `value` is neither `0` nor `1`.\n    \"\"\"\n    tf_keras_backend.set_learning_phase(value)",
                "def get_session():\n    \"\"\"Returns the TF session to be used by the backend.\n\n    If a default TensorFlow session is available, we will return it.\n\n    Else, we will return the global Keras session.\n\n    If no global Keras session exists at this point:\n    we will create a new global session.\n\n    Note that you can manually set the global session\n    via `K.set_session(sess)`.\n\n    # Returns\n        A TensorFlow session.\n\n    # Raises\n        RuntimeError: if no session is available\n            (e.g. when using TensorFlow 2.0).\n    \"\"\"\n    if not _is_tf_1():\n        raise RuntimeError(\n            '`get_session` is not available '\n            'when using TensorFlow 2.0.')\n    if tf.executing_eagerly():\n        raise RuntimeError(\n            '`get_session` is not available when '\n            'TensorFlow is executing eagerly.')\n    return tf_keras_backend.get_session()",
                "def set_session(session):\n    \"\"\"Sets the global TensorFlow session.\n\n    # Arguments\n        session: A TF Session.\n\n    # Raises\n        RuntimeError: if no session is available\n            (e.g. when using TensorFlow 2.0).\n    \"\"\"\n    if not _is_tf_1():\n        raise RuntimeError(\n            '`set_session` is not available '\n            'when using TensorFlow 2.0.')\n    if tf.executing_eagerly():\n        raise RuntimeError(\n            '`set_session` is not available when '\n            'TensorFlow is executing eagerly.')\n    tf_keras_backend.set_session(session)",
                "def clear_session():\n    \"\"\"Destroys the current Keras graph and creates a new one.\n\n    Useful to avoid clutter from old models / layers.\n    \"\"\"\n    tf_keras_backend.clear_session()\n    global _LEARNING_PHASE_CACHE\n    _LEARNING_PHASE_CACHE = {}",
                "def v1_variable_initialization():\n    session = get_session()\n    with session.graph.as_default():\n        variables = tf.global_variables()\n        candidate_vars = []\n        for v in variables:\n            if not getattr(v, '_keras_initialized', False):\n                candidate_vars.append(v)\n        if candidate_vars:\n            # This step is expensive, so we only run it on variables\n            # not already marked as initialized.\n            is_initialized = session.run(\n                [tf.is_variable_initialized(v) for v in candidate_vars])\n            uninitialized_vars = []\n            for flag, v in zip(is_initialized, candidate_vars):\n                if not flag:\n                    uninitialized_vars.append(v)\n                v._keras_initialized = True\n            if uninitialized_vars:\n                session.run(tf.variables_initializer(uninitialized_vars))",
                "def _get_current_tf_device():\n    \"\"\"Return explicit device of current context, otherwise returns `None`.\n\n    # Returns\n        If the current device scope is explicitly set, it returns a string with\n        the device (`CPU` or `GPU`). If the scope is not explicitly set, it will\n        return `None`.\n    \"\"\"\n    g = get_graph()\n    op = _TfDeviceCaptureOp()\n    g._apply_device_functions(op)\n    return op.device",
                "def _is_current_explicit_device(device_type):\n    \"\"\"Check if the current device is explicitly set on the device type specified.\n\n    # Arguments\n        device_type: A string containing `GPU` or `CPU` (case-insensitive).\n\n    # Returns\n        A boolean indicating if the current device\n        scope is explicitly set on the device type.\n\n    # Raises\n        ValueError: If the `device_type` string indicates an unsupported device.\n    \"\"\"\n    device_type = device_type.lower()\n    if device_type not in ['cpu', 'gpu']:\n        raise ValueError('`device_type` should be either \"cpu\" or \"gpu\".')\n    device = _get_current_tf_device()\n    return (device is not None and device.device_type.lower() == device_type)",
                "def _get_available_gpus():\n    \"\"\"Get a list of available gpu devices (formatted as strings).\n\n    # Returns\n        A list of available GPU devices.\n    \"\"\"\n    global _LOCAL_DEVICES\n    if _LOCAL_DEVICES is None:\n        if _is_tf_1():\n            devices = get_session().list_devices()\n            _LOCAL_DEVICES = [x.name for x in devices]\n        else:\n            _LOCAL_DEVICES = tf.config.experimental_list_devices()\n    return [x for x in _LOCAL_DEVICES if 'device:gpu' in x.lower()]",
                "def _has_nchw_support():\n    \"\"\"Check whether the current scope supports NCHW ops.\n\n    TensorFlow does not support NCHW on CPU.\n    Therefore we check if we are not explicitly put on\n    CPU, and have GPUs available.\n    In this case there will be soft-placing on the GPU device.\n\n    # Returns\n        bool: if the current scope device placement would support nchw\n    \"\"\"\n    explicitly_on_cpu = _is_current_explicit_device('cpu')\n    gpus_available = len(_get_available_gpus()) > 0\n    return (not explicitly_on_cpu and gpus_available)",
                "@symbolic\ndef _to_tensor(x, dtype):\n    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n\n    # Arguments\n        x: An object to be converted (numpy array, list, tensors).\n        dtype: The destination type.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.convert_to_tensor(x, dtype=dtype)",
                "def is_sparse(tensor):\n    \"\"\"Returns whether a tensor is a sparse tensor.\n\n    # Arguments\n        tensor: A tensor instance.\n\n    # Returns\n        A boolean.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> a = K.placeholder((2, 2), sparse=False)\n        >>> print(K.is_sparse(a))\n        False\n        >>> b = K.placeholder((2, 2), sparse=True)\n        >>> print(K.is_sparse(b))\n        True\n    ```\n    \"\"\"\n    return isinstance(tensor, tf.SparseTensor)",
                "@symbolic\ndef to_dense(tensor):\n    \"\"\"Converts a sparse tensor into a dense tensor and returns it.\n\n    # Arguments\n        tensor: A tensor instance (potentially sparse).\n\n    # Returns\n        A dense tensor.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> b = K.placeholder((2, 2), sparse=True)\n        >>> print(K.is_sparse(b))\n        True\n        >>> c = K.to_dense(b)\n        >>> print(K.is_sparse(c))\n        False\n    ```\n    \"\"\"\n    if is_sparse(tensor):\n        return tf.sparse.to_dense(tensor)\n    else:\n        return tensor",
                "def variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val, dtype='float64', name='example_var')\n        >>> K.dtype(kvar)\n        'float64'\n        >>> print(kvar)\n        example_var\n        >>> K.eval(kvar)\n        array([[ 1.,  2.],\n               [ 3.,  4.]])\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    v = tf_keras_backend.variable(\n        value, dtype=dtype, name=name, constraint=constraint)\n    if hasattr(value, 'tocoo'):\n        v._keras_shape = value.tocoo().shape\n    elif isinstance(value, np.ndarray):\n        v._keras_shape = value.shape\n    elif hasattr(value, 'shape'):\n        v._keras_shape = int_shape(value)\n    v._uses_learning_phase = False\n    return v",
                "def constant(value, dtype=None, shape=None, name=None):\n    \"\"\"Creates a constant tensor.\n\n    # Arguments\n        value: A constant value (or list)\n        dtype: The type of the elements of the resulting tensor.\n        shape: Optional dimensions of resulting tensor.\n        name: Optional name for the tensor.\n\n    # Returns\n        A Constant Tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    with tf_ops.init_scope():\n        return tf.constant(value, dtype=dtype, shape=shape, name=name)",
                "def is_keras_tensor(x):\n    \"\"\"Returns whether `x` is a Keras tensor.\n\n    A \"Keras tensor\" is a tensor that was returned by a Keras layer,\n    (`Layer` class) or by `Input`.\n\n    # Arguments\n        x: A candidate tensor.\n\n    # Returns\n        A boolean: Whether the argument is a Keras tensor.\n\n    # Raises\n        ValueError: In case `x` is not a symbolic tensor.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> from keras.layers import Input, Dense\n        >>> np_var = numpy.array([1, 2])\n        >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n        ValueError\n        >>> k_var = tf.placeholder('float32', shape=(1,1))\n        >>> # A variable indirectly created outside of keras is not a Keras tensor.\n        >>> K.is_keras_tensor(k_var)\n        False\n        >>> keras_var = K.variable(np_var)\n        >>> # A variable created with the keras backend is not a Keras tensor.\n        >>> K.is_keras_tensor(keras_var)\n        False\n        >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n        >>> # A placeholder is not a Keras tensor.\n        >>> K.is_keras_tensor(keras_placeholder)\n        False\n        >>> keras_input = Input([10])\n        >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\n        True\n        >>> keras_layer_output = Dense(10)(keras_input)\n        >>> # Any Keras layer output is a Keras tensor.\n        >>> K.is_keras_tensor(keras_layer_output)\n        True\n    ```\n    \"\"\"\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')",
                "def is_tensor(x):\n    return isinstance(x, tf_ops._TensorLike) or tf_ops.is_dense_tensor_like(x)",
                "@symbolic\ndef placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n    \"\"\"Instantiates a placeholder tensor and returns it.\n\n    # Arguments\n        shape: Shape of the placeholder\n            (integer tuple, may include `None` entries).\n        ndim: Number of axes of the tensor.\n            At least one of {`shape`, `ndim`} must be specified.\n            If both are specified, `shape` is used.\n        dtype: Placeholder type.\n        sparse: Boolean, whether the placeholder should have a sparse type.\n        name: Optional name string for the placeholder.\n\n    # Returns\n        Tensor instance (with Keras metadata included).\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> input_ph = K.placeholder(shape=(2, 4, 5))\n        >>> input_ph._keras_shape\n        (2, 4, 5)\n        >>> input_ph\n        <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    x = tf_keras_backend.placeholder(\n        shape=shape, ndim=ndim, dtype=dtype, sparse=sparse, name=name)\n    if shape is None:\n        if ndim is not None:\n            shape = tuple(None for _ in range(ndim))\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    return x",
                "@symbolic\ndef is_placeholder(x):\n    \"\"\"Returns whether `x` is a placeholder.\n\n    # Arguments\n        x: A candidate placeholder.\n\n    # Returns\n        Boolean.\n    \"\"\"\n    try:\n        return x.op.type == 'Placeholder'\n    except AttributeError:\n        return False",
                "def shape(x):\n    \"\"\"Returns the symbolic shape of a tensor or variable.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A symbolic shape (which is itself a tensor).\n\n    # Examples\n    ```python\n        # TensorFlow example\n        >>> from keras import backend as K\n        >>> tf_session = K.get_session()\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val)\n        >>> inputs = keras.backend.placeholder(shape=(2, 4, 5))\n        >>> K.shape(kvar)\n        <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32>\n        >>> K.shape(inputs)\n        <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32>\n        # To get integer shape (Instead, you can use K.int_shape(x))\n        >>> K.shape(kvar).eval(session=tf_session)\n        array([2, 2], dtype=int32)\n        >>> K.shape(inputs).eval(session=tf_session)\n        array([2, 4, 5], dtype=int32)\n    ```\n    \"\"\"\n    return tf.shape(x)",
                "def int_shape(x):\n    \"\"\"Returns the shape of tensor or variable as a tuple of int or None entries.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tuple of integers (or None entries).\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> inputs = K.placeholder(shape=(2, 4, 5))\n        >>> K.int_shape(inputs)\n        (2, 4, 5)\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val)\n        >>> K.int_shape(kvar)\n        (2, 2)\n    ```\n\n    {{np_implementation}}\n    \"\"\"\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n    try:\n        if isinstance(x.shape, tuple):\n            return x.shape\n        return tuple(x.shape.as_list())\n    except ValueError:\n        return None",
                "def ndim(x):\n    \"\"\"Returns the number of axes in a tensor, as an integer.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        Integer (scalar), number of axes.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> inputs = K.placeholder(shape=(2, 4, 5))\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val)\n        >>> K.ndim(inputs)\n        3\n        >>> K.ndim(kvar)\n        2\n    ```\n\n    {{np_implementation}}\n    \"\"\"\n    return x.shape.rank",
                "def dtype(x):\n    \"\"\"Returns the dtype of a Keras tensor or variable, as a string.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        String, dtype of `x`.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> K.dtype(K.placeholder(shape=(2,4,5)))\n        'float32'\n        >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))\n        'float32'\n        >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))\n        'float64'\n        # Keras variable\n        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]))\n        >>> K.dtype(kvar)\n        'float32_ref'\n        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n        >>> K.dtype(kvar)\n        'float32_ref'\n    ```\n    {{np_implementation}}\n    \"\"\"\n    return x.dtype.base_dtype.name",
                "def eval(x):\n    \"\"\"Evaluates the value of a tensor.\n\n    # Arguments\n        x: A tensor.\n\n    # Returns\n        A Numpy array.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n        >>> K.eval(kvar)\n        array([[ 1.,  2.],\n               [ 3.,  4.]], dtype=float32)\n    ```\n    {{np_implementation}}\n    \"\"\"\n    if _is_tf_1():\n        return to_dense(x).eval(session=get_session())\n    if hasattr(x, 'numpy'):\n        with context.eager_mode():\n            return x.numpy()\n    eval_fn = function([], [x])\n    return eval_fn([])[0]",
                "def zeros(shape, dtype=None, name=None):\n    \"\"\"Instantiates an all-zeros variable and returns it.\n\n    # Arguments\n        shape: Tuple of integers, shape of returned Keras variable\n        dtype: String, data type of returned Keras variable\n        name: String, name of returned Keras variable\n\n    # Returns\n        A variable (including Keras metadata), filled with `0.0`.\n        Note that if `shape` was symbolic, we cannot return a variable,\n        and will return a dynamically-shaped tensor instead.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.zeros((3,4))\n        >>> K.eval(kvar)\n        array([[ 0.,  0.,  0.,  0.],\n               [ 0.,  0.,  0.,  0.],\n               [ 0.,  0.,  0.,  0.]], dtype=float32)\n    ```\n    {{np_implementation}}\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    with tf_ops.init_scope():\n        v = tf.zeros(shape=shape, dtype=dtype, name=name)\n        if py_all(v.shape.as_list()):\n            return variable(v, dtype=dtype, name=name)\n        return v",
                "def ones(shape, dtype=None, name=None):\n    \"\"\"Instantiates an all-ones variable and returns it.\n\n    # Arguments\n        shape: Tuple of integers, shape of returned Keras variable.\n        dtype: String, data type of returned Keras variable.\n        name: String, name of returned Keras variable.\n\n    # Returns\n        A Keras variable, filled with `1.0`.\n        Note that if `shape` was symbolic, we cannot return a variable,\n        and will return a dynamically-shaped tensor instead.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.ones((3,4))\n        >>> K.eval(kvar)\n        array([[ 1.,  1.,  1.,  1.],\n               [ 1.,  1.,  1.,  1.],\n               [ 1.,  1.,  1.,  1.]], dtype=float32)\n    ```\n    {{np_implementation}}\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    with tf_ops.init_scope():\n        v = tf.ones(shape=shape, dtype=dtype, name=name)\n        if py_all(v.shape.as_list()):\n            return variable(v, dtype=dtype, name=name)\n        return v",
                "def eye(size, dtype=None, name=None):\n    \"\"\"Instantiate an identity matrix and returns it.\n\n    # Arguments\n        size: Integer, number of rows/columns.\n        dtype: String, data type of returned Keras variable.\n        name: String, name of returned Keras variable.\n\n    # Returns\n        A Keras variable, an identity matrix.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.eye(3)\n        >>> K.eval(kvar)\n        array([[ 1.,  0.,  0.],\n               [ 0.,  1.,  0.],\n               [ 0.,  0.,  1.]], dtype=float32)\n    ```\n    {{np_implementation}}\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    with tf_ops.init_scope():\n        return variable(tf.eye(size, dtype=dtype), dtype, name)",
                "@symbolic\ndef zeros_like(x, dtype=None, name=None):\n    \"\"\"Instantiates an all-zeros variable of the same shape as another tensor.\n\n    # Arguments\n        x: Keras variable or Keras tensor.\n        dtype: String, dtype of returned Keras variable.\n             None uses the dtype of x.\n        name: String, name for the variable to create.\n\n    # Returns\n        A Keras variable with the shape of x filled with zeros.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.variable(np.random.random((2,3)))\n        >>> kvar_zeros = K.zeros_like(kvar)\n        >>> K.eval(kvar_zeros)\n        array([[ 0.,  0.,  0.],\n               [ 0.,  0.,  0.]], dtype=float32)\n    ```\n    {{np_implementation}}\n    \"\"\"\n    return tf.zeros_like(x, dtype=dtype, name=name)",
                "@symbolic\ndef ones_like(x, dtype=None, name=None):\n    \"\"\"Instantiates an all-ones variable of the same shape as another tensor.\n\n    # Arguments\n        x: Keras variable or tensor.\n        dtype: String, dtype of returned Keras variable.\n             None uses the dtype of x.\n        name: String, name for the variable to create.\n\n    # Returns\n        A Keras variable with the shape of x filled with ones.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.variable(np.random.random((2,3)))\n        >>> kvar_ones = K.ones_like(kvar)\n        >>> K.eval(kvar_ones)\n        array([[ 1.,  1.,  1.],\n               [ 1.,  1.,  1.]], dtype=float32)\n    ```\n    {{np_implementation}}\n    \"\"\"\n    return tf.ones_like(x, dtype=dtype, name=name)",
                "@symbolic\ndef identity(x, name=None):\n    \"\"\"Returns a tensor with the same content as the input tensor.\n\n    # Arguments\n        x: The input tensor.\n        name: String, name for the variable to create.\n\n    # Returns\n        A tensor of the same shape, type and content.\n    \"\"\"\n    return tf.identity(x, name)",
                "def random_uniform_variable(shape, low, high,\n                            dtype=None,\n                            name=None,\n                            seed=None):\n    \"\"\"Instantiates a variable with values drawn from a uniform distribution.\n\n    # Arguments\n        shape: Tuple of integers, shape of returned Keras variable.\n        low: Float, lower boundary of the output interval.\n        high: Float, upper boundary of the output interval.\n        dtype: String, dtype of returned Keras variable.\n        name: String, name of returned Keras variable.\n        seed: Integer, random seed.\n\n    # Returns\n        A Keras variable, filled with drawn samples.\n\n    # Example\n    ```python\n        # TensorFlow example\n        >>> kvar = K.random_uniform_variable((2,3), 0, 1)\n        >>> kvar\n        <tensorflow.python.ops.variables.Variable object at 0x10ab40b10>\n        >>> K.eval(kvar)\n        array([[ 0.10940075,  0.10047495,  0.476143  ],\n               [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)\n    ```\n    {{np_implementation}}\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e8)\n    with tf_ops.init_scope():\n        value = tf.random_uniform_initializer(\n            low, high, seed=seed)(shape, dtype=dtype)\n        return variable(value, dtype=dtype, name=name)",
                "def random_normal_variable(shape, mean, scale, dtype=None,\n                           name=None, seed=None):\n    \"\"\"Instantiates a variable with values drawn from a normal distribution.\n\n    # Arguments\n        shape: Tuple of integers, shape of returned Keras variable.\n        mean: Float, mean of the normal distribution.\n        scale: Float, standard deviation of the normal distribution.\n        dtype: String, dtype of returned Keras variable.\n        name: String, name of returned Keras variable.\n        seed: Integer, random seed.\n\n    # Returns\n        A Keras variable, filled with drawn samples.\n\n    # Example\n    ```python\n        # TensorFlow example\n        >>> kvar = K.random_normal_variable((2,3), 0, 1)\n        >>> kvar\n        <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0>\n        >>> K.eval(kvar)\n        array([[ 1.19591331,  0.68685907, -0.63814116],\n               [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)\n    ```\n    {{np_implementation}}\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e8)\n    with tf_ops.init_scope():\n        value = tf.random_normal_initializer(\n            mean, scale, seed=seed)(shape, dtype=dtype)\n        return variable(value, dtype=dtype, name=name)",
                "def count_params(x):\n    \"\"\"Returns the static number of elements in a Keras variable or tensor.\n\n    # Arguments\n        x: Keras variable or tensor.\n\n    # Returns\n        Integer, the number of elements in `x`, i.e., the product of the\n        array's static dimensions.\n\n    # Example\n    ```python\n        >>> kvar = K.zeros((2,3))\n        >>> K.count_params(kvar)\n        6\n        >>> K.eval(kvar)\n        array([[ 0.,  0.,  0.],\n               [ 0.,  0.,  0.]], dtype=float32)\n    ```\n    {{np_implementation}}\n    \"\"\"\n    return np.prod(int_shape(x))",
                "@symbolic\ndef cast(x, dtype):\n    \"\"\"Casts a tensor to a different dtype and returns it.\n\n    You can cast a Keras variable but it still returns a Keras tensor.\n\n    # Arguments\n        x: Keras tensor (or variable).\n        dtype: String, either (`'float16'`, `'float32'`, or `'float64'`).\n\n    # Returns\n        Keras tensor with dtype `dtype`.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> input = K.placeholder((2, 3), dtype='float32')\n        >>> input\n        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n        # It doesn't work in-place as below.\n        >>> K.cast(input, dtype='float16')\n        <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16>\n        >>> input\n        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n        # you need to assign it.\n        >>> input = K.cast(input, dtype='float16')\n        >>> input\n        <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>\n    ```\n    \"\"\"\n    return tf.cast(x, dtype)",
                "@symbolic\ndef update(x, new_x):\n    \"\"\"Update the value of `x` to `new_x`.\n\n    # Arguments\n        x: A `Variable`.\n        new_x: A tensor of same shape as `x`.\n\n    # Returns\n        The variable `x` updated.\n    \"\"\"\n    return tf_state_ops.assign(x, new_x)",
                "@symbolic\ndef update_add(x, increment):\n    \"\"\"Update the value of `x` by adding `increment`.\n\n    # Arguments\n        x: A `Variable`.\n        increment: A tensor of same shape as `x`.\n\n    # Returns\n        The variable `x` updated.\n    \"\"\"\n    return tf_state_ops.assign_add(x, increment)",
                "@symbolic\ndef update_sub(x, decrement):\n    \"\"\"Update the value of `x` by subtracting `decrement`.\n\n    # Arguments\n        x: A `Variable`.\n        decrement: A tensor of same shape as `x`.\n\n    # Returns\n        The variable `x` updated.\n    \"\"\"\n    return tf_state_ops.assign_sub(x, decrement)",
                "@symbolic\ndef moving_average_update(x, value, momentum):\n    \"\"\"Compute the moving average of a variable.\n\n    # Arguments\n        x: A `Variable`.\n        value: A tensor with the same shape as `x`.\n        momentum: The moving average momentum.\n\n    # Returns\n        An operation to update the variable.\n    \"\"\"\n    with tf_ops.colocate_with(x):\n        decay = tf_ops.convert_to_tensor(1.0 - momentum)\n        if decay.dtype != x.dtype.base_dtype:\n            decay = tf_math_ops.cast(decay, x.dtype.base_dtype)\n        update_delta = (x - tf_math_ops.cast(value, x.dtype)) * decay\n        return tf_state_ops.assign_sub(x, update_delta)",
                "def dot(x, y):\n    \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n\n    When attempting to multiply a nD tensor\n    with a nD tensor, it reproduces the Theano behavior.\n    (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A tensor, dot product of `x` and `y`.\n\n    # Examples\n    ```python\n        # dot product between tensors\n        >>> x = K.placeholder(shape=(2, 3))\n        >>> y = K.placeholder(shape=(3, 4))\n        >>> xy = K.dot(x, y)\n        >>> xy\n        <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>\n    ```\n\n    ```python\n        # dot product between tensors\n        >>> x = K.placeholder(shape=(32, 28, 3))\n        >>> y = K.placeholder(shape=(3, 4))\n        >>> xy = K.dot(x, y)\n        >>> xy\n        <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32>\n    ```\n\n    ```python\n        # Theano-like behavior example\n        >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)\n        >>> y = K.ones((4, 3, 5))\n        >>> xy = K.dot(x, y)\n        >>> K.int_shape(xy)\n        (2, 4, 5)\n    ```\n    {{np_implementation}}\n    \"\"\"\n    if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\n        x_shape = []\n        for i, s in zip(int_shape(x), tf.unstack(tf.shape(x))):\n            if i is not None:\n                x_shape.append(i)\n            else:\n                x_shape.append(s)\n        x_shape = tuple(x_shape)\n        y_shape = []\n        for i, s in zip(int_shape(y), tf.unstack(tf.shape(y))):\n            if i is not None:\n                y_shape.append(i)\n            else:\n                y_shape.append(s)\n        y_shape = tuple(y_shape)\n        y_permute_dim = list(range(ndim(y)))\n        y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim\n        xt = tf.reshape(x, [-1, x_shape[-1]])\n        yt = tf.reshape(tf.transpose(y, perm=y_permute_dim), [y_shape[-2], -1])\n        return tf.reshape(tf.matmul(xt, yt),\n                          x_shape[:-1] + y_shape[:-2] + y_shape[-1:])\n    if is_sparse(x):\n        out = tf.sparse.sparse_dense_matmul(x, y)\n    else:\n        out = tf.matmul(x, y)\n    return out",
                "def batch_dot(x, y, axes=None):\n    \"\"\"Batchwise dot product.\n\n    `batch_dot` is used to compute dot product of `x` and `y` when\n    `x` and `y` are data in batches, i.e. in a shape of\n    `(batch_size, :)`.\n    `batch_dot` results in a tensor or variable with less dimensions\n    than the input. If the number of dimensions is reduced to 1,\n    we use `expand_dims` to make sure that ndim is at least 2.\n\n    # Arguments\n        x: Keras tensor or variable with `ndim >= 2`.\n        y: Keras tensor or variable with `ndim >= 2`.\n        axes: int or tuple(int, int). Target dimensions to be reduced.\n\n    # Returns\n        A tensor with shape equal to the concatenation of `x`'s shape\n        (less the dimension that was summed over) and `y`'s shape\n        (less the batch dimension and the dimension that was summed over).\n        If the final rank is 1, we reshape it to `(batch_size, 1)`.\n\n    # Examples\n        Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n        `batch_dot(x, y, axes=1) = [[17], [53]]` which is the main diagonal\n        of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n        elements.\n\n        Pseudocode:\n        ```\n        inner_products = []\n        for xi, yi in zip(x, y):\n            inner_products.append(xi.dot(yi))\n        result = stack(inner_products)\n        ```\n\n        Shape inference:\n        Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n        If `axes` is (1, 2), to find the output shape of resultant tensor,\n            loop through each dimension in `x`'s shape and `y`'s shape:\n\n        * `x.shape[0]` : 100 : append to output shape\n        * `x.shape[1]` : 20 : do not append to output shape,\n            dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n        * `y.shape[0]` : 100 : do not append to output shape,\n            always ignore first dimension of `y`\n        * `y.shape[1]` : 30 : append to output shape\n        * `y.shape[2]` : 20 : do not append to output shape,\n            dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n        `output_shape` = `(100, 30)`\n\n    ```python\n        >>> x_batch = K.ones(shape=(32, 20, 1))\n        >>> y_batch = K.ones(shape=(32, 30, 20))\n        >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 2))\n        >>> K.int_shape(xy_batch_dot)\n        (32, 1, 30)\n    ```\n\n    {{np_implementation}}\n    \"\"\"\n    x_shape = int_shape(x)\n    y_shape = int_shape(y)\n\n    x_ndim = len(x_shape)\n    y_ndim = len(y_shape)\n\n    if x_ndim < 2 or y_ndim < 2:\n        raise ValueError('Can not do batch_dot on inputs '\n                         'with rank < 2. '\n                         'Received inputs with shapes ' +\n                         str(x_shape) + ' and ' +\n                         str(y_shape) + '.')\n\n    x_batch_size = x_shape[0]\n    y_batch_size = y_shape[0]\n\n    if x_batch_size is not None and y_batch_size is not None:\n        if x_batch_size != y_batch_size:\n            raise ValueError('Can not do batch_dot on inputs '\n                             'with different batch sizes. '\n                             'Received inputs with shapes ' +\n                             str(x_shape) + ' and ' +\n                             str(y_shape) + '.')\n\n    if isinstance(axes, int):\n        axes = [axes, axes]\n\n    if axes is None:\n        if y_ndim == 2:\n            axes = [x_ndim - 1, y_ndim - 1]\n        else:\n            axes = [x_ndim - 1, y_ndim - 2]\n\n    if py_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n\n    # if tuple, convert to list.\n    axes = list(axes)\n\n    # convert negative indices.\n    if axes[0] < 0:\n        axes[0] += x_ndim\n    if axes[1] < 0:\n        axes[1] += y_ndim\n\n    # sanity checks\n    if 0 in axes:\n        raise ValueError('Can not perform batch_dot over axis 0.'\n                         'If your inputs are not batched,'\n                         ' add a dummy batch dimension to your '\n                         'inputs using K.expand_dims(x, 0)')\n\n    a0, a1 = axes\n    d1 = x_shape[a0]\n    d2 = y_shape[a1]\n\n    if d1 is not None and d2 is not None and d1 != d2:\n        raise ValueError('Can not do batch_dot on inputs with shapes ' +\n                         str(x_shape) + ' and ' + str(y_shape) +\n                         ' with axes=' + str(axes) + '. x.shape[%d] != '\n                         'y.shape[%d] (%d != %d).' % (axes[0], axes[1], d1, d2))\n\n    # backup ndims. Need them later.\n    orig_x_ndim = x_ndim\n    orig_y_ndim = y_ndim\n\n    # if rank is 2, expand to 3.\n    if x_ndim == 2:\n        x = tf.expand_dims(x, 1)\n        a0 += 1\n        x_ndim += 1\n    if y_ndim == 2:\n        y = tf.expand_dims(y, 2)\n        y_ndim += 1\n\n    # bring x's dimension to be reduced to last axis.\n    if a0 != x_ndim - 1:\n        pattern = list(range(x_ndim))\n        for i in range(a0, x_ndim - 1):\n            pattern[i] = pattern[i + 1]\n        pattern[-1] = a0\n        x = tf.transpose(x, pattern)\n\n    # bring y's dimension to be reduced to axis 1.\n    if a1 != 1:\n        pattern = list(range(y_ndim))\n        for i in range(a1, 1, -1):\n            pattern[i] = pattern[i - 1]\n        pattern[1] = a1\n        y = tf.transpose(y, pattern)\n\n    # normalize both inputs to rank 3.\n    if x_ndim > 3:\n        # squash middle dimensions of x.\n        x_shape = shape(x)\n        x_mid_dims = x_shape[1:-1]\n        x_squashed_dim = tf.reduce_prod(x_mid_dims)\n        x_squashed_shape = tf.stack([x_shape[0], x_squashed_dim, x_shape[-1]])\n        x = tf.reshape(x, x_squashed_shape)\n        x_squashed = True\n    else:\n        x_squashed = False\n\n    if y_ndim > 3:\n        # squash trailing dimensions of y.\n        y_shape = shape(y)\n        y_trail_dims = y_shape[2:]\n        y_squashed_dim = tf.reduce_prod(y_trail_dims)\n        y_squashed_shape = tf.stack([y_shape[0], y_shape[1], y_squashed_dim])\n        y = tf.reshape(y, y_squashed_shape)\n        y_squashed = True\n    else:\n        y_squashed = False\n\n    result = tf.matmul(x, y)\n\n    # if inputs were squashed, we have to reshape the matmul output.\n    output_shape = tf.shape(result)\n    do_reshape = False\n\n    if x_squashed:\n        output_shape = tf.concat([output_shape[:1],\n                                  x_mid_dims,\n                                  output_shape[-1:]], 0)\n        do_reshape = True\n\n    if y_squashed:\n        output_shape = tf.concat([output_shape[:-1], y_trail_dims], 0)\n        do_reshape = True\n\n    if do_reshape:\n        result = tf.reshape(result, output_shape)\n\n    # if the inputs were originally rank 2, we remove the added 1 dim.\n    if orig_x_ndim == 2:\n        result = tf.squeeze(result, 1)\n    elif orig_y_ndim == 2:\n        result = tf.squeeze(result, -1)\n\n    return result",
                "def transpose(x):\n    \"\"\"Transposes a tensor and returns it.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n\n    # Examples\n    ```python\n        >>> var = K.variable([[1, 2, 3], [4, 5, 6]])\n        >>> K.eval(var)\n        array([[ 1.,  2.,  3.],\n               [ 4.,  5.,  6.]], dtype=float32)\n        >>> var_transposed = K.transpose(var)\n        >>> K.eval(var_transposed)\n        array([[ 1.,  4.],\n               [ 2.,  5.],\n               [ 3.,  6.]], dtype=float32)\n    ```\n\n    ```python\n        >>> inputs = K.placeholder((2, 3))\n        >>> inputs\n        <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32>\n        >>> input_transposed = K.transpose(inputs)\n        >>> input_transposed\n        <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32>\n\n    ```\n    {{np_implementation}}\n    \"\"\"\n    return tf.transpose(x)",
                "def gather(reference, indices):\n    \"\"\"Retrieves the elements of indices `indices` in the tensor `reference`.\n\n    # Arguments\n        reference: A tensor.\n        indices: An integer tensor of indices.\n\n    # Returns\n        A tensor of same type as `reference`.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.nn.embedding_lookup(reference, indices)",
                "def max(x, axis=None, keepdims=False):\n    \"\"\"Maximum value in a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to find maximum values. If `None` (default), finds the\n            maximum over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with maximum values of `x`.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.reduce_max(x, axis, keepdims)",
                "def min(x, axis=None, keepdims=False):\n    \"\"\"Minimum value in a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to find minimum values. If `None` (default), finds the\n            minimum over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with miminum values of `x`.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.reduce_min(x, axis, keepdims)",
                "def sum(x, axis=None, keepdims=False):\n    \"\"\"Sum of the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to sum over. If `None` (default), sums over all\n            dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with sum of `x`.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.reduce_sum(x, axis, keepdims)",
                "def prod(x, axis=None, keepdims=False):\n    \"\"\"Multiplies the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the product. If `None` (default), computes\n            the product over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with the product of elements of `x`.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.reduce_prod(x, axis, keepdims)",
                "def cumsum(x, axis=0):\n    \"\"\"Cumulative sum of the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer, the axis to compute the sum.\n\n    # Returns\n        A tensor of the cumulative sum of values of `x` along `axis`.\n    {{np_implementation}}\n    \"\"\"\n    return tf_math_ops.cumsum(x, axis=axis)",
                "def cumprod(x, axis=0):\n    \"\"\"Cumulative product of the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer, the axis to compute the product.\n\n    # Returns\n        A tensor of the cumulative product of values of `x` along `axis`.\n    {{np_implementation}}\n    \"\"\"\n    return tf_math_ops.cumprod(x, axis=axis)",
                "def var(x, axis=None, keepdims=False):\n    \"\"\"Variance of a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the variance. If `None` (default), computes\n            the variance over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with the variance of elements of `x`.\n    {{np_implementation}}\n    \"\"\"\n    if x.dtype.base_dtype == tf.bool:\n        x = tf.cast(x, floatx())\n    m = tf.reduce_mean(x, axis, True)\n    devs_squared = tf.square(x - m)\n    return tf.reduce_mean(devs_squared,\n                          axis,\n                          keepdims)",
                "def std(x, axis=None, keepdims=False):\n    \"\"\"Standard deviation of a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the standard deviation. If `None` (default),\n            computes the standard deviation over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with the standard deviation of elements of `x`.\n    {{np_implementation}}\n    \"\"\"\n    return tf.sqrt(var(x, axis=axis, keepdims=keepdims))",
                "def mean(x, axis=None, keepdims=False):\n    \"\"\"Mean of a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the mean. If `None` (default), computes\n            the mean over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1 for each entry in `axis`. If `keepdims` is `True`,\n            the reduced dimensions are retained with length 1.\n\n    # Returns\n        A tensor with the mean of elements of `x`.\n    {{np_implementation}}\n    \"\"\"\n    if x.dtype.base_dtype == tf.bool:\n        x = tf.cast(x, floatx())\n    return tf.reduce_mean(x, axis, keepdims)",
                "def any(x, axis=None, keepdims=False):\n    \"\"\"Bitwise reduction (logical OR).\n\n    # Arguments\n        x: Tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the logical or. If `None` (default), computes\n            the logical or over all dimensions.\n        keepdims: whether the drop or broadcast the reduction axes.\n\n    # Returns\n        A uint8 tensor (0s and 1s).\n    {{np_implementation}}\n    \"\"\"\n    x = tf.cast(x, tf.bool)\n    return tf.reduce_any(x, axis, keepdims)",
                "def all(x, axis=None, keepdims=False):\n    \"\"\"Bitwise reduction (logical AND).\n\n    # Arguments\n        x: Tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the logical and. If `None` (default), computes\n            the logical and over all dimensions.\n        keepdims: whether the drop or broadcast the reduction axes.\n\n    # Returns\n        A uint8 tensor (0s and 1s).\n    {{np_implementation}}\n    \"\"\"\n    x = tf.cast(x, tf.bool)\n    return tf.reduce_all(x, axis, keepdims)",
                "def argmax(x, axis=-1):\n    \"\"\"Returns the index of the maximum value along an axis.\n\n    # Arguments\n        x: Tensor or variable.\n        axis: axis along which to perform the reduction.\n\n    # Returns\n        A tensor.\n    {{np_implementation}}\n    \"\"\"\n    return tf.argmax(x, axis)",
                "def argmin(x, axis=-1):\n    \"\"\"Returns the index of the minimum value along an axis.\n\n    # Arguments\n        x: Tensor or variable.\n        axis: axis along which to perform the reduction.\n\n    # Returns\n        A tensor.\n    {{np_implementation}}\n    \"\"\"\n    return tf.argmin(x, axis)",
                "def square(x):\n    \"\"\"Element-wise square.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.square(x)",
                "def abs(x):\n    \"\"\"Element-wise absolute value.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.abs(x)",
                "def sqrt(x):\n    \"\"\"Element-wise square root.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    {{np_implementation}}\n    \"\"\"\n    zero = _to_tensor(0., x.dtype.base_dtype)\n    inf = _to_tensor(np.inf, x.dtype.base_dtype)\n    x = tf.clip_by_value(x, zero, inf)\n    return tf.sqrt(x)",
                "def exp(x):\n    \"\"\"Element-wise exponential.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.exp(x)",
                "def log(x):\n    \"\"\"Element-wise log.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf_math_ops.log(x)",
                "def logsumexp(x, axis=None, keepdims=False):\n    \"\"\"Computes log(sum(exp(elements across dimensions of a tensor))).\n\n    This function is more numerically stable than log(sum(exp(x))).\n    It avoids overflows caused by taking the exp of large inputs and\n    underflows caused by taking the log of small inputs.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the logsumexp. If `None` (default), computes\n            the logsumexp over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`, the reduced dimension is\n            retained with length 1.\n\n    # Returns\n        The reduced tensor.\n    {{np_implementation}}\n    \"\"\"\n    return tf.reduce_logsumexp(x, axis, keepdims)",
                "def round(x):\n    \"\"\"Element-wise rounding to the closest integer.\n\n    In case of tie, the rounding mode used is \"half to even\".\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.round(x)",
                "def sign(x):\n    \"\"\"Element-wise sign.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.sign(x)",
                "def pow(x, a):\n    \"\"\"Element-wise exponentiation.\n\n    # Arguments\n        x: Tensor or variable.\n        a: Python integer.\n\n    # Returns\n        A tensor.\n    {{np_implementation}}\n    \"\"\"\n    return tf.pow(x, a)",
                "def clip(x, min_value, max_value):\n    \"\"\"Element-wise value clipping.\n\n    # Arguments\n        x: Tensor or variable.\n        min_value: Python float, integer or tensor.\n        max_value: Python float, integer or tensor.\n\n    # Returns\n        A tensor.\n    {{np_implementation}}\n    \"\"\"\n    if (isinstance(min_value, (int, float)) and\n            isinstance(max_value, (int, float))):\n        if max_value < min_value:\n            max_value = min_value\n    if min_value is None:\n        min_value = -np.inf\n    if max_value is None:\n        max_value = np.inf\n    return tf.clip_by_value(x, min_value, max_value)",
                "def equal(x, y):\n    \"\"\"Element-wise equality between two tensors.\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.equal(x, y)",
                "def not_equal(x, y):\n    \"\"\"Element-wise inequality between two tensors.\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.not_equal(x, y)",
                "def greater(x, y):\n    \"\"\"Element-wise truth value of (x > y).\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.greater(x, y)",
                "def greater_equal(x, y):\n    \"\"\"Element-wise truth value of (x >= y).\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.greater_equal(x, y)",
                "def less(x, y):\n    \"\"\"Element-wise truth value of (x < y).\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.less(x, y)",
                "def less_equal(x, y):\n    \"\"\"Element-wise truth value of (x <= y).\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.less_equal(x, y)",
                "def maximum(x, y):\n    \"\"\"Element-wise maximum of two tensors.\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.maximum(x, y)",
                "def minimum(x, y):\n    \"\"\"Element-wise minimum of two tensors.\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.minimum(x, y)",
                "def sin(x):\n    \"\"\"Computes sin of x element-wise.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.sin(x)",
                "def cos(x):\n    \"\"\"Computes cos of x element-wise.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.cos(x)",
                "def _regular_normalize_batch_in_training(x, gamma, beta,\n                                         reduction_axes, epsilon=1e-3):\n    \"\"\"Non-fused version of `normalize_batch_in_training`.\n\n    # Arguments\n        x: Input tensor or variable.\n        gamma: Tensor by which to scale the input.\n        beta: Tensor with which to center the input.\n        reduction_axes: iterable of integers,\n            axes over which to normalize.\n        epsilon: Fuzz factor.\n\n    # Returns\n        A tuple length of 3, `(normalized_tensor, mean, variance)`.\n    \"\"\"\n    mean, var = tf.nn.moments(x, reduction_axes,\n                              None, None, False)\n    normed = tf.nn.batch_normalization(x, mean, var,\n                                       beta, gamma,\n                                       epsilon)\n    return normed, mean, var",
                "def _broadcast_normalize_batch_in_training(x, gamma, beta,\n                                           reduction_axes, epsilon=1e-3):\n    \"\"\"Non-fused, broadcast version of `normalize_batch_in_training`.\n\n    # Arguments\n        x: Input tensor or variable.\n        gamma: Tensor by which to scale the input.\n        beta: Tensor with which to center the input.\n        reduction_axes: iterable of integers,\n            axes over which to normalize.\n        epsilon: Fuzz factor.\n\n    # Returns\n        A tuple length of 3, `(normalized_tensor, mean, variance)`.\n    \"\"\"\n    mean, var = tf.nn.moments(x, reduction_axes,\n                              None, None, False)\n    target_shape = []\n    for axis in range(ndim(x)):\n        if axis in reduction_axes:\n            target_shape.append(1)\n        else:\n            target_shape.append(tf.shape(x)[axis])\n    target_shape = tf.stack(target_shape)\n\n    broadcast_mean = tf.reshape(mean, target_shape)\n    broadcast_var = tf.reshape(var, target_shape)\n    if gamma is None:\n        broadcast_gamma = None\n    else:\n        broadcast_gamma = tf.reshape(gamma, target_shape)\n    if beta is None:\n        broadcast_beta = None\n    else:\n        broadcast_beta = tf.reshape(beta, target_shape)\n\n    normed = tf.nn.batch_normalization(\n        x,\n        broadcast_mean,\n        broadcast_var,\n        broadcast_beta,\n        broadcast_gamma,\n        epsilon)\n    return normed, mean, var",
                "def _fused_normalize_batch_in_training(x, gamma, beta, reduction_axes,\n                                       epsilon=1e-3):\n    \"\"\"Fused version of `normalize_batch_in_training`.\n\n    # Arguments\n        x: Input tensor or variable.\n        gamma: Tensor by which to scale the input.\n        beta: Tensor with which to center the input.\n        reduction_axes: iterable of integers,\n            axes over which to normalize.\n        epsilon: Fuzz factor.\n\n    # Returns\n        A tuple length of 3, `(normalized_tensor, mean, variance)`.\n    \"\"\"\n    if list(reduction_axes) == [0, 1, 2]:\n        normalization_axis = 3\n        tf_data_format = 'NHWC'\n    else:\n        normalization_axis = 1\n        tf_data_format = 'NCHW'\n\n    if gamma is None:\n        gamma = tf.constant(1.0,\n                            dtype=x.dtype,\n                            shape=[x.shape[normalization_axis]])\n    if beta is None:\n        beta = tf.constant(0.0,\n                           dtype=x.dtype,\n                           shape=[x.shape[normalization_axis]])\n\n    if gamma.dtype != tf.float32:\n        gamma = tf.cast(gamma, tf.float32)\n    if beta.dtype != tf.float32:\n        beta = tf.cast(beta, tf.float32)\n\n    return tf.nn.fused_batch_norm(\n        x,\n        gamma,\n        beta,\n        epsilon=epsilon,\n        data_format=tf_data_format)",
                "def normalize_batch_in_training(x, gamma, beta,\n                                reduction_axes, epsilon=1e-3):\n    \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n\n    # Arguments\n        x: Input tensor or variable.\n        gamma: Tensor by which to scale the input.\n        beta: Tensor with which to center the input.\n        reduction_axes: iterable of integers,\n            axes over which to normalize.\n        epsilon: Fuzz factor.\n\n    # Returns\n        A tuple length of 3, `(normalized_tensor, mean, variance)`.\n    \"\"\"\n    if (ndim(x) == 4 and\n            list(reduction_axes) in [[0, 1, 2], [0, 2, 3]] and\n            _is_tf_1()):\n        if not _has_nchw_support() and list(reduction_axes) == [0, 2, 3]:\n            return _broadcast_normalize_batch_in_training(x, gamma, beta,\n                                                          reduction_axes,\n                                                          epsilon=epsilon)\n        return _fused_normalize_batch_in_training(\n            x, gamma, beta, reduction_axes,\n            epsilon=epsilon)\n    else:\n        if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:\n            return _regular_normalize_batch_in_training(x, gamma, beta,\n                                                        reduction_axes,\n                                                        epsilon=epsilon)\n        else:\n            return _broadcast_normalize_batch_in_training(x, gamma, beta,\n                                                          reduction_axes,\n                                                          epsilon=epsilon)",
                "def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n    \"\"\"Applies batch normalization on x given mean, var, beta and gamma.\n\n    I.e. returns:\n    `output = (x - mean) / sqrt(var + epsilon) * gamma + beta`\n\n    # Arguments\n        x: Input tensor or variable.\n        mean: Mean of batch.\n        var: Variance of batch.\n        beta: Tensor with which to center the input.\n        gamma: Tensor by which to scale the input.\n        axis: Integer, the axis that should be normalized.\n            (typically the features axis).\n        epsilon: Fuzz factor.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if ndim(x) == 4:\n        # The CPU implementation of FusedBatchNorm only support NHWC\n        if axis == 1 or axis == -3:\n            tf_data_format = 'NCHW'\n        elif axis == 3 or axis == -1:\n            tf_data_format = 'NHWC'\n        else:\n            tf_data_format = None\n\n        if ((tf_data_format == 'NHWC' or\n                (tf_data_format == 'NCHW' and\n                 _has_nchw_support())) and\n                _is_tf_1()):\n            # The mean / var / beta / gamma may be processed by broadcast\n            # so it may have extra axes with 1,\n            # it is not needed and should be removed\n            if ndim(mean) > 1:\n                mean = tf.reshape(mean, [-1])\n            if ndim(var) > 1:\n                var = tf.reshape(var, [-1])\n            if beta is None:\n                beta = zeros_like(mean)\n            elif ndim(beta) > 1:\n                beta = tf.reshape(beta, [-1])\n            if gamma is None:\n                gamma = ones_like(mean)\n            elif ndim(gamma) > 1:\n                gamma = tf.reshape(gamma, [-1])\n\n            if gamma.dtype != tf.float32:\n                gamma = tf.cast(gamma, tf.float32)\n            if beta.dtype != tf.float32:\n                beta = tf.cast(beta, tf.float32)\n            if mean.dtype != tf.float32:\n                mean = tf.cast(mean, tf.float32)\n            if var.dtype != tf.float32:\n                var = tf.cast(var, tf.float32)\n\n            y, _, _ = tf.nn.fused_batch_norm(\n                x,\n                gamma,\n                beta,\n                epsilon=epsilon,\n                mean=mean,\n                variance=var,\n                data_format=tf_data_format,\n                is_training=False\n            )\n            return y\n    # default\n    return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)",
                "def concatenate(tensors, axis=-1):\n    \"\"\"Concatenates a list of tensors alongside the specified axis.\n\n    # Arguments\n        tensors: list of tensors to concatenate.\n        axis: concatenation axis.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if axis < 0:\n        rank = ndim(tensors[0])\n        if rank:\n            axis %= rank\n        else:\n            axis = 0\n    if py_all([is_sparse(x) for x in tensors]):\n        return tf.sparse.concat(axis, tensors)\n    else:\n        return tf.concat([to_dense(x) for x in tensors], axis)",
                "def reshape(x, shape):\n    \"\"\"Reshapes a tensor to the specified shape.\n\n    # Arguments\n        x: Tensor or variable.\n        shape: Target shape tuple.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.reshape(x, shape)",
                "def permute_dimensions(x, pattern):\n    \"\"\"Permutes axes in a tensor.\n\n    # Arguments\n        x: Tensor or variable.\n        pattern: A tuple of\n            dimension indices, e.g. `(0, 2, 1)`.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.transpose(x, perm=pattern)",
                "def resize_images(x,\n                  height_factor,\n                  width_factor,\n                  data_format,\n                  interpolation='nearest'):\n    \"\"\"Resizes the images contained in a 4D tensor.\n\n    # Arguments\n        x: Tensor or variable to resize.\n        height_factor: Positive integer.\n        width_factor: Positive integer.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        interpolation: A string, one of `nearest` or `bilinear`.\n\n    # Returns\n        A tensor.\n\n    # Raises\n        ValueError: if `data_format` is\n        neither `\"channels_last\"` or `\"channels_first\"`.\n    \"\"\"\n    if data_format == 'channels_first':\n        rows, cols = 2, 3\n    else:\n        rows, cols = 1, 2\n\n    original_shape = int_shape(x)\n    new_shape = tf.shape(x)[rows:cols + 1]\n    new_shape *= tf.constant(np.array([height_factor, width_factor],\n                             dtype='int32'))\n    if data_format == 'channels_first':\n        x = permute_dimensions(x, [0, 2, 3, 1])\n    if interpolation == 'nearest':\n        x = tf_image_ops.resize_nearest_neighbor(x, new_shape)\n    elif interpolation == 'bilinear':\n        x = tf_image_ops.resize_bilinear(x, new_shape)\n    else:\n        raise ValueError('interpolation should be one '\n                         'of \"nearest\" or \"bilinear\".')\n    if data_format == 'channels_first':\n        x = permute_dimensions(x, [0, 3, 1, 2])\n\n    if original_shape[rows] is None:\n        new_height = None\n    else:\n        new_height = original_shape[rows] * height_factor\n\n    if original_shape[cols] is None:\n        new_width = None\n    else:\n        new_width = original_shape[cols] * width_factor\n\n    output_shape = (None, new_height, new_width, None)\n    x.set_shape(transpose_shape(output_shape, data_format,\n                                spatial_axes=(1, 2)))\n    return x",
                "def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    \"\"\"Resizes the volume contained in a 5D tensor.\n\n    # Arguments\n        x: Tensor or variable to resize.\n        depth_factor: Positive integer.\n        height_factor: Positive integer.\n        width_factor: Positive integer.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A tensor.\n\n    # Raises\n        ValueError: if `data_format` is\n        neither `\"channels_last\"` or `\"channels_first\"`.\n    \"\"\"\n    if data_format == 'channels_first':\n        output = repeat_elements(x, depth_factor, axis=2)\n        output = repeat_elements(output, height_factor, axis=3)\n        output = repeat_elements(output, width_factor, axis=4)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, depth_factor, axis=1)\n        output = repeat_elements(output, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    else:\n        raise ValueError('Unknown data_format: ' + str(data_format))",
                "def repeat_elements(x, rep, axis):\n    \"\"\"Repeats the elements of a tensor along an axis, like `np.repeat`.\n\n    If `x` has shape `(s1, s2, s3)` and `axis` is `1`, the output\n    will have shape `(s1, s2 * rep, s3)`.\n\n    # Arguments\n        x: Tensor or variable.\n        rep: Python integer, number of times to repeat.\n        axis: Axis along which to repeat.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    x_shape = x.shape.as_list()\n    # For static axis\n    if x_shape[axis] is not None:\n        # slices along the repeat axis\n        splits = tf.split(value=x, num_or_size_splits=x_shape[axis], axis=axis)\n        # repeat each slice the given number of reps\n        x_rep = [s for s in splits for _ in range(rep)]\n        return concatenate(x_rep, axis)\n\n    # Here we use tf.tile to mimic behavior of np.repeat so that\n    # we can handle dynamic shapes (that include None).\n    # To do that, we need an auxiliary axis to repeat elements along\n    # it and then merge them along the desired axis.\n\n    # Repeating\n    auxiliary_axis = axis + 1\n    x_shape = tf.shape(x)\n    x_rep = tf.expand_dims(x, axis=auxiliary_axis)\n    reps = np.ones(len(x.shape) + 1)\n    reps[auxiliary_axis] = rep\n    x_rep = tf.tile(x_rep, reps)\n\n    # Merging\n    reps = np.delete(reps, auxiliary_axis)\n    reps[axis] = rep\n    reps = tf.constant(reps, dtype='int32')\n    x_shape = x_shape * reps\n    x_rep = tf.reshape(x_rep, x_shape)\n\n    # Fix shape representation\n    x_shape = x.shape.as_list()\n    x_rep.set_shape(x_shape)\n    x_rep._keras_shape = tuple(x_shape)\n    return x_rep",
                "def repeat(x, n):\n    \"\"\"Repeats a 2D tensor.\n\n    if `x` has shape (samples, dim) and `n` is `2`,\n    the output will have shape `(samples, 2, dim)`.\n\n    # Arguments\n        x: Tensor or variable.\n        n: Python integer, number of times to repeat.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    assert ndim(x) == 2\n    x = tf.expand_dims(x, 1)\n    pattern = tf.stack([1, n, 1])\n    return tf.tile(x, pattern)",
                "def arange(start, stop=None, step=1, dtype='int32'):\n    \"\"\"Creates a 1D tensor containing a sequence of integers.\n\n    The function arguments use the same convention as\n    Theano's arange: if only one argument is provided,\n    it is in fact the \"stop\" argument and \"start\" is 0.\n\n    The default type of the returned tensor is `'int32'` to\n    match TensorFlow's default.\n\n    # Arguments\n        start: Start value.\n        stop: Stop value.\n        step: Difference between two successive values.\n        dtype: Integer dtype to use.\n\n    # Returns\n        An integer tensor.\n\n    \"\"\"\n    # Match the behavior of numpy and Theano by returning an empty sequence.\n    if stop is None:\n        try:\n            if start < 0:\n                start = 0\n        except TypeError:\n            # Handle case where start is a tensor\n            start = tf.cond(start < 0,\n                            true_fn=lambda: tf.constant(0, dtype=start.dtype),\n                            false_fn=lambda: start)\n\n    result = tf.range(start, limit=stop, delta=step, name='arange')\n    if dtype != 'int32':\n        result = cast(result, dtype)\n    return result",
                "def tile(x, n):\n    \"\"\"Creates a tensor by tiling `x` by `n`.\n\n    # Arguments\n        x: A tensor or variable\n        n: A list of integer. The length must be the same as the number of\n            dimensions in `x`.\n\n    # Returns\n        A tiled tensor.\n    \"\"\"\n    if isinstance(n, int):\n        n = [n]\n    return tf.tile(x, n)",
                "def flatten(x):\n    \"\"\"Flatten a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor, reshaped into 1-D\n    \"\"\"\n    return tf.reshape(x, [-1])",
                "def batch_flatten(x):\n    \"\"\"Turn a nD tensor into a 2D tensor with same 0th dimension.\n\n    In other words, it flattens each data samples of a batch.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    x = tf.reshape(x, tf.stack([-1, prod(shape(x)[1:])]))\n    return x",
                "def expand_dims(x, axis=-1):\n    \"\"\"Adds a 1-sized dimension at index \"axis\".\n\n    # Arguments\n        x: A tensor or variable.\n        axis: Position where to add a new axis.\n\n    # Returns\n        A tensor with expanded dimensions.\n    \"\"\"\n    return tf.expand_dims(x, axis)",
                "def squeeze(x, axis):\n    \"\"\"Removes a 1-dimension from the tensor at index \"axis\".\n\n    # Arguments\n        x: A tensor or variable.\n        axis: Axis to drop.\n\n    # Returns\n        A tensor with the same data as `x` but reduced dimensions.\n    \"\"\"\n    return tf.squeeze(x, [axis])",
                "def temporal_padding(x, padding=(1, 1)):\n    \"\"\"Pads the middle dimension of a 3D tensor.\n\n    # Arguments\n        x: Tensor or variable.\n        padding: Tuple of 2 integers, how many zeros to\n            add at the start and end of dim 1.\n\n    # Returns\n        A padded 3D tensor.\n    \"\"\"\n    assert len(padding) == 2\n    pattern = [[0, 0], [padding[0], padding[1]], [0, 0]]\n    return tf.pad(x, pattern)",
                "def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n    \"\"\"Pads the 2nd and 3rd dimensions of a 4D tensor.\n\n    # Arguments\n        x: Tensor or variable.\n        padding: Tuple of 2 tuples, padding pattern.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A padded 4D tensor.\n\n    # Raises\n        ValueError: if `data_format` is\n        neither `\"channels_last\"` or `\"channels_first\"`.\n    \"\"\"\n    assert len(padding) == 2\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    data_format = normalize_data_format(data_format)\n\n    pattern = [[0, 0],\n               list(padding[0]),\n               list(padding[1]),\n               [0, 0]]\n    pattern = transpose_shape(pattern, data_format, spatial_axes=(1, 2))\n    return tf.pad(x, pattern)",
                "def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n    \"\"\"Pads 5D tensor with zeros along the depth, height, width dimensions.\n\n    Pads these dimensions with respectively\n    \"padding[0]\", \"padding[1]\" and \"padding[2]\" zeros left and right.\n\n    For 'channels_last' data_format,\n    the 2nd, 3rd and 4th dimension will be padded.\n    For 'channels_first' data_format,\n    the 3rd, 4th and 5th dimension will be padded.\n\n    # Arguments\n        x: Tensor or variable.\n        padding: Tuple of 3 tuples, padding pattern.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A padded 5D tensor.\n\n    # Raises\n        ValueError: if `data_format` is\n        neither `\"channels_last\"` or `\"channels_first\"`.\n\n    \"\"\"\n    assert len(padding) == 3\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    assert len(padding[2]) == 2\n    data_format = normalize_data_format(data_format)\n\n    pattern = [\n        [0, 0],\n        [padding[0][0], padding[0][1]],\n        [padding[1][0], padding[1][1]],\n        [padding[2][0], padding[2][1]],\n        [0, 0]\n    ]\n    pattern = transpose_shape(pattern, data_format, spatial_axes=(1, 2, 3))\n\n    return tf.pad(x, pattern)",
                "def stack(x, axis=0):\n    \"\"\"Stacks a list of rank `R` tensors into a rank `R+1` tensor.\n\n    # Arguments\n        x: List of tensors.\n        axis: Axis along which to perform stacking.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.stack(x, axis=axis)",
                "def one_hot(indices, num_classes):\n    \"\"\"Computes the one-hot representation of an integer tensor.\n\n    # Arguments\n        indices: nD integer tensor of shape\n            `(batch_size, dim1, dim2, ... dim(n-1))`\n        num_classes: Integer, number of classes to consider.\n\n    # Returns\n        (n + 1)D one hot representation of the input\n        with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n    \"\"\"\n    return tf.one_hot(indices, depth=num_classes, axis=-1)",
                "def reverse(x, axes):\n    \"\"\"Reverses a tensor along the specified axes.\n\n    # Arguments\n        x: Tensor to reverse.\n        axes: Integer or iterable of integers.\n            Axes to reverse.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    if isinstance(axes, int):\n        axes = [axes]\n    return tf.reverse(x, axes)",
                "def slice(x, start, size):\n    \"\"\"Extracts a slice from a tensor.\n\n    # Arguments\n        x: Input tensor.\n        start: Integer list/tuple or tensor\n            indicating the start indices of the slice\n            along each axis.\n        size: Integer list/tuple or tensor\n            indicating how many dimensions to slice\n            along each axis.\n\n    # Returns\n        A sliced tensor:\n        ```python\n        new_x = x[start[0]: start[0] + size[0], ..., start[-1]: start[-1] + size[-1]]\n        ```\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.slice(x, start, size)",
                "def get_value(x):\n    \"\"\"Returns the value of a variable.\n\n    # Arguments\n        x: input variable.\n\n    # Returns\n        A Numpy array.\n    \"\"\"\n    if _is_tf_1():\n        return x.eval(session=get_session())\n    else:\n        return x.numpy()",
                "def batch_get_value(ops):\n    \"\"\"Returns the value of more than one tensor variable.\n\n    # Arguments\n        ops: list of ops to run.\n\n    # Returns\n        A list of Numpy arrays.\n    \"\"\"\n    return tf_keras_backend.batch_get_value(ops)",
                "def set_value(x, value):\n    \"\"\"Sets the value of a variable, from a Numpy array.\n\n    # Arguments\n        x: Variable to set to a new value.\n        value: Value to set the tensor to, as a Numpy array\n            (of the same shape).\n    \"\"\"\n    tf_keras_backend.set_value(x, value)",
                "def batch_set_value(tuples):\n    \"\"\"Sets the values of many tensor variables at once.\n\n    # Arguments\n        tuples: a list of tuples `(tensor, value)`.\n            `value` should be a Numpy array.\n    \"\"\"\n    tf_keras_backend.batch_set_value(tuples)",
                "def get_variable_shape(x):\n    \"\"\"Returns the shape of a variable.\n\n    # Arguments\n        x: A variable.\n\n    # Returns\n        A tuple of integers.\n    \"\"\"\n    return int_shape(x)",
                "def print_tensor(x, message=''):\n    \"\"\"Prints `message` and the tensor value when evaluated.\n\n     Note that `print_tensor` returns a new tensor identical to `x`\n     which should be used in the following code. Otherwise the\n     print operation is not taken into account during evaluation.\n\n     # Example\n     ```python\n         >>> x = K.print_tensor(x, message=\"x is: \")\n     ```\n\n    # Arguments\n        x: Tensor to print.\n        message: Message to print jointly with the tensor.\n\n    # Returns\n        The same tensor `x`, unchanged.\n    \"\"\"\n    # TODO\n    return tf.Print(x, [x], message)",
                "def function(inputs, outputs, updates=None, **kwargs):\n    if _is_tf_1():\n        v1_variable_initialization()\n    return tf_keras_backend.function(inputs, outputs,\n                                     updates=updates,\n                                     **kwargs)",
                "@symbolic\ndef gradients(loss, variables):\n    \"\"\"Returns the gradients of `loss` w.r.t. `variables`.\n\n    # Arguments\n        loss: Scalar tensor to minimize.\n        variables: List of variables.\n\n    # Returns\n        A gradients tensor.\n    \"\"\"\n    if _is_tf_1():\n        return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n    return tf.gradients(loss, variables)",
                "@symbolic\ndef stop_gradient(variables):\n    \"\"\"Returns `variables` but with zero gradient w.r.t. every other variable.\n\n    # Arguments\n        variables: tensor or list of tensors to consider constant with respect\n            to any other variable.\n\n    # Returns\n        A single tensor or a list of tensors (depending on the passed argument)\n            that has constant gradient with respect to any other variable.\n    \"\"\"\n    if isinstance(variables, (list, tuple)):\n        return map(tf.stop_gradient, variables)\n    else:\n        return tf.stop_gradient(variables)",
                "def rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n    \"\"\"Iterates over the time dimension of a tensor.\n\n    # Arguments\n        step_function:\n            Parameters:\n                inputs: Tensor with shape (samples, ...) (no time dimension),\n                    representing input for the batch of samples at a certain\n                    time step.\n                states: List of tensors.\n            Returns:\n                outputs: Tensor with shape (samples, ...) (no time dimension),\n                new_states: List of tensors, same length and shapes\n                    as 'states'.\n        inputs: Tensor of temporal data of shape (samples, time, ...)\n            (at least 3D).\n        initial_states: Tensor with shape (samples, ...) (no time dimension),\n            containing the initial values for the states used in\n            the step function.\n        go_backwards: Boolean. If True, do the iteration over the time\n            dimension in reverse order and return the reversed sequence.\n        mask: Binary tensor with shape (samples, time),\n            with a zero for every element that is masked.\n        constants: A list of constant values passed at each step.\n        unroll: Whether to unroll the RNN or to use a symbolic loop\n            (`while_loop` or `scan` depending on backend).\n        input_length: Static number of timesteps in the input.\n\n    # Returns\n        A tuple, `(last_output, outputs, new_states)`.\n\n        last_output: The latest output of the rnn, of shape `(samples, ...)`\n        outputs: Tensor with shape `(samples, time, ...)` where each\n            entry `outputs[s, t]` is the output of the step function\n            at time `t` for sample `s`.\n        new_states: List of tensors, latest states returned by\n            the step function, of shape `(samples, ...)`.\n\n    # Raises\n        ValueError: If input dimension is less than 3.\n        ValueError: If `unroll` is `True`\n            but input timestep is not a fixed number.\n        ValueError: If `mask` is provided (not `None`)\n            but states is not provided (`len(states)` == 0).\n\n    {{np_implementation}}\n    \"\"\"\n    last_output, outputs, new_states = tf_keras_backend.rnn(\n        step_function, inputs, initial_states,\n        go_backwards=go_backwards,\n        mask=mask,\n        constants=constants,\n        unroll=unroll,\n        input_length=input_length)\n    reachable = tf_utils.get_reachable_from_inputs([learning_phase()],\n                                                   targets=[last_output])\n    if last_output in reachable:\n        last_output._uses_learning_phase = True\n    return last_output, outputs, new_states",
                "@symbolic\ndef switch(condition, then_expression, else_expression):\n    \"\"\"Switches between two operations depending on a scalar value.\n\n    Note that both `then_expression` and `else_expression`\n    should be symbolic tensors of the *same shape*.\n\n    # Arguments\n        condition: tensor (`int` or `bool`).\n        then_expression: either a tensor, or a callable that returns a tensor.\n        else_expression: either a tensor, or a callable that returns a tensor.\n\n    # Returns\n        The selected tensor.\n\n    # Raises\n        ValueError: If rank of `condition` is greater than rank of expressions.\n\n    {{np_implementation}}\n    \"\"\"\n    if condition.dtype != tf.bool:\n        condition = tf.cast(condition, 'bool')\n    cond_ndim = ndim(condition)\n    if not cond_ndim:\n        if not callable(then_expression):\n            def then_expression_fn():\n                return then_expression\n        else:\n            then_expression_fn = then_expression\n        if not callable(else_expression):\n            def else_expression_fn():\n                return else_expression\n        else:\n            else_expression_fn = else_expression\n        x = tf.cond(condition,\n                    then_expression_fn,\n                    else_expression_fn)\n    else:\n        # tf.where needs its condition tensor\n        # to be the same shape as its two\n        # result tensors\n        if callable(then_expression):\n            then_expression = then_expression()\n        if callable(else_expression):\n            else_expression = else_expression()\n        expr_ndim = ndim(then_expression)\n        if cond_ndim > expr_ndim:\n            raise ValueError('Rank of `condition` should be less than or'\n                             ' equal to rank of `then_expression` and '\n                             '`else_expression`. ndim(condition)=' +\n                             str(cond_ndim) + ', ndim(then_expression)'\n                             '=' + str(expr_ndim))\n        if cond_ndim > 1:\n            ndim_diff = expr_ndim - cond_ndim\n            cond_shape = tf.concat([tf.shape(condition), [1] * ndim_diff], axis=0)\n            condition = tf.reshape(condition, cond_shape)\n            expr_shape = tf.shape(then_expression)\n            shape_diff = expr_shape - cond_shape\n            zero_expr_shape = tf.ones_like(expr_shape)\n            tile_shape = tf.where(shape_diff > 0, expr_shape, zero_expr_shape)\n            condition = tf.tile(condition, tile_shape)\n        x = tf.where(condition, then_expression, else_expression)\n    return x",
                "@symbolic\ndef in_train_phase(x, alt, training=None):\n    \"\"\"Selects `x` in train phase, and `alt` otherwise.\n\n    Note that `alt` should have the *same shape* as `x`.\n\n    # Arguments\n        x: What to return in train phase\n            (tensor or callable that returns a tensor).\n        alt: What to return otherwise\n            (tensor or callable that returns a tensor).\n        training: Optional scalar tensor\n            (or Python boolean, or Python integer)\n            specifying the learning phase.\n\n    # Returns\n        Either `x` or `alt` based on the `training` flag.\n        the `training` flag defaults to `K.learning_phase()`.\n    \"\"\"\n    if training is None:\n        training = learning_phase()\n        uses_learning_phase = True\n    else:\n        uses_learning_phase = False\n\n    if training is 1 or training is True:\n        if callable(x):\n            return x()\n        else:\n            return x\n\n    elif training is 0 or training is False:\n        if callable(alt):\n            return alt()\n        else:\n            return alt\n\n    # else: assume learning phase is a placeholder tensor.\n    x = switch(training, x, alt)\n    if uses_learning_phase:\n        x._uses_learning_phase = True\n    return x",
                "@symbolic\ndef in_test_phase(x, alt, training=None):\n    \"\"\"Selects `x` in test phase, and `alt` otherwise.\n\n    Note that `alt` should have the *same shape* as `x`.\n\n    # Arguments\n        x: What to return in test phase\n            (tensor or callable that returns a tensor).\n        alt: What to return otherwise\n            (tensor or callable that returns a tensor).\n        training: Optional scalar tensor\n            (or Python boolean, or Python integer)\n            specifying the learning phase.\n\n    # Returns\n        Either `x` or `alt` based on `K.learning_phase`.\n    \"\"\"\n    return in_train_phase(alt, x, training=training)",
                "def relu(x, alpha=0., max_value=None, threshold=0.):\n    \"\"\"Rectified linear unit.\n\n    With default values, it returns element-wise `max(x, 0)`.\n\n    Otherwise, it follows:\n    `f(x) = max_value` for `x >= max_value`,\n    `f(x) = x` for `threshold <= x < max_value`,\n    `f(x) = alpha * (x - threshold)` otherwise.\n\n    # Arguments\n        x: A tensor or variable.\n        alpha: A scalar, slope of negative section (default=`0.`).\n        max_value: float. Saturation threshold.\n        threshold: float. Threshold value for thresholded activation.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n\n    if alpha != 0.:\n        if max_value is None and threshold == 0.:\n            return tf.nn.leaky_relu(x, alpha=alpha)\n\n        if threshold != 0.:\n            negative_part = tf.nn.relu(-x + threshold)\n        else:\n            negative_part = tf.nn.relu(-x)\n\n    clip_max = max_value is not None\n\n    if threshold != 0:\n        # computes x for x > threshold else 0\n        x = x * tf.cast(tf.greater(x, threshold), floatx())\n    elif max_value == 6:\n        # if no threshold, then can use nn.relu6 native TF op for performance\n        x = tf.nn.relu6(x)\n        clip_max = False\n    else:\n        x = tf.nn.relu(x)\n\n    if clip_max:\n        max_value = _to_tensor(max_value, x.dtype.base_dtype)\n        zero = _to_tensor(0., x.dtype.base_dtype)\n        x = tf.clip_by_value(x, zero, max_value)\n\n    if alpha != 0:\n        alpha = _to_tensor(alpha, x.dtype.base_dtype)\n        x -= alpha * negative_part\n    return x",
                "def elu(x, alpha=1.):\n    \"\"\"Exponential linear unit.\n\n    # Arguments\n        x: A tensor or variable to compute the activation function for.\n        alpha: A scalar, slope of negative section.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    res = tf.nn.elu(x)\n    if alpha == 1:\n        return res\n    else:\n        return tf.where(x > 0, res, alpha * res)",
                "def softmax(x, axis=-1):\n    \"\"\"Softmax of a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: The dimension softmax would be performed on.\n            The default is -1 which indicates the last dimension.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.nn.softmax(x, axis=axis)",
                "def softplus(x):\n    \"\"\"Softplus of a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.nn.softplus(x)",
                "def softsign(x):\n    \"\"\"Softsign of a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.nn.softsign(x)",
                "def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n\n    # Arguments\n        target: A tensor of the same shape as `output`.\n        output: A tensor resulting from a softmax\n            (unless `from_logits` is True, in which\n            case `output` is expected to be the logits).\n        from_logits: Boolean, whether `output` is the\n            result of a softmax, or is a tensor of logits.\n        axis: Int specifying the channels axis. `axis=-1`\n            corresponds to data format `channels_last`,\n            and `axis=1` corresponds to data format\n            `channels_first`.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: if `axis` is neither -1 nor one of\n            the axes of `output`.\n    \"\"\"\n    return tf_keras_backend.categorical_crossentropy(\n        target, output, from_logits=from_logits, axis=axis)",
                "def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    \"\"\"Categorical crossentropy with integer targets.\n\n    # Arguments\n        target: An integer tensor.\n        output: A tensor resulting from a softmax\n            (unless `from_logits` is True, in which\n            case `output` is expected to be the logits).\n        from_logits: Boolean, whether `output` is the\n            result of a softmax, or is a tensor of logits.\n        axis: Int specifying the channels axis. `axis=-1`\n            corresponds to data format `channels_last`,\n            and `axis=1` corresponds to data format\n            `channels_first`.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: if `axis` is neither -1 nor one of\n            the axes of `output`.\n    \"\"\"\n    return tf_keras_backend.sparse_categorical_crossentropy(\n        target, output, from_logits=from_logits, axis=axis)",
                "def binary_crossentropy(target, output, from_logits=False):\n    \"\"\"Binary crossentropy between an output tensor and a target tensor.\n\n    # Arguments\n        target: A tensor with the same shape as `output`.\n        output: A tensor.\n        from_logits: Whether `output` is expected to be a logits tensor.\n            By default, we consider that `output`\n            encodes a probability distribution.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf_keras_backend.binary_crossentropy(\n        target, output, from_logits=from_logits)",
                "def sigmoid(x):\n    \"\"\"Element-wise sigmoid.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.nn.sigmoid(x)",
                "def hard_sigmoid(x):\n    \"\"\"Segment-wise linear approximation of sigmoid.\n\n    Faster than sigmoid.\n    Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n    In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf_keras_backend.hard_sigmoid(x)",
                "def tanh(x):\n    \"\"\"Element-wise tanh.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.nn.tanh(x)",
                "def dropout(x, level, noise_shape=None, seed=None):\n    \"\"\"Sets entries in `x` to zero at random, while scaling the entire tensor.\n\n    # Arguments\n        x: tensor\n        level: fraction of the entries in the tensor\n            that will be set to 0.\n        noise_shape: shape for randomly generated keep/drop flags,\n            must be broadcastable to the shape of `x`\n        seed: random seed to ensure determinism.\n\n    # Returns\n        A tensor.\n    {{np_implementation}}\n    \"\"\"\n    if seed is None:\n        seed = np.random.randint(10e6)\n    return tf.nn.dropout(x, rate=level, noise_shape=noise_shape, seed=seed)",
                "def l2_normalize(x, axis=None):\n    \"\"\"Normalizes a tensor wrt the L2 norm alongside the specified axis.\n\n    # Arguments\n        x: Tensor or variable.\n        axis: axis along which to perform normalization.\n\n    # Returns\n        A tensor.\n\n    {{np_implementation}}\n    \"\"\"\n    return tf.nn.l2_normalize(x, axis=axis)",
                "def in_top_k(predictions, targets, k):\n    \"\"\"Returns whether the `targets` are in the top `k` `predictions`.\n\n    # Arguments\n        predictions: A tensor of shape `(batch_size, classes)` and type `float32`.\n        targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.\n        k: An `int`, number of top elements to consider.\n\n    # Returns\n        A 1D tensor of length `batch_size` and type `bool`.\n        `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`\n        values of `predictions[i]`.\n    \"\"\"\n    # Note that the order of the 2 first positional arguments\n    # has been inverted in TF 2.\n    return tf.nn.in_top_k(predictions=predictions,\n                          targets=targets,\n                          k=k)",
                "def _preprocess_conv1d_input(x, data_format):\n    \"\"\"Transpose and cast the input before the conv1d.\n\n    # Arguments\n        x: input tensor.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    # tensorflow doesn't support float64 for conv layer before 1.8.0\n    if (dtype(x) == 'float64' and\n            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\n        x = tf.cast(x, 'float32')\n    tf_data_format = 'NWC'  # to pass TF Conv2dNative operations\n    if data_format == 'channels_first':\n        if not _has_nchw_support():\n            x = tf.transpose(x, (0, 2, 1))  # NCW -> NWC\n        else:\n            tf_data_format = 'NCW'\n    return x, tf_data_format",
                "def _preprocess_conv2d_input(x, data_format, force_transpose=False):\n    \"\"\"Transpose and cast the input before the conv2d.\n\n    # Arguments\n        x: input tensor.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n                        if the `data_format` is `\"channels_first\"`.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    # tensorflow doesn't support float64 for conv layer before 1.8.0\n    if (dtype(x) == 'float64' and\n            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\n        x = tf.cast(x, 'float32')\n    tf_data_format = 'NHWC'\n    if data_format == 'channels_first':\n        if not _has_nchw_support() or force_transpose:\n            x = tf.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC\n        else:\n            tf_data_format = 'NCHW'\n    return x, tf_data_format",
                "def _preprocess_conv3d_input(x, data_format):\n    \"\"\"Transpose and cast the input before the conv3d.\n\n    # Arguments\n        x: input tensor.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    # tensorflow doesn't support float64 for conv layer before 1.8.0\n    if (dtype(x) == 'float64' and\n            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\n        x = tf.cast(x, 'float32')\n    tf_data_format = 'NDHWC'\n    if data_format == 'channels_first':\n        if not _has_nchw_support():\n            x = tf.transpose(x, (0, 2, 3, 4, 1))\n        else:\n            tf_data_format = 'NCDHW'\n    return x, tf_data_format",
                "def _preprocess_padding(padding):\n    \"\"\"Convert keras' padding to tensorflow's padding.\n\n    # Arguments\n        padding: string, `\"same\"` or `\"valid\"`.\n\n    # Returns\n        a string, `\"SAME\"` or `\"VALID\"`.\n\n    # Raises\n        ValueError: if `padding` is invalid.\n    \"\"\"\n    if padding == 'same':\n        padding = 'SAME'\n    elif padding == 'valid':\n        padding = 'VALID'\n    else:\n        raise ValueError('Invalid padding: ' + str(padding))\n    return padding",
                "def conv1d(x, kernel, strides=1, padding='valid',\n           data_format=None, dilation_rate=1):\n    \"\"\"1D convolution.\n\n    # Arguments\n        x: Tensor or variable.\n        kernel: kernel tensor.\n        strides: stride integer.\n        padding: string, `\"same\"`, `\"causal\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: integer dilate rate.\n\n    # Returns\n        A tensor, result of 1D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    kernel_shape = kernel.shape.as_list()\n    if padding == 'causal':\n        if data_format != 'channels_last':\n            raise ValueError('When using causal padding in `conv1d`, '\n                             '`data_format` must be \"channels_last\" '\n                             '(temporal data).')\n        # causal (dilated) convolution:\n        left_pad = dilation_rate * (kernel_shape[0] - 1)\n        x = temporal_padding(x, (left_pad, 0))\n        padding = 'valid'\n    padding = _preprocess_padding(padding)\n    x, tf_data_format = _preprocess_conv1d_input(x, data_format)\n\n    # TF 2 arg conversion\n    kwargs = {}\n    if _is_tf_1():\n        kwargs['dilation_rate'] = (dilation_rate,)\n    else:\n        kwargs['dilations'] = (dilation_rate,)\n\n    x = tf.nn.convolution(\n        x, kernel,\n        strides=(strides,),\n        padding=padding,\n        data_format=tf_data_format,\n        **kwargs)\n\n    if data_format == 'channels_first' and tf_data_format == 'NWC':\n        x = tf.transpose(x, (0, 2, 1))  # NWC -> NCW\n    return x",
                "def conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D convolution.\n\n    # Arguments\n        x: Tensor or variable.\n        kernel: kernel tensor.\n        strides: strides tuple.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n            Whether to use Theano or TensorFlow/CNTK data format\n            for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n\n    # Returns\n        A tensor, result of 2D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n\n    padding = _preprocess_padding(padding)\n\n    # TF 2 arg conversion\n    kwargs = {}\n    if _is_tf_1():\n        kwargs['dilation_rate'] = dilation_rate\n    else:\n        kwargs['dilations'] = dilation_rate\n\n    x = tf.nn.convolution(\n        x, kernel,\n        strides=strides,\n        padding=padding,\n        data_format=tf_data_format,\n        **kwargs)\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
                "def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D deconvolution (i.e. transposed convolution).\n\n    # Arguments\n        x: Tensor or variable.\n        kernel: kernel tensor.\n        output_shape: 1D int tensor for the output shape.\n        strides: strides tuple.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n            Whether to use Theano or TensorFlow/CNTK data format\n            for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n\n    # Returns\n        A tensor, result of transposed 2D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n    if isinstance(output_shape, (tuple, list)):\n        output_shape = tf.stack(output_shape)\n\n    # tf.nn.atrous_conv2d_transpose input only supports NHWC format\n    if data_format == 'channels_first' and dilation_rate != (1, 1):\n        force_transpose = True\n    else:\n        force_transpose = False\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format, force_transpose)\n\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        output_shape = (output_shape[0],\n                        output_shape[2],\n                        output_shape[3],\n                        output_shape[1])\n    if output_shape[0] is None:\n        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\n        output_shape = tf.stack(list(output_shape))\n\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NHWC':\n        strides = (1,) + strides + (1,)\n    else:\n        strides = (1, 1) + strides\n\n    if dilation_rate == (1, 1):\n        x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,\n                                   padding=padding,\n                                   data_format=tf_data_format)\n    else:\n        assert dilation_rate[0] == dilation_rate[1]\n        x = tf.nn.atrous_conv2d_transpose(\n            x, kernel, output_shape, dilation_rate[0], padding)\n\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
                "def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                     padding='valid', data_format=None, dilation_rate=1):\n    \"\"\"1D convolution with separable filters.\n\n    # Arguments\n        x: input tensor\n        depthwise_kernel: convolution kernel for the depthwise convolution.\n        pointwise_kernel: kernel for the 1x1 convolution.\n        strides: stride integer.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: integer dilation rate.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n    if isinstance(strides, int):\n        strides = (strides,)\n    if isinstance(dilation_rate, int):\n        dilation_rate = (dilation_rate,)\n\n    x, tf_data_format = _preprocess_conv1d_input(x, data_format)\n    if tf_data_format == 'NWC':\n        tf_data_format = 'NHWC'\n    else:\n        tf_data_format = 'NCHW'\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NHWC':\n        spatial_start_dim = 1\n        strides = (1,) + strides * 2 + (1,)\n    else:\n        spatial_start_dim = 2\n        strides = (1, 1) + strides * 2\n    x = tf.expand_dims(x, spatial_start_dim)\n    depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n    pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)\n    dilation_rate = (1,) + dilation_rate\n\n    # TF 2 arg conversion\n    kwargs = {}\n    if _is_tf_1():\n        kwargs['rate'] = dilation_rate\n    else:\n        kwargs['dilations'] = dilation_rate\n\n    x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,\n                               strides=strides,\n                               padding=padding,\n                               data_format=tf_data_format,\n                               **kwargs)\n\n    x = tf.squeeze(x, [spatial_start_dim])\n\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 2, 1))  # NWC -> NCW\n\n    return x",
                "def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D convolution with separable filters.\n\n    # Arguments\n        x: input tensor\n        depthwise_kernel: convolution kernel for the depthwise convolution.\n        pointwise_kernel: kernel for the 1x1 convolution.\n        strides: strides tuple (length 2).\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: tuple of integers,\n            dilation rates for the separable convolution.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NHWC':\n        strides = (1,) + strides + (1,)\n    else:\n        strides = (1, 1) + strides\n\n    # TF 2 arg conversion\n    kwargs = {}\n    if _is_tf_1():\n        kwargs['rate'] = dilation_rate\n    else:\n        kwargs['dilations'] = dilation_rate\n\n    x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,\n                               strides=strides,\n                               padding=padding,\n                               data_format=tf_data_format,\n                               **kwargs)\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
                "def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                     data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D convolution with separable filters.\n\n    # Arguments\n        x: input tensor\n        depthwise_kernel: convolution kernel for the depthwise convolution.\n        strides: strides tuple (length 2).\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: tuple of integers,\n            dilation rates for the separable convolution.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NHWC':\n        strides = (1,) + strides + (1,)\n    else:\n        strides = (1, 1) + strides\n\n    # TF 2 arg conversion\n    kwargs = {}\n    if _is_tf_1():\n        kwargs['rate'] = dilation_rate\n    else:\n        kwargs['dilations'] = dilation_rate\n\n    x = tf.nn.depthwise_conv2d(x, depthwise_kernel,\n                               strides=strides,\n                               padding=padding,\n                               data_format=tf_data_format,\n                               **kwargs)\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
                "def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1, 1)):\n    \"\"\"3D convolution.\n\n    # Arguments\n        x: Tensor or variable.\n        kernel: kernel tensor.\n        strides: strides tuple.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n            Whether to use Theano or TensorFlow/CNTK data format\n            for inputs/kernels/outputs.\n        dilation_rate: tuple of 3 integers.\n\n    # Returns\n        A tensor, result of 3D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n    padding = _preprocess_padding(padding)\n\n    # TF 2 arg conversion\n    kwargs = {}\n    if _is_tf_1():\n        kwargs['dilation_rate'] = dilation_rate\n    else:\n        kwargs['dilations'] = dilation_rate\n\n    x = tf.nn.convolution(\n        x, kernel,\n        strides=strides,\n        padding=padding,\n        data_format=tf_data_format,\n        **kwargs)\n    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n        x = tf.transpose(x, (0, 4, 1, 2, 3))\n    return x",
                "def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                     padding='valid', data_format=None):\n    \"\"\"3D deconvolution (i.e. transposed convolution).\n\n    # Arguments\n        x: input tensor.\n        kernel: kernel tensor.\n        output_shape: 1D int tensor for the output shape.\n        strides: strides tuple.\n        padding: string, \"same\" or \"valid\".\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n            Whether to use Theano or TensorFlow/CNTK data format\n            for inputs/kernels/outputs.\n\n    # Returns\n        A tensor, result of transposed 3D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n    if isinstance(output_shape, (tuple, list)):\n        output_shape = tf.stack(output_shape)\n\n    x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n\n    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n        output_shape = (output_shape[0],\n                        output_shape[2],\n                        output_shape[3],\n                        output_shape[4],\n                        output_shape[1])\n    if output_shape[0] is None:\n        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\n        output_shape = tf.stack(list(output_shape))\n\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NDHWC':\n        strides = (1,) + strides + (1,)\n    else:\n        strides = (1, 1) + strides\n\n    x = tf.nn.conv3d_transpose(x, kernel, output_shape, strides,\n                               padding=padding,\n                               data_format=tf_data_format)\n    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n        x = tf.transpose(x, (0, 4, 1, 2, 3))\n    return x",
                "def pool2d(x, pool_size, strides=(1, 1),\n           padding='valid', data_format=None,\n           pool_mode='max'):\n    \"\"\"2D Pooling.\n\n    # Arguments\n        x: Tensor or variable.\n        pool_size: tuple of 2 integers.\n        strides: tuple of 2 integers.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        pool_mode: string, `\"max\"` or `\"avg\"`.\n\n    # Returns\n        A tensor, result of 2D pooling.\n\n    # Raises\n        ValueError: if `data_format` is\n        neither `\"channels_last\"` or `\"channels_first\"`.\n        ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NHWC':\n        strides = (1,) + strides + (1,)\n        pool_size = (1,) + pool_size + (1,)\n    else:\n        strides = (1, 1) + strides\n        pool_size = (1, 1) + pool_size\n\n    if pool_mode == 'max':\n        x = tf.nn.max_pool(x, pool_size, strides,\n                           padding=padding,\n                           data_format=tf_data_format)\n    elif pool_mode == 'avg':\n        x = tf.nn.avg_pool(x, pool_size, strides,\n                           padding=padding,\n                           data_format=tf_data_format)\n    else:\n        raise ValueError('Invalid pool_mode: ' + str(pool_mode))\n\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
                "def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    \"\"\"3D Pooling.\n\n    # Arguments\n        x: Tensor or variable.\n        pool_size: tuple of 3 integers.\n        strides: tuple of 3 integers.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        pool_mode: string, `\"max\"` or `\"avg\"`.\n\n    # Returns\n        A tensor, result of 3D pooling.\n\n    # Raises\n        ValueError: if `data_format` is\n        neither `\"channels_last\"` or `\"channels_first\"`.\n        ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NDHWC':\n        strides = (1,) + strides + (1,)\n        pool_size = (1,) + pool_size + (1,)\n    else:\n        strides = (1, 1) + strides\n        pool_size = (1, 1) + pool_size\n\n    if pool_mode == 'max':\n        x = tf.nn.max_pool3d(x, pool_size, strides,\n                             padding=padding,\n                             data_format=tf_data_format)\n    elif pool_mode == 'avg':\n        x = tf.nn.avg_pool3d(x, pool_size, strides,\n                             padding=padding,\n                             data_format=tf_data_format)\n    else:\n        raise ValueError('Invalid pool_mode: ' + str(pool_mode))\n\n    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n        x = tf.transpose(x, (0, 4, 1, 2, 3))\n    return x",
                "def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n    \"\"\"Apply 1D conv with un-shared weights.\n\n    # Arguments\n        inputs: 3D tensor with shape: (batch_size, steps, input_dim)\n        kernel: the unshared weight for convolution,\n                with shape (output_length, feature_dim, filters)\n        kernel_size: a tuple of a single integer,\n                     specifying the length of the 1D convolution window\n        strides: a tuple of a single integer,\n                 specifying the stride length of the convolution\n        data_format: the data format, channels_first or channels_last\n\n    # Returns\n        the tensor after 1d conv with un-shared weights,\n        with shape (batch_size, output_length, filters)\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    stride = strides[0]\n    kernel_shape = int_shape(kernel)\n    output_length, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_length):\n        slice_length = py_slice(i * stride,\n                                i * stride + kernel_size[0])\n        xs.append(reshape(inputs[:, slice_length, :],\n                          (1, -1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=0)\n    # Shape: `(output_length, batch_size, filters)`.\n    output = batch_dot(x_aggregate, kernel)\n    return permute_dimensions(output, (1, 0, 2))",
                "def local_conv2d(inputs,\n                 kernel,\n                 kernel_size,\n                 strides,\n                 output_shape,\n                 data_format=None):\n    \"\"\"Apply 2D conv with un-shared weights.\n\n    # Arguments\n        inputs: 4D tensor with shape:\n                (batch_size, filters, new_rows, new_cols)\n                if data_format='channels_first'\n                or 4D tensor with shape:\n                (batch_size, new_rows, new_cols, filters)\n                if data_format='channels_last'.\n        kernel: the unshared weight for convolution,\n                with shape (output_items, feature_dim, filters)\n        kernel_size: a tuple of 2 integers, specifying the\n                     width and height of the 2D convolution window.\n        strides: a tuple of 2 integers, specifying the strides\n                 of the convolution along the width and height.\n        output_shape: a tuple with (output_row, output_col)\n        data_format: the data format, channels_first or channels_last\n\n    # Returns\n        A 4d tensor with shape:\n        (batch_size, filters, new_rows, new_cols)\n        if data_format='channels_first'\n        or 4D tensor with shape:\n        (batch_size, new_rows, new_cols, filters)\n        if data_format='channels_last'.\n\n    # Raises\n        ValueError: if `data_format` is neither\n                    `channels_last` or `channels_first`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    stride_row, stride_col = strides\n    output_row, output_col = output_shape\n    kernel_shape = int_shape(kernel)\n    _, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_row):\n        for j in range(output_col):\n            slice_row = py_slice(i * stride_row,\n                                 i * stride_row + kernel_size[0])\n            slice_col = py_slice(j * stride_col,\n                                 j * stride_col + kernel_size[1])\n            if data_format == 'channels_first':\n                xs.append(reshape(inputs[:, :, slice_row, slice_col],\n                                  (1, -1, feature_dim)))\n            else:\n                xs.append(reshape(inputs[:, slice_row, slice_col, :],\n                                  (1, -1, feature_dim)))\n\n    x_aggregate = concatenate(xs, axis=0)\n    output = batch_dot(x_aggregate, kernel)\n    output = reshape(output,\n                     (output_row, output_col, -1, filters))\n\n    if data_format == 'channels_first':\n        output = permute_dimensions(output, (2, 3, 0, 1))\n    else:\n        output = permute_dimensions(output, (2, 0, 1, 3))\n    return output",
                "def bias_add(x, bias, data_format=None):\n    \"\"\"Adds a bias vector to a tensor.\n\n    # Arguments\n        x: Tensor or variable.\n        bias: Bias tensor to add.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: In one of the two cases below:\n                    1. invalid `data_format` argument.\n                    2. invalid bias shape.\n                       the bias should be either a vector or\n                       a tensor with ndim(x) - 1 dimension\n    {{np_implementation}}\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n    bias_shape = int_shape(bias)\n    if len(bias_shape) != 1 and len(bias_shape) != ndim(x) - 1:\n        raise ValueError('Unexpected bias dimensions %d, '\n                         'expect to be 1 or %d dimensions'\n                         % (len(bias_shape), ndim(x)))\n    if ndim(x) == 5:\n        if len(bias_shape) == 1:\n            new_shape = (1, 1, 1, 1, bias_shape[0])\n        else:\n            new_shape = (1,) + bias_shape\n        new_shape = transpose_shape(new_shape, data_format,\n                                    spatial_axes=(1, 2, 3))\n        x = x + reshape(bias, new_shape)\n    elif ndim(x) == 4:\n        if data_format == 'channels_first':\n            if len(bias_shape) == 1:\n                if _has_nchw_support():\n                    x = tf.nn.bias_add(x, bias,\n                                       data_format='NCHW')\n                else:\n                    x = x + reshape(bias, (1, bias_shape[0], 1, 1))\n            else:\n                x = x + reshape(bias, (1, bias_shape[2]) + bias_shape[:2])\n        elif data_format == 'channels_last':\n            if len(bias_shape) == 1:\n                x = tf.nn.bias_add(x, bias,\n                                   data_format='NHWC')\n            else:\n                x = x + reshape(bias, (1,) + bias_shape)\n    elif ndim(x) == 3:\n        if len(bias_shape) == 1:\n            new_shape = (1, 1, bias_shape[0])\n        else:\n            new_shape = (1,) + bias_shape\n        new_shape = transpose_shape(new_shape, data_format,\n                                    spatial_axes=(1,))\n        x = x + reshape(bias, new_shape)\n    else:\n        x = tf.nn.bias_add(x, bias)\n    return x",
                "def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    \"\"\"Returns a tensor with normal distribution of values.\n\n    # Arguments\n        shape: A tuple of integers, the shape of tensor to create.\n        mean: A float, mean of the normal distribution to draw samples.\n        stddev: A float, standard deviation of the normal distribution\n            to draw samples.\n        dtype: String, dtype of returned tensor.\n        seed: Integer, random seed.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(10e6)\n    if py_any(list(is_symbolic(x) for x in (shape, mean, stddev))):\n        with get_graph().as_default():\n            return tf_keras_backend.random_normal(\n                shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n    with tf_ops.init_scope():\n        return tf_keras_backend.random_normal(\n            shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)",
                "def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    \"\"\"Returns a tensor with uniform distribution of values.\n\n    # Arguments\n        shape: A tuple of integers, the shape of tensor to create.\n        minval: A float, lower boundary of the uniform distribution\n            to draw samples.\n        maxval: A float, upper boundary of the uniform distribution\n            to draw samples.\n        dtype: String, dtype of returned tensor.\n        seed: Integer, random seed.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(10e6)\n    if py_any(list(is_symbolic(x) for x in (shape, minval, maxval))):\n        with get_graph().as_default():\n            return tf_keras_backend.random_uniform(\n                shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n    with tf_ops.init_scope():\n        return tf_keras_backend.random_uniform(\n            shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)",
                "def random_binomial(shape, p=0.0, dtype=None, seed=None):\n    \"\"\"Returns a tensor with random binomial distribution of values.\n\n    # Arguments\n        shape: A tuple of integers, the shape of tensor to create.\n        p: A float, `0. <= p <= 1`, probability of binomial distribution.\n        dtype: String, dtype of returned tensor.\n        seed: Integer, random seed.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(10e6)\n    if py_any(list(is_symbolic(x) for x in (shape, p))):\n        with get_graph().as_default():\n            return tf_keras_backend.random_binomial(\n                shape, p=p, dtype=dtype, seed=seed)\n    with tf_ops.init_scope():\n        return tf_keras_backend.random_binomial(\n            shape, p=p, dtype=dtype, seed=seed)",
                "def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    \"\"\"Returns a tensor with truncated random normal distribution of values.\n\n    The generated values follow a normal distribution\n    with specified mean and standard deviation,\n    except that values whose magnitude is more than\n    two standard deviations from the mean are dropped and re-picked.\n\n    # Arguments\n        shape: A tuple of integers, the shape of tensor to create.\n        mean: Mean of the values.\n        stddev: Standard deviation of the values.\n        dtype: String, dtype of returned tensor.\n        seed: Integer, random seed.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(10e6)\n    if py_any(list(is_symbolic(x) for x in (shape, mean, stddev))):\n        with get_graph().as_default():\n            return tf_keras_backend.truncated_normal(\n                shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n    with tf_ops.init_scope():\n        return tf_keras_backend.truncated_normal(\n            shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)",
                "def ctc_label_dense_to_sparse(labels, label_lengths):\n    \"\"\"Converts CTC labels from dense to sparse.\n\n    # Arguments\n        labels: dense CTC labels.\n        label_lengths: length of the labels.\n\n    # Returns\n        A sparse tensor representation of the labels.\n    \"\"\"\n    label_shape = tf.shape(labels)\n    num_batches_tns = tf.stack([label_shape[0]])\n    max_num_labels_tns = tf.stack([label_shape[1]])\n\n    def range_less_than(_, current_input):\n        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(\n            max_num_labels_tns, current_input)\n\n    init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)\n    dense_mask = functional_ops.scan(range_less_than, label_lengths,\n                                     initializer=init, parallel_iterations=1)\n    dense_mask = dense_mask[:, 0, :]\n\n    label_array = tf.reshape(tf.tile(tf.range(label_shape[1]), num_batches_tns),\n                             label_shape)\n    label_ind = tf.boolean_mask(label_array, dense_mask)\n\n    tmp = tf.tile(tf.range(label_shape[0]), max_num_labels_tns)\n    batch_array = tf.transpose(tf.reshape(tmp, reverse(label_shape, 0)))\n    batch_ind = tf.boolean_mask(batch_array, dense_mask)\n\n    indices = concatenate([batch_ind, label_ind], axis=0)\n    indices = tf.transpose(tf.reshape(indices, [2, -1]))\n\n    vals_sparse = tf.gather_nd(labels, indices)\n\n    indices = tf.cast(indices, tf.int64)\n    label_shape = tf.cast(label_shape, tf.int64)\n    return tf.SparseTensor(indices, vals_sparse, label_shape)",
                "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n    \"\"\"Runs CTC loss algorithm on each batch element.\n\n    # Arguments\n        y_true: tensor `(samples, max_string_length)`\n            containing the truth labels.\n        y_pred: tensor `(samples, time_steps, num_categories)`\n            containing the prediction, or output of the softmax.\n        input_length: tensor `(samples, 1)` containing the sequence length for\n            each batch item in `y_pred`.\n        label_length: tensor `(samples, 1)` containing the sequence length for\n            each batch item in `y_true`.\n\n    # Returns\n        Tensor with shape (samples,1) containing the\n            CTC loss of each element.\n    \"\"\"\n    label_length = tf.cast(tf.squeeze(label_length, axis=-1), tf.int32)\n    input_length = tf.cast(tf.squeeze(input_length, axis=-1), tf.int32)\n    sparse_labels = tf.cast(\n        ctc_label_dense_to_sparse(y_true, label_length), tf.int32)\n    y_pred = tf_math_ops.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n    return tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\n                                       labels=sparse_labels,\n                                       sequence_length=input_length), 1)",
                "def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n               top_paths=1, merge_repeated=False):\n    \"\"\"Decodes the output of a softmax.\n\n    Can use either greedy search (also known as best path)\n    or a constrained dictionary search.\n\n    # Arguments\n        y_pred: tensor `(samples, time_steps, num_categories)`\n            containing the prediction, or output of the softmax.\n        input_length: tensor `(samples, )` containing the sequence length for\n            each batch item in `y_pred`.\n        greedy: perform much faster best-path search if `True`.\n            This does not use a dictionary.\n        beam_width: if `greedy` is `False`: a beam search decoder will be used\n            with a beam of this width.\n        top_paths: if `greedy` is `False`,\n            how many of the most probable paths will be returned.\n        merge_repeated: if `greedy` is `False`,\n            merge repeated classes in the output beams.\n\n    # Returns\n        Tuple:\n            List: if `greedy` is `True`, returns a list of one element that\n                contains the decoded sequence.\n                If `False`, returns the `top_paths` most probable\n                decoded sequences.\n                Important: blank labels are returned as `-1`.\n            Tensor `(top_paths, )` that contains\n                the log probability of each decoded sequence.\n    \"\"\"\n    y_pred = tf_math_ops.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n    input_length = tf.cast(input_length, tf.int32)\n\n    if greedy:\n        (decoded, log_prob) = ctc.ctc_greedy_decoder(\n            inputs=y_pred,\n            sequence_length=input_length)\n    else:\n        (decoded, log_prob) = ctc.ctc_beam_search_decoder(\n            inputs=y_pred,\n            sequence_length=input_length, beam_width=beam_width,\n            top_paths=top_paths, merge_repeated=merge_repeated)\n\n    decoded_dense = []\n    for st in decoded:\n        dense_tensor = tf.sparse.to_dense(st, default_value=-1)\n        decoded_dense.append(dense_tensor)\n    return (decoded_dense, log_prob)",
                "def map_fn(fn, elems, name=None, dtype=None):\n    \"\"\"Map the function fn over the elements elems and return the outputs.\n\n    # Arguments\n        fn: Callable that will be called upon each element in elems\n        elems: tensor\n        name: A string name for the map node in the graph\n        dtype: Output data type.\n\n    # Returns\n        Tensor with dtype `dtype`.\n    \"\"\"\n    return tf.map_fn(fn, elems, name=name, dtype=dtype)",
                "def foldl(fn, elems, initializer=None, name=None):\n    \"\"\"Reduce elems using fn to combine them from left to right.\n\n    # Arguments\n        fn: Callable that will be called upon each element in elems and an\n            accumulator, for instance `lambda acc, x: acc + x`\n        elems: tensor\n        initializer: The first value used (`elems[0]` in case of None)\n        name: A string name for the foldl node in the graph\n\n    # Returns\n        Tensor with same type and shape as `initializer`.\n    \"\"\"\n    return tf.foldl(fn, elems, initializer=initializer, name=name)",
                "def foldr(fn, elems, initializer=None, name=None):\n    \"\"\"Reduce elems using fn to combine them from right to left.\n\n    # Arguments\n        fn: Callable that will be called upon each element in elems and an\n            accumulator, for instance `lambda acc, x: acc + x`\n        elems: tensor\n        initializer: The first value used (`elems[-1]` in case of None)\n        name: A string name for the foldr node in the graph\n\n    # Returns\n        Tensor with same type and shape as `initializer`.\n    \"\"\"\n    return tf.foldr(fn, elems, initializer=initializer, name=name)",
                "@functools.wraps(func)\ndef symbolic_fn_wrapper(*args, **kwargs):\n    if _SYMBOLIC_SCOPE.value:\n        with get_graph().as_default():\n            return func(*args, **kwargs)\n    else:\n        return func(*args, **kwargs)",
                "@functools.wraps(func)\ndef eager_fn_wrapper(*args, **kwargs):\n    prev_value = _SYMBOLIC_SCOPE.value\n    try:\n        _SYMBOLIC_SCOPE.value = False\n        with context.eager_mode():\n            out = func(*args, **kwargs)\n    finally:\n        _SYMBOLIC_SCOPE.value = prev_value\n    return out",
                "def __init__(self):\n    self.device = None",
                "def _set_device(self, device):\n    \"\"\"This method captures TF's explicit device scope setting.\"\"\"\n    self.device = device",
                "def range_less_than(_, current_input):\n    return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(\n        max_num_labels_tns, current_input)",
                "def then_expression_fn():\n    return then_expression",
                "def else_expression_fn():\n    return else_expression"
            ],
            "inscope_function_signatures": [
                "_is_tf_1()",
                "symbolic(func)",
                "is_symbolic(x)",
                "eager(func)",
                "get_uid(prefix='')",
                "manual_variable_initialization(value)",
                "epsilon()",
                "reset_uids()",
                "set_epsilon(e)",
                "floatx()",
                "set_floatx(floatx)",
                "cast_to_floatx(x)",
                "image_data_format()",
                "set_image_data_format(data_format)",
                "normalize_data_format(value)",
                "learning_phase()",
                "set_learning_phase(value)",
                "get_session()",
                "set_session(session)",
                "clear_session()",
                "v1_variable_initialization()",
                "_get_current_tf_device()",
                "_is_current_explicit_device(device_type)",
                "_get_available_gpus()",
                "_has_nchw_support()",
                "_to_tensor(x, dtype)",
                "is_sparse(tensor)",
                "to_dense(tensor)",
                "variable(value, dtype=None, name=None, constraint=None)",
                "constant(value, dtype=None, shape=None, name=None)",
                "is_keras_tensor(x)",
                "is_tensor(x)",
                "placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None)",
                "is_placeholder(x)",
                "shape(x)",
                "int_shape(x)",
                "ndim(x)",
                "dtype(x)",
                "eval(x)",
                "zeros(shape, dtype=None, name=None)",
                "ones(shape, dtype=None, name=None)",
                "eye(size, dtype=None, name=None)",
                "zeros_like(x, dtype=None, name=None)",
                "ones_like(x, dtype=None, name=None)",
                "identity(x, name=None)",
                "random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None)",
                "random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None)",
                "count_params(x)",
                "cast(x, dtype)",
                "update(x, new_x)",
                "update_add(x, increment)",
                "update_sub(x, decrement)",
                "moving_average_update(x, value, momentum)",
                "dot(x, y)",
                "batch_dot(x, y, axes=None)",
                "transpose(x)",
                "gather(reference, indices)",
                "max(x, axis=None, keepdims=False)",
                "min(x, axis=None, keepdims=False)",
                "sum(x, axis=None, keepdims=False)",
                "prod(x, axis=None, keepdims=False)",
                "cumsum(x, axis=0)",
                "cumprod(x, axis=0)",
                "var(x, axis=None, keepdims=False)",
                "std(x, axis=None, keepdims=False)",
                "mean(x, axis=None, keepdims=False)",
                "any(x, axis=None, keepdims=False)",
                "all(x, axis=None, keepdims=False)",
                "argmax(x, axis=-1)",
                "argmin(x, axis=-1)",
                "square(x)",
                "abs(x)",
                "sqrt(x)",
                "exp(x)",
                "log(x)",
                "logsumexp(x, axis=None, keepdims=False)",
                "round(x)",
                "sign(x)",
                "pow(x, a)",
                "clip(x, min_value, max_value)",
                "equal(x, y)",
                "not_equal(x, y)",
                "greater(x, y)",
                "greater_equal(x, y)",
                "less(x, y)",
                "less_equal(x, y)",
                "maximum(x, y)",
                "minimum(x, y)",
                "sin(x)",
                "cos(x)",
                "_regular_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
                "_broadcast_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
                "_fused_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
                "normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
                "batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=0.001)",
                "concatenate(tensors, axis=-1)",
                "reshape(x, shape)",
                "permute_dimensions(x, pattern)",
                "resize_images(x, height_factor, width_factor, data_format, interpolation='nearest')",
                "resize_volumes(x, depth_factor, height_factor, width_factor, data_format)",
                "repeat_elements(x, rep, axis)",
                "repeat(x, n)",
                "arange(start, stop=None, step=1, dtype='int32')",
                "tile(x, n)",
                "flatten(x)",
                "batch_flatten(x)",
                "expand_dims(x, axis=-1)",
                "squeeze(x, axis)",
                "temporal_padding(x, padding=(1, 1))",
                "spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None)",
                "spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None)",
                "stack(x, axis=0)",
                "one_hot(indices, num_classes)",
                "reverse(x, axes)",
                "slice(x, start, size)",
                "get_value(x)",
                "batch_get_value(ops)",
                "set_value(x, value)",
                "batch_set_value(tuples)",
                "get_variable_shape(x)",
                "print_tensor(x, message='')",
                "function(inputs, outputs, updates=None, **kwargs)",
                "gradients(loss, variables)",
                "stop_gradient(variables)",
                "rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)",
                "switch(condition, then_expression, else_expression)",
                "in_train_phase(x, alt, training=None)",
                "in_test_phase(x, alt, training=None)",
                "relu(x, alpha=0.0, max_value=None, threshold=0.0)",
                "elu(x, alpha=1.0)",
                "softmax(x, axis=-1)",
                "softplus(x)",
                "softsign(x)",
                "categorical_crossentropy(target, output, from_logits=False, axis=-1)",
                "sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1)",
                "binary_crossentropy(target, output, from_logits=False)",
                "sigmoid(x)",
                "hard_sigmoid(x)",
                "tanh(x)",
                "dropout(x, level, noise_shape=None, seed=None)",
                "l2_normalize(x, axis=None)",
                "in_top_k(predictions, targets, k)",
                "_preprocess_conv1d_input(x, data_format)",
                "_preprocess_conv2d_input(x, data_format, force_transpose=False)",
                "_preprocess_conv3d_input(x, data_format)",
                "_preprocess_padding(padding)",
                "conv1d(x, kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)",
                "conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "conv2d_transpose(x, kernel, output_shape, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)",
                "separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "conv3d(x, kernel, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1))",
                "conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1), padding='valid', data_format=None)",
                "pool2d(x, pool_size, strides=(1, 1), padding='valid', data_format=None, pool_mode='max')",
                "pool3d(x, pool_size, strides=(1, 1, 1), padding='valid', data_format=None, pool_mode='max')",
                "local_conv1d(inputs, kernel, kernel_size, strides, data_format=None)",
                "local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None)",
                "bias_add(x, bias, data_format=None)",
                "random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)",
                "random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None)",
                "random_binomial(shape, p=0.0, dtype=None, seed=None)",
                "truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)",
                "ctc_label_dense_to_sparse(labels, label_lengths)",
                "ctc_batch_cost(y_true, y_pred, input_length, label_length)",
                "ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1, merge_repeated=False)",
                "map_fn(fn, elems, name=None, dtype=None)",
                "foldl(fn, elems, initializer=None, name=None)",
                "foldr(fn, elems, initializer=None, name=None)",
                "symbolic_fn_wrapper(*args, **kwargs)",
                "eager_fn_wrapper(*args, **kwargs)",
                "__init__(self)",
                "_set_device(self, device)",
                "range_less_than(_, current_input)",
                "then_expression_fn()",
                "else_expression_fn()"
            ],
            "variables_in_file": {
                "py_all": [
                    25,
                    923,
                    956,
                    2366
                ],
                "all": [
                    25
                ],
                "py_any": [
                    4256,
                    4228,
                    1423,
                    4312,
                    4281,
                    26
                ],
                "any": [
                    26
                ],
                "py_sum": [
                    27
                ],
                "sum": [
                    27
                ],
                "py_slice": [
                    4066,
                    4122,
                    28,
                    4124
                ],
                "slice": [
                    28
                ],
                "_LOCAL_DEVICES": [
                    35,
                    487,
                    490,
                    492,
                    493
                ],
                "_SYMBOLIC_SCOPE": [
                    99,
                    101,
                    37,
                    38,
                    71,
                    105
                ],
                "threading.local": [
                    37
                ],
                "threading": [
                    37
                ],
                "_SYMBOLIC_SCOPE.value": [
                    99,
                    101,
                    38,
                    71,
                    105
                ],
                "_LEARNING_PHASE_CACHE": [
                    39,
                    328,
                    329,
                    332,
                    408
                ],
                "tf.__version__.startswith": [
                    43
                ],
                "tf.__version__": [
                    3497,
                    3449,
                    3474,
                    43
                ],
                "tf": [
                    3073,
                    3074,
                    3075,
                    3076,
                    2055,
                    525,
                    1040,
                    2070,
                    2584,
                    2585,
                    2588,
                    1054,
                    1567,
                    3616,
                    548,
                    2085,
                    3623,
                    2552,
                    43,
                    2553,
                    1582,
                    2607,
                    2100,
                    54,
                    2619,
                    573,
                    2112,
                    3651,
                    1092,
                    1606,
                    2633,
                    2124,
                    4391,
                    80,
                    3578,
                    3667,
                    3668,
                    4182,
                    2647,
                    1627,
                    3677,
                    2142,
                    4190,
                    2144,
                    3682,
                    3171,
                    2660,
                    3174,
                    3686,
                    3176,
                    1130,
                    4203,
                    3182,
                    1648,
                    3185,
                    2676,
                    2165,
                    3188,
                    3193,
                    634,
                    2172,
                    2173,
                    2175,
                    2176,
                    2180,
                    1669,
                    2184,
                    2186,
                    3213,
                    2704,
                    3217,
                    3728,
                    3729,
                    3730,
                    3740,
                    3233,
                    3746,
                    1189,
                    3749,
                    2219,
                    2223,
                    3247,
                    2227,
                    2228,
                    1717,
                    1718,
                    1719,
                    1720,
                    1721,
                    2229,
                    2230,
                    2232,
                    2746,
                    3261,
                    2761,
                    1743,
                    3791,
                    3797,
                    2776,
                    1763,
                    1764,
                    1765,
                    2794,
                    4338,
                    4339,
                    4340,
                    1782,
                    1783,
                    4343,
                    4346,
                    3837,
                    4351,
                    2817,
                    4353,
                    3843,
                    4355,
                    4356,
                    774,
                    4357,
                    1800,
                    1801,
                    2312,
                    2314,
                    4360,
                    4362,
                    2318,
                    4364,
                    3344,
                    4365,
                    2322,
                    4366,
                    2324,
                    2325,
                    2326,
                    1815,
                    1304,
                    2327,
                    2328,
                    2329,
                    2330,
                    2331,
                    2333,
                    1311,
                    4386,
                    4387,
                    4388,
                    1829,
                    4389,
                    1319,
                    1320,
                    1321,
                    2345,
                    3880,
                    1324,
                    4390,
                    1326,
                    3887,
                    3376,
                    1841,
                    1853,
                    2367,
                    2369,
                    3396,
                    3914,
                    331,
                    1868,
                    1869,
                    2382,
                    4427,
                    4428,
                    3411,
                    3925,
                    3926,
                    2903,
                    1881,
                    4442,
                    2396,
                    3934,
                    3938,
                    3429,
                    4461,
                    2928,
                    2929,
                    373,
                    3449,
                    2426,
                    2427,
                    3450,
                    1917,
                    3454,
                    4477,
                    2945,
                    2947,
                    3975,
                    394,
                    1931,
                    3979,
                    4493,
                    3474,
                    3475,
                    3986,
                    1943,
                    3479,
                    922,
                    414,
                    1957,
                    423,
                    3497,
                    3498,
                    430,
                    3502,
                    1460,
                    4022,
                    1464,
                    4026,
                    955,
                    1980,
                    1473,
                    4033,
                    1481,
                    2506,
                    1995,
                    1488,
                    1489,
                    1490,
                    2518,
                    2519,
                    986,
                    1499,
                    1500,
                    1501,
                    2010,
                    2522,
                    2527,
                    2529,
                    1506,
                    3035,
                    3036,
                    1509,
                    1513,
                    2025,
                    3049,
                    492,
                    1519,
                    3570,
                    1523,
                    1013,
                    1527,
                    2040,
                    1529,
                    2554,
                    3069,
                    3070,
                    3071
                ],
                "tf_keras_backend.set_floatx": [
                    216,
                    46
                ],
                "tf_keras_backend": [
                    128,
                    3329,
                    258,
                    4230,
                    4233,
                    142,
                    398,
                    277,
                    406,
                    157,
                    2847,
                    162,
                    3362,
                    4258,
                    4261,
                    2858,
                    46,
                    47,
                    48,
                    52,
                    181,
                    2868,
                    3001,
                    4283,
                    4286,
                    322,
                    197,
                    720,
                    3286,
                    216,
                    346,
                    4314,
                    4317,
                    607,
                    2911,
                    3312,
                    243,
                    377
                ],
                "floatx": [
                    4225,
                    1764,
                    1125,
                    632,
                    46,
                    719,
                    3182,
                    920,
                    1718,
                    984,
                    216,
                    953,
                    4278,
                    4309,
                    4253,
                    606,
                    1087
                ],
                "tf_keras_backend.set_epsilon": [
                    181,
                    47
                ],
                "epsilon": [
                    2273,
                    2146,
                    2337,
                    4390,
                    2345,
                    4427,
                    47,
                    2192,
                    2261,
                    2264,
                    2236,
                    2269
                ],
                "tf_keras_backend.set_image_data_format": [
                    48,
                    277
                ],
                "image_data_format": [
                    48,
                    302
                ],
                "get_graph": [
                    4257,
                    4229,
                    454,
                    72,
                    52,
                    4313,
                    4282
                ],
                "tf_keras_backend.get_graph": [
                    52
                ],
                "name_scope": [
                    330,
                    54
                ],
                "tf.name_scope": [
                    54
                ],
                "_is_tf_1": [
                    66,
                    323,
                    2307,
                    3875,
                    390,
                    488,
                    3786,
                    3565,
                    2927,
                    2832,
                    369,
                    2257,
                    886,
                    3735,
                    3832,
                    3611,
                    92,
                    2909
                ],
                "func": [
                    97,
                    67,
                    69,
                    103,
                    73,
                    75,
                    93
                ],
                "as_default": [
                    4257,
                    4229,
                    72,
                    4313,
                    4282
                ],
                "args": [
                    73,
                    75,
                    103
                ],
                "kwargs": [
                    3841,
                    3734,
                    3736,
                    3738,
                    3610,
                    3612,
                    3614,
                    3744,
                    3874,
                    3876,
                    3621,
                    3878,
                    3885,
                    73,
                    3785,
                    75,
                    3787,
                    3789,
                    3831,
                    3795,
                    2913,
                    103,
                    3564,
                    3566,
                    3568,
                    3575,
                    3833,
                    3835
                ],
                "functools.wraps": [
                    97,
                    69
                ],
                "functools": [
                    97,
                    69
                ],
                "symbolic_fn_wrapper": [
                    76
                ],
                "isinstance": [
                    2944,
                    3713,
                    3650,
                    611,
                    548,
                    803,
                    326,
                    1414,
                    2792,
                    3913,
                    2605,
                    1423,
                    80,
                    688,
                    1972,
                    1973,
                    3711
                ],
                "x": [
                    3076,
                    3077,
                    2055,
                    525,
                    1040,
                    3605,
                    2070,
                    1054,
                    1567,
                    3616,
                    3617,
                    3106,
                    3107,
                    2085,
                    3109,
                    3623,
                    2552,
                    3624,
                    3118,
                    2607,
                    3120,
                    3121,
                    2100,
                    2619,
                    2112,
                    1606,
                    3142,
                    4166,
                    2633,
                    2634,
                    3659,
                    2124,
                    4169,
                    4170,
                    80,
                    4177,
                    3578,
                    3667,
                    4178,
                    4182,
                    2647,
                    4185,
                    1627,
                    4187,
                    3677,
                    2142,
                    4190,
                    2144,
                    4193,
                    3682,
                    3171,
                    2660,
                    3683,
                    3174,
                    3686,
                    3176,
                    3687,
                    4194,
                    4201,
                    4203,
                    4204,
                    3182,
                    1648,
                    3185,
                    2676,
                    2165,
                    3188,
                    3191,
                    2168,
                    3192,
                    3193,
                    2172,
                    3196,
                    3197,
                    3198,
                    1156,
                    1669,
                    3716,
                    4228,
                    2187,
                    3213,
                    2704,
                    3217,
                    3728,
                    1683,
                    3740,
                    4256,
                    1697,
                    3233,
                    3746,
                    1189,
                    3749,
                    3751,
                    680,
                    682,
                    684,
                    2220,
                    2221,
                    3247,
                    688,
                    2224,
                    2225,
                    1717,
                    1206,
                    1718,
                    1719,
                    1720,
                    2233,
                    2746,
                    4281,
                    3261,
                    3777,
                    1220,
                    2761,
                    1743,
                    720,
                    2255,
                    1234,
                    2259,
                    3791,
                    725,
                    726,
                    727,
                    2263,
                    3797,
                    2266,
                    2267,
                    3798,
                    4312,
                    2271,
                    1249,
                    1251,
                    1252,
                    741,
                    1253,
                    1254,
                    1763,
                    1764,
                    1765,
                    2794,
                    3823,
                    243,
                    1782,
                    1783,
                    2295,
                    3837,
                    2817,
                    3843,
                    3844,
                    774,
                    1800,
                    1801,
                    3344,
                    2833,
                    2835,
                    1302,
                    1815,
                    1304,
                    2334,
                    3870,
                    800,
                    801,
                    3362,
                    803,
                    804,
                    805,
                    1829,
                    1319,
                    3880,
                    2345,
                    2858,
                    1323,
                    1324,
                    3881,
                    1326,
                    3887,
                    3376,
                    1841,
                    3888,
                    1853,
                    2366,
                    2880,
                    833,
                    2369,
                    3396,
                    1866,
                    1867,
                    1868,
                    1869,
                    2382,
                    3916,
                    3411,
                    3925,
                    2903,
                    1881,
                    2396,
                    3934,
                    864,
                    3938,
                    3939,
                    1893,
                    1390,
                    887,
                    888,
                    2425,
                    890,
                    891,
                    2426,
                    1917,
                    2430,
                    3448,
                    2432,
                    3450,
                    2434,
                    3454,
                    3457,
                    3965,
                    2439,
                    3975,
                    1931,
                    3979,
                    3473,
                    3986,
                    3475,
                    2452,
                    3987,
                    2454,
                    1943,
                    3479,
                    3482,
                    1957,
                    3496,
                    3498,
                    2475,
                    4012,
                    3502,
                    2480,
                    3505,
                    1460,
                    4022,
                    4026,
                    1980,
                    1473,
                    4033,
                    4034,
                    2502,
                    2506,
                    1995,
                    1486,
                    1490,
                    2518,
                    2519,
                    2520,
                    2010,
                    1506,
                    2532,
                    3558,
                    2025,
                    490,
                    3049,
                    3561,
                    493,
                    3570,
                    3571,
                    1013,
                    2551,
                    2040,
                    2554,
                    3579
                ],
                "tf.Tensor": [
                    80
                ],
                "hasattr": [
                    800,
                    609,
                    613,
                    684,
                    80,
                    888
                ],
                "prev_value": [
                    105,
                    99
                ],
                "context.eager_mode": [
                    889,
                    102
                ],
                "context": [
                    889,
                    102
                ],
                "out": [
                    103,
                    106,
                    1324,
                    1326,
                    1327
                ],
                "eager_fn_wrapper": [
                    107
                ],
                "tf_keras_backend.get_uid": [
                    128
                ],
                "prefix": [
                    128
                ],
                "tf_keras_backend.manual_variable_initialization": [
                    142
                ],
                "value": [
                    142,
                    2858,
                    301,
                    302,
                    303,
                    307,
                    1092,
                    1094,
                    346,
                    608,
                    609,
                    610,
                    611,
                    612,
                    613,
                    614,
                    1253,
                    1130,
                    1132,
                    634
                ],
                "tf_keras_backend.epsilon": [
                    157
                ],
                "tf_keras_backend.reset_uids": [
                    162
                ],
                "e": [
                    181
                ],
                "tf_keras_backend.floatx": [
                    197
                ],
                "tf_keras_backend.cast_to_floatx": [
                    243
                ],
                "tf_keras_backend.image_data_format": [
                    258
                ],
                "data_format": [
                    3842,
                    3716,
                    2438,
                    2697,
                    2703,
                    4112,
                    3985,
                    3603,
                    2452,
                    277,
                    3477,
                    3605,
                    3868,
                    3870,
                    4126,
                    3748,
                    3622,
                    2474,
                    4010,
                    3500,
                    4012,
                    3886,
                    303,
                    304,
                    2479,
                    2735,
                    4138,
                    308,
                    2485,
                    2744,
                    3775,
                    4032,
                    3649,
                    3777,
                    4164,
                    3654,
                    3912,
                    3659,
                    3916,
                    3661,
                    3918,
                    4175,
                    4179,
                    3796,
                    4058,
                    3548,
                    4188,
                    3552,
                    3937,
                    3685,
                    4199,
                    3561,
                    3821,
                    3965,
                    3823,
                    2420,
                    3577,
                    3963,
                    3452,
                    2429,
                    3710
                ],
                "value.lower": [
                    303
                ],
                "ValueError": [
                    1408,
                    3553,
                    2436,
                    3525,
                    806,
                    4167,
                    681,
                    1449,
                    3983,
                    1424,
                    305,
                    1397,
                    2485,
                    3062,
                    475,
                    4030,
                    1439
                ],
                "str": [
                    1411,
                    1412,
                    3525,
                    682,
                    1450,
                    1451,
                    3983,
                    1426,
                    307,
                    2485,
                    1400,
                    1401,
                    3066,
                    3065,
                    4030
                ],
                "lp": [
                    322,
                    324,
                    326,
                    327,
                    328,
                    329,
                    331,
                    332
                ],
                "tf_keras_backend.learning_phase": [
                    322
                ],
                "int": [
                    3713,
                    1414,
                    326,
                    2792,
                    2605,
                    1972,
                    1973,
                    3711
                ],
                "int_lp": [
                    331,
                    332,
                    333
                ],
                "tf.cast": [
                    1800,
                    4364,
                    4365,
                    3475,
                    2325,
                    2327,
                    2329,
                    2331,
                    4386,
                    4387,
                    4388,
                    1189,
                    3498,
                    2228,
                    1718,
                    2230,
                    331,
                    4428,
                    3450,
                    3036,
                    1764,
                    3182,
                    1782,
                    4346
                ],
                "symbolic": [
                    514,
                    1159,
                    3080,
                    1043,
                    551,
                    1195,
                    691,
                    3124,
                    311,
                    1209,
                    1223,
                    3015,
                    336,
                    1237,
                    730,
                    989,
                    2916,
                    2932,
                    1016
                ],
                "tf_keras_backend.set_learning_phase": [
                    346
                ],
                "RuntimeError": [
                    370,
                    395,
                    374,
                    391
                ],
                "tf.executing_eagerly": [
                    394,
                    373
                ],
                "tf_keras_backend.get_session": [
                    377
                ],
                "tf_keras_backend.set_session": [
                    398
                ],
                "session": [
                    422,
                    430,
                    398,
                    412,
                    413
                ],
                "tf_keras_backend.clear_session": [
                    406
                ],
                "get_session": [
                    489,
                    412,
                    2833,
                    887
                ],
                "session.graph.as_default": [
                    413
                ],
                "session.graph": [
                    413
                ],
                "variables": [
                    416,
                    2944,
                    2945,
                    2947,
                    2928,
                    2929,
                    414
                ],
                "tf.global_variables": [
                    414
                ],
                "candidate_vars": [
                    418,
                    419,
                    423,
                    425,
                    415
                ],
                "v": [
                    922,
                    923,
                    924,
                    925,
                    416,
                    417,
                    418,
                    423,
                    425,
                    427,
                    428,
                    955,
                    956,
                    957,
                    958,
                    607,
                    610,
                    612,
                    614,
                    615,
                    616
                ],
                "getattr": [
                    417
                ],
                "candidate_vars.append": [
                    418
                ],
                "is_initialized": [
                    425,
                    422
                ],
                "session.run": [
                    430,
                    422
                ],
                "tf.is_variable_initialized": [
                    423
                ],
                "uninitialized_vars": [
                    424,
                    427,
                    429,
                    430
                ],
                "flag": [
                    425,
                    426
                ],
                "zip": [
                    1304,
                    425,
                    1311
                ],
                "uninitialized_vars.append": [
                    427
                ],
                "v._keras_initialized": [
                    428
                ],
                "tf.variables_initializer": [
                    430
                ],
                "object": [
                    435
                ],
                "self.device": [
                    443,
                    439
                ],
                "self": [
                    443,
                    439
                ],
                "device": [
                    443,
                    476,
                    477
                ],
                "g": [
                    456,
                    454
                ],
                "op": [
                    456,
                    457,
                    455
                ],
                "_TfDeviceCaptureOp": [
                    455
                ],
                "g._apply_device_functions": [
                    456
                ],
                "op.device": [
                    457
                ],
                "device_type": [
                    473,
                    474,
                    477
                ],
                "device_type.lower": [
                    473
                ],
                "_get_current_tf_device": [
                    476
                ],
                "device.device_type.lower": [
                    477
                ],
                "device.device_type": [
                    477
                ],
                "devices": [
                    489,
                    490
                ],
                "list_devices": [
                    489
                ],
                "x.name": [
                    490
                ],
                "tf.config.experimental_list_devices": [
                    492
                ],
                "tf.config": [
                    492
                ],
                "x.lower": [
                    493
                ],
                "explicitly_on_cpu": [
                    507,
                    509
                ],
                "_is_current_explicit_device": [
                    507
                ],
                "gpus_available": [
                    508,
                    509
                ],
                "len": [
                    4195,
                    2694,
                    2695,
                    2696,
                    4166,
                    4169,
                    2731,
                    2732,
                    2733,
                    2734,
                    4171,
                    1393,
                    1394,
                    2674,
                    4180,
                    2520,
                    508,
                    4189
                ],
                "_get_available_gpus": [
                    508
                ],
                "tf.convert_to_tensor": [
                    525
                ],
                "dtype": [
                    4224,
                    4225,
                    4231,
                    4234,
                    525,
                    1040,
                    3473,
                    919,
                    920,
                    922,
                    924,
                    2589,
                    2590,
                    4252,
                    4253,
                    4259,
                    1189,
                    4262,
                    3496,
                    4277,
                    4278,
                    952,
                    953,
                    955,
                    4284,
                    957,
                    1086,
                    1087,
                    4287,
                    3448,
                    1093,
                    1094,
                    718,
                    719,
                    721,
                    4308,
                    4309,
                    983,
                    984,
                    986,
                    4315,
                    605,
                    606,
                    4318,
                    608,
                    1124,
                    1125,
                    1131,
                    1132,
                    4461,
                    1013,
                    631,
                    632,
                    634
                ],
                "tensor": [
                    548,
                    573,
                    572,
                    575
                ],
                "tf.SparseTensor": [
                    548,
                    4366
                ],
                "is_sparse": [
                    1323,
                    572,
                    2366
                ],
                "tf.sparse.to_dense": [
                    4442,
                    573
                ],
                "tf.sparse": [
                    4442,
                    1324,
                    573,
                    2367
                ],
                "tf_keras_backend.variable": [
                    607
                ],
                "name": [
                    608,
                    986,
                    1094,
                    1132,
                    4461,
                    4493,
                    1040,
                    721,
                    1013,
                    4477,
                    634,
                    955,
                    924,
                    957,
                    1054,
                    922
                ],
                "constraint": [
                    608
                ],
                "v._keras_shape": [
                    610,
                    612,
                    614
                ],
                "shape": [
                    4228,
                    4231,
                    4234,
                    922,
                    4256,
                    4259,
                    4262,
                    4281,
                    955,
                    4284,
                    4287,
                    1093,
                    2633,
                    1486,
                    2382,
                    721,
                    722,
                    724,
                    725,
                    4312,
                    1497,
                    4315,
                    4318,
                    610,
                    1131,
                    634
                ],
                "value.tocoo": [
                    610
                ],
                "np.ndarray": [
                    611
                ],
                "np": [
                    1090,
                    611,
                    1156,
                    3395,
                    4227,
                    1128,
                    1867,
                    2427,
                    4311,
                    2520,
                    1977,
                    1979,
                    2525,
                    4280,
                    4255
                ],
                "value.shape": [
                    612
                ],
                "int_shape": [
                    2880,
                    1156,
                    4165,
                    614,
                    1390,
                    1391,
                    4116,
                    1304,
                    2425,
                    4061,
                    1311
                ],
                "v._uses_learning_phase": [
                    615
                ],
                "tf_ops.init_scope": [
                    921,
                    1091,
                    4260,
                    4232,
                    1129,
                    4316,
                    633,
                    954,
                    985,
                    4285
                ],
                "tf_ops": [
                    954,
                    1249,
                    1250,
                    1091,
                    4260,
                    4232,
                    1129,
                    688,
                    4316,
                    633,
                    921,
                    985,
                    4285
                ],
                "tf.constant": [
                    2219,
                    2223,
                    2585,
                    634,
                    2427,
                    2527
                ],
                "is_tensor": [
                    680
                ],
                "type": [
                    682
                ],
                "tf_ops._TensorLike": [
                    688
                ],
                "tf_ops.is_dense_tensor_like": [
                    688
                ],
                "tf_keras_backend.placeholder": [
                    720
                ],
                "ndim": [
                    2311,
                    2313,
                    2317,
                    2321,
                    1302,
                    2295,
                    1317,
                    2361,
                    4166,
                    4169,
                    4170,
                    2255,
                    721,
                    4178,
                    723,
                    724,
                    2266,
                    3037,
                    4194,
                    3060,
                    2551,
                    2168
                ],
                "sparse": [
                    721
                ],
                "tuple": [
                    2944,
                    3650,
                    803,
                    1316,
                    805,
                    2534,
                    3913,
                    1423,
                    3667,
                    724,
                    3925,
                    1309
                ],
                "_": [
                    4117,
                    2508,
                    724,
                    2333
                ],
                "range": [
                    4065,
                    1477,
                    1317,
                    1478,
                    2508,
                    724,
                    2168,
                    4120,
                    2266,
                    4121,
                    1469,
                    1470
                ],
                "x._keras_shape": [
                    801,
                    725
                ],
                "x._uses_learning_phase": [
                    3120,
                    726
                ],
                "x.op.type": [
                    741
                ],
                "x.op": [
                    741
                ],
                "AttributeError": [
                    742
                ],
                "tf.shape": [
                    1509,
                    774,
                    4338,
                    3667,
                    3925,
                    2518,
                    1304,
                    2426,
                    3071,
                    2172,
                    3069,
                    1311
                ],
                "x.shape": [
                    833,
                    803,
                    804,
                    805,
                    2502,
                    2532,
                    2221,
                    2225,
                    2520
                ],
                "x.shape.as_list": [
                    2532,
                    805,
                    2502
                ],
                "x.shape.rank": [
                    833
                ],
                "x.dtype.base_dtype.name": [
                    864
                ],
                "x.dtype.base_dtype": [
                    864,
                    1251,
                    1252,
                    1763,
                    1866,
                    1867,
                    1717,
                    3191,
                    3192,
                    3196
                ],
                "x.dtype": [
                    864,
                    1251,
                    1252,
                    1253,
                    1763,
                    1866,
                    1867,
                    2220,
                    2224,
                    1717,
                    3191,
                    3192,
                    3196
                ],
                "eval": [
                    887
                ],
                "to_dense": [
                    2369,
                    887
                ],
                "x.numpy": [
                    890,
                    2835
                ],
                "eval_fn": [
                    891,
                    892
                ],
                "function": [
                    891
                ],
                "tf.zeros": [
                    922
                ],
                "v.shape.as_list": [
                    923,
                    956
                ],
                "v.shape": [
                    923,
                    956
                ],
                "variable": [
                    1094,
                    1132,
                    986,
                    924,
                    957
                ],
                "tf.ones": [
                    955
                ],
                "tf.eye": [
                    986
                ],
                "size": [
                    2817,
                    986
                ],
                "tf.zeros_like": [
                    1013
                ],
                "tf.ones_like": [
                    1040,
                    3073
                ],
                "tf.identity": [
                    1054
                ],
                "seed": [
                    4226,
                    4227,
                    4231,
                    4234,
                    4254,
                    4255,
                    4259,
                    4262,
                    4279,
                    4280,
                    4284,
                    4287,
                    1088,
                    1090,
                    3394,
                    3395,
                    1093,
                    3396,
                    4310,
                    4311,
                    4315,
                    4318,
                    1126,
                    1128,
                    1131
                ],
                "np.random.randint": [
                    1090,
                    3395,
                    4227,
                    1128,
                    4311,
                    4280,
                    4255
                ],
                "np.random": [
                    1090,
                    3395,
                    4227,
                    1128,
                    4311,
                    4280,
                    4255
                ],
                "tf.random_uniform_initializer": [
                    1092
                ],
                "low": [
                    1093
                ],
                "high": [
                    1093
                ],
                "tf.random_normal_initializer": [
                    1130
                ],
                "mean": [
                    4228,
                    2311,
                    2312,
                    4231,
                    4234,
                    2316,
                    2320,
                    2193,
                    2328,
                    2329,
                    2338,
                    2345,
                    4312,
                    4315,
                    2142,
                    4318,
                    2144,
                    2147,
                    1131,
                    2165,
                    2175
                ],
                "scale": [
                    1131
                ],
                "np.prod": [
                    1156
                ],
                "tf_state_ops.assign": [
                    1206
                ],
                "tf_state_ops": [
                    1254,
                    1234,
                    1220,
                    1206
                ],
                "new_x": [
                    1206
                ],
                "tf_state_ops.assign_add": [
                    1220
                ],
                "increment": [
                    1220
                ],
                "tf_state_ops.assign_sub": [
                    1234,
                    1254
                ],
                "decrement": [
                    1234
                ],
                "tf_ops.colocate_with": [
                    1249
                ],
                "decay": [
                    1250,
                    1251,
                    1252,
                    1253
                ],
                "tf_ops.convert_to_tensor": [
                    1250
                ],
                "momentum": [
                    1250
                ],
                "decay.dtype": [
                    1251
                ],
                "tf_math_ops.cast": [
                    1252,
                    1253
                ],
                "tf_math_ops": [
                    1697,
                    1252,
                    1893,
                    1253,
                    4390,
                    4427,
                    1683
                ],
                "update_delta": [
                    1253,
                    1254
                ],
                "y": [
                    2055,
                    1302,
                    2070,
                    2333,
                    1311,
                    1317,
                    2085,
                    2343,
                    1320,
                    1324,
                    1326,
                    2100,
                    1464,
                    1481,
                    1995,
                    1497,
                    2010,
                    1501,
                    1506,
                    2025,
                    1391,
                    2040
                ],
                "x_shape": [
                    1411,
                    1303,
                    1306,
                    1308,
                    1309,
                    1445,
                    1319,
                    1322,
                    1450,
                    2502,
                    2504,
                    2506,
                    1486,
                    1487,
                    1489,
                    2518,
                    2528,
                    2529,
                    2532,
                    2533,
                    2534,
                    1390,
                    1393,
                    1400,
                    1403
                ],
                "i": [
                    1312,
                    1313,
                    4065,
                    4066,
                    4067,
                    4120,
                    1478,
                    1479,
                    4122,
                    1471,
                    1304,
                    1305,
                    1306,
                    4123,
                    1470,
                    1311
                ],
                "s": [
                    1315,
                    2508,
                    1304,
                    1308,
                    1311
                ],
                "tf.unstack": [
                    1304,
                    1311
                ],
                "x_shape.append": [
                    1306,
                    1308
                ],
                "y_shape": [
                    1313,
                    1315,
                    1316,
                    1412,
                    1446,
                    1320,
                    1322,
                    1450,
                    1497,
                    1391,
                    1394,
                    1401,
                    1498,
                    1500,
                    1404,
                    1310
                ],
                "y_shape.append": [
                    1313,
                    1315
                ],
                "y_permute_dim": [
                    1320,
                    1317,
                    1318
                ],
                "list": [
                    2944,
                    4228,
                    2700,
                    2701,
                    1423,
                    1429,
                    4256,
                    2211,
                    1317,
                    4281,
                    1469,
                    3650,
                    1477,
                    3913,
                    2256,
                    2258,
                    3668,
                    3926,
                    4312,
                    2266
                ],
                "y_permute_dim.pop": [
                    1318
                ],
                "xt": [
                    1321,
                    1319
                ],
                "tf.reshape": [
                    2176,
                    4351,
                    2180,
                    4356,
                    2184,
                    2312,
                    2314,
                    4360,
                    2318,
                    2322,
                    1319,
                    1320,
                    1321,
                    2619,
                    2633,
                    2382,
                    1490,
                    1501,
                    2529,
                    1523,
                    3070,
                    2175
                ],
                "yt": [
                    1320,
                    1321
                ],
                "tf.transpose": [
                    3843,
                    4356,
                    4360,
                    3986,
                    3479,
                    1567,
                    3749,
                    4390,
                    3623,
                    1320,
                    3502,
                    3887,
                    1473,
                    4033,
                    1481,
                    4427,
                    3797,
                    2396,
                    3938,
                    3686,
                    3578,
                    3454
                ],
                "tf.matmul": [
                    1321,
                    1506,
                    1326
                ],
                "tf.sparse.sparse_dense_matmul": [
                    1324
                ],
                "x_ndim": [
                    1419,
                    1484,
                    1421,
                    1455,
                    1393,
                    1459,
                    1396,
                    1462,
                    1433,
                    1468,
                    1469,
                    1470
                ],
                "y_ndim": [
                    1495,
                    1477,
                    1418,
                    1419,
                    1421,
                    1456,
                    1394,
                    1396,
                    1463,
                    1465,
                    1435
                ],
                "x_batch_size": [
                    1403,
                    1406,
                    1407
                ],
                "y_batch_size": [
                    1404,
                    1406,
                    1407
                ],
                "axes": [
                    1414,
                    1415,
                    1417,
                    1419,
                    1421,
                    1423,
                    1426,
                    1429,
                    1432,
                    1433,
                    1434,
                    1435,
                    1438,
                    1444,
                    1451,
                    1452,
                    2792,
                    2793,
                    2794
                ],
                "a": [
                    1957,
                    1423
                ],
                "a0": [
                    1472,
                    1444,
                    1445,
                    1461,
                    1468,
                    1470
                ],
                "a1": [
                    1476,
                    1444,
                    1478,
                    1446,
                    1480
                ],
                "d1": [
                    1448,
                    1452,
                    1445
                ],
                "d2": [
                    1448,
                    1452,
                    1446
                ],
                "orig_x_ndim": [
                    1526,
                    1455
                ],
                "orig_y_ndim": [
                    1456,
                    1528
                ],
                "tf.expand_dims": [
                    2519,
                    4391,
                    2552,
                    3728,
                    3729,
                    3730,
                    1460,
                    2647,
                    1464,
                    4343
                ],
                "pattern": [
                    2699,
                    2703,
                    2704,
                    2737,
                    2744,
                    2746,
                    1469,
                    1471,
                    1472,
                    1473,
                    1477,
                    1479,
                    1480,
                    1481,
                    2396,
                    2675,
                    2676,
                    2553,
                    2554
                ],
                "x_mid_dims": [
                    1488,
                    1514,
                    1487
                ],
                "x_squashed_dim": [
                    1488,
                    1489
                ],
                "tf.reduce_prod": [
                    1488,
                    1499,
                    1669
                ],
                "x_squashed_shape": [
                    1489,
                    1490
                ],
                "tf.stack": [
                    3651,
                    2633,
                    2761,
                    3914,
                    1489,
                    4339,
                    3668,
                    4340,
                    3926,
                    2553,
                    1500,
                    2173
                ],
                "x_squashed": [
                    1512,
                    1491,
                    1493
                ],
                "y_trail_dims": [
                    1498,
                    1499,
                    1519
                ],
                "y_squashed_dim": [
                    1499,
                    1500
                ],
                "y_squashed_shape": [
                    1500,
                    1501
                ],
                "y_squashed": [
                    1504,
                    1518,
                    1502
                ],
                "result": [
                    1506,
                    1509,
                    1523,
                    1527,
                    1529,
                    1531,
                    2588,
                    2590,
                    2591
                ],
                "output_shape": [
                    2451,
                    2452,
                    4115,
                    3650,
                    3651,
                    3913,
                    3914,
                    3662,
                    3663,
                    3664,
                    3665,
                    3666,
                    3667,
                    3668,
                    3919,
                    3920,
                    3921,
                    3922,
                    3923,
                    3924,
                    3925,
                    3926,
                    3677,
                    3934,
                    3683,
                    1509,
                    1513,
                    1515,
                    1519,
                    1523
                ],
                "do_reshape": [
                    1520,
                    1522,
                    1516,
                    1510
                ],
                "tf.concat": [
                    2369,
                    1513,
                    3069,
                    1519
                ],
                "tf.squeeze": [
                    3746,
                    4386,
                    2660,
                    4387,
                    1527,
                    1529
                ],
                "tf.nn.embedding_lookup": [
                    1582
                ],
                "tf.nn": [
                    3975,
                    2186,
                    3979,
                    3213,
                    3344,
                    3740,
                    2333,
                    3616,
                    3233,
                    3880,
                    2345,
                    1582,
                    3247,
                    3376,
                    4022,
                    2232,
                    4026,
                    3261,
                    3396,
                    3791,
                    3411,
                    4182,
                    3677,
                    2142,
                    3934,
                    2144,
                    4190,
                    3682,
                    3171,
                    3429,
                    3174,
                    3176,
                    4203,
                    3185,
                    3570,
                    3188,
                    2165,
                    3837
                ],
                "reference": [
                    1582
                ],
                "indices": [
                    4359,
                    4360,
                    4362,
                    4364,
                    1582,
                    4366,
                    2776
                ],
                "tf.reduce_max": [
                    1606
                ],
                "axis": [
                    1669,
                    1801,
                    1683,
                    1815,
                    1697,
                    2169,
                    3233,
                    1829,
                    1719,
                    2360,
                    1722,
                    2363,
                    2365,
                    2367,
                    2369,
                    1606,
                    2504,
                    2761,
                    2506,
                    2509,
                    1743,
                    3411,
                    2517,
                    2647,
                    3287,
                    1627,
                    2526,
                    2660,
                    1765,
                    1648,
                    3313,
                    1783,
                    2168,
                    2297,
                    2299,
                    2172,
                    1917
                ],
                "keepdims": [
                    1669,
                    1606,
                    1765,
                    1801,
                    1743,
                    1648,
                    1783,
                    1627,
                    1723,
                    1917
                ],
                "tf.reduce_min": [
                    1627
                ],
                "tf.reduce_sum": [
                    1648
                ],
                "tf_math_ops.cumsum": [
                    1683
                ],
                "tf_math_ops.cumprod": [
                    1697
                ],
                "tf.bool": [
                    1763,
                    1800,
                    1717,
                    1782,
                    4346,
                    3035
                ],
                "m": [
                    1720,
                    1719
                ],
                "tf.reduce_mean": [
                    1721,
                    1765,
                    1719
                ],
                "devs_squared": [
                    1720,
                    1721
                ],
                "tf.square": [
                    1720,
                    1841
                ],
                "tf.sqrt": [
                    1869,
                    1743
                ],
                "var": [
                    2144,
                    2176,
                    2147,
                    2339,
                    2313,
                    2314,
                    2345,
                    1743,
                    2193,
                    2165,
                    2330,
                    2331,
                    2142
                ],
                "tf.reduce_any": [
                    1783
                ],
                "tf.reduce_all": [
                    1801
                ],
                "tf.argmax": [
                    1815
                ],
                "tf.argmin": [
                    1829
                ],
                "tf.abs": [
                    1853
                ],
                "zero": [
                    3192,
                    3193,
                    1866,
                    1868
                ],
                "_to_tensor": [
                    1866,
                    1867,
                    3191,
                    3192,
                    3196
                ],
                "inf": [
                    1867,
                    1868
                ],
                "np.inf": [
                    1977,
                    1867,
                    1979
                ],
                "tf.clip_by_value": [
                    3193,
                    1980,
                    1868
                ],
                "tf.exp": [
                    1881
                ],
                "tf_math_ops.log": [
                    4427,
                    1893,
                    4390
                ],
                "tf.reduce_logsumexp": [
                    1917
                ],
                "tf.round": [
                    1931
                ],
                "tf.sign": [
                    1943
                ],
                "tf.pow": [
                    1957
                ],
                "min_value": [
                    1972,
                    1974,
                    1975,
                    1976,
                    1977,
                    1980
                ],
                "float": [
                    1972,
                    1973
                ],
                "max_value": [
                    3170,
                    3178,
                    3183,
                    3191,
                    1973,
                    1974,
                    1975,
                    3193,
                    1978,
                    1979,
                    1980
                ],
                "tf.equal": [
                    1995
                ],
                "tf.not_equal": [
                    2010
                ],
                "tf.greater": [
                    2025,
                    3182
                ],
                "tf.greater_equal": [
                    2040
                ],
                "tf.less": [
                    2055
                ],
                "tf.less_equal": [
                    2070
                ],
                "tf.maximum": [
                    2085
                ],
                "tf.minimum": [
                    2100
                ],
                "tf.sin": [
                    2112
                ],
                "tf.cos": [
                    2124
                ],
                "tf.nn.moments": [
                    2165,
                    2142
                ],
                "reduction_axes": [
                    2272,
                    2211,
                    2256,
                    2258,
                    2260,
                    2165,
                    2263,
                    2169,
                    2266,
                    2268,
                    2142
                ],
                "normed": [
                    2144,
                    2193,
                    2186,
                    2147
                ],
                "tf.nn.batch_normalization": [
                    2144,
                    2345,
                    2186
                ],
                "beta": [
                    2181,
                    2184,
                    2315,
                    2316,
                    2317,
                    2318,
                    2326,
                    2327,
                    2336,
                    2345,
                    2222,
                    2223,
                    2229,
                    2230,
                    2235,
                    2259,
                    2263,
                    2267,
                    2271,
                    2145
                ],
                "gamma": [
                    2177,
                    2180,
                    2319,
                    2320,
                    2321,
                    2322,
                    2324,
                    2325,
                    2335,
                    2345,
                    2218,
                    2219,
                    2227,
                    2228,
                    2234,
                    2259,
                    2263,
                    2267,
                    2271,
                    2145
                ],
                "target_shape": [
                    2176,
                    2180,
                    2184,
                    2167,
                    2170,
                    2172,
                    2173,
                    2175
                ],
                "target_shape.append": [
                    2170,
                    2172
                ],
                "broadcast_mean": [
                    2188,
                    2175
                ],
                "broadcast_var": [
                    2176,
                    2189
                ],
                "broadcast_gamma": [
                    2178,
                    2180,
                    2191
                ],
                "broadcast_beta": [
                    2184,
                    2190,
                    2182
                ],
                "normalization_axis": [
                    2225,
                    2212,
                    2221,
                    2215
                ],
                "tf_data_format": [
                    2304,
                    2305,
                    3456,
                    3457,
                    3716,
                    3717,
                    3718,
                    3840,
                    3720,
                    3842,
                    3722,
                    3977,
                    3981,
                    3985,
                    3476,
                    3605,
                    3481,
                    3482,
                    3870,
                    3743,
                    2340,
                    2213,
                    3620,
                    3622,
                    2216,
                    3748,
                    3499,
                    3884,
                    4012,
                    3886,
                    4014,
                    3504,
                    3505,
                    4024,
                    4028,
                    2237,
                    4032,
                    3777,
                    3779,
                    3659,
                    3916,
                    3661,
                    3918,
                    3794,
                    3796,
                    3671,
                    3929,
                    3679,
                    3936,
                    3937,
                    3685,
                    3561,
                    3823,
                    3825,
                    3574,
                    3577,
                    2298,
                    3451,
                    2300,
                    3965,
                    2302,
                    3967
                ],
                "gamma.dtype": [
                    2227,
                    2324
                ],
                "tf.float32": [
                    2328,
                    2329,
                    2227,
                    2324,
                    2228,
                    2229,
                    2230,
                    2325,
                    2326,
                    2330,
                    2331,
                    2327
                ],
                "beta.dtype": [
                    2229,
                    2326
                ],
                "tf.nn.fused_batch_norm": [
                    2232,
                    2333
                ],
                "_has_nchw_support": [
                    2306,
                    3501,
                    2258,
                    4181,
                    3478,
                    3453
                ],
                "_broadcast_normalize_batch_in_training": [
                    2259,
                    2271
                ],
                "_fused_normalize_batch_in_training": [
                    2262
                ],
                "sorted": [
                    2266
                ],
                "_regular_normalize_batch_in_training": [
                    2267
                ],
                "zeros_like": [
                    2316
                ],
                "ones_like": [
                    2320
                ],
                "mean.dtype": [
                    2328
                ],
                "var.dtype": [
                    2330
                ],
                "rank": [
                    2361,
                    2362,
                    2363
                ],
                "tensors": [
                    2369,
                    2361,
                    2366,
                    2367
                ],
                "tf.sparse.concat": [
                    2367
                ],
                "rows": [
                    2441,
                    2444,
                    2421,
                    2423,
                    2426
                ],
                "cols": [
                    2446,
                    2449,
                    2421,
                    2423,
                    2426
                ],
                "original_shape": [
                    2441,
                    2444,
                    2446,
                    2449,
                    2425
                ],
                "new_shape": [
                    2432,
                    2434,
                    4196,
                    4198,
                    4199,
                    4201,
                    4172,
                    4174,
                    4175,
                    4177,
                    2426,
                    2427
                ],
                "np.array": [
                    2427
                ],
                "height_factor": [
                    2481,
                    2476,
                    2427,
                    2444
                ],
                "width_factor": [
                    2449,
                    2482,
                    2427,
                    2477
                ],
                "permute_dimensions": [
                    2439,
                    4073,
                    4139,
                    4141,
                    2430
                ],
                "interpolation": [
                    2433,
                    2431
                ],
                "tf_image_ops.resize_nearest_neighbor": [
                    2432
                ],
                "tf_image_ops": [
                    2432,
                    2434
                ],
                "tf_image_ops.resize_bilinear": [
                    2434
                ],
                "new_height": [
                    2442,
                    2451,
                    2444
                ],
                "new_width": [
                    2449,
                    2451,
                    2447
                ],
                "x.set_shape": [
                    2452
                ],
                "transpose_shape": [
                    4199,
                    4175,
                    2703,
                    2452,
                    2744
                ],
                "output": [
                    3330,
                    4134,
                    4135,
                    4072,
                    4073,
                    2475,
                    2476,
                    2477,
                    2478,
                    4139,
                    2480,
                    2481,
                    2482,
                    2483,
                    3313,
                    4141,
                    4142,
                    3287
                ],
                "repeat_elements": [
                    2475,
                    2476,
                    2477,
                    2480,
                    2481,
                    2482
                ],
                "depth_factor": [
                    2480,
                    2475
                ],
                "splits": [
                    2506,
                    2508
                ],
                "tf.split": [
                    2506
                ],
                "x_rep": [
                    2529,
                    2533,
                    2534,
                    2535,
                    2508,
                    2509,
                    2519,
                    2522
                ],
                "rep": [
                    2521,
                    2508,
                    2526
                ],
                "concatenate": [
                    4133,
                    2509,
                    4070,
                    4359
                ],
                "auxiliary_axis": [
                    2525,
                    2521,
                    2517,
                    2519
                ],
                "reps": [
                    2528,
                    2520,
                    2521,
                    2522,
                    2525,
                    2526,
                    2527
                ],
                "np.ones": [
                    2520
                ],
                "tf.tile": [
                    3075,
                    4355,
                    2607,
                    2554,
                    2522,
                    4351
                ],
                "np.delete": [
                    2525
                ],
                "x_rep.set_shape": [
                    2533
                ],
                "x_rep._keras_shape": [
                    2534
                ],
                "n": [
                    2553,
                    2605,
                    2606,
                    2607
                ],
                "stop": [
                    2578,
                    2588
                ],
                "start": [
                    2817,
                    2580,
                    2581,
                    2584,
                    2585,
                    2586,
                    2588
                ],
                "TypeError": [
                    2582
                ],
                "tf.cond": [
                    2584,
                    3049
                ],
                "start.dtype": [
                    2585
                ],
                "tf.range": [
                    4355,
                    4351,
                    2588,
                    4343
                ],
                "step": [
                    2588
                ],
                "cast": [
                    2590
                ],
                "prod": [
                    2633
                ],
                "padding": [
                    2694,
                    2695,
                    2696,
                    3721,
                    3976,
                    2700,
                    2701,
                    3980,
                    3607,
                    3742,
                    3871,
                    3619,
                    2731,
                    2732,
                    2733,
                    2734,
                    3883,
                    4013,
                    2739,
                    2740,
                    2741,
                    4023,
                    4027,
                    3520,
                    3521,
                    3522,
                    3523,
                    3778,
                    3525,
                    3526,
                    3793,
                    3670,
                    3928,
                    3678,
                    3551,
                    3935,
                    3683,
                    3559,
                    3560,
                    3824,
                    2674,
                    2675,
                    3573,
                    3966,
                    3839
                ],
                "tf.pad": [
                    2704,
                    2746,
                    2676
                ],
                "normalize_data_format": [
                    3649,
                    4164,
                    3868,
                    3912,
                    2697,
                    4010,
                    3821,
                    2735,
                    4112,
                    3603,
                    4058,
                    3963,
                    3548,
                    3710,
                    3775
                ],
                "tf.one_hot": [
                    2776
                ],
                "num_classes": [
                    2776
                ],
                "tf.reverse": [
                    2794
                ],
                "tf.slice": [
                    2817
                ],
                "x.eval": [
                    2833
                ],
                "tf_keras_backend.batch_get_value": [
                    2847
                ],
                "ops": [
                    2847
                ],
                "tf_keras_backend.set_value": [
                    2858
                ],
                "tf_keras_backend.batch_set_value": [
                    2868
                ],
                "tuples": [
                    2868
                ],
                "tf.Print": [
                    2903
                ],
                "message": [
                    2903
                ],
                "v1_variable_initialization": [
                    2910
                ],
                "tf_keras_backend.function": [
                    2911
                ],
                "inputs": [
                    4130,
                    4068,
                    2911,
                    3002,
                    4127
                ],
                "outputs": [
                    3001,
                    3012,
                    2911
                ],
                "updates": [
                    2912
                ],
                "tf.gradients": [
                    2928,
                    2929
                ],
                "loss": [
                    2928,
                    2929
                ],
                "map": [
                    2945
                ],
                "tf.stop_gradient": [
                    2945,
                    2947
                ],
                "last_output": [
                    3009,
                    3010,
                    3011,
                    3012,
                    3001
                ],
                "new_states": [
                    3001,
                    3012
                ],
                "tf_keras_backend.rnn": [
                    3001
                ],
                "step_function": [
                    3002
                ],
                "initial_states": [
                    3002
                ],
                "go_backwards": [
                    3003
                ],
                "mask": [
                    3004
                ],
                "constants": [
                    3005
                ],
                "unroll": [
                    3006
                ],
                "input_length": [
                    4387,
                    4393,
                    4428,
                    4433,
                    4437,
                    3007
                ],
                "reachable": [
                    3008,
                    3010
                ],
                "tf_utils.get_reachable_from_inputs": [
                    3008
                ],
                "tf_utils": [
                    3008
                ],
                "learning_phase": [
                    3008,
                    3100
                ],
                "last_output._uses_learning_phase": [
                    3011
                ],
                "condition.dtype": [
                    3035
                ],
                "condition": [
                    3075,
                    3076,
                    3049,
                    3037,
                    3035,
                    3036,
                    3069,
                    3070
                ],
                "cond_ndim": [
                    3061,
                    3065,
                    3067,
                    3068,
                    3037,
                    3038
                ],
                "callable": [
                    3106,
                    3044,
                    3112,
                    3056,
                    3058,
                    3039
                ],
                "then_expression": [
                    3041,
                    3043,
                    3076,
                    3056,
                    3057,
                    3060,
                    3071,
                    3039
                ],
                "then_expression_fn": [
                    3050,
                    3043
                ],
                "else_expression": [
                    3044,
                    3076,
                    3046,
                    3048,
                    3058,
                    3059
                ],
                "else_expression_fn": [
                    3048,
                    3051
                ],
                "expr_ndim": [
                    3066,
                    3060,
                    3061,
                    3068
                ],
                "ndim_diff": [
                    3068,
                    3069
                ],
                "cond_shape": [
                    3072,
                    3069,
                    3070
                ],
                "expr_shape": [
                    3072,
                    3073,
                    3074,
                    3071
                ],
                "shape_diff": [
                    3072,
                    3074
                ],
                "zero_expr_shape": [
                    3073,
                    3074
                ],
                "tile_shape": [
                    3074,
                    3075
                ],
                "tf.where": [
                    3217,
                    3074,
                    3076
                ],
                "training": [
                    3105,
                    3142,
                    3111,
                    3118,
                    3099,
                    3100
                ],
                "uses_learning_phase": [
                    3119,
                    3101,
                    3103
                ],
                "alt": [
                    3142,
                    3112,
                    3113,
                    3115,
                    3118
                ],
                "switch": [
                    3118
                ],
                "in_train_phase": [
                    3142
                ],
                "alpha": [
                    3169,
                    3171,
                    3214,
                    3217,
                    3195,
                    3196,
                    3197
                ],
                "threshold": [
                    3170,
                    3173,
                    3174,
                    3180,
                    3182
                ],
                "tf.nn.leaky_relu": [
                    3171
                ],
                "negative_part": [
                    3176,
                    3197,
                    3174
                ],
                "tf.nn.relu": [
                    3176,
                    3188,
                    3174
                ],
                "clip_max": [
                    3178,
                    3186,
                    3190
                ],
                "tf.nn.relu6": [
                    3185
                ],
                "res": [
                    3217,
                    3213,
                    3215
                ],
                "tf.nn.elu": [
                    3213
                ],
                "tf.nn.softmax": [
                    3233
                ],
                "tf.nn.softplus": [
                    3247
                ],
                "tf.nn.softsign": [
                    3261
                ],
                "tf_keras_backend.categorical_crossentropy": [
                    3286
                ],
                "target": [
                    3313,
                    3330,
                    3287
                ],
                "from_logits": [
                    3313,
                    3330,
                    3287
                ],
                "tf_keras_backend.sparse_categorical_crossentropy": [
                    3312
                ],
                "tf_keras_backend.binary_crossentropy": [
                    3329
                ],
                "tf.nn.sigmoid": [
                    3344
                ],
                "tf_keras_backend.hard_sigmoid": [
                    3362
                ],
                "tf.nn.tanh": [
                    3376
                ],
                "tf.nn.dropout": [
                    3396
                ],
                "level": [
                    3396
                ],
                "noise_shape": [
                    3396
                ],
                "tf.nn.l2_normalize": [
                    3411
                ],
                "tf.nn.in_top_k": [
                    3429
                ],
                "predictions": [
                    3429
                ],
                "targets": [
                    3430
                ],
                "k": [
                    3431
                ],
                "StrictVersion": [
                    3449,
                    3474,
                    3497
                ],
                "tf.__version__.split": [
                    3449,
                    3474,
                    3497
                ],
                "force_transpose": [
                    3657,
                    3659,
                    3478,
                    3655
                ],
                "kernel_shape": [
                    3557,
                    3550,
                    4116,
                    4117,
                    4061,
                    4062
                ],
                "kernel.shape.as_list": [
                    3550
                ],
                "kernel.shape": [
                    3550
                ],
                "kernel": [
                    3617,
                    3683,
                    4134,
                    4072,
                    3881,
                    4061,
                    3934,
                    3571,
                    4116,
                    3677,
                    3550
                ],
                "left_pad": [
                    3557,
                    3558
                ],
                "dilation_rate": [
                    3713,
                    3714,
                    3731,
                    3736,
                    3738,
                    3612,
                    3614,
                    3876,
                    3878,
                    3654,
                    3787,
                    3789,
                    3676,
                    3681,
                    3683,
                    3557,
                    3566,
                    3568,
                    3833,
                    3835
                ],
                "temporal_padding": [
                    3558
                ],
                "_preprocess_padding": [
                    3778,
                    3560,
                    3721,
                    4013,
                    3824,
                    3670,
                    3607,
                    3928,
                    3966,
                    3871
                ],
                "_preprocess_conv1d_input": [
                    3561,
                    3716
                ],
                "tf.nn.convolution": [
                    3616,
                    3570,
                    3880
                ],
                "strides": [
                    3712,
                    3968,
                    3971,
                    3975,
                    3979,
                    3724,
                    3727,
                    4114,
                    3741,
                    3618,
                    3882,
                    4015,
                    4018,
                    4022,
                    4026,
                    3780,
                    3782,
                    3792,
                    3672,
                    3930,
                    3674,
                    3932,
                    3677,
                    3934,
                    4060,
                    3826,
                    3572,
                    3828,
                    3838,
                    3711
                ],
                "_preprocess_conv2d_input": [
                    3777,
                    3659,
                    3823,
                    3605,
                    3965
                ],
                "tf.nn.conv2d_transpose": [
                    3677
                ],
                "tf.nn.atrous_conv2d_transpose": [
                    3682
                ],
                "spatial_start_dim": [
                    3728,
                    3746,
                    3723,
                    3726
                ],
                "depthwise_kernel": [
                    3729,
                    3740,
                    3837,
                    3791
                ],
                "pointwise_kernel": [
                    3730,
                    3740,
                    3791
                ],
                "tf.nn.separable_conv2d": [
                    3740,
                    3791
                ],
                "tf.nn.depthwise_conv2d": [
                    3837
                ],
                "_preprocess_conv3d_input": [
                    4012,
                    3916,
                    3870
                ],
                "tf.nn.conv3d_transpose": [
                    3934
                ],
                "pool_size": [
                    3969,
                    3972,
                    3975,
                    3979,
                    4016,
                    4019,
                    4022,
                    4026
                ],
                "pool_mode": [
                    3974,
                    3978,
                    3983,
                    4021,
                    4025,
                    4030
                ],
                "tf.nn.max_pool": [
                    3975
                ],
                "tf.nn.avg_pool": [
                    3979
                ],
                "tf.nn.max_pool3d": [
                    4022
                ],
                "tf.nn.avg_pool3d": [
                    4026
                ],
                "stride": [
                    4066,
                    4067,
                    4060
                ],
                "output_length": [
                    4065,
                    4062
                ],
                "feature_dim": [
                    4128,
                    4131,
                    4069,
                    4117,
                    4062
                ],
                "filters": [
                    4136,
                    4117,
                    4062
                ],
                "xs": [
                    4064,
                    4130,
                    4068,
                    4133,
                    4070,
                    4119,
                    4127
                ],
                "slice_length": [
                    4066,
                    4068
                ],
                "kernel_size": [
                    4123,
                    4067,
                    4125
                ],
                "xs.append": [
                    4130,
                    4068,
                    4127
                ],
                "reshape": [
                    4193,
                    4130,
                    4068,
                    4135,
                    4201,
                    4177,
                    4185,
                    4187,
                    4127
                ],
                "x_aggregate": [
                    4072,
                    4134,
                    4133,
                    4070
                ],
                "batch_dot": [
                    4072,
                    4134
                ],
                "stride_row": [
                    4123,
                    4114,
                    4122
                ],
                "stride_col": [
                    4114,
                    4124,
                    4125
                ],
                "output_row": [
                    4120,
                    4136,
                    4115
                ],
                "output_col": [
                    4136,
                    4121,
                    4115
                ],
                "j": [
                    4121,
                    4124,
                    4125
                ],
                "slice_row": [
                    4122,
                    4130,
                    4127
                ],
                "slice_col": [
                    4130,
                    4124,
                    4127
                ],
                "bias_shape": [
                    4193,
                    4195,
                    4196,
                    4165,
                    4166,
                    4198,
                    4169,
                    4171,
                    4172,
                    4174,
                    4180,
                    4185,
                    4187,
                    4189
                ],
                "bias": [
                    4193,
                    4165,
                    4201,
                    4203,
                    4177,
                    4182,
                    4185,
                    4187,
                    4190
                ],
                "tf.nn.bias_add": [
                    4190,
                    4203,
                    4182
                ],
                "is_symbolic": [
                    4256,
                    4281,
                    4228,
                    4312
                ],
                "stddev": [
                    4228,
                    4231,
                    4234,
                    4312,
                    4315,
                    4318
                ],
                "tf_keras_backend.random_normal": [
                    4233,
                    4230
                ],
                "minval": [
                    4256,
                    4259,
                    4262
                ],
                "maxval": [
                    4256,
                    4259,
                    4262
                ],
                "tf_keras_backend.random_uniform": [
                    4258,
                    4261
                ],
                "p": [
                    4281,
                    4284,
                    4287
                ],
                "tf_keras_backend.random_binomial": [
                    4283,
                    4286
                ],
                "tf_keras_backend.truncated_normal": [
                    4314,
                    4317
                ],
                "label_shape": [
                    4352,
                    4355,
                    4356,
                    4365,
                    4366,
                    4338,
                    4339,
                    4340,
                    4343,
                    4346,
                    4351
                ],
                "labels": [
                    4338,
                    4362
                ],
                "num_batches_tns": [
                    4339,
                    4351
                ],
                "max_num_labels_tns": [
                    4344,
                    4355,
                    4340
                ],
                "tf.fill": [
                    4346,
                    4343
                ],
                "current_input": [
                    4344
                ],
                "init": [
                    4346,
                    4348
                ],
                "dense_mask": [
                    4353,
                    4357,
                    4347,
                    4349
                ],
                "functional_ops.scan": [
                    4347
                ],
                "functional_ops": [
                    4347
                ],
                "range_less_than": [
                    4347
                ],
                "label_lengths": [
                    4347
                ],
                "label_array": [
                    4353,
                    4351
                ],
                "label_ind": [
                    4353,
                    4359
                ],
                "tf.boolean_mask": [
                    4353,
                    4357
                ],
                "tmp": [
                    4355,
                    4356
                ],
                "batch_array": [
                    4356,
                    4357
                ],
                "reverse": [
                    4356
                ],
                "batch_ind": [
                    4357,
                    4359
                ],
                "vals_sparse": [
                    4362,
                    4366
                ],
                "tf.gather_nd": [
                    4362
                ],
                "tf.int64": [
                    4364,
                    4365
                ],
                "label_length": [
                    4386,
                    4389
                ],
                "tf.int32": [
                    4386,
                    4387,
                    4428,
                    4389
                ],
                "sparse_labels": [
                    4392,
                    4388
                ],
                "ctc_label_dense_to_sparse": [
                    4389
                ],
                "y_true": [
                    4389
                ],
                "y_pred": [
                    4390,
                    4391,
                    4427,
                    4432,
                    4436
                ],
                "ctc.ctc_loss": [
                    4391
                ],
                "ctc": [
                    4435,
                    4431,
                    4391
                ],
                "greedy": [
                    4430
                ],
                "decoded": [
                    4441,
                    4435,
                    4431
                ],
                "log_prob": [
                    4435,
                    4444,
                    4431
                ],
                "ctc.ctc_greedy_decoder": [
                    4431
                ],
                "ctc.ctc_beam_search_decoder": [
                    4435
                ],
                "beam_width": [
                    4437
                ],
                "top_paths": [
                    4438
                ],
                "merge_repeated": [
                    4438
                ],
                "decoded_dense": [
                    4440,
                    4443,
                    4444
                ],
                "st": [
                    4441,
                    4442
                ],
                "dense_tensor": [
                    4442,
                    4443
                ],
                "decoded_dense.append": [
                    4443
                ],
                "tf.map_fn": [
                    4461
                ],
                "fn": [
                    4493,
                    4461,
                    4477
                ],
                "elems": [
                    4493,
                    4461,
                    4477
                ],
                "tf.foldl": [
                    4477
                ],
                "initializer": [
                    4477,
                    4493
                ],
                "tf.foldr": [
                    4493
                ]
            },
            "filtered_variables_in_file": {
                "py_all": [
                    25,
                    923,
                    956,
                    2366
                ],
                "py_any": [
                    4256,
                    4228,
                    1423,
                    4312,
                    4281,
                    26
                ],
                "py_sum": [
                    27
                ],
                "py_slice": [
                    4066,
                    4122,
                    28,
                    4124
                ],
                "_LOCAL_DEVICES": [
                    35,
                    487,
                    490,
                    492,
                    493
                ],
                "_SYMBOLIC_SCOPE": [
                    99,
                    101,
                    37,
                    38,
                    71,
                    105
                ],
                "threading.local": [
                    37
                ],
                "threading": [
                    37
                ],
                "_SYMBOLIC_SCOPE.value": [
                    99,
                    101,
                    38,
                    71,
                    105
                ],
                "_LEARNING_PHASE_CACHE": [
                    39,
                    328,
                    329,
                    332,
                    408
                ],
                "tf.__version__.startswith": [
                    43
                ],
                "tf.__version__": [
                    3497,
                    3449,
                    3474,
                    43
                ],
                "tf": [
                    3073,
                    3074,
                    3075,
                    3076,
                    2055,
                    525,
                    1040,
                    2070,
                    2584,
                    2585,
                    2588,
                    1054,
                    1567,
                    3616,
                    548,
                    2085,
                    3623,
                    2552,
                    43,
                    2553,
                    1582,
                    2607,
                    2100,
                    54,
                    2619,
                    573,
                    2112,
                    3651,
                    1092,
                    1606,
                    2633,
                    2124,
                    4391,
                    80,
                    3578,
                    3667,
                    3668,
                    4182,
                    2647,
                    1627,
                    3677,
                    2142,
                    4190,
                    2144,
                    3682,
                    3171,
                    2660,
                    3174,
                    3686,
                    3176,
                    1130,
                    4203,
                    3182,
                    1648,
                    3185,
                    2676,
                    2165,
                    3188,
                    3193,
                    634,
                    2172,
                    2173,
                    2175,
                    2176,
                    2180,
                    1669,
                    2184,
                    2186,
                    3213,
                    2704,
                    3217,
                    3728,
                    3729,
                    3730,
                    3740,
                    3233,
                    3746,
                    1189,
                    3749,
                    2219,
                    2223,
                    3247,
                    2227,
                    2228,
                    1717,
                    1718,
                    1719,
                    1720,
                    1721,
                    2229,
                    2230,
                    2232,
                    2746,
                    3261,
                    2761,
                    1743,
                    3791,
                    3797,
                    2776,
                    1763,
                    1764,
                    1765,
                    2794,
                    4338,
                    4339,
                    4340,
                    1782,
                    1783,
                    4343,
                    4346,
                    3837,
                    4351,
                    2817,
                    4353,
                    3843,
                    4355,
                    4356,
                    774,
                    4357,
                    1800,
                    1801,
                    2312,
                    2314,
                    4360,
                    4362,
                    2318,
                    4364,
                    3344,
                    4365,
                    2322,
                    4366,
                    2324,
                    2325,
                    2326,
                    1815,
                    1304,
                    2327,
                    2328,
                    2329,
                    2330,
                    2331,
                    2333,
                    1311,
                    4386,
                    4387,
                    4388,
                    1829,
                    4389,
                    1319,
                    1320,
                    1321,
                    2345,
                    3880,
                    1324,
                    4390,
                    1326,
                    3887,
                    3376,
                    1841,
                    1853,
                    2367,
                    2369,
                    3396,
                    3914,
                    331,
                    1868,
                    1869,
                    2382,
                    4427,
                    4428,
                    3411,
                    3925,
                    3926,
                    2903,
                    1881,
                    4442,
                    2396,
                    3934,
                    3938,
                    3429,
                    4461,
                    2928,
                    2929,
                    373,
                    3449,
                    2426,
                    2427,
                    3450,
                    1917,
                    3454,
                    4477,
                    2945,
                    2947,
                    3975,
                    394,
                    1931,
                    3979,
                    4493,
                    3474,
                    3475,
                    3986,
                    1943,
                    3479,
                    922,
                    414,
                    1957,
                    423,
                    3497,
                    3498,
                    430,
                    3502,
                    1460,
                    4022,
                    1464,
                    4026,
                    955,
                    1980,
                    1473,
                    4033,
                    1481,
                    2506,
                    1995,
                    1488,
                    1489,
                    1490,
                    2518,
                    2519,
                    986,
                    1499,
                    1500,
                    1501,
                    2010,
                    2522,
                    2527,
                    2529,
                    1506,
                    3035,
                    3036,
                    1509,
                    1513,
                    2025,
                    3049,
                    492,
                    1519,
                    3570,
                    1523,
                    1013,
                    1527,
                    2040,
                    1529,
                    2554,
                    3069,
                    3070,
                    3071
                ],
                "tf_keras_backend.set_floatx": [
                    216,
                    46
                ],
                "tf_keras_backend": [
                    128,
                    3329,
                    258,
                    4230,
                    4233,
                    142,
                    398,
                    277,
                    406,
                    157,
                    2847,
                    162,
                    3362,
                    4258,
                    4261,
                    2858,
                    46,
                    47,
                    48,
                    52,
                    181,
                    2868,
                    3001,
                    4283,
                    4286,
                    322,
                    197,
                    720,
                    3286,
                    216,
                    346,
                    4314,
                    4317,
                    607,
                    2911,
                    3312,
                    243,
                    377
                ],
                "floatx": [
                    4225,
                    1764,
                    1125,
                    632,
                    46,
                    719,
                    3182,
                    920,
                    1718,
                    984,
                    216,
                    953,
                    4278,
                    4309,
                    4253,
                    606,
                    1087
                ],
                "tf_keras_backend.set_epsilon": [
                    181,
                    47
                ],
                "epsilon": [
                    2273,
                    2146,
                    2337,
                    4390,
                    2345,
                    4427,
                    47,
                    2192,
                    2261,
                    2264,
                    2236,
                    2269
                ],
                "tf_keras_backend.set_image_data_format": [
                    48,
                    277
                ],
                "image_data_format": [
                    48,
                    302
                ],
                "get_graph": [
                    4257,
                    4229,
                    454,
                    72,
                    52,
                    4313,
                    4282
                ],
                "tf_keras_backend.get_graph": [
                    52
                ],
                "name_scope": [
                    330,
                    54
                ],
                "tf.name_scope": [
                    54
                ],
                "_is_tf_1": [
                    66,
                    323,
                    2307,
                    3875,
                    390,
                    488,
                    3786,
                    3565,
                    2927,
                    2832,
                    369,
                    2257,
                    886,
                    3735,
                    3832,
                    3611,
                    92,
                    2909
                ],
                "func": [
                    97,
                    67,
                    69,
                    103,
                    73,
                    75,
                    93
                ],
                "as_default": [
                    4257,
                    4229,
                    72,
                    4313,
                    4282
                ],
                "args": [
                    73,
                    75,
                    103
                ],
                "kwargs": [
                    3841,
                    3734,
                    3736,
                    3738,
                    3610,
                    3612,
                    3614,
                    3744,
                    3874,
                    3876,
                    3621,
                    3878,
                    3885,
                    73,
                    3785,
                    75,
                    3787,
                    3789,
                    3831,
                    3795,
                    2913,
                    103,
                    3564,
                    3566,
                    3568,
                    3575,
                    3833,
                    3835
                ],
                "functools.wraps": [
                    97,
                    69
                ],
                "functools": [
                    97,
                    69
                ],
                "symbolic_fn_wrapper": [
                    76
                ],
                "x": [
                    3076,
                    3077,
                    2055,
                    525,
                    1040,
                    3605,
                    2070,
                    1054,
                    1567,
                    3616,
                    3617,
                    3106,
                    3107,
                    2085,
                    3109,
                    3623,
                    2552,
                    3624,
                    3118,
                    2607,
                    3120,
                    3121,
                    2100,
                    2619,
                    2112,
                    1606,
                    3142,
                    4166,
                    2633,
                    2634,
                    3659,
                    2124,
                    4169,
                    4170,
                    80,
                    4177,
                    3578,
                    3667,
                    4178,
                    4182,
                    2647,
                    4185,
                    1627,
                    4187,
                    3677,
                    2142,
                    4190,
                    2144,
                    4193,
                    3682,
                    3171,
                    2660,
                    3683,
                    3174,
                    3686,
                    3176,
                    3687,
                    4194,
                    4201,
                    4203,
                    4204,
                    3182,
                    1648,
                    3185,
                    2676,
                    2165,
                    3188,
                    3191,
                    2168,
                    3192,
                    3193,
                    2172,
                    3196,
                    3197,
                    3198,
                    1156,
                    1669,
                    3716,
                    4228,
                    2187,
                    3213,
                    2704,
                    3217,
                    3728,
                    1683,
                    3740,
                    4256,
                    1697,
                    3233,
                    3746,
                    1189,
                    3749,
                    3751,
                    680,
                    682,
                    684,
                    2220,
                    2221,
                    3247,
                    688,
                    2224,
                    2225,
                    1717,
                    1206,
                    1718,
                    1719,
                    1720,
                    2233,
                    2746,
                    4281,
                    3261,
                    3777,
                    1220,
                    2761,
                    1743,
                    720,
                    2255,
                    1234,
                    2259,
                    3791,
                    725,
                    726,
                    727,
                    2263,
                    3797,
                    2266,
                    2267,
                    3798,
                    4312,
                    2271,
                    1249,
                    1251,
                    1252,
                    741,
                    1253,
                    1254,
                    1763,
                    1764,
                    1765,
                    2794,
                    3823,
                    243,
                    1782,
                    1783,
                    2295,
                    3837,
                    2817,
                    3843,
                    3844,
                    774,
                    1800,
                    1801,
                    3344,
                    2833,
                    2835,
                    1302,
                    1815,
                    1304,
                    2334,
                    3870,
                    800,
                    801,
                    3362,
                    803,
                    804,
                    805,
                    1829,
                    1319,
                    3880,
                    2345,
                    2858,
                    1323,
                    1324,
                    3881,
                    1326,
                    3887,
                    3376,
                    1841,
                    3888,
                    1853,
                    2366,
                    2880,
                    833,
                    2369,
                    3396,
                    1866,
                    1867,
                    1868,
                    1869,
                    2382,
                    3916,
                    3411,
                    3925,
                    2903,
                    1881,
                    2396,
                    3934,
                    864,
                    3938,
                    3939,
                    1893,
                    1390,
                    887,
                    888,
                    2425,
                    890,
                    891,
                    2426,
                    1917,
                    2430,
                    3448,
                    2432,
                    3450,
                    2434,
                    3454,
                    3457,
                    3965,
                    2439,
                    3975,
                    1931,
                    3979,
                    3473,
                    3986,
                    3475,
                    2452,
                    3987,
                    2454,
                    1943,
                    3479,
                    3482,
                    1957,
                    3496,
                    3498,
                    2475,
                    4012,
                    3502,
                    2480,
                    3505,
                    1460,
                    4022,
                    4026,
                    1980,
                    1473,
                    4033,
                    4034,
                    2502,
                    2506,
                    1995,
                    1486,
                    1490,
                    2518,
                    2519,
                    2520,
                    2010,
                    1506,
                    2532,
                    3558,
                    2025,
                    490,
                    3049,
                    3561,
                    493,
                    3570,
                    3571,
                    1013,
                    2551,
                    2040,
                    2554,
                    3579
                ],
                "tf.Tensor": [
                    80
                ],
                "prev_value": [
                    105,
                    99
                ],
                "context.eager_mode": [
                    889,
                    102
                ],
                "context": [
                    889,
                    102
                ],
                "out": [
                    103,
                    106,
                    1324,
                    1326,
                    1327
                ],
                "eager_fn_wrapper": [
                    107
                ],
                "tf_keras_backend.get_uid": [
                    128
                ],
                "prefix": [
                    128
                ],
                "tf_keras_backend.manual_variable_initialization": [
                    142
                ],
                "value": [
                    142,
                    2858,
                    301,
                    302,
                    303,
                    307,
                    1092,
                    1094,
                    346,
                    608,
                    609,
                    610,
                    611,
                    612,
                    613,
                    614,
                    1253,
                    1130,
                    1132,
                    634
                ],
                "tf_keras_backend.epsilon": [
                    157
                ],
                "tf_keras_backend.reset_uids": [
                    162
                ],
                "e": [
                    181
                ],
                "tf_keras_backend.floatx": [
                    197
                ],
                "tf_keras_backend.cast_to_floatx": [
                    243
                ],
                "tf_keras_backend.image_data_format": [
                    258
                ],
                "data_format": [
                    3842,
                    3716,
                    2438,
                    2697,
                    2703,
                    4112,
                    3985,
                    3603,
                    2452,
                    277,
                    3477,
                    3605,
                    3868,
                    3870,
                    4126,
                    3748,
                    3622,
                    2474,
                    4010,
                    3500,
                    4012,
                    3886,
                    303,
                    304,
                    2479,
                    2735,
                    4138,
                    308,
                    2485,
                    2744,
                    3775,
                    4032,
                    3649,
                    3777,
                    4164,
                    3654,
                    3912,
                    3659,
                    3916,
                    3661,
                    3918,
                    4175,
                    4179,
                    3796,
                    4058,
                    3548,
                    4188,
                    3552,
                    3937,
                    3685,
                    4199,
                    3561,
                    3821,
                    3965,
                    3823,
                    2420,
                    3577,
                    3963,
                    3452,
                    2429,
                    3710
                ],
                "value.lower": [
                    303
                ],
                "lp": [
                    322,
                    324,
                    326,
                    327,
                    328,
                    329,
                    331,
                    332
                ],
                "tf_keras_backend.learning_phase": [
                    322
                ],
                "int_lp": [
                    331,
                    332,
                    333
                ],
                "tf.cast": [
                    1800,
                    4364,
                    4365,
                    3475,
                    2325,
                    2327,
                    2329,
                    2331,
                    4386,
                    4387,
                    4388,
                    1189,
                    3498,
                    2228,
                    1718,
                    2230,
                    331,
                    4428,
                    3450,
                    3036,
                    1764,
                    3182,
                    1782,
                    4346
                ],
                "symbolic": [
                    514,
                    1159,
                    3080,
                    1043,
                    551,
                    1195,
                    691,
                    3124,
                    311,
                    1209,
                    1223,
                    3015,
                    336,
                    1237,
                    730,
                    989,
                    2916,
                    2932,
                    1016
                ],
                "tf_keras_backend.set_learning_phase": [
                    346
                ],
                "tf.executing_eagerly": [
                    394,
                    373
                ],
                "tf_keras_backend.get_session": [
                    377
                ],
                "tf_keras_backend.set_session": [
                    398
                ],
                "session": [
                    422,
                    430,
                    398,
                    412,
                    413
                ],
                "tf_keras_backend.clear_session": [
                    406
                ],
                "get_session": [
                    489,
                    412,
                    2833,
                    887
                ],
                "session.graph.as_default": [
                    413
                ],
                "session.graph": [
                    413
                ],
                "variables": [
                    416,
                    2944,
                    2945,
                    2947,
                    2928,
                    2929,
                    414
                ],
                "tf.global_variables": [
                    414
                ],
                "candidate_vars": [
                    418,
                    419,
                    423,
                    425,
                    415
                ],
                "v": [
                    922,
                    923,
                    924,
                    925,
                    416,
                    417,
                    418,
                    423,
                    425,
                    427,
                    428,
                    955,
                    956,
                    957,
                    958,
                    607,
                    610,
                    612,
                    614,
                    615,
                    616
                ],
                "candidate_vars.append": [
                    418
                ],
                "is_initialized": [
                    425,
                    422
                ],
                "session.run": [
                    430,
                    422
                ],
                "tf.is_variable_initialized": [
                    423
                ],
                "uninitialized_vars": [
                    424,
                    427,
                    429,
                    430
                ],
                "flag": [
                    425,
                    426
                ],
                "uninitialized_vars.append": [
                    427
                ],
                "v._keras_initialized": [
                    428
                ],
                "tf.variables_initializer": [
                    430
                ],
                "self.device": [
                    443,
                    439
                ],
                "self": [
                    443,
                    439
                ],
                "device": [
                    443,
                    476,
                    477
                ],
                "g": [
                    456,
                    454
                ],
                "op": [
                    456,
                    457,
                    455
                ],
                "_TfDeviceCaptureOp": [
                    455
                ],
                "g._apply_device_functions": [
                    456
                ],
                "op.device": [
                    457
                ],
                "device_type": [
                    473,
                    474,
                    477
                ],
                "device_type.lower": [
                    473
                ],
                "_get_current_tf_device": [
                    476
                ],
                "device.device_type.lower": [
                    477
                ],
                "device.device_type": [
                    477
                ],
                "devices": [
                    489,
                    490
                ],
                "list_devices": [
                    489
                ],
                "x.name": [
                    490
                ],
                "tf.config.experimental_list_devices": [
                    492
                ],
                "tf.config": [
                    492
                ],
                "x.lower": [
                    493
                ],
                "explicitly_on_cpu": [
                    507,
                    509
                ],
                "_is_current_explicit_device": [
                    507
                ],
                "gpus_available": [
                    508,
                    509
                ],
                "_get_available_gpus": [
                    508
                ],
                "tf.convert_to_tensor": [
                    525
                ],
                "dtype": [
                    4224,
                    4225,
                    4231,
                    4234,
                    525,
                    1040,
                    3473,
                    919,
                    920,
                    922,
                    924,
                    2589,
                    2590,
                    4252,
                    4253,
                    4259,
                    1189,
                    4262,
                    3496,
                    4277,
                    4278,
                    952,
                    953,
                    955,
                    4284,
                    957,
                    1086,
                    1087,
                    4287,
                    3448,
                    1093,
                    1094,
                    718,
                    719,
                    721,
                    4308,
                    4309,
                    983,
                    984,
                    986,
                    4315,
                    605,
                    606,
                    4318,
                    608,
                    1124,
                    1125,
                    1131,
                    1132,
                    4461,
                    1013,
                    631,
                    632,
                    634
                ],
                "tensor": [
                    548,
                    573,
                    572,
                    575
                ],
                "tf.SparseTensor": [
                    548,
                    4366
                ],
                "is_sparse": [
                    1323,
                    572,
                    2366
                ],
                "tf.sparse.to_dense": [
                    4442,
                    573
                ],
                "tf.sparse": [
                    4442,
                    1324,
                    573,
                    2367
                ],
                "tf_keras_backend.variable": [
                    607
                ],
                "name": [
                    608,
                    986,
                    1094,
                    1132,
                    4461,
                    4493,
                    1040,
                    721,
                    1013,
                    4477,
                    634,
                    955,
                    924,
                    957,
                    1054,
                    922
                ],
                "constraint": [
                    608
                ],
                "v._keras_shape": [
                    610,
                    612,
                    614
                ],
                "shape": [
                    4228,
                    4231,
                    4234,
                    922,
                    4256,
                    4259,
                    4262,
                    4281,
                    955,
                    4284,
                    4287,
                    1093,
                    2633,
                    1486,
                    2382,
                    721,
                    722,
                    724,
                    725,
                    4312,
                    1497,
                    4315,
                    4318,
                    610,
                    1131,
                    634
                ],
                "value.tocoo": [
                    610
                ],
                "np.ndarray": [
                    611
                ],
                "np": [
                    1090,
                    611,
                    1156,
                    3395,
                    4227,
                    1128,
                    1867,
                    2427,
                    4311,
                    2520,
                    1977,
                    1979,
                    2525,
                    4280,
                    4255
                ],
                "value.shape": [
                    612
                ],
                "int_shape": [
                    2880,
                    1156,
                    4165,
                    614,
                    1390,
                    1391,
                    4116,
                    1304,
                    2425,
                    4061,
                    1311
                ],
                "v._uses_learning_phase": [
                    615
                ],
                "tf_ops.init_scope": [
                    921,
                    1091,
                    4260,
                    4232,
                    1129,
                    4316,
                    633,
                    954,
                    985,
                    4285
                ],
                "tf_ops": [
                    954,
                    1249,
                    1250,
                    1091,
                    4260,
                    4232,
                    1129,
                    688,
                    4316,
                    633,
                    921,
                    985,
                    4285
                ],
                "tf.constant": [
                    2219,
                    2223,
                    2585,
                    634,
                    2427,
                    2527
                ],
                "is_tensor": [
                    680
                ],
                "tf_ops._TensorLike": [
                    688
                ],
                "tf_ops.is_dense_tensor_like": [
                    688
                ],
                "tf_keras_backend.placeholder": [
                    720
                ],
                "ndim": [
                    2311,
                    2313,
                    2317,
                    2321,
                    1302,
                    2295,
                    1317,
                    2361,
                    4166,
                    4169,
                    4170,
                    2255,
                    721,
                    4178,
                    723,
                    724,
                    2266,
                    3037,
                    4194,
                    3060,
                    2551,
                    2168
                ],
                "sparse": [
                    721
                ],
                "_": [
                    4117,
                    2508,
                    724,
                    2333
                ],
                "x._keras_shape": [
                    801,
                    725
                ],
                "x._uses_learning_phase": [
                    3120,
                    726
                ],
                "x.op.type": [
                    741
                ],
                "x.op": [
                    741
                ],
                "tf.shape": [
                    1509,
                    774,
                    4338,
                    3667,
                    3925,
                    2518,
                    1304,
                    2426,
                    3071,
                    2172,
                    3069,
                    1311
                ],
                "x.shape": [
                    833,
                    803,
                    804,
                    805,
                    2502,
                    2532,
                    2221,
                    2225,
                    2520
                ],
                "x.shape.as_list": [
                    2532,
                    805,
                    2502
                ],
                "x.shape.rank": [
                    833
                ],
                "x.dtype.base_dtype.name": [
                    864
                ],
                "x.dtype.base_dtype": [
                    864,
                    1251,
                    1252,
                    1763,
                    1866,
                    1867,
                    1717,
                    3191,
                    3192,
                    3196
                ],
                "x.dtype": [
                    864,
                    1251,
                    1252,
                    1253,
                    1763,
                    1866,
                    1867,
                    2220,
                    2224,
                    1717,
                    3191,
                    3192,
                    3196
                ],
                "to_dense": [
                    2369,
                    887
                ],
                "x.numpy": [
                    890,
                    2835
                ],
                "eval_fn": [
                    891,
                    892
                ],
                "function": [
                    891
                ],
                "tf.zeros": [
                    922
                ],
                "v.shape.as_list": [
                    923,
                    956
                ],
                "v.shape": [
                    923,
                    956
                ],
                "variable": [
                    1094,
                    1132,
                    986,
                    924,
                    957
                ],
                "tf.ones": [
                    955
                ],
                "tf.eye": [
                    986
                ],
                "size": [
                    2817,
                    986
                ],
                "tf.zeros_like": [
                    1013
                ],
                "tf.ones_like": [
                    1040,
                    3073
                ],
                "tf.identity": [
                    1054
                ],
                "seed": [
                    4226,
                    4227,
                    4231,
                    4234,
                    4254,
                    4255,
                    4259,
                    4262,
                    4279,
                    4280,
                    4284,
                    4287,
                    1088,
                    1090,
                    3394,
                    3395,
                    1093,
                    3396,
                    4310,
                    4311,
                    4315,
                    4318,
                    1126,
                    1128,
                    1131
                ],
                "np.random.randint": [
                    1090,
                    3395,
                    4227,
                    1128,
                    4311,
                    4280,
                    4255
                ],
                "np.random": [
                    1090,
                    3395,
                    4227,
                    1128,
                    4311,
                    4280,
                    4255
                ],
                "tf.random_uniform_initializer": [
                    1092
                ],
                "low": [
                    1093
                ],
                "high": [
                    1093
                ],
                "tf.random_normal_initializer": [
                    1130
                ],
                "mean": [
                    4228,
                    2311,
                    2312,
                    4231,
                    4234,
                    2316,
                    2320,
                    2193,
                    2328,
                    2329,
                    2338,
                    2345,
                    4312,
                    4315,
                    2142,
                    4318,
                    2144,
                    2147,
                    1131,
                    2165,
                    2175
                ],
                "scale": [
                    1131
                ],
                "np.prod": [
                    1156
                ],
                "tf_state_ops.assign": [
                    1206
                ],
                "tf_state_ops": [
                    1254,
                    1234,
                    1220,
                    1206
                ],
                "new_x": [
                    1206
                ],
                "tf_state_ops.assign_add": [
                    1220
                ],
                "increment": [
                    1220
                ],
                "tf_state_ops.assign_sub": [
                    1234,
                    1254
                ],
                "decrement": [
                    1234
                ],
                "tf_ops.colocate_with": [
                    1249
                ],
                "decay": [
                    1250,
                    1251,
                    1252,
                    1253
                ],
                "tf_ops.convert_to_tensor": [
                    1250
                ],
                "momentum": [
                    1250
                ],
                "decay.dtype": [
                    1251
                ],
                "tf_math_ops.cast": [
                    1252,
                    1253
                ],
                "tf_math_ops": [
                    1697,
                    1252,
                    1893,
                    1253,
                    4390,
                    4427,
                    1683
                ],
                "update_delta": [
                    1253,
                    1254
                ],
                "y": [
                    2055,
                    1302,
                    2070,
                    2333,
                    1311,
                    1317,
                    2085,
                    2343,
                    1320,
                    1324,
                    1326,
                    2100,
                    1464,
                    1481,
                    1995,
                    1497,
                    2010,
                    1501,
                    1506,
                    2025,
                    1391,
                    2040
                ],
                "x_shape": [
                    1411,
                    1303,
                    1306,
                    1308,
                    1309,
                    1445,
                    1319,
                    1322,
                    1450,
                    2502,
                    2504,
                    2506,
                    1486,
                    1487,
                    1489,
                    2518,
                    2528,
                    2529,
                    2532,
                    2533,
                    2534,
                    1390,
                    1393,
                    1400,
                    1403
                ],
                "i": [
                    1312,
                    1313,
                    4065,
                    4066,
                    4067,
                    4120,
                    1478,
                    1479,
                    4122,
                    1471,
                    1304,
                    1305,
                    1306,
                    4123,
                    1470,
                    1311
                ],
                "s": [
                    1315,
                    2508,
                    1304,
                    1308,
                    1311
                ],
                "tf.unstack": [
                    1304,
                    1311
                ],
                "x_shape.append": [
                    1306,
                    1308
                ],
                "y_shape": [
                    1313,
                    1315,
                    1316,
                    1412,
                    1446,
                    1320,
                    1322,
                    1450,
                    1497,
                    1391,
                    1394,
                    1401,
                    1498,
                    1500,
                    1404,
                    1310
                ],
                "y_shape.append": [
                    1313,
                    1315
                ],
                "y_permute_dim": [
                    1320,
                    1317,
                    1318
                ],
                "y_permute_dim.pop": [
                    1318
                ],
                "xt": [
                    1321,
                    1319
                ],
                "tf.reshape": [
                    2176,
                    4351,
                    2180,
                    4356,
                    2184,
                    2312,
                    2314,
                    4360,
                    2318,
                    2322,
                    1319,
                    1320,
                    1321,
                    2619,
                    2633,
                    2382,
                    1490,
                    1501,
                    2529,
                    1523,
                    3070,
                    2175
                ],
                "yt": [
                    1320,
                    1321
                ],
                "tf.transpose": [
                    3843,
                    4356,
                    4360,
                    3986,
                    3479,
                    1567,
                    3749,
                    4390,
                    3623,
                    1320,
                    3502,
                    3887,
                    1473,
                    4033,
                    1481,
                    4427,
                    3797,
                    2396,
                    3938,
                    3686,
                    3578,
                    3454
                ],
                "tf.matmul": [
                    1321,
                    1506,
                    1326
                ],
                "tf.sparse.sparse_dense_matmul": [
                    1324
                ],
                "x_ndim": [
                    1419,
                    1484,
                    1421,
                    1455,
                    1393,
                    1459,
                    1396,
                    1462,
                    1433,
                    1468,
                    1469,
                    1470
                ],
                "y_ndim": [
                    1495,
                    1477,
                    1418,
                    1419,
                    1421,
                    1456,
                    1394,
                    1396,
                    1463,
                    1465,
                    1435
                ],
                "x_batch_size": [
                    1403,
                    1406,
                    1407
                ],
                "y_batch_size": [
                    1404,
                    1406,
                    1407
                ],
                "axes": [
                    1414,
                    1415,
                    1417,
                    1419,
                    1421,
                    1423,
                    1426,
                    1429,
                    1432,
                    1433,
                    1434,
                    1435,
                    1438,
                    1444,
                    1451,
                    1452,
                    2792,
                    2793,
                    2794
                ],
                "a": [
                    1957,
                    1423
                ],
                "a0": [
                    1472,
                    1444,
                    1445,
                    1461,
                    1468,
                    1470
                ],
                "a1": [
                    1476,
                    1444,
                    1478,
                    1446,
                    1480
                ],
                "d1": [
                    1448,
                    1452,
                    1445
                ],
                "d2": [
                    1448,
                    1452,
                    1446
                ],
                "orig_x_ndim": [
                    1526,
                    1455
                ],
                "orig_y_ndim": [
                    1456,
                    1528
                ],
                "tf.expand_dims": [
                    2519,
                    4391,
                    2552,
                    3728,
                    3729,
                    3730,
                    1460,
                    2647,
                    1464,
                    4343
                ],
                "pattern": [
                    2699,
                    2703,
                    2704,
                    2737,
                    2744,
                    2746,
                    1469,
                    1471,
                    1472,
                    1473,
                    1477,
                    1479,
                    1480,
                    1481,
                    2396,
                    2675,
                    2676,
                    2553,
                    2554
                ],
                "x_mid_dims": [
                    1488,
                    1514,
                    1487
                ],
                "x_squashed_dim": [
                    1488,
                    1489
                ],
                "tf.reduce_prod": [
                    1488,
                    1499,
                    1669
                ],
                "x_squashed_shape": [
                    1489,
                    1490
                ],
                "tf.stack": [
                    3651,
                    2633,
                    2761,
                    3914,
                    1489,
                    4339,
                    3668,
                    4340,
                    3926,
                    2553,
                    1500,
                    2173
                ],
                "x_squashed": [
                    1512,
                    1491,
                    1493
                ],
                "y_trail_dims": [
                    1498,
                    1499,
                    1519
                ],
                "y_squashed_dim": [
                    1499,
                    1500
                ],
                "y_squashed_shape": [
                    1500,
                    1501
                ],
                "y_squashed": [
                    1504,
                    1518,
                    1502
                ],
                "result": [
                    1506,
                    1509,
                    1523,
                    1527,
                    1529,
                    1531,
                    2588,
                    2590,
                    2591
                ],
                "output_shape": [
                    2451,
                    2452,
                    4115,
                    3650,
                    3651,
                    3913,
                    3914,
                    3662,
                    3663,
                    3664,
                    3665,
                    3666,
                    3667,
                    3668,
                    3919,
                    3920,
                    3921,
                    3922,
                    3923,
                    3924,
                    3925,
                    3926,
                    3677,
                    3934,
                    3683,
                    1509,
                    1513,
                    1515,
                    1519,
                    1523
                ],
                "do_reshape": [
                    1520,
                    1522,
                    1516,
                    1510
                ],
                "tf.concat": [
                    2369,
                    1513,
                    3069,
                    1519
                ],
                "tf.squeeze": [
                    3746,
                    4386,
                    2660,
                    4387,
                    1527,
                    1529
                ],
                "tf.nn.embedding_lookup": [
                    1582
                ],
                "tf.nn": [
                    3975,
                    2186,
                    3979,
                    3213,
                    3344,
                    3740,
                    2333,
                    3616,
                    3233,
                    3880,
                    2345,
                    1582,
                    3247,
                    3376,
                    4022,
                    2232,
                    4026,
                    3261,
                    3396,
                    3791,
                    3411,
                    4182,
                    3677,
                    2142,
                    3934,
                    2144,
                    4190,
                    3682,
                    3171,
                    3429,
                    3174,
                    3176,
                    4203,
                    3185,
                    3570,
                    3188,
                    2165,
                    3837
                ],
                "reference": [
                    1582
                ],
                "indices": [
                    4359,
                    4360,
                    4362,
                    4364,
                    1582,
                    4366,
                    2776
                ],
                "tf.reduce_max": [
                    1606
                ],
                "axis": [
                    1669,
                    1801,
                    1683,
                    1815,
                    1697,
                    2169,
                    3233,
                    1829,
                    1719,
                    2360,
                    1722,
                    2363,
                    2365,
                    2367,
                    2369,
                    1606,
                    2504,
                    2761,
                    2506,
                    2509,
                    1743,
                    3411,
                    2517,
                    2647,
                    3287,
                    1627,
                    2526,
                    2660,
                    1765,
                    1648,
                    3313,
                    1783,
                    2168,
                    2297,
                    2299,
                    2172,
                    1917
                ],
                "keepdims": [
                    1669,
                    1606,
                    1765,
                    1801,
                    1743,
                    1648,
                    1783,
                    1627,
                    1723,
                    1917
                ],
                "tf.reduce_min": [
                    1627
                ],
                "tf.reduce_sum": [
                    1648
                ],
                "tf_math_ops.cumsum": [
                    1683
                ],
                "tf_math_ops.cumprod": [
                    1697
                ],
                "tf.bool": [
                    1763,
                    1800,
                    1717,
                    1782,
                    4346,
                    3035
                ],
                "m": [
                    1720,
                    1719
                ],
                "tf.reduce_mean": [
                    1721,
                    1765,
                    1719
                ],
                "devs_squared": [
                    1720,
                    1721
                ],
                "tf.square": [
                    1720,
                    1841
                ],
                "tf.sqrt": [
                    1869,
                    1743
                ],
                "var": [
                    2144,
                    2176,
                    2147,
                    2339,
                    2313,
                    2314,
                    2345,
                    1743,
                    2193,
                    2165,
                    2330,
                    2331,
                    2142
                ],
                "tf.reduce_any": [
                    1783
                ],
                "tf.reduce_all": [
                    1801
                ],
                "tf.argmax": [
                    1815
                ],
                "tf.argmin": [
                    1829
                ],
                "tf.abs": [
                    1853
                ],
                "zero": [
                    3192,
                    3193,
                    1866,
                    1868
                ],
                "_to_tensor": [
                    1866,
                    1867,
                    3191,
                    3192,
                    3196
                ],
                "inf": [
                    1867,
                    1868
                ],
                "np.inf": [
                    1977,
                    1867,
                    1979
                ],
                "tf.clip_by_value": [
                    3193,
                    1980,
                    1868
                ],
                "tf.exp": [
                    1881
                ],
                "tf_math_ops.log": [
                    4427,
                    1893,
                    4390
                ],
                "tf.reduce_logsumexp": [
                    1917
                ],
                "tf.round": [
                    1931
                ],
                "tf.sign": [
                    1943
                ],
                "tf.pow": [
                    1957
                ],
                "min_value": [
                    1972,
                    1974,
                    1975,
                    1976,
                    1977,
                    1980
                ],
                "max_value": [
                    3170,
                    3178,
                    3183,
                    3191,
                    1973,
                    1974,
                    1975,
                    3193,
                    1978,
                    1979,
                    1980
                ],
                "tf.equal": [
                    1995
                ],
                "tf.not_equal": [
                    2010
                ],
                "tf.greater": [
                    2025,
                    3182
                ],
                "tf.greater_equal": [
                    2040
                ],
                "tf.less": [
                    2055
                ],
                "tf.less_equal": [
                    2070
                ],
                "tf.maximum": [
                    2085
                ],
                "tf.minimum": [
                    2100
                ],
                "tf.sin": [
                    2112
                ],
                "tf.cos": [
                    2124
                ],
                "tf.nn.moments": [
                    2165,
                    2142
                ],
                "reduction_axes": [
                    2272,
                    2211,
                    2256,
                    2258,
                    2260,
                    2165,
                    2263,
                    2169,
                    2266,
                    2268,
                    2142
                ],
                "normed": [
                    2144,
                    2193,
                    2186,
                    2147
                ],
                "tf.nn.batch_normalization": [
                    2144,
                    2345,
                    2186
                ],
                "beta": [
                    2181,
                    2184,
                    2315,
                    2316,
                    2317,
                    2318,
                    2326,
                    2327,
                    2336,
                    2345,
                    2222,
                    2223,
                    2229,
                    2230,
                    2235,
                    2259,
                    2263,
                    2267,
                    2271,
                    2145
                ],
                "gamma": [
                    2177,
                    2180,
                    2319,
                    2320,
                    2321,
                    2322,
                    2324,
                    2325,
                    2335,
                    2345,
                    2218,
                    2219,
                    2227,
                    2228,
                    2234,
                    2259,
                    2263,
                    2267,
                    2271,
                    2145
                ],
                "target_shape": [
                    2176,
                    2180,
                    2184,
                    2167,
                    2170,
                    2172,
                    2173,
                    2175
                ],
                "target_shape.append": [
                    2170,
                    2172
                ],
                "broadcast_mean": [
                    2188,
                    2175
                ],
                "broadcast_var": [
                    2176,
                    2189
                ],
                "broadcast_gamma": [
                    2178,
                    2180,
                    2191
                ],
                "broadcast_beta": [
                    2184,
                    2190,
                    2182
                ],
                "normalization_axis": [
                    2225,
                    2212,
                    2221,
                    2215
                ],
                "tf_data_format": [
                    2304,
                    2305,
                    3456,
                    3457,
                    3716,
                    3717,
                    3718,
                    3840,
                    3720,
                    3842,
                    3722,
                    3977,
                    3981,
                    3985,
                    3476,
                    3605,
                    3481,
                    3482,
                    3870,
                    3743,
                    2340,
                    2213,
                    3620,
                    3622,
                    2216,
                    3748,
                    3499,
                    3884,
                    4012,
                    3886,
                    4014,
                    3504,
                    3505,
                    4024,
                    4028,
                    2237,
                    4032,
                    3777,
                    3779,
                    3659,
                    3916,
                    3661,
                    3918,
                    3794,
                    3796,
                    3671,
                    3929,
                    3679,
                    3936,
                    3937,
                    3685,
                    3561,
                    3823,
                    3825,
                    3574,
                    3577,
                    2298,
                    3451,
                    2300,
                    3965,
                    2302,
                    3967
                ],
                "gamma.dtype": [
                    2227,
                    2324
                ],
                "tf.float32": [
                    2328,
                    2329,
                    2227,
                    2324,
                    2228,
                    2229,
                    2230,
                    2325,
                    2326,
                    2330,
                    2331,
                    2327
                ],
                "beta.dtype": [
                    2229,
                    2326
                ],
                "tf.nn.fused_batch_norm": [
                    2232,
                    2333
                ],
                "_has_nchw_support": [
                    2306,
                    3501,
                    2258,
                    4181,
                    3478,
                    3453
                ],
                "_broadcast_normalize_batch_in_training": [
                    2259,
                    2271
                ],
                "_fused_normalize_batch_in_training": [
                    2262
                ],
                "_regular_normalize_batch_in_training": [
                    2267
                ],
                "zeros_like": [
                    2316
                ],
                "ones_like": [
                    2320
                ],
                "mean.dtype": [
                    2328
                ],
                "var.dtype": [
                    2330
                ],
                "rank": [
                    2361,
                    2362,
                    2363
                ],
                "tensors": [
                    2369,
                    2361,
                    2366,
                    2367
                ],
                "tf.sparse.concat": [
                    2367
                ],
                "rows": [
                    2441,
                    2444,
                    2421,
                    2423,
                    2426
                ],
                "cols": [
                    2446,
                    2449,
                    2421,
                    2423,
                    2426
                ],
                "original_shape": [
                    2441,
                    2444,
                    2446,
                    2449,
                    2425
                ],
                "new_shape": [
                    2432,
                    2434,
                    4196,
                    4198,
                    4199,
                    4201,
                    4172,
                    4174,
                    4175,
                    4177,
                    2426,
                    2427
                ],
                "np.array": [
                    2427
                ],
                "height_factor": [
                    2481,
                    2476,
                    2427,
                    2444
                ],
                "width_factor": [
                    2449,
                    2482,
                    2427,
                    2477
                ],
                "permute_dimensions": [
                    2439,
                    4073,
                    4139,
                    4141,
                    2430
                ],
                "interpolation": [
                    2433,
                    2431
                ],
                "tf_image_ops.resize_nearest_neighbor": [
                    2432
                ],
                "tf_image_ops": [
                    2432,
                    2434
                ],
                "tf_image_ops.resize_bilinear": [
                    2434
                ],
                "new_height": [
                    2442,
                    2451,
                    2444
                ],
                "new_width": [
                    2449,
                    2451,
                    2447
                ],
                "x.set_shape": [
                    2452
                ],
                "transpose_shape": [
                    4199,
                    4175,
                    2703,
                    2452,
                    2744
                ],
                "output": [
                    3330,
                    4134,
                    4135,
                    4072,
                    4073,
                    2475,
                    2476,
                    2477,
                    2478,
                    4139,
                    2480,
                    2481,
                    2482,
                    2483,
                    3313,
                    4141,
                    4142,
                    3287
                ],
                "repeat_elements": [
                    2475,
                    2476,
                    2477,
                    2480,
                    2481,
                    2482
                ],
                "depth_factor": [
                    2480,
                    2475
                ],
                "splits": [
                    2506,
                    2508
                ],
                "tf.split": [
                    2506
                ],
                "x_rep": [
                    2529,
                    2533,
                    2534,
                    2535,
                    2508,
                    2509,
                    2519,
                    2522
                ],
                "rep": [
                    2521,
                    2508,
                    2526
                ],
                "concatenate": [
                    4133,
                    2509,
                    4070,
                    4359
                ],
                "auxiliary_axis": [
                    2525,
                    2521,
                    2517,
                    2519
                ],
                "reps": [
                    2528,
                    2520,
                    2521,
                    2522,
                    2525,
                    2526,
                    2527
                ],
                "np.ones": [
                    2520
                ],
                "tf.tile": [
                    3075,
                    4355,
                    2607,
                    2554,
                    2522,
                    4351
                ],
                "np.delete": [
                    2525
                ],
                "x_rep.set_shape": [
                    2533
                ],
                "x_rep._keras_shape": [
                    2534
                ],
                "n": [
                    2553,
                    2605,
                    2606,
                    2607
                ],
                "stop": [
                    2578,
                    2588
                ],
                "start": [
                    2817,
                    2580,
                    2581,
                    2584,
                    2585,
                    2586,
                    2588
                ],
                "tf.cond": [
                    2584,
                    3049
                ],
                "start.dtype": [
                    2585
                ],
                "tf.range": [
                    4355,
                    4351,
                    2588,
                    4343
                ],
                "step": [
                    2588
                ],
                "cast": [
                    2590
                ],
                "prod": [
                    2633
                ],
                "padding": [
                    2694,
                    2695,
                    2696,
                    3721,
                    3976,
                    2700,
                    2701,
                    3980,
                    3607,
                    3742,
                    3871,
                    3619,
                    2731,
                    2732,
                    2733,
                    2734,
                    3883,
                    4013,
                    2739,
                    2740,
                    2741,
                    4023,
                    4027,
                    3520,
                    3521,
                    3522,
                    3523,
                    3778,
                    3525,
                    3526,
                    3793,
                    3670,
                    3928,
                    3678,
                    3551,
                    3935,
                    3683,
                    3559,
                    3560,
                    3824,
                    2674,
                    2675,
                    3573,
                    3966,
                    3839
                ],
                "tf.pad": [
                    2704,
                    2746,
                    2676
                ],
                "normalize_data_format": [
                    3649,
                    4164,
                    3868,
                    3912,
                    2697,
                    4010,
                    3821,
                    2735,
                    4112,
                    3603,
                    4058,
                    3963,
                    3548,
                    3710,
                    3775
                ],
                "tf.one_hot": [
                    2776
                ],
                "num_classes": [
                    2776
                ],
                "tf.reverse": [
                    2794
                ],
                "tf.slice": [
                    2817
                ],
                "x.eval": [
                    2833
                ],
                "tf_keras_backend.batch_get_value": [
                    2847
                ],
                "ops": [
                    2847
                ],
                "tf_keras_backend.set_value": [
                    2858
                ],
                "tf_keras_backend.batch_set_value": [
                    2868
                ],
                "tuples": [
                    2868
                ],
                "tf.Print": [
                    2903
                ],
                "message": [
                    2903
                ],
                "v1_variable_initialization": [
                    2910
                ],
                "tf_keras_backend.function": [
                    2911
                ],
                "inputs": [
                    4130,
                    4068,
                    2911,
                    3002,
                    4127
                ],
                "outputs": [
                    3001,
                    3012,
                    2911
                ],
                "updates": [
                    2912
                ],
                "tf.gradients": [
                    2928,
                    2929
                ],
                "loss": [
                    2928,
                    2929
                ],
                "tf.stop_gradient": [
                    2945,
                    2947
                ],
                "last_output": [
                    3009,
                    3010,
                    3011,
                    3012,
                    3001
                ],
                "new_states": [
                    3001,
                    3012
                ],
                "tf_keras_backend.rnn": [
                    3001
                ],
                "step_function": [
                    3002
                ],
                "initial_states": [
                    3002
                ],
                "go_backwards": [
                    3003
                ],
                "mask": [
                    3004
                ],
                "constants": [
                    3005
                ],
                "unroll": [
                    3006
                ],
                "input_length": [
                    4387,
                    4393,
                    4428,
                    4433,
                    4437,
                    3007
                ],
                "reachable": [
                    3008,
                    3010
                ],
                "tf_utils.get_reachable_from_inputs": [
                    3008
                ],
                "tf_utils": [
                    3008
                ],
                "learning_phase": [
                    3008,
                    3100
                ],
                "last_output._uses_learning_phase": [
                    3011
                ],
                "condition.dtype": [
                    3035
                ],
                "condition": [
                    3075,
                    3076,
                    3049,
                    3037,
                    3035,
                    3036,
                    3069,
                    3070
                ],
                "cond_ndim": [
                    3061,
                    3065,
                    3067,
                    3068,
                    3037,
                    3038
                ],
                "then_expression": [
                    3041,
                    3043,
                    3076,
                    3056,
                    3057,
                    3060,
                    3071,
                    3039
                ],
                "then_expression_fn": [
                    3050,
                    3043
                ],
                "else_expression": [
                    3044,
                    3076,
                    3046,
                    3048,
                    3058,
                    3059
                ],
                "else_expression_fn": [
                    3048,
                    3051
                ],
                "expr_ndim": [
                    3066,
                    3060,
                    3061,
                    3068
                ],
                "ndim_diff": [
                    3068,
                    3069
                ],
                "cond_shape": [
                    3072,
                    3069,
                    3070
                ],
                "expr_shape": [
                    3072,
                    3073,
                    3074,
                    3071
                ],
                "shape_diff": [
                    3072,
                    3074
                ],
                "zero_expr_shape": [
                    3073,
                    3074
                ],
                "tile_shape": [
                    3074,
                    3075
                ],
                "tf.where": [
                    3217,
                    3074,
                    3076
                ],
                "training": [
                    3105,
                    3142,
                    3111,
                    3118,
                    3099,
                    3100
                ],
                "uses_learning_phase": [
                    3119,
                    3101,
                    3103
                ],
                "alt": [
                    3142,
                    3112,
                    3113,
                    3115,
                    3118
                ],
                "switch": [
                    3118
                ],
                "in_train_phase": [
                    3142
                ],
                "alpha": [
                    3169,
                    3171,
                    3214,
                    3217,
                    3195,
                    3196,
                    3197
                ],
                "threshold": [
                    3170,
                    3173,
                    3174,
                    3180,
                    3182
                ],
                "tf.nn.leaky_relu": [
                    3171
                ],
                "negative_part": [
                    3176,
                    3197,
                    3174
                ],
                "tf.nn.relu": [
                    3176,
                    3188,
                    3174
                ],
                "clip_max": [
                    3178,
                    3186,
                    3190
                ],
                "tf.nn.relu6": [
                    3185
                ],
                "res": [
                    3217,
                    3213,
                    3215
                ],
                "tf.nn.elu": [
                    3213
                ],
                "tf.nn.softmax": [
                    3233
                ],
                "tf.nn.softplus": [
                    3247
                ],
                "tf.nn.softsign": [
                    3261
                ],
                "tf_keras_backend.categorical_crossentropy": [
                    3286
                ],
                "target": [
                    3313,
                    3330,
                    3287
                ],
                "from_logits": [
                    3313,
                    3330,
                    3287
                ],
                "tf_keras_backend.sparse_categorical_crossentropy": [
                    3312
                ],
                "tf_keras_backend.binary_crossentropy": [
                    3329
                ],
                "tf.nn.sigmoid": [
                    3344
                ],
                "tf_keras_backend.hard_sigmoid": [
                    3362
                ],
                "tf.nn.tanh": [
                    3376
                ],
                "tf.nn.dropout": [
                    3396
                ],
                "level": [
                    3396
                ],
                "noise_shape": [
                    3396
                ],
                "tf.nn.l2_normalize": [
                    3411
                ],
                "tf.nn.in_top_k": [
                    3429
                ],
                "predictions": [
                    3429
                ],
                "targets": [
                    3430
                ],
                "k": [
                    3431
                ],
                "StrictVersion": [
                    3449,
                    3474,
                    3497
                ],
                "tf.__version__.split": [
                    3449,
                    3474,
                    3497
                ],
                "force_transpose": [
                    3657,
                    3659,
                    3478,
                    3655
                ],
                "kernel_shape": [
                    3557,
                    3550,
                    4116,
                    4117,
                    4061,
                    4062
                ],
                "kernel.shape.as_list": [
                    3550
                ],
                "kernel.shape": [
                    3550
                ],
                "kernel": [
                    3617,
                    3683,
                    4134,
                    4072,
                    3881,
                    4061,
                    3934,
                    3571,
                    4116,
                    3677,
                    3550
                ],
                "left_pad": [
                    3557,
                    3558
                ],
                "dilation_rate": [
                    3713,
                    3714,
                    3731,
                    3736,
                    3738,
                    3612,
                    3614,
                    3876,
                    3878,
                    3654,
                    3787,
                    3789,
                    3676,
                    3681,
                    3683,
                    3557,
                    3566,
                    3568,
                    3833,
                    3835
                ],
                "temporal_padding": [
                    3558
                ],
                "_preprocess_padding": [
                    3778,
                    3560,
                    3721,
                    4013,
                    3824,
                    3670,
                    3607,
                    3928,
                    3966,
                    3871
                ],
                "_preprocess_conv1d_input": [
                    3561,
                    3716
                ],
                "tf.nn.convolution": [
                    3616,
                    3570,
                    3880
                ],
                "strides": [
                    3712,
                    3968,
                    3971,
                    3975,
                    3979,
                    3724,
                    3727,
                    4114,
                    3741,
                    3618,
                    3882,
                    4015,
                    4018,
                    4022,
                    4026,
                    3780,
                    3782,
                    3792,
                    3672,
                    3930,
                    3674,
                    3932,
                    3677,
                    3934,
                    4060,
                    3826,
                    3572,
                    3828,
                    3838,
                    3711
                ],
                "_preprocess_conv2d_input": [
                    3777,
                    3659,
                    3823,
                    3605,
                    3965
                ],
                "tf.nn.conv2d_transpose": [
                    3677
                ],
                "tf.nn.atrous_conv2d_transpose": [
                    3682
                ],
                "spatial_start_dim": [
                    3728,
                    3746,
                    3723,
                    3726
                ],
                "depthwise_kernel": [
                    3729,
                    3740,
                    3837,
                    3791
                ],
                "pointwise_kernel": [
                    3730,
                    3740,
                    3791
                ],
                "tf.nn.separable_conv2d": [
                    3740,
                    3791
                ],
                "tf.nn.depthwise_conv2d": [
                    3837
                ],
                "_preprocess_conv3d_input": [
                    4012,
                    3916,
                    3870
                ],
                "tf.nn.conv3d_transpose": [
                    3934
                ],
                "pool_size": [
                    3969,
                    3972,
                    3975,
                    3979,
                    4016,
                    4019,
                    4022,
                    4026
                ],
                "pool_mode": [
                    3974,
                    3978,
                    3983,
                    4021,
                    4025,
                    4030
                ],
                "tf.nn.max_pool": [
                    3975
                ],
                "tf.nn.avg_pool": [
                    3979
                ],
                "tf.nn.max_pool3d": [
                    4022
                ],
                "tf.nn.avg_pool3d": [
                    4026
                ],
                "stride": [
                    4066,
                    4067,
                    4060
                ],
                "output_length": [
                    4065,
                    4062
                ],
                "feature_dim": [
                    4128,
                    4131,
                    4069,
                    4117,
                    4062
                ],
                "filters": [
                    4136,
                    4117,
                    4062
                ],
                "xs": [
                    4064,
                    4130,
                    4068,
                    4133,
                    4070,
                    4119,
                    4127
                ],
                "slice_length": [
                    4066,
                    4068
                ],
                "kernel_size": [
                    4123,
                    4067,
                    4125
                ],
                "xs.append": [
                    4130,
                    4068,
                    4127
                ],
                "reshape": [
                    4193,
                    4130,
                    4068,
                    4135,
                    4201,
                    4177,
                    4185,
                    4187,
                    4127
                ],
                "x_aggregate": [
                    4072,
                    4134,
                    4133,
                    4070
                ],
                "batch_dot": [
                    4072,
                    4134
                ],
                "stride_row": [
                    4123,
                    4114,
                    4122
                ],
                "stride_col": [
                    4114,
                    4124,
                    4125
                ],
                "output_row": [
                    4120,
                    4136,
                    4115
                ],
                "output_col": [
                    4136,
                    4121,
                    4115
                ],
                "j": [
                    4121,
                    4124,
                    4125
                ],
                "slice_row": [
                    4122,
                    4130,
                    4127
                ],
                "slice_col": [
                    4130,
                    4124,
                    4127
                ],
                "bias_shape": [
                    4193,
                    4195,
                    4196,
                    4165,
                    4166,
                    4198,
                    4169,
                    4171,
                    4172,
                    4174,
                    4180,
                    4185,
                    4187,
                    4189
                ],
                "bias": [
                    4193,
                    4165,
                    4201,
                    4203,
                    4177,
                    4182,
                    4185,
                    4187,
                    4190
                ],
                "tf.nn.bias_add": [
                    4190,
                    4203,
                    4182
                ],
                "is_symbolic": [
                    4256,
                    4281,
                    4228,
                    4312
                ],
                "stddev": [
                    4228,
                    4231,
                    4234,
                    4312,
                    4315,
                    4318
                ],
                "tf_keras_backend.random_normal": [
                    4233,
                    4230
                ],
                "minval": [
                    4256,
                    4259,
                    4262
                ],
                "maxval": [
                    4256,
                    4259,
                    4262
                ],
                "tf_keras_backend.random_uniform": [
                    4258,
                    4261
                ],
                "p": [
                    4281,
                    4284,
                    4287
                ],
                "tf_keras_backend.random_binomial": [
                    4283,
                    4286
                ],
                "tf_keras_backend.truncated_normal": [
                    4314,
                    4317
                ],
                "label_shape": [
                    4352,
                    4355,
                    4356,
                    4365,
                    4366,
                    4338,
                    4339,
                    4340,
                    4343,
                    4346,
                    4351
                ],
                "labels": [
                    4338,
                    4362
                ],
                "num_batches_tns": [
                    4339,
                    4351
                ],
                "max_num_labels_tns": [
                    4344,
                    4355,
                    4340
                ],
                "tf.fill": [
                    4346,
                    4343
                ],
                "current_input": [
                    4344
                ],
                "init": [
                    4346,
                    4348
                ],
                "dense_mask": [
                    4353,
                    4357,
                    4347,
                    4349
                ],
                "functional_ops.scan": [
                    4347
                ],
                "functional_ops": [
                    4347
                ],
                "range_less_than": [
                    4347
                ],
                "label_lengths": [
                    4347
                ],
                "label_array": [
                    4353,
                    4351
                ],
                "label_ind": [
                    4353,
                    4359
                ],
                "tf.boolean_mask": [
                    4353,
                    4357
                ],
                "tmp": [
                    4355,
                    4356
                ],
                "batch_array": [
                    4356,
                    4357
                ],
                "reverse": [
                    4356
                ],
                "batch_ind": [
                    4357,
                    4359
                ],
                "vals_sparse": [
                    4362,
                    4366
                ],
                "tf.gather_nd": [
                    4362
                ],
                "tf.int64": [
                    4364,
                    4365
                ],
                "label_length": [
                    4386,
                    4389
                ],
                "tf.int32": [
                    4386,
                    4387,
                    4428,
                    4389
                ],
                "sparse_labels": [
                    4392,
                    4388
                ],
                "ctc_label_dense_to_sparse": [
                    4389
                ],
                "y_true": [
                    4389
                ],
                "y_pred": [
                    4390,
                    4391,
                    4427,
                    4432,
                    4436
                ],
                "ctc.ctc_loss": [
                    4391
                ],
                "ctc": [
                    4435,
                    4431,
                    4391
                ],
                "greedy": [
                    4430
                ],
                "decoded": [
                    4441,
                    4435,
                    4431
                ],
                "log_prob": [
                    4435,
                    4444,
                    4431
                ],
                "ctc.ctc_greedy_decoder": [
                    4431
                ],
                "ctc.ctc_beam_search_decoder": [
                    4435
                ],
                "beam_width": [
                    4437
                ],
                "top_paths": [
                    4438
                ],
                "merge_repeated": [
                    4438
                ],
                "decoded_dense": [
                    4440,
                    4443,
                    4444
                ],
                "st": [
                    4441,
                    4442
                ],
                "dense_tensor": [
                    4442,
                    4443
                ],
                "decoded_dense.append": [
                    4443
                ],
                "tf.map_fn": [
                    4461
                ],
                "fn": [
                    4493,
                    4461,
                    4477
                ],
                "elems": [
                    4493,
                    4461,
                    4477
                ],
                "tf.foldl": [
                    4477
                ],
                "initializer": [
                    4477,
                    4493
                ],
                "tf.foldr": [
                    4493
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_1/keras/backend/theano_backend.py": {
            "buggy_functions": [
                {
                    "function_name": "print_tensor",
                    "function_code": "def print_tensor(x, message=''):\n    \"\"\"Print the message and the tensor when evaluated and return the same\n    tensor.\n    \"\"\"\n    p_op = Print(message)\n    return p_op(x)\n",
                    "decorators": [],
                    "docstring": "Print the message and the tensor when evaluated and return the same\ntensor.",
                    "start_line": 1380,
                    "end_line": 1385,
                    "variables": {
                        "p_op": [
                            1384,
                            1385
                        ],
                        "Print": [
                            1384
                        ],
                        "message": [
                            1384
                        ],
                        "x": [
                            1385
                        ]
                    },
                    "filtered_variables": {
                        "p_op": [
                            1384,
                            1385
                        ],
                        "Print": [
                            1384
                        ],
                        "message": [
                            1384
                        ],
                        "x": [
                            1385
                        ]
                    },
                    "diff_line_number": 1381,
                    "class_data": null,
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "inscope_functions": [
                "def learning_phase():\n    # False = test, True = train\n    return _LEARNING_PHASE",
                "def set_learning_phase(value):\n    global _LEARNING_PHASE\n    if value not in {0, 1}:\n        raise ValueError('Expected learning phase to be '\n                         '0 or 1.')\n    _LEARNING_PHASE = value",
                "def get_uid(prefix=''):\n    \"\"\"Provides a unique UID given a string prefix.\n\n    # Arguments\n        prefix: string.\n\n    # Returns\n        An integer.\n\n    # Example\n    ```python\n        >>> keras.backend.get_uid('dense')\n        1\n        >>> keras.backend.get_uid('dense')\n        2\n    ```\n\n    \"\"\"\n    _UID_PREFIXES[prefix] += 1\n    return _UID_PREFIXES[prefix]",
                "def reset_uids():\n    global _UID_PREFIXES\n    _UID_PREFIXES = defaultdict(int)",
                "def _assert_sparse_module():\n    if not th_sparse_module:\n        raise ImportError(\"Failed to import theano.sparse\\n\"\n                          \"You probably need to pip install nose-parameterized\")",
                "def is_sparse(tensor):\n    return th_sparse_module and isinstance(tensor.type, th_sparse_module.SparseType)",
                "def to_dense(tensor):\n    if is_sparse(tensor):\n        return th_sparse_module.dense_from_sparse(tensor)\n    else:\n        return tensor",
                "def _is_explicit_shape(shape):\n    if hasattr(shape, '__iter__'):\n        for x in shape:\n            if x is not None:\n                if not isinstance(x, int):\n                    return False\n        return True\n    return False",
                "@contextmanager\ndef name_scope(name):\n    global NAME_SCOPE_STACK\n    NAME_SCOPE_STACK.append(name)\n    yield\n    NAME_SCOPE_STACK.pop()",
                "def _prepare_name(name, default):\n    prefix = '/'.join(NAME_SCOPE_STACK)\n    if name is None:\n        return prefix + '/' + default\n    return prefix + '/' + name",
                "def variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if hasattr(value, 'tocoo'):\n        _assert_sparse_module()\n        variable = th_sparse_module.as_sparse_variable(\n            value, name=_prepare_name(name, 'variable'))\n    else:\n        if isinstance(value, (theano.tensor.TensorVariable,\n                              theano.tensor.sharedvar.TensorSharedVariable,\n                              theano.tensor.TensorConstant)):\n            # Support for RandomStreams().normal(), .uniform().\n            value = value.eval()\n        value = np.asarray(value, dtype=dtype)\n        variable = theano.shared(value=value,\n                                 name=_prepare_name(name, 'variable'),\n                                 strict=False)\n    variable._keras_shape = value.shape\n    variable._uses_learning_phase = False\n    variable.constraint = constraint\n    return variable",
                "def constant(value, dtype=None, shape=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    if shape is None:\n        shape = ()\n    np_value = value * np.ones(shape)\n    const = T.constant(np_value,\n                       dtype=dtype,\n                       name=_prepare_name(name, 'constant'))\n    const._keras_shape = shape\n    const._uses_learning_phase = False\n    return const",
                "def is_keras_tensor(x):\n    \"\"\"Returns whether `x` is a Keras tensor.\n\n    A \"Keras tensor\" is a tensor that was returned by a Keras layer,\n    (`Layer` class) or by `Input`.\n\n    # Arguments\n        x: A candidate tensor.\n\n    # Returns\n        A boolean: Whether the argument is a Keras tensor.\n\n    # Raises\n        ValueError: In case `x` is not a symbolic tensor.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> from keras.layers import Input, Dense\n        >>> np_var = numpy.array([1, 2])\n        >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n        ValueError\n        >>> k_var = tf.placeholder('float32', shape=(1,1))\n        >>> # A variable indirectly created outside of keras is not a Keras tensor.\n        >>> K.is_keras_tensor(k_var)\n        False\n        >>> keras_var = K.variable(np_var)\n        >>> # A variable created with the keras backend is not a Keras tensor.\n        >>> K.is_keras_tensor(keras_var)\n        False\n        >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n        >>> # A placeholder is not a Keras tensor.\n        >>> K.is_keras_tensor(keras_placeholder)\n        False\n        >>> keras_input = Input([10])\n        >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\n        True\n        >>> keras_layer_output = Dense(10)(keras_input)\n        >>> # Any Keras layer output is a Keras tensor.\n        >>> K.is_keras_tensor(keras_layer_output)\n        True\n    ```\n    \"\"\"\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')",
                "def is_tensor(x):\n    return isinstance(x, (T.TensorVariable,\n                          T.sharedvar.TensorSharedVariable))",
                "def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n    \"\"\"Instantiate an input data placeholder variable.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if shape is None and ndim is None:\n        raise ValueError('Specify either a shape or ndim value.')\n    if shape is not None:\n        ndim = len(shape)\n    else:\n        shape = tuple([None for _ in range(ndim)])\n\n    name = _prepare_name(name, 'placeholder')\n    broadcast = (False,) * ndim\n    if sparse:\n        _assert_sparse_module()\n        x = th_sparse_module.csr_matrix(name=name, dtype=dtype)\n    else:\n        x = T.TensorType(dtype, broadcast)(name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    x._theano_placeholder = True\n    return x",
                "def is_placeholder(x):\n    \"\"\"Returns whether `x` is a placeholder.\n\n    # Arguments\n        x: A candidate placeholder.\n\n    # Returns\n        Boolean.\n    \"\"\"\n    return hasattr(x, '_theano_placeholder') and x._theano_placeholder",
                "def shape(x):\n    \"\"\"Returns the shape of a tensor.\n\n    Warning: type returned will be different for\n    Theano backend (Theano tensor type) and TF backend (TF TensorShape).\n    \"\"\"\n    return x.shape",
                "def int_shape(x):\n    \"\"\"Returns the shape of a Keras tensor or a Keras variable as a tuple of\n    integers or None entries.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tuple of integers (or None entries).\n    \"\"\"\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n    else:\n        return None",
                "def ndim(x):\n    return x.ndim",
                "def dtype(x):\n    return x.dtype",
                "def eval(x):\n    \"\"\"Returns the value of a tensor.\n    \"\"\"\n    return to_dense(x).eval()",
                "def zeros(shape, dtype=None, name=None):\n    \"\"\"Instantiates an all-zeros variable.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    return variable(np.zeros(shape), dtype, name)",
                "def ones(shape, dtype=None, name=None):\n    \"\"\"Instantiates an all-ones variable.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    return variable(np.ones(shape), dtype, name)",
                "def eye(size, dtype=None, name=None):\n    \"\"\"Instantiates an identity matrix.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    return variable(np.eye(size), dtype, name)",
                "def ones_like(x, dtype=None, name=None):\n    return T.ones_like(x, dtype=dtype)",
                "def zeros_like(x, dtype=None, name=None):\n    return T.zeros_like(x, dtype=dtype)",
                "def identity(x, name=None):\n    \"\"\"Returns a tensor with the same content as the input tensor.\n\n    # Arguments\n        x: The input tensor.\n        name: String, name for the variable to create.\n\n    # Returns\n        A tensor of the same shape, type and content.\n    \"\"\"\n    return x.copy(name=name)",
                "def random_uniform_variable(shape, low, high, dtype=None, name=None):\n    return variable(np.random.uniform(low=low, high=high, size=shape),\n                    dtype=dtype, name=name)",
                "def random_normal_variable(shape, mean, scale, dtype=None, name=None):\n    return variable(np.random.normal(loc=0.0, scale=scale, size=shape),\n                    dtype=dtype, name=name)",
                "def count_params(x):\n    \"\"\"Returns the number of scalars in a tensor.\n\n    Return: numpy integer.\n    \"\"\"\n    # We don't want those compilation to show up in Theano profiler.\n    f = theano.function([], x.shape, profile=False)\n    return np.prod(f())",
                "def cast(x, dtype):\n    return T.cast(x, dtype)",
                "def update(x, new_x):\n    return (x, new_x)",
                "def update_add(x, increment):\n    return (x, x + increment)",
                "def update_sub(x, decrement):\n    return (x, x - decrement)",
                "def moving_average_update(variable, value, momentum):\n    return (variable, variable * momentum + value * (1. - momentum))",
                "def dot(x, y):\n    if is_sparse(x):\n        out = th_sparse_module.basic.structured_dot(x, y)\n    else:\n        out = T.dot(x, y)\n    if hasattr(x, '_keras_shape') and hasattr(y, '_keras_shape'):\n        x_shape = list(x._keras_shape)\n        y_shape = list(y._keras_shape)\n        if len(x_shape) > 0:\n            x_shape.pop()\n        if len(y_shape) == 1:\n            y_shape.pop()\n        elif len(y_shape) > 1:\n            y_shape.pop(-2)\n        out._keras_shape = tuple(x_shape + y_shape)\n    return out",
                "def batch_dot(x, y, axes=None):\n    \"\"\"Batchwise dot product.\n\n    batch_dot results in a tensor with less dimensions than the input.\n    If the number of dimensions is reduced to 1, we use `expand_dims` to\n    make sure that ndim is at least 2.\n\n    # Arguments\n        x, y: tensors with ndim >= 2\n        axes: list (or single) int with target dimensions\n\n    # Returns\n        A tensor with shape equal to the concatenation of x's shape\n        (less the dimension that was summed over) and y's shape\n        (less the batch dimension and the dimension that was summed over).\n        If the final rank is 1, we reshape it to (batch_size, 1).\n\n    # Examples\n        Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\n        batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\n        of x.dot(y.T), although we never have to calculate the off-diagonal\n        elements.\n\n        Shape inference:\n        Let x's shape be (100, 20) and y's shape be (100, 30, 20).\n        If dot_axes is (1, 2), to find the output shape of resultant tensor,\n            loop through each dimension in x's shape and y's shape:\n        x.shape[0] : 100 : append to output shape\n        x.shape[1] : 20 : do not append to output shape,\n            dimension 1 of x has been summed over. (dot_axes[0] = 1)\n        y.shape[0] : 100 : do not append to output shape,\n            always ignore first dimension of y\n        y.shape[1] : 30 : append to output shape\n        y.shape[2] : 20 : do not append to output shape,\n            dimension 2 of y has been summed over. (dot_axes[1] = 2)\n\n        output_shape = (100, 30)\n    \"\"\"\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        if y.ndim == 2:\n            axes = [x.ndim - 1, y.ndim - 1]\n        else:\n            axes = [x.ndim - 1, y.ndim - 2]\n    if py_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n    if isinstance(axes, tuple):\n        axes = list(axes)\n\n    if 0 in axes:\n        raise ValueError('Can not perform batch_dot over axis 0.'\n                         'If your inputs are not batched,'\n                         ' add a dummy batch dimension to your '\n                         'inputs using K.expand_dims(x, 0)')\n\n    out = T.batched_tensordot(x, y, axes=axes)\n    if ndim(out) == 1:\n        out = expand_dims(out, 1)\n\n    if hasattr(x, '_keras_shape') and hasattr(y, '_keras_shape'):\n        shape = []\n        for axis in range(len(x._keras_shape)):\n            if axis != axes[0]:\n                shape.append(x._keras_shape[axis])\n        for axis in range(1, len(y._keras_shape)):\n            if axis != axes[1]:\n                shape.append(y._keras_shape[axis])\n        if len(shape) == 1:\n            shape.append(1)     # Expand dims if ndim == 1\n        out._keras_shape = tuple(shape)\n    return out",
                "def transpose(x):\n    y = T.transpose(x)\n    if hasattr(x, '_keras_shape'):\n        y._keras_shape = tuple(reversed(x._keras_shape))\n    return y",
                "def gather(reference, indices):\n    \"\"\"Retrieves the elements of indices `indices` in the tensor `reference`.\n\n    # Arguments\n        reference: A tensor.\n        indices: An integer tensor of indices.\n\n    # Returns\n        A tensor of same type as `reference`.\n    \"\"\"\n    y = reference[indices]\n    if hasattr(reference, '_keras_shape') and hasattr(indices, '_keras_shape'):\n        y._keras_shape = indices._keras_shape + reference._keras_shape[1:]\n    return y",
                "def max(x, axis=None, keepdims=False):\n    return T.max(x, axis=axis, keepdims=keepdims)",
                "def min(x, axis=None, keepdims=False):\n    return T.min(x, axis=axis, keepdims=keepdims)",
                "def sum(x, axis=None, keepdims=False):\n    \"\"\"Sum of the values in a tensor, alongside the specified axis.\n    \"\"\"\n    return T.sum(x, axis=axis, keepdims=keepdims)",
                "def prod(x, axis=None, keepdims=False):\n    \"\"\"Multiply the values in a tensor, alongside the specified axis.\n    \"\"\"\n    return T.prod(x, axis=axis, keepdims=keepdims)",
                "def cumsum(x, axis=0):\n    \"\"\"Cumulative sum of the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer, the axis to compute the sum.\n\n    # Returns\n        A tensor of the cumulative sum of values of `x` along `axis`.\n    \"\"\"\n    return T.extra_ops.cumsum(x, axis=axis)",
                "def cumprod(x, axis=0):\n    \"\"\"Cumulative product of the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer, the axis to compute the product.\n\n    # Returns\n        A tensor of the cumulative product of values of `x` along `axis`.\n    \"\"\"\n    return T.extra_ops.cumprod(x, axis=axis)",
                "def mean(x, axis=None, keepdims=False):\n    \"\"\"Mean of a tensor, alongside the specified axis.\n    \"\"\"\n    dtype = None\n    # bool is available since theano v0.9dev\n    if 'int' in x.dtype or x.dtype == 'bool':\n        dtype = floatx()\n    return T.mean(x, axis=axis, keepdims=keepdims, dtype=dtype)",
                "def std(x, axis=None, keepdims=False):\n    return T.std(x, axis=axis, keepdims=keepdims)",
                "def var(x, axis=None, keepdims=False):\n    return T.var(x, axis=axis, keepdims=keepdims)",
                "def any(x, axis=None, keepdims=False):\n    \"\"\"Bitwise reduction (logical OR).\n    \"\"\"\n    y = T.any(x, axis=axis, keepdims=keepdims)\n    y = _set_keras_shape_for_reduction(x, y, axis, keepdims)\n    return y",
                "def all(x, axis=None, keepdims=False):\n    \"\"\"Bitwise reduction (logical AND).\n    \"\"\"\n    y = T.all(x, axis=axis, keepdims=keepdims)\n    y = _set_keras_shape_for_reduction(x, y, axis, keepdims)\n    return y",
                "def _set_keras_shape_for_reduction(x, y, axis, keepdims):\n    if hasattr(x, '_keras_shape'):\n        if axis is None:\n            y._keras_shape = (1,) * len(x._keras_shape) if keepdims else (1,)\n        else:\n            if isinstance(axis, int):\n                axis_list = [axis]\n            else:\n                axis_list = list(set(int(a) for a in axis))\n            keras_shape_list = list(x._keras_shape)\n            if keepdims:\n                for a in axis_list:\n                    keras_shape_list[a] = 1\n            else:\n                for a in axis_list[::-1]:\n                    keras_shape_list.pop(a)\n                if not keras_shape_list:\n                    keras_shape_list = (1,)\n            y._keras_shape = tuple(keras_shape_list)\n    return y",
                "def argmax(x, axis=-1):\n    return T.argmax(x, axis=axis, keepdims=False)",
                "def argmin(x, axis=-1):\n    return T.argmin(x, axis=axis, keepdims=False)",
                "def square(x):\n    return T.sqr(x)",
                "def abs(x):\n    return T.abs_(x)",
                "def sqrt(x):\n    x = T.clip(x, 0., np.inf)\n    return T.sqrt(x)",
                "def exp(x):\n    return T.exp(x)",
                "def log(x):\n    return T.log(x)",
                "def logsumexp(x, axis=None, keepdims=False):\n    \"\"\"Computes log(sum(exp(elements across dimensions of a tensor))).\n\n    This function is more numerically stable than log(sum(exp(x))).\n    It avoids overflows caused by taking the exp of large inputs and\n    underflows caused by taking the log of small inputs.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer, the axis to reduce over.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`, the reduced dimension is\n            retained with length 1.\n\n    # Returns\n        The reduced tensor.\n    \"\"\"\n    # Theano has a built-in optimization for logsumexp\n    # (see https://github.com/Theano/Theano/pull/4736)\n    # so we can just write the expression directly:\n    return T.log(T.sum(T.exp(x), axis=axis, keepdims=keepdims))",
                "def round(x):\n    return T.round(x, mode='half_to_even')",
                "def sign(x):\n    return T.sgn(x)",
                "def pow(x, a):\n    return T.pow(x, a)",
                "def clip(x, min_value, max_value):\n    if (isinstance(min_value, (int, float)) and\n            isinstance(max_value, (int, float))):\n        if max_value < min_value:\n            max_value = min_value\n    if min_value is None:\n        min_value = -np.inf\n    if max_value is None:\n        max_value = np.inf\n    return T.clip(x, min_value, max_value)",
                "def equal(x, y):\n    return T.eq(x, y)",
                "def not_equal(x, y):\n    z = T.neq(x, y)\n    if hasattr(x, '_keras_shape'):\n        z._keras_shape = x._keras_shape\n    elif hasattr(y, '_keras_shape'):\n        z._keras_shape = y._keras_shape\n    return z",
                "def greater(x, y):\n    return T.gt(x, y)",
                "def greater_equal(x, y):\n    return T.ge(x, y)",
                "def less(x, y):\n    return T.lt(x, y)",
                "def less_equal(x, y):\n    return T.le(x, y)",
                "def maximum(x, y):\n    return T.maximum(x, y)",
                "def minimum(x, y):\n    return T.minimum(x, y)",
                "def sin(x):\n    return T.sin(x)",
                "def cos(x):\n    return T.cos(x)",
                "def normalize_batch_in_training(x, gamma, beta,\n                                reduction_axes, epsilon=1e-3):\n    \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n    \"\"\"\n    # TODO remove this if statement when Theano without\n    # T.nnet.bn.batch_normalization_train is deprecated\n    if not hasattr(T.nnet.bn, 'batch_normalization_train'):\n        return _old_normalize_batch_in_training(\n            x, gamma, beta, reduction_axes, epsilon)\n\n    if gamma is None:\n        if beta is None:\n            gamma = ones_like(x)\n        else:\n            gamma = ones_like(beta)\n    if beta is None:\n        if gamma is None:\n            beta = zeros_like(x)\n        beta = zeros_like(gamma)\n\n    normed, mean, stdinv = T.nnet.bn.batch_normalization_train(\n        x, gamma, beta, reduction_axes, epsilon)\n\n    return normed, mean, T.inv(stdinv ** 2)",
                "def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n    \"\"\"Apply batch normalization on x given mean, var, beta and gamma.\n    \"\"\"\n    # TODO remove this if statement when Theano without\n    # T.nnet.bn.batch_normalization_test is deprecated\n    if not hasattr(T.nnet.bn, 'batch_normalization_test'):\n        return _old_batch_normalization(x, mean, var, beta, gamma, epsilon)\n\n    if gamma is None:\n        gamma = ones_like(var)\n    if beta is None:\n        beta = zeros_like(mean)\n\n    if mean.ndim == 1:\n        # based on TensorFlow's default: normalize along rightmost dimension\n        reduction_axes = list(range(x.ndim - 1))\n    else:\n        reduction_axes = [i for i in range(x.ndim) if mean.broadcastable[i]]\n\n    return T.nnet.bn.batch_normalization_test(\n        x, gamma, beta, mean, var, reduction_axes, epsilon)",
                "def _old_normalize_batch_in_training(x, gamma, beta, reduction_axes,\n                                     epsilon=1e-3):  # pragma: no cover\n    \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n    \"\"\"\n    if gamma is None:\n        gamma = ones_like(x)\n    if beta is None:\n        beta = zeros_like(x)\n\n    dev = theano.config.device\n    use_cudnn = (ndim(x) < 5 and\n                 reduction_axes == [0, 2, 3] and\n                 (dev.startswith('cuda') or dev.startswith('gpu')))\n    if use_cudnn:\n        broadcast_beta = beta.dimshuffle('x', 0, 'x', 'x')\n        broadcast_gamma = gamma.dimshuffle('x', 0, 'x', 'x')\n        try:\n            trained = theano.sandbox.cuda.dnn.dnn_batch_normalization_train(\n                x, broadcast_gamma, broadcast_beta, 'spatial', epsilon)\n            normed, mean, stdinv = trained\n            normed = theano.tensor.as_tensor_variable(normed)\n            mean = theano.tensor.as_tensor_variable(mean)\n            stdinv = theano.tensor.as_tensor_variable(stdinv)\n            var = T.inv(stdinv ** 2)\n            return normed, T.flatten(mean), T.flatten(var)\n        except AttributeError:\n            pass\n\n    var = x.var(reduction_axes)\n    mean = x.mean(reduction_axes)\n\n    target_shape = []\n    for axis in range(ndim(x)):\n        if axis in reduction_axes:\n            target_shape.append(1)\n        else:\n            target_shape.append(x.shape[axis])\n    target_shape = T.stack(*target_shape)\n\n    broadcast_mean = T.reshape(mean, target_shape)\n    broadcast_var = T.reshape(var, target_shape)\n    broadcast_beta = T.reshape(beta, target_shape)\n    broadcast_gamma = T.reshape(gamma, target_shape)\n    normed = batch_normalization(x, broadcast_mean, broadcast_var,\n                                 broadcast_beta, broadcast_gamma,\n                                 epsilon)\n    return normed, mean, var",
                "def _old_batch_normalization(x, mean, var, beta, gamma,\n                             epsilon=1e-3):  # pragma: no cover\n    \"\"\"Apply batch normalization on x given mean, var, beta and gamma.\n    \"\"\"\n    if gamma is None:\n        gamma = ones_like(var)\n    if beta is None:\n        beta = zeros_like(mean)\n\n    if mean.ndim == 1 and x.ndim > 1:\n        # in TensorFlow's batch_normalization, if the parameters are vectors\n        # the batch normalization should be applied along the rightmost axis.\n        # Theano expects the parameters to always have x.ndim dimensions.\n        shuffle_pattern = ['x'] * (x.ndim - 1) + [0]\n        mean = mean.dimshuffle(shuffle_pattern)\n        var = var.dimshuffle(shuffle_pattern)\n        beta = beta.dimshuffle(shuffle_pattern)\n        gamma = gamma.dimshuffle(shuffle_pattern)\n\n    ndim = x.ndim\n    dev = theano.config.device\n    use_cudnn = ndim < 5 and (dev.startswith('cuda') or dev.startswith('gpu'))\n    if use_cudnn:\n        try:\n            axis = mean.broadcastable.index(False)\n            if axis != 1:\n                shuffle_pattern = list(range(ndim))\n                shuffle_pattern[1] = shuffle_pattern[axis]\n                shuffle_pattern[axis] = 1\n                result = theano.sandbox.cuda.dnn.dnn_batch_normalization_test(\n                    x.dimshuffle(shuffle_pattern),\n                    gamma.dimshuffle(shuffle_pattern),\n                    beta.dimshuffle(shuffle_pattern),\n                    mean.dimshuffle(shuffle_pattern),\n                    var.dimshuffle(shuffle_pattern),\n                    'spatial', epsilon).dimshuffle(shuffle_pattern)\n            else:\n                result = theano.sandbox.cuda.dnn.dnn_batch_normalization_test(\n                    x, gamma, beta, mean, var, 'spatial', epsilon)\n            return theano.tensor.as_tensor_variable(result)\n        except AttributeError:\n            pass\n        except ValueError:\n            pass\n    return T.nnet.bn.batch_normalization(x, gamma, beta, mean, sqrt(var + epsilon),\n                                         mode='high_mem')",
                "def concatenate(tensors, axis=-1):\n    if py_all([is_sparse(x) for x in tensors]):\n        axis = axis % ndim(tensors[0])\n        if axis == 0:\n            output = th_sparse_module.basic.vstack(tensors, format='csr')\n        elif axis == 1:\n            output = th_sparse_module.basic.hstack(tensors, format='csr')\n        else:\n            raise ValueError('Invalid concat axis for sparse matrix:', axis)\n    else:\n        output = T.concatenate([to_dense(x) for x in tensors], axis=axis)\n\n    if py_all([hasattr(tensor, '_keras_shape') for tensor in tensors]):\n        input_shapes = [tensor._keras_shape for tensor in tensors]\n        output_shape = list(input_shapes[0])\n        for shape in input_shapes[1:]:\n            if output_shape[axis] is None or shape[axis] is None:\n                output_shape[axis] = None\n                break\n            output_shape[axis] += shape[axis]\n        output._keras_shape = tuple(output_shape)\n\n    return output",
                "def reshape(x, shape):\n    y = T.reshape(x, shape)\n    shape = tuple(x if isinstance(x, int) and x > 0 else None for x in shape)\n    y._keras_shape = shape\n    if hasattr(x, '_uses_learning_phase'):\n        y._uses_learning_phase = x._uses_learning_phase\n    else:\n        y._uses_learning_phase = False\n    return y",
                "def permute_dimensions(x, pattern):\n    \"\"\"Transpose dimensions.\n\n    pattern should be a tuple or list of\n    dimension indices, e.g. [0, 2, 1].\n    \"\"\"\n    pattern = tuple(pattern)\n    y = x.dimshuffle(pattern)\n    if hasattr(x, '_keras_shape'):\n        y._keras_shape = tuple(np.asarray(x._keras_shape)[list(pattern)])\n    return y",
                "def repeat_elements(x, rep, axis):\n    \"\"\"Repeat the elements of a tensor along an axis, like np.repeat.\n\n    If x has shape (s1, s2, s3) and axis=1, the output\n    will have shape (s1, s2 * rep, s3).\n    \"\"\"\n    y = T.repeat(x, rep, axis=axis)\n    if hasattr(x, '_keras_shape'):\n        y._keras_shape = list(x._keras_shape)\n        repeat_dim = x._keras_shape[axis]\n        if repeat_dim is not None:\n                y._keras_shape[axis] = repeat_dim * rep\n        y._keras_shape = tuple(y._keras_shape)\n    return y",
                "def resize_images(x,\n                  height_factor,\n                  width_factor,\n                  data_format,\n                  interpolation='nearest'):\n    \"\"\"Resize the images contained in a 4D tensor of shape\n    - [batch, channels, height, width] (for 'channels_first' data_format)\n    - [batch, height, width, channels] (for 'channels_last' data_format)\n    by a factor of (height_factor, width_factor). Both factors should be\n    positive integers.\n    \"\"\"\n    if data_format == 'channels_first':\n        axis_1 = 2\n        axis_2 = 3\n    elif data_format == 'channels_last':\n        axis_1 = 1\n        axis_2 = 2\n    else:\n        raise ValueError('Invalid data_format:', data_format)\n\n    if interpolation == 'nearest':\n        output = repeat_elements(x, height_factor, axis=axis_1)\n        output = repeat_elements(output, width_factor, axis=axis_2)\n    elif interpolation == 'bilinear':\n        if not (height_factor == width_factor == 2):\n            raise NotImplementedError(\n                'Bilinear upscaling with factors other than (2, 2)'\n                'is not available when using the Theano backend.')\n        if data_format == 'channels_last':\n            output = permute_dimensions(x, [0, 3, 1, 2])\n        else:\n            output = x\n        output = T.nnet.abstract_conv.bilinear_upsampling(output,\n                                                          ratio=height_factor)\n        if data_format == 'channels_last':\n            output = permute_dimensions(output, [0, 2, 3, 1])\n        if hasattr(x, '_keras_shape'):\n            output._keras_shape = list(x._keras_shape)\n            output._keras_shape[axis_1] *= height_factor\n            output._keras_shape[axis_2] *= width_factor\n            output._keras_shape = tuple(output._keras_shape)\n    else:\n        raise ValueError('interpolation should be one of \"nearest\" or \"bilinear\".')\n\n    return output",
                "def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    \"\"\"Resize the volume contained in a 5D tensor of shape\n    - [batch, channels, depth, height, width] (for 'channels_first' data_format)\n    - [batch, depth, height, width, channels] (for 'channels_last' data_format)\n    by a factor of (depth_factor, height_factor, width_factor).\n    Both factors should be positive integers.\n    \"\"\"\n    if data_format == 'channels_first':\n        output = repeat_elements(x, depth_factor, axis=2)\n        output = repeat_elements(output, height_factor, axis=3)\n        output = repeat_elements(output, width_factor, axis=4)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, depth_factor, axis=1)\n        output = repeat_elements(output, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    else:\n        raise ValueError('Invalid data_format:', data_format)",
                "def repeat(x, n):\n    \"\"\"Repeat a 2D tensor.\n\n    If x has shape (samples, dim) and n=2,\n    the output will have shape (samples, 2, dim).\n    \"\"\"\n    assert x.ndim == 2\n    y = x.dimshuffle((0, 'x', 1))\n    y = T.extra_ops.repeat(y, n, axis=1)\n    if hasattr(x, '_keras_shape'):\n        shape = list(x._keras_shape)\n        shape.insert(1, n)\n        y._keras_shape = tuple(shape)\n\n    return y",
                "def arange(start, stop=None, step=1, dtype='int32'):\n    \"\"\"Creates a 1-D tensor containing a sequence of integers.\n\n    The function arguments use the same convention as\n    Theano's arange: if only one argument is provided,\n    it is in fact the \"stop\" argument.\n\n    The default type of the returned tensor is 'int32' to\n    match TensorFlow's default.\n    \"\"\"\n    return T.arange(start, stop=stop, step=step, dtype=dtype)",
                "def tile(x, n):\n    y = T.tile(x, n)\n    if hasattr(x, '_keras_shape'):\n        if _is_explicit_shape(n):\n            output_shape = x._keras_shape[:-len(n)]\n            for i, j in zip(x._keras_shape, n):\n                if i is None:\n                    output_shape += (None,)\n                else:\n                    output_shape += (i * j,)\n        elif isinstance(n, int):\n            output_shape = x._keras_shape[:-1]\n            if x._keras_shape[-1] is None:\n                output_shape += (None,)\n            else:\n                output_shape += (x._keras_shape[-1] * n,)\n        else:\n            # symbolic n\n            if n.ndim == 0:\n                # n is a scalar\n                output_shape = x._keras_shape[:-1] + (None,)\n            elif hasattr(n, '_keras_shape'):\n                # n is a vector\n                n_size = n._keras_shape[0]\n                output_shape = x._keras_shape[:-n_size] + (None,) * n_size\n            else:\n                output_shape = (None,) * x.ndim\n        y._keras_shape = output_shape\n    return y",
                "def flatten(x):\n    y = T.flatten(x)\n    if hasattr(x, '_keras_shape'):\n        if None in x._keras_shape:\n            y._keras_shape = (None,)\n        else:\n            y._keras_shape = (np.prod(x._keras_shape), )\n    return y",
                "def batch_flatten(x):\n    \"\"\"Turn a n-D tensor into a 2D tensor where\n    the first dimension is conserved.\n    \"\"\"\n    y = T.reshape(x, (x.shape[0], T.prod(x.shape[1:])))\n    if hasattr(x, '_keras_shape'):\n        if None in x._keras_shape[1:]:\n            y._keras_shape = (x._keras_shape[0], None)\n        else:\n            y._keras_shape = (x._keras_shape[0], np.prod(x._keras_shape[1:]))\n    return y",
                "def expand_dims(x, axis=-1):\n    \"\"\"Add a 1-sized dimension at index \"dim\".\n    \"\"\"\n    pattern = [i for i in range(x.type.ndim)]\n    if axis < 0:\n        if x.type.ndim == 0:\n            axis = 0\n        else:\n            axis = axis % x.type.ndim + 1\n    pattern.insert(axis, 'x')\n    y = x.dimshuffle(pattern)\n    if hasattr(x, '_keras_shape'):\n        shape = list(x._keras_shape)\n        shape.insert(axis, 1)\n        y._keras_shape = tuple(shape)\n    return y",
                "def squeeze(x, axis):\n    \"\"\"Remove a 1-dimension from the tensor at index \"axis\".\n    \"\"\"\n    shape = list(x.shape)\n    shape.pop(axis)\n    y = T.reshape(x, tuple(shape))\n    if hasattr(x, '_keras_shape'):\n        kshape = list(x._keras_shape)\n        kshape.pop(axis)\n        y._keras_shape = tuple(kshape)\n    return y",
                "def temporal_padding(x, padding=(1, 1)):\n    \"\"\"Pad the middle dimension of a 3D tensor\n    with \"padding\" zeros left and right.\n\n    Apologies for the inane API, but Theano makes this\n    really hard.\n    \"\"\"\n    assert len(padding) == 2\n    input_shape = x.shape\n    output_shape = (input_shape[0],\n                    input_shape[1] + padding[0] + padding[1],\n                    input_shape[2])\n    output = T.zeros(output_shape)\n    result = T.set_subtensor(output[:, padding[0]:x.shape[1] + padding[0], :], x)\n    if hasattr(x, '_keras_shape'):\n        result._keras_shape = (x._keras_shape[0],\n                               x._keras_shape[1] + py_sum(padding),\n                               x._keras_shape[2])\n    return result",
                "def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n    \"\"\"Pad the 2nd and 3rd dimensions of a 4D tensor\n    with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n    \"\"\"\n    assert len(padding) == 2\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    top_pad, bottom_pad = padding[0]\n    left_pad, right_pad = padding[1]\n    data_format = normalize_data_format(data_format)\n\n    input_shape = x.shape\n    if data_format == 'channels_first':\n        output_shape = (input_shape[0],\n                        input_shape[1],\n                        input_shape[2] + top_pad + bottom_pad,\n                        input_shape[3] + left_pad + right_pad)\n        output = T.zeros(output_shape)\n        indices = (py_slice(None),\n                   py_slice(None),\n                   py_slice(top_pad, input_shape[2] + top_pad),\n                   py_slice(left_pad, input_shape[3] + left_pad))\n\n    else:\n        output_shape = (input_shape[0],\n                        input_shape[1] + top_pad + bottom_pad,\n                        input_shape[2] + left_pad + right_pad,\n                        input_shape[3])\n        output = T.zeros(output_shape)\n        indices = (py_slice(None),\n                   py_slice(top_pad, input_shape[1] + top_pad),\n                   py_slice(left_pad, input_shape[2] + left_pad),\n                   py_slice(None))\n    y = T.set_subtensor(output[indices], x)\n    if hasattr(x, '_keras_shape'):\n        if data_format == 'channels_first':\n            if x._keras_shape[2] is not None:\n                h = x._keras_shape[2] + top_pad + bottom_pad\n            else:\n                h = None\n            if x._keras_shape[3] is not None:\n                w = x._keras_shape[3] + left_pad + right_pad\n            else:\n                w = None\n            output_keras_shape = (x._keras_shape[0],\n                                  x._keras_shape[1],\n                                  h,\n                                  w)\n        else:\n            if x._keras_shape[1] is not None:\n                h = x._keras_shape[1] + top_pad + bottom_pad\n            else:\n                h = None\n            if x._keras_shape[2] is not None:\n                w = x._keras_shape[2] + left_pad + right_pad\n            else:\n                w = None\n            output_keras_shape = (x._keras_shape[0],\n                                  h,\n                                  w,\n                                  x._keras_shape[3])\n        y._keras_shape = output_keras_shape\n    return y",
                "def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n    \"\"\"Pad the 2nd, 3rd and 4th dimensions of a 5D tensor\n    with \"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    input_shape = x.shape\n    if data_format == 'channels_first':\n        output_shape = (input_shape[0],\n                        input_shape[1],\n                        input_shape[2] + padding[0][0] + padding[0][1],\n                        input_shape[3] + padding[1][0] + padding[1][1],\n                        input_shape[4] + padding[2][0] + padding[2][1])\n        output = T.zeros(output_shape)\n        indices = (py_slice(None),\n                   py_slice(None),\n                   py_slice(padding[0][0], input_shape[2] + padding[0][0]),\n                   py_slice(padding[1][0], input_shape[3] + padding[1][0]),\n                   py_slice(padding[2][0], input_shape[4] + padding[2][0]))\n\n    else:\n        output_shape = (input_shape[0],\n                        input_shape[1] + padding[0][0] + padding[0][1],\n                        input_shape[2] + padding[1][0] + padding[1][1],\n                        input_shape[3] + padding[2][0] + padding[2][1],\n                        input_shape[4])\n        output = T.zeros(output_shape)\n        indices = (py_slice(None),\n                   py_slice(padding[0][0], input_shape[1] + padding[0][0]),\n                   py_slice(padding[1][0], input_shape[2] + padding[1][0]),\n                   py_slice(padding[2][0], input_shape[3] + padding[2][0]),\n                   py_slice(None))\n    y = T.set_subtensor(output[indices], x)\n    if hasattr(x, '_keras_shape'):\n        if data_format == 'channels_first':\n            if x._keras_shape[2] is not None:\n                h = x._keras_shape[2] + padding[0][0] + padding[0][1]\n            else:\n                h = None\n            if x._keras_shape[3] is not None:\n                w = x._keras_shape[3] + padding[1][0] + padding[1][1]\n            else:\n                w = None\n            if x._keras_shape[4] is not None:\n                d = x._keras_shape[4] + padding[2][0] + padding[2][1]\n            else:\n                d = None\n            output_keras_shape = (x._keras_shape[0],\n                                  x._keras_shape[1],\n                                  h,\n                                  w,\n                                  d)\n        else:\n            if x._keras_shape[1] is not None:\n                h = x._keras_shape[1] + padding[0][0] + padding[0][1]\n            else:\n                h = None\n            if x._keras_shape[2] is not None:\n                w = x._keras_shape[2] + padding[1][0] + padding[1][1]\n            else:\n                w = None\n            if x._keras_shape[3] is not None:\n                d = x._keras_shape[3] + padding[2][0] + padding[2][1]\n            else:\n                d = None\n            output_keras_shape = (x._keras_shape[0],\n                                  h,\n                                  w,\n                                  d,\n                                  x._keras_shape[4])\n        y._keras_shape = output_keras_shape\n    return y",
                "def stack(x, axis=0):\n    return T.stack(x, axis=axis)",
                "def one_hot(indices, num_classes):\n    \"\"\"Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))\n    Output: (n + 1)D one hot representation of the input\n    with shape (batch_size, dim1, dim2, ... dim(n-1), num_classes)\n    \"\"\"\n    input_shape = tuple((indices.shape[i] for i in range(indices.ndim)))\n    indices = T.flatten(indices)\n    oh = T.extra_ops.to_one_hot(indices, num_classes)\n    oh = T.reshape(oh, input_shape + (num_classes,))\n    return oh",
                "def reverse(x, axes):\n    \"\"\"Reverse a tensor along the specified axes\n    \"\"\"\n    if isinstance(axes, int):\n        axes = [axes]\n    slices = []\n    for i in range(x.ndim):\n        if i in axes:\n            slices.append(py_slice(None, None, -1))\n        else:\n            slices.append(py_slice(None, None, None))\n    return x[slices]",
                "def slice(x, start, size):\n    raise NotImplementedError",
                "def pattern_broadcast(x, broadcastable):\n    return T.patternbroadcast(x, broadcastable)",
                "def get_value(x):\n    if not hasattr(x, 'get_value'):\n        raise TypeError('`get_value` can only be called on a variable. '\n                        'If you have an expression instead, use `eval()`.')\n    return x.get_value()",
                "def batch_get_value(xs):\n    \"\"\"Returns the value of more than one tensor variable,\n    as a list of Numpy arrays.\n    \"\"\"\n    return [get_value(x) for x in xs]",
                "def set_value(x, value):\n    x.set_value(np.asarray(value, dtype=x.dtype))",
                "def batch_set_value(tuples):\n    for x, value in tuples:\n        x.set_value(np.asarray(value, dtype=x.dtype))",
                "def get_variable_shape(x):\n    return x.get_value(borrow=True, return_internal_type=True).shape",
                "def print_tensor(x, message=''):\n    \"\"\"Print the message and the tensor when evaluated and return the same\n    tensor.\n    \"\"\"\n    p_op = Print(message)\n    return p_op(x)",
                "def _raise_invalid_arg(key):\n    msg = 'Invalid argument \"%s\" passed to K.function with Theano backend' % key\n    raise ValueError(msg)",
                "def function(inputs, outputs, updates=[], **kwargs):\n    if len(kwargs) > 0:\n        for key in kwargs.keys():\n            if not has_arg(theano.function, key, True):\n                _raise_invalid_arg(key)\n    return Function(inputs, outputs, updates=updates, **kwargs)",
                "def gradients(loss, variables):\n    return T.grad(loss, variables)",
                "def stop_gradient(variables):\n    \"\"\"Returns `variables` but with zero gradient w.r.t. every other variable.\n\n    # Arguments\n        variables: tensor or list of tensors to consider constant with respect\n            to any other variable.\n\n    # Returns\n        A single tensor or a list of tensors (depending on the passed argument)\n            that has constant gradient with respect to any other variable.\n    \"\"\"\n    if isinstance(variables, (list, tuple)):\n        return map(theano.gradient.disconnected_grad, variables)\n    else:\n        return theano.gradient.disconnected_grad(variables)",
                "def rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n    \"\"\"Iterates over the time dimension of a tensor.\n\n    # Arguments\n        step_function:\n            Parameters:\n                inputs: Tensor with shape (samples, ...) (no time dimension),\n                    representing input for the batch of samples at a certain\n                    time step.\n                states: List of tensors.\n            Returns:\n                outputs: Tensor with shape (samples, ...) (no time dimension),\n                new_states: List of tensors, same length and shapes\n                    as 'states'.\n        inputs: Tensor of temporal data of shape (samples, time, ...)\n            (at least 3D).\n        initial_states: Tensor with shape (samples, ...) (no time dimension),\n            containing the initial values for the states used in\n            the step function.\n        go_backwards: Boolean. If True, do the iteration over the time\n            dimension in reverse order and return the reversed sequence.\n        mask: Binary tensor with shape (samples, time),\n            with a zero for every element that is masked.\n        constants: A list of constant values passed at each step.\n        unroll: Whether to unroll the RNN or to use a symbolic loop\n            (`while_loop` or `scan` depending on backend).\n        input_length: Static number of timesteps in the input.\n            Must be specified if using `unroll`.\n\n    # Returns\n        A tuple (last_output, outputs, new_states).\n\n        last_output: The latest output of the rnn, of shape `(samples, ...)`\n        outputs: Tensor with shape `(samples, time, ...)` where each\n            entry `outputs[s, t]` is the output of the step function\n            at time `t` for sample `s`.\n        new_states: List of tensors, latest states returned by\n            the step function, of shape `(samples, ...)`.\n    \"\"\"\n    ndim = inputs.ndim\n    assert ndim >= 3, 'Input should be at least 3D.'\n\n    if unroll:\n        if input_length is None:\n            raise ValueError('When specifying `unroll=True`, '\n                             'an `input_length` '\n                             'must be provided to `rnn`.')\n\n    axes = [1, 0] + list(range(2, ndim))\n    inputs = inputs.dimshuffle(axes)\n\n    if constants is None:\n        constants = []\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if mask is not None:\n        if mask.ndim != 2:\n            raise ValueError(\n                'mask should have `shape=(samples, time)`, '\n                'got {}'.format(mask.shape))\n        mask = mask.dimshuffle([1, 0])\n\n        def get_matching_mask(mask_t, ref_tensor_t):\n            # tf.where needs its condition tensor\n            # to be the same shape as its two\n            # result tensors\n            ndim = ref_tensor_t.ndim\n            for _ in range(ndim - 1):\n                mask_t = expand_dims(mask_t)\n            add_shape = ref_tensor_t.shape[1:]\n            reps = T.concatenate([[1], add_shape], 0)\n            return T.tile(mask_t, reps, ndim=ndim)\n\n        if unroll:\n            indices = list(range(input_length))\n            if go_backwards:\n                indices = indices[::-1]\n\n            successive_outputs = []\n            successive_states = []\n            states = initial_states\n            for i in indices:\n                output, new_states = step_function(inputs[i], states + constants)\n                if getattr(output, '_uses_learning_phase', False):\n                    uses_learning_phase = True\n\n                if len(successive_outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = successive_outputs[-1]\n\n                output_mask = get_matching_mask(mask[i], output)\n                output = T.switch(output_mask, output, prev_output)\n                kept_states = []\n                for state, new_state in zip(states, new_states):\n                    state_mask = get_matching_mask(mask[i], state)\n                    kept_states.append(T.switch(state_mask, new_state, state))\n                states = kept_states\n\n                successive_outputs.append(output)\n                successive_states.append(states)\n\n            outputs = T.stack(*successive_outputs)\n            states = []\n            for i in range(len(successive_states[-1])):\n                new_states = []\n                for states_at_step in successive_states:\n                    new_states.append(states_at_step[i])\n                states.append(T.stack(*new_states))\n        else:\n            # build an all-zero tensor of shape (samples, output_dim)\n            initial_output = step_function(inputs[0], initial_states + constants)\n            initial_output = initial_output[0] * 0\n            # Theano gets confused by broadcasting patterns in the scan op\n            initial_output = T.unbroadcast(initial_output, 0, 1)\n            if len(initial_states) > 0:\n                initial_states[0] = T.unbroadcast(initial_states[0], 0, 1)\n\n            def _step(inputs, mask, output_tm1, *states):\n                outputs, new_states = step_function(inputs, states)\n                if getattr(outputs, '_uses_learning_phase', False):\n                    global uses_learning_phase\n                    uses_learning_phase = True\n                # output previous output if masked.\n                output_mask = get_matching_mask(mask, outputs)\n                outputs = T.switch(output_mask, outputs, output_tm1)\n                return_states = []\n                for state, new_state in zip(states, new_states):\n                    state_mask = get_matching_mask(mask, state)\n                    return_states.append(T.switch(state_mask, new_state, state))\n                return [outputs] + return_states\n\n            results, _ = theano.scan(\n                _step,\n                sequences=[inputs, mask],\n                outputs_info=[initial_output] + initial_states,\n                non_sequences=constants,\n                go_backwards=go_backwards)\n\n            # deal with Theano API inconsistency\n            if isinstance(results, list):\n                outputs = results[0]\n                states = results[1:]\n            else:\n                outputs = results\n                states = []\n    else:\n        if unroll:\n            indices = list(range(input_length))\n            if go_backwards:\n                indices = indices[::-1]\n\n            successive_outputs = []\n            successive_states = []\n            states = initial_states\n            for i in indices:\n                outputs, states = step_function(inputs[i], states + constants)\n                if getattr(outputs, '_uses_learning_phase', False):\n                    uses_learning_phase = True\n                successive_outputs.append(outputs)\n                successive_states.append(states)\n            outputs = T.stack(*successive_outputs)\n            states = []\n            for i in range(len(successive_states[-1])):\n                states.append(T.stack(\n                    *[states_at_step[i] for states_at_step in successive_states]))\n\n        else:\n            def _step(inputs, *states):\n                outputs, new_states = step_function(inputs, states)\n                if getattr(outputs, '_uses_learning_phase', False):\n                    global uses_learning_phase\n                    uses_learning_phase = True\n                return [outputs] + new_states\n\n            # Theano likes to make shape==1 dimensions\n            # in the initial states (outputs_info) broadcastable\n            if len(initial_states) > 0:\n                initial_states[0] = T.unbroadcast(initial_states[0], 0, 1)\n\n            results, _ = theano.scan(\n                _step,\n                sequences=inputs,\n                outputs_info=[None] + initial_states,\n                non_sequences=constants,\n                go_backwards=go_backwards)\n\n            # deal with Theano API inconsistency\n            if isinstance(results, list):\n                outputs = results[0]\n                states = results[1:]\n            else:\n                outputs = results\n                states = []\n\n    outputs = T.squeeze(outputs)\n    last_output = outputs[-1]\n\n    axes = [1, 0] + list(range(2, outputs.ndim))\n    outputs = outputs.dimshuffle(axes)\n    states = [T.squeeze(state[-1]) for state in states]\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, outputs, states",
                "def switch(condition, then_expression, else_expression):\n    \"\"\"Switches between two operations depending on a scalar value.\n\n    Note that both `then_expression` and `else_expression`\n    should be symbolic tensors of the *same shape*.\n\n    # Arguments\n        condition: scalar tensor (`int` or `bool`).\n        then_expression: either a tensor, or a callable that returns a tensor.\n        else_expression: either a tensor, or a callable that returns a tensor.\n\n    # Returns\n        The selected tensor.\n    \"\"\"\n    if callable(then_expression):\n        then_expression = then_expression()\n    if callable(else_expression):\n        else_expression = else_expression()\n    cond_ndim = ndim(condition)\n    expr_ndim = ndim(then_expression)\n    if cond_ndim < expr_ndim:\n        ndim_diff = expr_ndim - cond_ndim\n        for _ in range(ndim_diff):\n            condition = expand_dims(condition)\n    return T.switch(condition, then_expression, else_expression)",
                "def in_train_phase(x, alt, training=None):\n    \"\"\"Selects `x` in train phase, and `alt` otherwise.\n\n    Note that `alt` should have the *same shape* as `x`.\n\n    # Returns\n        Either `x` or `alt` based on the `training` flag.\n        the `training` flag defaults to `K.learning_phase()`.\n    \"\"\"\n    if training is None:\n        training = learning_phase()\n        uses_learning_phase = True\n    else:\n        uses_learning_phase = False\n\n    if training is 1 or training is True:\n        if callable(x):\n            return x()\n        else:\n            return x\n\n    elif training is 0 or training is False:\n        if callable(alt):\n            return alt()\n        else:\n            return alt\n\n    if callable(x):\n        x = x()\n    if callable(alt):\n        alt = alt()\n\n    # else: assume learning phase is a placeholder tensor.\n    x = ifelse(training, x, alt)\n    if uses_learning_phase:\n        x._uses_learning_phase = True\n    return x",
                "def in_test_phase(x, alt, training=None):\n    \"\"\"Selects `x` in test phase, and `alt` otherwise.\n    Note that `alt` should have the *same shape* as `x`.\n\n    # Returns\n        Either `x` or `alt` based on `K.learning_phase`.\n    \"\"\"\n    return in_train_phase(alt, x, training=training)",
                "def _assert_has_capability(module, func):\n    if not hasattr(module, func):\n        raise EnvironmentError(\n            'It looks like like your version of '\n            'Theano is out of date. '\n            'Install the latest version with:\\n'\n            'pip install git+git://github.com/Theano/Theano.git '\n            '--upgrade --no-deps')",
                "def elu(x, alpha=1.0):\n    \"\"\" Exponential linear unit\n\n    # Arguments\n        x: Tensor to compute the activation function for.\n        alpha: scalar\n    \"\"\"\n    _assert_has_capability(T.nnet, 'elu')\n    return T.nnet.elu(x, alpha)",
                "def relu(x, alpha=0., max_value=None, threshold=0.):\n    _assert_has_capability(T.nnet, 'relu')\n\n    if alpha != 0.:\n        if threshold != 0.:\n            negative_part = T.nnet.relu(-x + threshold)\n        else:\n            negative_part = T.nnet.relu(-x)\n\n    if threshold != 0.:\n        x = x * T.cast(T.gt(x, threshold), floatx())\n    else:\n        x = T.nnet.relu(x)\n\n    if max_value is not None:\n        x = T.clip(x, 0.0, max_value)\n\n    if alpha != 0.:\n        x -= alpha * negative_part\n\n    return x",
                "def softmax(x, axis=-1):\n    if (axis == -1 or axis == x.ndim - 1) and x.ndim == 2:\n        return T.nnet.softmax(x)\n    xm = x.max(axis=axis, keepdims=True)\n    return T.exp(x - xm) / T.exp(\n        x - xm).sum(axis=axis, keepdims=True)",
                "def softplus(x):\n    return T.nnet.softplus(x)",
                "def softsign(x):\n    return T_softsign(x)",
                "def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    output_dimensions = list(range(len(int_shape(output))))\n    if axis != -1 and axis not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(int_shape(output)))))\n    # If the channels are not in the last axis, move them to be there:\n    if axis != -1 and axis != output_dimensions[-1]:\n        permutation = output_dimensions[:axis]\n        permutation += output_dimensions[axis + 1:] + [axis]\n        output = permute_dimensions(output, permutation)\n        target = permute_dimensions(target, permutation)\n    if from_logits:\n        output = T.nnet.softmax(output)\n    else:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= output.sum(axis=-1, keepdims=True)\n    # avoid numerical instability with _EPSILON clipping\n    output = T.clip(output, epsilon(), 1.0 - epsilon())\n    return T.nnet.categorical_crossentropy(output, target)",
                "def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    output_dimensions = list(range(len(int_shape(output))))\n    if axis != -1 and axis not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(int_shape(output)))))\n    # If the channels are not in the last axis, move them to be there:\n    if axis != -1 and axis != output_dimensions[-1]:\n        permutation = output_dimensions[:axis]\n        permutation += output_dimensions[axis + 1:] + [axis]\n        output = permute_dimensions(output, permutation)\n        target = permute_dimensions(target, permutation)\n    target = T.cast(T.flatten(target), 'int32')\n    target = T.extra_ops.to_one_hot(target, nb_class=output.shape[-1])\n    target = reshape(target, shape(output))\n    return categorical_crossentropy(target, output, from_logits, axis=-1)",
                "def binary_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        output = T.nnet.sigmoid(output)\n    # avoid numerical instability with _EPSILON clipping\n    output = T.clip(output, epsilon(), 1.0 - epsilon())\n    return T.nnet.binary_crossentropy(output, target)",
                "def sigmoid(x):\n    return T.nnet.sigmoid(x)",
                "def hard_sigmoid(x):\n    return T.nnet.hard_sigmoid(x)",
                "def tanh(x):\n    return T.tanh(x)",
                "def dropout(x, level, noise_shape=None, seed=None):\n    \"\"\"Sets entries in `x` to zero at random,\n    while scaling the entire tensor.\n\n    # Arguments\n        x: tensor\n        level: fraction of the entries in the tensor\n            that will be set to 0.\n        noise_shape: shape for randomly generated keep/drop flags,\n            must be broadcastable to the shape of `x`\n        seed: random seed to ensure determinism.\n    \"\"\"\n    if level < 0. or level >= 1:\n        raise ValueError('Dropout level must be in interval [0, 1[.')\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    if isinstance(noise_shape, list):\n        noise_shape = tuple(noise_shape)\n\n    rng = RandomStreams(seed=seed)\n    retain_prob = 1. - level\n\n    if noise_shape is None:\n        random_tensor = rng.binomial(x.shape, p=retain_prob, dtype=x.dtype)\n    else:\n        random_tensor = rng.binomial(noise_shape, p=retain_prob, dtype=x.dtype)\n        random_tensor = T.patternbroadcast(random_tensor,\n                                           [dim == 1 for dim in noise_shape])\n    x *= random_tensor\n    x /= retain_prob\n    return x",
                "def l2_normalize(x, axis=None):\n    square_sum = T.sum(T.square(x), axis=axis, keepdims=True)\n    norm = T.sqrt(T.maximum(square_sum, epsilon()))\n    return x / norm",
                "def in_top_k(predictions, targets, k):\n    \"\"\"Returns whether the `targets` are in the top `k` `predictions`.\n\n    # Arguments\n        predictions: A tensor of shape `(batch_size, classes)` and type `float32`.\n        targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.\n        k: An `int`, number of top elements to consider.\n\n    # Returns\n        A 1D tensor of length `batch_size` and type `bool`.\n        `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`\n        values of `predictions[i]`.\n    \"\"\"\n    # handle k < 1 and k >= predictions.shape[1] cases to match TF behavior\n    if k < 1:\n        # dtype='bool' is only available since Theano 0.9.0\n        try:\n            return T.zeros_like(targets, dtype='bool')\n        except TypeError:\n            return T.zeros_like(targets, dtype='int8')\n\n    if k >= int_shape(predictions)[1]:\n        try:\n            return T.ones_like(targets, dtype='bool')\n        except TypeError:\n            return T.ones_like(targets, dtype='int8')\n\n    predictions_k = T.sort(predictions)[:, -k]\n    targets_values = predictions[T.arange(targets.shape[0]), targets]\n    return T.ge(targets_values, predictions_k)",
                "def _preprocess_conv2d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols)\n        # TF input shape: (samples, rows, cols, input_depth)\n        x = x.dimshuffle((0, 3, 1, 2))\n    return x",
                "def _preprocess_conv3d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols, slices)\n        # TF input shape: (samples, rows, cols, slices, input_depth)\n        x = x.dimshuffle((0, 4, 1, 2, 3))\n    return x",
                "def _preprocess_conv2d_kernel(kernel, data_format):\n    # As of Keras 2.0.0, all kernels are normalized\n    # on the format `(rows, cols, input_depth, depth)`,\n    # independently of `data_format`.\n    # Theano expects `(depth, input_depth, rows, cols)`.\n    kernel = kernel.dimshuffle((3, 2, 0, 1))\n    return kernel",
                "def _preprocess_conv2d_depthwise_kernel(kernel, kernel_shape, data_format):\n    # As of Keras 2.0.0, all kernels are normalized\n    # on the format `(rows, cols, input_depth, depth)`,\n    # independently of `data_format`.\n    # Theano expects `(input_depth * depth, 1, rows, cols)`\n    # for depthwise convolution.\n    kernel = kernel[::-1, ::-1, :, :]\n    kernel = kernel.dimshuffle((2, 3, 0, 1))\n    kernel = reshape(kernel, kernel_shape)\n    return kernel",
                "def _preprocess_conv3d_kernel(kernel, data_format):\n    # As of Keras 2.0.0, all kernels are normalized\n    # on the format `(space, input_depth, depth)`,\n    # independently of `data_format`.\n    # Theano expects `(depth, input_depth, space)`.\n    kernel = kernel.dimshuffle((4, 3, 0, 1, 2))\n    return kernel",
                "def _preprocess_padding(padding):\n    if padding == 'same':\n        th_padding = 'half'\n    elif padding == 'valid':\n        th_padding = 'valid'\n    elif padding == 'full':\n        th_padding = 'full'\n    else:\n        raise ValueError('Border mode not supported:', str(padding))\n    return th_padding",
                "def _preprocess_conv2d_image_shape(image_shape, data_format):\n    # Theano might not accept long type\n    def int_or_none(value):\n        try:\n            return int(value)\n        except TypeError:\n            return None\n    if data_format == 'channels_last':\n        if image_shape:\n            image_shape = transpose_shape(image_shape, 'channels_first',\n                                          spatial_axes=(1, 2))\n    if image_shape is not None:\n        image_shape = tuple(int_or_none(v) for v in image_shape)\n    return image_shape",
                "def _preprocess_conv3d_volume_shape(volume_shape, data_format):\n    # Theano might not accept long type\n    def int_or_none(value):\n        try:\n            return int(value)\n        except TypeError:\n            return None\n    if data_format == 'channels_last':\n        if volume_shape:\n            volume_shape = (volume_shape[0], volume_shape[4],\n                            volume_shape[1], volume_shape[2], volume_shape[3])\n    if volume_shape is not None:\n        volume_shape = tuple(int_or_none(v) for v in volume_shape)\n    return volume_shape",
                "def _preprocess_conv2d_filter_shape(filter_shape, data_format):\n    # Theano might not accept long type\n    def int_or_none(value):\n        try:\n            return int(value)\n        except TypeError:\n            return None\n    if filter_shape:\n        filter_shape = (filter_shape[3], filter_shape[2],\n                        filter_shape[0], filter_shape[1])\n    if filter_shape is not None:\n        filter_shape = tuple(int_or_none(v) for v in filter_shape)\n    return filter_shape",
                "def _preprocess_conv2d_depthwise_filter_shape(filter_shape, data_format):\n    # Theano might not accept long type\n    def int_or_none(value):\n        try:\n            return int(value)\n        except TypeError:\n            return None\n    if filter_shape:\n        filter_shape = (filter_shape[3] * filter_shape[2], 1,\n                        filter_shape[0], filter_shape[1])\n    if filter_shape is not None:\n        filter_shape = tuple(int_or_none(v) for v in filter_shape)\n    return filter_shape",
                "def _preprocess_conv3d_filter_shape(filter_shape, data_format):\n    # Theano might not accept long type\n    def int_or_none(value):\n        try:\n            return int(value)\n        except TypeError:\n            return None\n    if filter_shape:\n        filter_shape = (filter_shape[4], filter_shape[3],\n                        filter_shape[0], filter_shape[1], filter_shape[2])\n    if filter_shape is not None:\n        filter_shape = tuple(int_or_none(v) for v in filter_shape)\n    return filter_shape",
                "def _postprocess_conv2d_output(conv_out, x,\n                               padding, kernel_shape,\n                               strides, data_format):\n    if padding == 'same':\n        if kernel_shape[2] % 2 == 0:\n            i = (x.shape[2] + strides[0] - 1) // strides[0]\n            conv_out = conv_out[:, :, :i, :]\n        if kernel_shape[3] % 2 == 0:\n            i = (x.shape[3] + strides[1] - 1) // strides[1]\n            conv_out = conv_out[:, :, :, :i]\n    if data_format == 'channels_last':\n        conv_out = conv_out.dimshuffle((0, 2, 3, 1))\n    return conv_out",
                "def _postprocess_conv3d_output(conv_out, x,\n                               padding, kernel_shape,\n                               strides, data_format):\n    if padding == 'same':\n        if kernel_shape[2] % 2 == 0:\n            i = (x.shape[2] + strides[0] - 1) // strides[0]\n            conv_out = conv_out[:, :, :i, :, :]\n        if kernel_shape[3] % 2 == 0:\n            i = (x.shape[3] + strides[1] - 1) // strides[1]\n            conv_out = conv_out[:, :, :, :i, :]\n        if kernel_shape[4] % 2 == 0:\n            i = (x.shape[4] + strides[2] - 1) // strides[2]\n            conv_out = conv_out[:, :, :, :, :i]\n    if data_format == 'channels_last':\n        conv_out = conv_out.dimshuffle((0, 2, 3, 4, 1))\n    return conv_out",
                "def conv1d(x, kernel, strides=1, padding='valid',\n           data_format=None, dilation_rate=1):\n    \"\"\"1D convolution.\n\n    # Arguments\n        kernel: kernel tensor.\n        strides: stride integer.\n        padding: string, `\"same\"`, `\"causal\"` or `\"valid\"`.\n        data_format: string, one of \"channels_last\", \"channels_first\"\n        dilation_rate: integer.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    kernel_shape = int_shape(kernel)\n    if padding == 'causal':\n        # causal (dilated) convolution:\n        if not kernel_shape:\n            raise AttributeError('Causal padding requires kernel._keras_shape set.')\n        left_pad = dilation_rate * (kernel_shape[0] - 1)\n        x = temporal_padding(x, (left_pad, 0))\n        padding = 'valid'\n    shape = int_shape(x)\n    if data_format == 'channels_last':\n        # original shape: (batch, length, input_dim)\n        # add dim to x to have (batch, length, 1, input_dim)\n        x = expand_dims(x, 2)\n        # update x._keras_shape\n        if shape is not None:\n            x._keras_shape = (shape[0], shape[1], 1, shape[2])\n    else:\n        # original shape: (batch, input_dim, length)\n        # add dim to x to have (batch, input_dim, length, 1)\n        x = expand_dims(x, 3)\n        # update x._keras_shape\n        if shape is not None:\n            x._keras_shape = (shape[0], shape[1], shape[2], 1)\n    # update dilation rate, strides\n    dilation_rate = (dilation_rate, 1)\n    strides = (strides, 1)\n    # add dim to kernel (always same format independently of data_format)\n    # i.e. (rows, 1, input_depth, depth)\n    kernel = expand_dims(kernel, 1)\n    output = conv2d(x, kernel,\n                    strides=strides, padding=padding,\n                    data_format=data_format, dilation_rate=dilation_rate)\n    # remove added dim\n    if data_format == 'channels_last':\n        output = squeeze(output, 2)\n    else:\n        output = squeeze(output, 3)\n    return output",
                "def conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D convolution.\n\n    # Arguments\n        kernel: kernel tensor.\n        strides: strides tuple.\n        padding: string, \"same\" or \"valid\".\n        data_format: \"channels_last\" or \"channels_first\".\n            Whether to use Theano or TensorFlow data format\n        in inputs/kernels/outputs.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n    kernel_shape = int_shape(kernel)\n    if kernel_shape is None:\n        kernel_shape = kernel.eval().shape  # in case of a shared variable\n    kernel_shape = _preprocess_conv2d_filter_shape(kernel_shape, data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    th_padding = _preprocess_padding(padding)\n\n    conv_out = T.nnet.conv2d(x, kernel,\n                             border_mode=th_padding,\n                             subsample=strides,\n                             input_shape=image_shape,\n                             filter_shape=kernel_shape,\n                             filter_dilation=dilation_rate)\n    conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                          kernel_shape, strides, data_format)\n    return conv_out",
                "def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D deconvolution (transposed convolution).\n\n    # Arguments\n        kernel: kernel tensor.\n        output_shape: desired dimensions of output.\n        strides: strides tuple.\n        padding: string, \"same\" or \"valid\".\n        data_format: \"channels_last\" or \"channels_first\".\n            Whether to use Theano or TensorFlow data format\n            in inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n\n    # Raises\n        ValueError: if using an even kernel size with padding 'same'.\n    \"\"\"\n    flip_filters = False\n    data_format = normalize_data_format(data_format)\n\n    if data_format == 'channels_last':\n        output_shape = (output_shape[0],\n                        output_shape[3],\n                        output_shape[1],\n                        output_shape[2])\n\n    kernel_shape = int_shape(kernel)\n    if kernel_shape is None:\n        kernel_shape = kernel.eval().shape  # in case of a shared variable\n\n    if padding == 'same' and kernel_shape[0] % 2 == 0:\n        raise ValueError('In `Conv2DTranspose`, with padding mode `same`, '\n                         'even kernel sizes are not supported with Theano. '\n                         'You can set `kernel_size` to an odd number.')\n\n    kernel_shape = _preprocess_conv2d_filter_shape(kernel_shape, data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n\n    th_padding = _preprocess_padding(padding)\n    op = T.nnet.abstract_conv.AbstractConv2d_gradInputs(\n        imshp=None,\n        kshp=kernel_shape,\n        subsample=strides,\n        border_mode=th_padding,\n        filter_flip=not flip_filters,\n        filter_dilation=dilation_rate)\n    conv_out = op(kernel, x, output_shape[2:])\n    conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                          kernel_shape, strides, data_format)\n    return conv_out",
                "def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                     padding='valid', data_format=None, dilation_rate=1):\n    \"\"\"1D convolution with separable filters.\n\n    # Arguments\n        x: input tensor\n        depthwise_kernel: convolution kernel for the depthwise convolution.\n        pointwise_kernel: kernel for the 1x1 convolution.\n        strides: strides integer.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: integer dilation rate.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: if `data_format` is neither `\"channels_last\"` or\n        `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n    if isinstance(strides, int):\n        strides = (strides,)\n    if isinstance(dilation_rate, int):\n        dilation_rate = (dilation_rate,)\n\n    if data_format == 'channels_last':\n        spatial_start_dim = 2\n    else:\n        spatial_start_dim = 3\n    x = expand_dims(x, spatial_start_dim)\n    depthwise_kernel = expand_dims(depthwise_kernel, 1)\n    pointwise_kernel = expand_dims(pointwise_kernel, 1)\n    strides = strides + (1,)\n    dilation_rate = dilation_rate + (1,)\n\n    image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n    depthwise_kernel_shape = int_shape(depthwise_kernel)\n    if depthwise_kernel_shape is None:\n        # in case of a shared variable\n        depthwise_kernel_shape = depthwise_kernel.eval().shape\n    depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(\n        depthwise_kernel_shape, data_format)\n    pointwise_kernel_shape = int_shape(pointwise_kernel)\n    if pointwise_kernel_shape is None:\n        # in case of a shared variable\n        pointwise_kernel_shape = pointwise_kernel.eval().shape\n    pointwise_kernel_shape = _preprocess_conv2d_filter_shape(\n        pointwise_kernel_shape, data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_depthwise_kernel(\n        depthwise_kernel, depthwise_kernel_shape, data_format)\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    th_padding = _preprocess_padding(padding)\n\n    conv_out = T.nnet.conv2d(x, depthwise_kernel,\n                             border_mode=th_padding,\n                             subsample=strides,\n                             input_shape=image_shape,\n                             filter_shape=depthwise_kernel_shape,\n                             filter_dilation=dilation_rate,\n                             num_groups=image_shape[1])\n    conv_out = T.nnet.conv2d(conv_out, pointwise_kernel,\n                             border_mode=th_padding,\n                             subsample=(1, 1),\n                             input_shape=None,\n                             filter_shape=pointwise_kernel_shape,\n                             filter_dilation=dilation_rate)\n    conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                          pointwise_kernel_shape,\n                                          strides, data_format)\n    conv_out = squeeze(conv_out, spatial_start_dim)\n    return conv_out",
                "def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D convolution with separable filters.\n\n    # Arguments\n        x: input tensor\n        depthwise_kernel: convolution kernel for the depthwise convolution.\n        pointwise_kernel: kernel for the 1x1 convolution.\n        strides: strides tuple (length 2).\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: tuple of integers,\n            dilation rates for the separable convolution.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: if `data_format` is neither `\"channels_last\"` or\n        `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n    depthwise_kernel_shape = int_shape(depthwise_kernel)\n    if depthwise_kernel_shape is None:\n        # in case of a shared variable\n        depthwise_kernel_shape = depthwise_kernel.eval().shape\n    depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(\n        depthwise_kernel_shape, data_format)\n    pointwise_kernel_shape = int_shape(pointwise_kernel)\n    if pointwise_kernel_shape is None:\n        # in case of a shared variable\n        pointwise_kernel_shape = pointwise_kernel.eval().shape\n    pointwise_kernel_shape = _preprocess_conv2d_filter_shape(\n        pointwise_kernel_shape, data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_depthwise_kernel(\n        depthwise_kernel, depthwise_kernel_shape, data_format)\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    th_padding = _preprocess_padding(padding)\n\n    conv_out = T.nnet.conv2d(x, depthwise_kernel,\n                             border_mode=th_padding,\n                             subsample=strides,\n                             input_shape=image_shape,\n                             filter_shape=depthwise_kernel_shape,\n                             filter_dilation=dilation_rate,\n                             num_groups=image_shape[1])\n    conv_out = T.nnet.conv2d(conv_out, pointwise_kernel,\n                             border_mode=th_padding,\n                             subsample=(1, 1),\n                             input_shape=None,\n                             filter_shape=pointwise_kernel_shape,\n                             filter_dilation=dilation_rate)\n    conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                          pointwise_kernel_shape,\n                                          strides, data_format)\n    return conv_out",
                "def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                     data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D convolution with separable filters.\n\n    # Arguments\n        x: input tensor\n        depthwise_kernel: convolution kernel for the depthwise convolution.\n        strides: strides tuple (length 2).\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: tuple of integers,\n            dilation rates for the separable convolution.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: if `data_format` is neither `\"channels_last\"` or\n        `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n    depthwise_kernel_shape = int_shape(depthwise_kernel)\n    if depthwise_kernel_shape is None:\n        # in case of a shared variable\n        depthwise_kernel_shape = depthwise_kernel.eval().shape\n    depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(\n        depthwise_kernel_shape, data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_depthwise_kernel(\n        depthwise_kernel, depthwise_kernel_shape, data_format)\n    th_padding = _preprocess_padding(padding)\n\n    conv_out = T.nnet.conv2d(x, depthwise_kernel,\n                             border_mode=th_padding,\n                             subsample=strides,\n                             input_shape=image_shape,\n                             filter_shape=depthwise_kernel_shape,\n                             filter_dilation=dilation_rate,\n                             num_groups=image_shape[1])\n    conv_out = _postprocess_conv2d_output(\n        conv_out, x, padding, depthwise_kernel_shape, strides, data_format)\n    return conv_out",
                "def conv3d(x, kernel, strides=(1, 1, 1),\n           padding='valid', data_format=None,\n           dilation_rate=(1, 1, 1)):\n    \"\"\"3D convolution.\n\n    # Arguments\n        kernel: kernel tensor.\n        strides: strides tuple.\n        padding: string, \"same\" or \"valid\".\n        data_format: \"channels_last\" or \"channels_first\".\n            Whether to use Theano or TensorFlow data format\n        in inputs/kernels/outputs.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    volume_shape = _preprocess_conv3d_volume_shape(int_shape(x), data_format)\n    kernel_shape = int_shape(kernel)\n    if kernel_shape is None:\n        kernel_shape = kernel.eval().shape  # in case of a shared variable\n    kernel_shape = _preprocess_conv3d_filter_shape(kernel_shape, data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    th_padding = _preprocess_padding(padding)\n\n    conv_out = T.nnet.conv3d(x, kernel,\n                             border_mode=th_padding,\n                             subsample=strides,\n                             input_shape=volume_shape,\n                             filter_shape=kernel_shape,\n                             filter_dilation=dilation_rate)\n    conv_out = _postprocess_conv3d_output(conv_out, x, padding,\n                                          kernel_shape, strides, data_format)\n    return conv_out",
                "def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                     padding='valid', data_format=None):\n    \"\"\"3D deconvolution (transposed convolution).\n\n    # Arguments\n        kernel: kernel tensor.\n        output_shape: desired dimensions of output.\n        strides: strides tuple.\n        padding: string, \"same\" or \"valid\".\n        data_format: \"channels_last\" or \"channels_first\".\n            Whether to use Theano or TensorFlow data format\n        in inputs/kernels/outputs.\n\n    # Raises\n        ValueError: if using an even kernel size with padding 'same'.\n    \"\"\"\n    flip_filters = False\n    data_format = normalize_data_format(data_format)\n\n    if data_format == 'channels_last':\n        output_shape = (output_shape[0],\n                        output_shape[4],\n                        output_shape[1],\n                        output_shape[2],\n                        output_shape[3])\n\n    kernel_shape = int_shape(kernel)\n    if kernel_shape is None:\n        kernel_shape = kernel.eval().shape  # in case of a shared variable\n\n    if padding == 'same' and kernel_shape[0] % 2 == 0:\n        raise ValueError('In `Conv3DTranspose`, with padding mode `same`, '\n                         'even kernel sizes are not supported with Theano. '\n                         'You can set `kernel_size` to an odd number.')\n\n    kernel_shape = _preprocess_conv3d_filter_shape(kernel_shape, data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n\n    th_padding = _preprocess_padding(padding)\n    op = T.nnet.abstract_conv.AbstractConv3d_gradInputs(imshp=None,\n                                                        kshp=kernel_shape,\n                                                        subsample=strides,\n                                                        border_mode=th_padding,\n                                                        filter_flip=not flip_filters)\n    conv_out = op(kernel, x, output_shape[2:])\n    conv_out = _postprocess_conv3d_output(conv_out, x, padding,\n                                          kernel_shape, strides, data_format)\n    return conv_out",
                "def pool2d(x, pool_size, strides=(1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    assert pool_size[0] >= 1 and pool_size[1] >= 1\n\n    if padding == 'same':\n        odd_pad_w = pool_size[0] > 2 and pool_size[0] % 2 == 1\n        w_pad = pool_size[0] - 2 if odd_pad_w else pool_size[0] - 1\n        odd_pad_h = pool_size[1] > 2 and pool_size[1] % 2 == 1\n        h_pad = pool_size[1] - 2 if odd_pad_h else pool_size[1] - 1\n        pad = (w_pad, h_pad)\n    elif padding == 'valid':\n        pad = (0, 0)\n    else:\n        raise ValueError('Invalid border mode:', padding)\n\n    if data_format == 'channels_last':\n        x = x.dimshuffle((0, 3, 1, 2))\n\n    if pool_mode == 'max':\n        pool_out = pool.pool_2d(x, ws=pool_size, stride=strides,\n                                ignore_border=True,\n                                pad=pad,\n                                mode='max')\n    elif pool_mode == 'avg':\n        pool_out = pool.pool_2d(x, ws=pool_size, stride=strides,\n                                ignore_border=True,\n                                pad=pad,\n                                mode='average_exc_pad')\n    else:\n        raise ValueError('Invalid pooling mode:', pool_mode)\n    if padding == 'same':\n        expected_width = (x.shape[2] + strides[0] - 1) // strides[0]\n        expected_height = (x.shape[3] + strides[1] - 1) // strides[1]\n        pool_out = pool_out[:, :,\n                            : expected_width,\n                            : expected_height]\n\n    if data_format == 'channels_last':\n        pool_out = pool_out.dimshuffle((0, 2, 3, 1))\n    return pool_out",
                "def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    if padding == 'same':\n        w_pad = pool_size[0] - 2 if pool_size[0] % 2 == 1 else pool_size[0] - 1\n        h_pad = pool_size[1] - 2 if pool_size[1] % 2 == 1 else pool_size[1] - 1\n        d_pad = pool_size[2] - 2 if pool_size[2] % 2 == 1 else pool_size[2] - 1\n        pad = (w_pad, h_pad, d_pad)\n    elif padding == 'valid':\n        pad = (0, 0, 0)\n    else:\n        raise ValueError('Invalid padding:', padding)\n\n    if data_format == 'channels_last':\n        x = x.dimshuffle((0, 4, 1, 2, 3))\n\n    if pool_mode == 'max':\n        pool_out = pool.pool_3d(x, ws=pool_size, stride=strides,\n                                ignore_border=True,\n                                pad=pad,\n                                mode='max')\n    elif pool_mode == 'avg':\n        pool_out = pool.pool_3d(x, ws=pool_size, stride=strides,\n                                ignore_border=True,\n                                pad=pad,\n                                mode='average_exc_pad')\n    else:\n        raise ValueError('Invalid pooling mode:', pool_mode)\n\n    if padding == 'same':\n        expected_width = (x.shape[2] + strides[0] - 1) // strides[0]\n        expected_height = (x.shape[3] + strides[1] - 1) // strides[1]\n        expected_depth = (x.shape[4] + strides[2] - 1) // strides[2]\n\n        pool_out = pool_out[:, :,\n                            : expected_width,\n                            : expected_height,\n                            : expected_depth]\n\n    if data_format == 'channels_last':\n        pool_out = pool_out.dimshuffle((0, 2, 3, 4, 1))\n    return pool_out",
                "def bias_add(x, bias, data_format=None):\n    data_format = normalize_data_format(data_format)\n    if ndim(bias) != 1 and ndim(bias) != ndim(x) - 1:\n        raise ValueError('Unexpected bias dimensions %d, '\n                         'expect to be 1 or %d dimensions'\n                         % (ndim(bias), ndim(x) - 1))\n    bias_shape = tuple(bias.shape)\n    if ndim(x) == 5:\n        if data_format == 'channels_first':\n            if ndim(bias) == 1:\n                x += reshape(bias, (1, bias_shape[0], 1, 1, 1))\n            else:\n                x += reshape(bias, (1, bias_shape[3]) + bias_shape[:3])\n        elif data_format == 'channels_last':\n            if ndim(bias) == 1:\n                x += reshape(bias, (1, 1, 1, 1, bias_shape[0]))\n            else:\n                x += reshape(bias, (1,) + bias_shape)\n    elif ndim(x) == 4:\n        if data_format == 'channels_first':\n            if ndim(bias) == 1:\n                x += reshape(bias, (1, bias_shape[0], 1, 1))\n            else:\n                x += reshape(bias, (1, bias_shape[2]) + bias_shape[:2])\n        elif data_format == 'channels_last':\n            if ndim(bias) == 1:\n                x += reshape(bias, (1, 1, 1, bias_shape[0]))\n            else:\n                x += reshape(bias, (1,) + bias_shape)\n    elif ndim(x) == 3:\n        if data_format == 'channels_first':\n            if ndim(bias) == 1:\n                x += reshape(bias, (1, bias_shape[0], 1))\n            else:\n                x += reshape(bias, (1, bias_shape[1], bias_shape[0]))\n        elif data_format == 'channels_last':\n            if ndim(bias) == 1:\n                x += reshape(bias, (1, 1, bias_shape[0]))\n            else:\n                x += reshape(bias, (1,) + bias_shape)\n    else:\n        x += bias\n    return x",
                "def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    rng = RandomStreams(seed=seed)\n    return rng.normal(size=shape, avg=mean, std=stddev, dtype=dtype)",
                "def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    rng = RandomStreams(seed=seed)\n    return rng.uniform(shape, low=minval, high=maxval, dtype=dtype)",
                "def random_binomial(shape, p=0.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    rng = RandomStreams(seed=seed)\n    return rng.binomial(shape, p=p, dtype=dtype)",
                "def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    rng = RandomStreams(seed=seed)\n\n    try:\n        return rng.normal(size=shape, avg=mean, std=stddev, dtype=dtype,\n                          truncate=True)\n    except TypeError:\n        normal_t = rng.normal(size=shape, avg=mean, std=stddev, dtype=dtype)\n        # Poor man's truncated normal: we literally clip the tensor\n        return T.clip(normal_t, mean - 2 * stddev, mean + 2 * stddev)",
                "def ctc_interleave_blanks(Y):\n    Y_ = T.alloc(-1, Y.shape[0] * 2 + 1)\n    Y_ = T.set_subtensor(Y_[T.arange(Y.shape[0]) * 2 + 1], Y)\n    return Y_",
                "def ctc_create_skip_idxs(Y):\n    skip_idxs = T.arange((Y.shape[0] - 3) // 2) * 2 + 1\n    non_repeats = T.neq(Y[skip_idxs], Y[skip_idxs + 2])\n    return skip_idxs[non_repeats.nonzero()]",
                "def ctc_update_log_p(skip_idxs, zeros, active, log_p_curr, log_p_prev):\n    active_skip_idxs = skip_idxs[(skip_idxs < active).nonzero()]\n    active_next = T.cast(T.minimum(\n        T.maximum(\n            active + 1,\n            T.max(T.concatenate([active_skip_idxs, [-1]])) + 2 + 1\n        ), log_p_curr.shape[0]), 'int32')\n\n    common_factor = T.max(log_p_prev[:active])\n    p_prev = T.exp(log_p_prev[:active] - common_factor)\n    _p_prev = zeros[:active_next]\n    # copy over\n    _p_prev = T.set_subtensor(_p_prev[:active], p_prev)\n    # previous transitions\n    _p_prev = T.inc_subtensor(_p_prev[1:], _p_prev[:-1])\n    # skip transitions\n    _p_prev = T.inc_subtensor(\n        _p_prev[active_skip_idxs + 2], p_prev[active_skip_idxs])\n    updated_log_p_prev = T.log(_p_prev) + common_factor\n\n    log_p_next = T.set_subtensor(\n        zeros[:active_next],\n        log_p_curr[:active_next] + updated_log_p_prev\n    )\n    return active_next, log_p_next",
                "def ctc_path_probs(predict, Y, alpha=1e-4):\n    smoothed = (1 - alpha) * predict[:, Y] + alpha * np.float32(1.) / Y.shape[0]\n    L = T.log(smoothed)\n    zeros = T.zeros_like(L[0])\n    log_first = zeros\n\n    f_skip_idxs = ctc_create_skip_idxs(Y)\n    # there should be a shortcut to calculating this\n    b_skip_idxs = ctc_create_skip_idxs(Y[::-1])\n\n    def step(log_f_curr, log_b_curr, f_active, log_f_prev, b_active, log_b_prev):\n        f_active_next, log_f_next = ctc_update_log_p(\n            f_skip_idxs, zeros, f_active, log_f_curr, log_f_prev)\n        b_active_next, log_b_next = ctc_update_log_p(\n            b_skip_idxs, zeros, b_active, log_b_curr, log_b_prev)\n        return f_active_next, log_f_next, b_active_next, log_b_next\n\n    [f_active, log_f_probs, b_active, log_b_probs], _ = theano.scan(\n        step,\n        sequences=[L, L[::-1, ::-1]],\n        outputs_info=[np.int32(1), log_first, np.int32(1), log_first])\n\n    idxs = T.arange(L.shape[1]).dimshuffle('x', 0)\n    mask = ((idxs < f_active.dimshuffle(0, 'x')) &\n            (idxs < b_active.dimshuffle(0, 'x'))[::-1, ::-1])\n    log_probs = log_f_probs + log_b_probs[::-1, ::-1] - L\n    return log_probs, mask",
                "def ctc_cost(predict, Y):\n    log_probs, mask = ctc_path_probs(predict, ctc_interleave_blanks(Y))\n    common_factor = T.max(log_probs)\n    total_log_prob = T.log(T.sum(T.exp(log_probs - common_factor)[mask.nonzero()]))\n    total_log_prob = total_log_prob + common_factor\n    return -total_log_prob",
                "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n    \"\"\"Runs CTC loss algorithm on each batch element.\n\n    # Arguments\n        y_true: tensor (samples, max_string_length) containing the truth labels\n        y_pred: tensor (samples, time_steps, num_categories) containing the\n                prediction, or output of the softmax\n        input_length: tensor (samples,1) containing the sequence length for\n                each batch item in y_pred\n        label_length: tensor (samples,1) containing the sequence length for\n                each batch item in y_true\n\n    # Returns\n        Tensor with shape (samples,1) containing the\n            CTC loss of each element\n    \"\"\"\n\n    def ctc_step(y_true_step, y_pred_step, input_length_step, label_length_step):\n        y_pred_step = y_pred_step[0: input_length_step[0]]\n        y_true_step = y_true_step[0:label_length_step[0]]\n        return ctc_cost(y_pred_step, y_true_step)\n\n    ret, _ = theano.scan(\n        fn=ctc_step,\n        outputs_info=None,\n        sequences=[y_true, y_pred, input_length, label_length]\n    )\n\n    ret = ret.dimshuffle('x', 0)\n    return ret",
                "def map_fn(fn, elems, name=None, dtype=None):\n    \"\"\"Map the function fn over the elements elems and return the outputs.\n\n    # Arguments\n        fn: Callable that will be called upon each element in elems\n        elems: tensor, at least 2 dimensional\n        name: A string name for the map node in the graph\n\n    # Returns\n        Tensor with first dimension equal to the elems and second depending on\n        fn\n    \"\"\"\n    return theano.map(fn, elems, name=name)[0]",
                "def foldl(fn, elems, initializer=None, name=None):\n    \"\"\"Reduce elems using fn to combine them from left to right.\n\n    # Arguments\n        fn: Callable that will be called upon each element in elems and an\n            accumulator, for instance lambda acc, x: acc + x\n        elems: tensor\n        initializer: The first value used (elems[0] in case of None)\n        name: A string name for the foldl node in the graph\n\n    # Returns\n        Same type and shape as initializer\n    \"\"\"\n    if initializer is None:\n        initializer = elems[0]\n        elems = elems[1:]\n\n    # We need to change the order of the arguments because theano accepts x as\n    # first parameter and accumulator as second\n    return theano.foldl(lambda x, acc: fn(acc, x),\n                        elems, initializer, name=name)[0]",
                "def foldr(fn, elems, initializer=None, name=None):\n    \"\"\"Reduce elems using fn to combine them from right to left.\n\n    # Arguments\n        fn: Callable that will be called upon each element in elems and an\n            accumulator, for instance lambda acc, x: acc + x\n        elems: tensor\n        initializer: The first value used (elems[-1] in case of None)\n        name: A string name for the foldr node in the graph\n\n    # Returns\n        Same type and shape as initializer\n    \"\"\"\n    if initializer is None:\n        initializer = elems[-1]\n        elems = elems[:-1]\n\n    # We need to change the order of the arguments because theano accepts x as\n    # first parameter and accumulator as second\n    return theano.foldr(lambda x, acc: fn(acc, x),\n                        elems, initializer, name=name)[0]",
                "def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride = strides[0]\n    kernel_shape = int_shape(kernel)\n    output_length, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_length):\n        slice_length = py_slice(i * stride,\n                                i * stride + kernel_size[0])\n        xs.append(reshape(inputs[:, slice_length, :],\n                          (1, -1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=0)\n    # Shape: `(output_length, batch_size, filters)`.\n    output = batch_dot(x_aggregate, kernel)\n    return permute_dimensions(output, (1, 0, 2))",
                "def local_conv2d(inputs,\n                 kernel,\n                 kernel_size,\n                 strides,\n                 output_shape,\n                 data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride_row, stride_col = strides\n    output_row, output_col = output_shape\n    kernel_shape = int_shape(kernel)\n    _, feature_dim, filters = kernel_shape\n\n    if data_format == 'channels_first':\n        output = []\n        for i in range(output_row):\n            for j in range(output_col):\n                slice_row = py_slice(i * stride_row,\n                                     i * stride_row + kernel_size[0])\n                slice_col = py_slice(j * stride_col,\n                                     j * stride_col + kernel_size[1])\n                x_flatten = reshape(inputs[:, :, slice_row, slice_col],\n                                    (1, -1, feature_dim))\n                output.append(dot(x_flatten,\n                                  kernel[i * output_col + j, :, :]))\n        output = concatenate(output, axis=0)\n        output = reshape(output,\n                         (output_row, output_col, -1, filters))\n        output = permute_dimensions(output, (2, 3, 0, 1))\n    else:\n        xs = []\n        for i in range(output_row):\n            for j in range(output_col):\n                slice_row = py_slice(i * stride_row,\n                                     i * stride_row + kernel_size[0])\n                slice_col = py_slice(j * stride_col,\n                                     j * stride_col + kernel_size[1])\n                xs.append(reshape(inputs[:, slice_row, slice_col, :],\n                                  (1, -1, feature_dim)))\n\n        x_aggregate = concatenate(xs, axis=0)\n        output = batch_dot(x_aggregate, kernel)\n        output = reshape(output,\n                         (output_row, output_col, -1, filters))\n        output = permute_dimensions(output, (2, 0, 1, 3))\n    return output",
                "def ctc_label_dense_to_sparse(labels, label_lengths):\n    raise NotImplementedError",
                "def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1,\n               merge_repeated=False):\n    raise NotImplementedError",
                "def __init__(self, inputs, outputs, updates=[], name=None, **kwargs):\n    unique_variables_to_update = {}\n    for v, nv in updates:\n        if v not in unique_variables_to_update:\n            unique_variables_to_update[v] = nv\n    updates = unique_variables_to_update.items()\n    self.function = theano.function(inputs, outputs, updates=updates,\n                                    allow_input_downcast=True,\n                                    on_unused_input='ignore',\n                                    name=name,\n                                    **kwargs)\n    self.name = name",
                "def __call__(self, inputs):\n    assert isinstance(inputs, (list, tuple))\n    return self.function(*inputs)",
                "def int_or_none(value):\n    try:\n        return int(value)\n    except TypeError:\n        return None",
                "def int_or_none(value):\n    try:\n        return int(value)\n    except TypeError:\n        return None",
                "def int_or_none(value):\n    try:\n        return int(value)\n    except TypeError:\n        return None",
                "def int_or_none(value):\n    try:\n        return int(value)\n    except TypeError:\n        return None",
                "def int_or_none(value):\n    try:\n        return int(value)\n    except TypeError:\n        return None",
                "def step(log_f_curr, log_b_curr, f_active, log_f_prev, b_active, log_b_prev):\n    f_active_next, log_f_next = ctc_update_log_p(\n        f_skip_idxs, zeros, f_active, log_f_curr, log_f_prev)\n    b_active_next, log_b_next = ctc_update_log_p(\n        b_skip_idxs, zeros, b_active, log_b_curr, log_b_prev)\n    return f_active_next, log_f_next, b_active_next, log_b_next",
                "def ctc_step(y_true_step, y_pred_step, input_length_step, label_length_step):\n    y_pred_step = y_pred_step[0: input_length_step[0]]\n    y_true_step = y_true_step[0:label_length_step[0]]\n    return ctc_cost(y_pred_step, y_true_step)",
                "def get_matching_mask(mask_t, ref_tensor_t):\n    # tf.where needs its condition tensor\n    # to be the same shape as its two\n    # result tensors\n    ndim = ref_tensor_t.ndim\n    for _ in range(ndim - 1):\n        mask_t = expand_dims(mask_t)\n    add_shape = ref_tensor_t.shape[1:]\n    reps = T.concatenate([[1], add_shape], 0)\n    return T.tile(mask_t, reps, ndim=ndim)",
                "def _step(inputs, mask, output_tm1, *states):\n    outputs, new_states = step_function(inputs, states)\n    if getattr(outputs, '_uses_learning_phase', False):\n        global uses_learning_phase\n        uses_learning_phase = True\n    # output previous output if masked.\n    output_mask = get_matching_mask(mask, outputs)\n    outputs = T.switch(output_mask, outputs, output_tm1)\n    return_states = []\n    for state, new_state in zip(states, new_states):\n        state_mask = get_matching_mask(mask, state)\n        return_states.append(T.switch(state_mask, new_state, state))\n    return [outputs] + return_states",
                "def _step(inputs, *states):\n    outputs, new_states = step_function(inputs, states)\n    if getattr(outputs, '_uses_learning_phase', False):\n        global uses_learning_phase\n        uses_learning_phase = True\n    return [outputs] + new_states"
            ],
            "inscope_function_signatures": [
                "learning_phase()",
                "set_learning_phase(value)",
                "get_uid(prefix='')",
                "reset_uids()",
                "_assert_sparse_module()",
                "is_sparse(tensor)",
                "to_dense(tensor)",
                "_is_explicit_shape(shape)",
                "name_scope(name)",
                "_prepare_name(name, default)",
                "variable(value, dtype=None, name=None, constraint=None)",
                "constant(value, dtype=None, shape=None, name=None)",
                "is_keras_tensor(x)",
                "is_tensor(x)",
                "placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None)",
                "is_placeholder(x)",
                "shape(x)",
                "int_shape(x)",
                "ndim(x)",
                "dtype(x)",
                "eval(x)",
                "zeros(shape, dtype=None, name=None)",
                "ones(shape, dtype=None, name=None)",
                "eye(size, dtype=None, name=None)",
                "ones_like(x, dtype=None, name=None)",
                "zeros_like(x, dtype=None, name=None)",
                "identity(x, name=None)",
                "random_uniform_variable(shape, low, high, dtype=None, name=None)",
                "random_normal_variable(shape, mean, scale, dtype=None, name=None)",
                "count_params(x)",
                "cast(x, dtype)",
                "update(x, new_x)",
                "update_add(x, increment)",
                "update_sub(x, decrement)",
                "moving_average_update(variable, value, momentum)",
                "dot(x, y)",
                "batch_dot(x, y, axes=None)",
                "transpose(x)",
                "gather(reference, indices)",
                "max(x, axis=None, keepdims=False)",
                "min(x, axis=None, keepdims=False)",
                "sum(x, axis=None, keepdims=False)",
                "prod(x, axis=None, keepdims=False)",
                "cumsum(x, axis=0)",
                "cumprod(x, axis=0)",
                "mean(x, axis=None, keepdims=False)",
                "std(x, axis=None, keepdims=False)",
                "var(x, axis=None, keepdims=False)",
                "any(x, axis=None, keepdims=False)",
                "all(x, axis=None, keepdims=False)",
                "_set_keras_shape_for_reduction(x, y, axis, keepdims)",
                "argmax(x, axis=-1)",
                "argmin(x, axis=-1)",
                "square(x)",
                "abs(x)",
                "sqrt(x)",
                "exp(x)",
                "log(x)",
                "logsumexp(x, axis=None, keepdims=False)",
                "round(x)",
                "sign(x)",
                "pow(x, a)",
                "clip(x, min_value, max_value)",
                "equal(x, y)",
                "not_equal(x, y)",
                "greater(x, y)",
                "greater_equal(x, y)",
                "less(x, y)",
                "less_equal(x, y)",
                "maximum(x, y)",
                "minimum(x, y)",
                "sin(x)",
                "cos(x)",
                "normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
                "batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=0.001)",
                "_old_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
                "_old_batch_normalization(x, mean, var, beta, gamma, epsilon=0.001)",
                "concatenate(tensors, axis=-1)",
                "reshape(x, shape)",
                "permute_dimensions(x, pattern)",
                "repeat_elements(x, rep, axis)",
                "resize_images(x, height_factor, width_factor, data_format, interpolation='nearest')",
                "resize_volumes(x, depth_factor, height_factor, width_factor, data_format)",
                "repeat(x, n)",
                "arange(start, stop=None, step=1, dtype='int32')",
                "tile(x, n)",
                "flatten(x)",
                "batch_flatten(x)",
                "expand_dims(x, axis=-1)",
                "squeeze(x, axis)",
                "temporal_padding(x, padding=(1, 1))",
                "spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None)",
                "spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None)",
                "stack(x, axis=0)",
                "one_hot(indices, num_classes)",
                "reverse(x, axes)",
                "slice(x, start, size)",
                "pattern_broadcast(x, broadcastable)",
                "get_value(x)",
                "batch_get_value(xs)",
                "set_value(x, value)",
                "batch_set_value(tuples)",
                "get_variable_shape(x)",
                "print_tensor(x, message='')",
                "_raise_invalid_arg(key)",
                "function(inputs, outputs, updates=[], **kwargs)",
                "gradients(loss, variables)",
                "stop_gradient(variables)",
                "rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)",
                "switch(condition, then_expression, else_expression)",
                "in_train_phase(x, alt, training=None)",
                "in_test_phase(x, alt, training=None)",
                "_assert_has_capability(module, func)",
                "elu(x, alpha=1.0)",
                "relu(x, alpha=0.0, max_value=None, threshold=0.0)",
                "softmax(x, axis=-1)",
                "softplus(x)",
                "softsign(x)",
                "categorical_crossentropy(target, output, from_logits=False, axis=-1)",
                "sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1)",
                "binary_crossentropy(target, output, from_logits=False)",
                "sigmoid(x)",
                "hard_sigmoid(x)",
                "tanh(x)",
                "dropout(x, level, noise_shape=None, seed=None)",
                "l2_normalize(x, axis=None)",
                "in_top_k(predictions, targets, k)",
                "_preprocess_conv2d_input(x, data_format)",
                "_preprocess_conv3d_input(x, data_format)",
                "_preprocess_conv2d_kernel(kernel, data_format)",
                "_preprocess_conv2d_depthwise_kernel(kernel, kernel_shape, data_format)",
                "_preprocess_conv3d_kernel(kernel, data_format)",
                "_preprocess_padding(padding)",
                "_preprocess_conv2d_image_shape(image_shape, data_format)",
                "_preprocess_conv3d_volume_shape(volume_shape, data_format)",
                "_preprocess_conv2d_filter_shape(filter_shape, data_format)",
                "_preprocess_conv2d_depthwise_filter_shape(filter_shape, data_format)",
                "_preprocess_conv3d_filter_shape(filter_shape, data_format)",
                "_postprocess_conv2d_output(conv_out, x, padding, kernel_shape, strides, data_format)",
                "_postprocess_conv3d_output(conv_out, x, padding, kernel_shape, strides, data_format)",
                "conv1d(x, kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)",
                "conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "conv2d_transpose(x, kernel, output_shape, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)",
                "separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "conv3d(x, kernel, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1))",
                "conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1), padding='valid', data_format=None)",
                "pool2d(x, pool_size, strides=(1, 1), padding='valid', data_format=None, pool_mode='max')",
                "pool3d(x, pool_size, strides=(1, 1, 1), padding='valid', data_format=None, pool_mode='max')",
                "bias_add(x, bias, data_format=None)",
                "random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)",
                "random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None)",
                "random_binomial(shape, p=0.0, dtype=None, seed=None)",
                "truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)",
                "ctc_interleave_blanks(Y)",
                "ctc_create_skip_idxs(Y)",
                "ctc_update_log_p(skip_idxs, zeros, active, log_p_curr, log_p_prev)",
                "ctc_path_probs(predict, Y, alpha=0.0001)",
                "ctc_cost(predict, Y)",
                "ctc_batch_cost(y_true, y_pred, input_length, label_length)",
                "map_fn(fn, elems, name=None, dtype=None)",
                "foldl(fn, elems, initializer=None, name=None)",
                "foldr(fn, elems, initializer=None, name=None)",
                "local_conv1d(inputs, kernel, kernel_size, strides, data_format=None)",
                "local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None)",
                "ctc_label_dense_to_sparse(labels, label_lengths)",
                "ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1, merge_repeated=False)",
                "__init__(self, inputs, outputs, updates=[], name=None, **kwargs)",
                "__call__(self, inputs)",
                "int_or_none(value)",
                "int_or_none(value)",
                "int_or_none(value)",
                "int_or_none(value)",
                "int_or_none(value)",
                "step(log_f_curr, log_b_curr, f_active, log_f_prev, b_active, log_b_prev)",
                "ctc_step(y_true_step, y_pred_step, input_length_step, label_length_step)",
                "get_matching_mask(mask_t, ref_tensor_t)",
                "_step(inputs, mask, output_tm1, *states)",
                "_step(inputs, *states)"
            ],
            "variables_in_file": {
                "ImportError": [
                    89,
                    19,
                    15
                ],
                "th_sparse_module": [
                    99,
                    909,
                    911,
                    16,
                    149,
                    88,
                    410,
                    252,
                    94
                ],
                "py_all": [
                    906,
                    917,
                    31
                ],
                "all": [
                    31
                ],
                "py_any": [
                    32,
                    472
                ],
                "any": [
                    32
                ],
                "py_sum": [
                    33,
                    1169
                ],
                "sum": [
                    33,
                    1781
                ],
                "py_slice": [
                    34,
                    1192,
                    1193,
                    1194,
                    1195,
                    1203,
                    1204,
                    1205,
                    1206,
                    1337,
                    1339,
                    2885,
                    2912,
                    2914,
                    1253,
                    1254,
                    1255,
                    1256,
                    1257,
                    2928,
                    1266,
                    1267,
                    1268,
                    1269,
                    1270,
                    2930
                ],
                "slice": [
                    34
                ],
                "theano.config.floatX": [
                    38
                ],
                "theano.config": [
                    875,
                    813,
                    38
                ],
                "theano": [
                    1418,
                    2827,
                    152,
                    153,
                    154,
                    158,
                    1439,
                    1441,
                    2849,
                    38,
                    813,
                    1582,
                    821,
                    824,
                    825,
                    826,
                    2872,
                    2760,
                    1630,
                    875,
                    2803,
                    884,
                    373,
                    1398,
                    892,
                    894
                ],
                "floatx": [
                    2656,
                    324,
                    581,
                    38,
                    1764,
                    169,
                    2665,
                    332,
                    240,
                    146,
                    2674,
                    2683,
                    316
                ],
                "_LEARNING_PHASE": [
                    40,
                    54,
                    46
                ],
                "T.scalar": [
                    40
                ],
                "T": [
                    1542,
                    1546,
                    1552,
                    530,
                    534,
                    1046,
                    1558,
                    540,
                    1564,
                    1566,
                    546,
                    1575,
                    40,
                    1065,
                    1579,
                    1069,
                    559,
                    572,
                    582,
                    586,
                    1611,
                    1100,
                    590,
                    1614,
                    596,
                    1113,
                    604,
                    1628,
                    1645,
                    1650,
                    632,
                    1145,
                    636,
                    640,
                    2179,
                    644,
                    2694,
                    648,
                    649,
                    653,
                    1165,
                    1166,
                    1679,
                    657,
                    2705,
                    2706,
                    2711,
                    2712,
                    2718,
                    2719,
                    2721,
                    2724,
                    2725,
                    1191,
                    2728,
                    681,
                    2730,
                    2732,
                    173,
                    685,
                    2734,
                    2736,
                    689,
                    1202,
                    693,
                    1207,
                    2231,
                    2745,
                    2746,
                    705,
                    709,
                    713,
                    2765,
                    722,
                    726,
                    1750,
                    1751,
                    2774,
                    730,
                    1755,
                    2775,
                    734,
                    1759,
                    1761,
                    738,
                    1252,
                    1764,
                    742,
                    1766,
                    232,
                    233,
                    746,
                    1769,
                    750,
                    1265,
                    1779,
                    1781,
                    759,
                    1271,
                    1786,
                    2300,
                    254,
                    2307,
                    773,
                    776,
                    784,
                    1808,
                    1813,
                    1814,
                    798,
                    1314,
                    1831,
                    1832,
                    1323,
                    1324,
                    1325,
                    1839,
                    1841,
                    1842,
                    1846,
                    1850,
                    827,
                    828,
                    2363,
                    1854,
                    2370,
                    1348,
                    841,
                    843,
                    844,
                    845,
                    846,
                    337,
                    341,
                    1883,
                    1891,
                    1892,
                    2417,
                    1913,
                    378,
                    1915,
                    1919,
                    1921,
                    899,
                    1923,
                    1924,
                    1925,
                    1424,
                    915,
                    2454,
                    412,
                    931,
                    960,
                    2506,
                    485,
                    1002,
                    1520,
                    1521,
                    504
                ],
                "_UID_PREFIXES": [
                    81,
                    41,
                    75,
                    76
                ],
                "defaultdict": [
                    81,
                    41
                ],
                "int": [
                    932,
                    2265,
                    614,
                    41,
                    617,
                    2058,
                    108,
                    1996,
                    2028,
                    2267,
                    464,
                    81,
                    1332,
                    1078,
                    697,
                    698,
                    2043,
                    2012
                ],
                "value": [
                    2058,
                    397,
                    147,
                    150,
                    152,
                    156,
                    157,
                    158,
                    161,
                    172,
                    51,
                    54,
                    1996,
                    1368,
                    2012,
                    1373,
                    1372,
                    2028,
                    2043
                ],
                "ValueError": [
                    897,
                    1412,
                    1796,
                    1035,
                    2573,
                    913,
                    1820,
                    2589,
                    2221,
                    2609,
                    52,
                    2496,
                    1988,
                    1870,
                    1492,
                    473,
                    988,
                    480,
                    225,
                    1507,
                    2532,
                    242,
                    1012,
                    2548
                ],
                "prefix": [
                    128,
                    129,
                    75,
                    76,
                    126
                ],
                "isinstance": [
                    152,
                    1438,
                    932,
                    1332,
                    1078,
                    1590,
                    697,
                    698,
                    464,
                    1873,
                    472,
                    2265,
                    2267,
                    476,
                    94,
                    614,
                    1638,
                    232,
                    108,
                    1406
                ],
                "tensor.type": [
                    94
                ],
                "tensor": [
                    98,
                    99,
                    101,
                    917,
                    918,
                    94
                ],
                "th_sparse_module.SparseType": [
                    94
                ],
                "is_sparse": [
                    409,
                    98,
                    906
                ],
                "th_sparse_module.dense_from_sparse": [
                    99
                ],
                "hasattr": [
                    521,
                    270,
                    1167,
                    784,
                    147,
                    917,
                    1047,
                    413,
                    292,
                    934,
                    1070,
                    949,
                    1208,
                    961,
                    1089,
                    1734,
                    714,
                    1354,
                    716,
                    1101,
                    1114,
                    610,
                    228,
                    105,
                    489,
                    1133,
                    1006,
                    759,
                    1272,
                    505,
                    1146
                ],
                "shape": [
                    2689,
                    2692,
                    2447,
                    920,
                    921,
                    1048,
                    1049,
                    924,
                    1050,
                    931,
                    932,
                    933,
                    1833,
                    170,
                    171,
                    172,
                    2218,
                    2347,
                    176,
                    2172,
                    2353,
                    317,
                    2493,
                    325,
                    2123,
                    2129,
                    2130,
                    1143,
                    2136,
                    2137,
                    1377,
                    2284,
                    2660,
                    358,
                    2408,
                    105,
                    106,
                    363,
                    490,
                    493,
                    1134,
                    1135,
                    496,
                    241,
                    497,
                    243,
                    244,
                    498,
                    246,
                    499,
                    1136,
                    1145,
                    2290,
                    2669,
                    1144,
                    2678,
                    255
                ],
                "x": [
                    2074,
                    2077,
                    2089,
                    2092,
                    2095,
                    2121,
                    2123,
                    2127,
                    2130,
                    2134,
                    2137,
                    2144,
                    106,
                    107,
                    108,
                    2169,
                    2175,
                    2179,
                    2185,
                    2227,
                    2238,
                    2239,
                    224,
                    226,
                    2274,
                    228,
                    232,
                    2280,
                    2294,
                    252,
                    2300,
                    254,
                    255,
                    256,
                    257,
                    258,
                    2313,
                    270,
                    279,
                    292,
                    293,
                    2343,
                    299,
                    303,
                    309,
                    2357,
                    2363,
                    2376,
                    337,
                    341,
                    354,
                    2404,
                    2412,
                    2417,
                    373,
                    2425,
                    378,
                    385,
                    389,
                    393,
                    2444,
                    2450,
                    2454,
                    409,
                    410,
                    412,
                    413,
                    414,
                    2460,
                    2502,
                    2511,
                    2512,
                    469,
                    471,
                    485,
                    2535,
                    489,
                    2538,
                    491,
                    493,
                    2543,
                    2550,
                    2551,
                    504,
                    505,
                    506,
                    2576,
                    530,
                    2579,
                    534,
                    2584,
                    540,
                    2592,
                    2593,
                    546,
                    2594,
                    559,
                    2608,
                    2611,
                    2613,
                    2616,
                    2618,
                    572,
                    2621,
                    2623,
                    2624,
                    2627,
                    580,
                    2629,
                    582,
                    2632,
                    586,
                    2634,
                    2635,
                    590,
                    2638,
                    2640,
                    2643,
                    596,
                    597,
                    2645,
                    2647,
                    2648,
                    604,
                    605,
                    610,
                    612,
                    618,
                    632,
                    636,
                    640,
                    644,
                    648,
                    649,
                    653,
                    657,
                    681,
                    685,
                    689,
                    693,
                    705,
                    709,
                    713,
                    714,
                    715,
                    722,
                    726,
                    730,
                    734,
                    738,
                    742,
                    746,
                    750,
                    761,
                    765,
                    770,
                    774,
                    785,
                    794,
                    796,
                    799,
                    2849,
                    809,
                    811,
                    814,
                    822,
                    2872,
                    832,
                    833,
                    836,
                    840,
                    847,
                    864,
                    868,
                    874,
                    885,
                    893,
                    899,
                    906,
                    915,
                    931,
                    932,
                    934,
                    935,
                    948,
                    949,
                    950,
                    960,
                    961,
                    962,
                    963,
                    991,
                    999,
                    1001,
                    1006,
                    1007,
                    1025,
                    1030,
                    1044,
                    1045,
                    1047,
                    1048,
                    1069,
                    1070,
                    1072,
                    1073,
                    1079,
                    1080,
                    1083,
                    1088,
                    1092,
                    1094,
                    1100,
                    1101,
                    1102,
                    1105,
                    1113,
                    1114,
                    1115,
                    1116,
                    1118,
                    1125,
                    1127,
                    1130,
                    1132,
                    1133,
                    1134,
                    1143,
                    1145,
                    1146,
                    1147,
                    1161,
                    1166,
                    1167,
                    1168,
                    1169,
                    1170,
                    1185,
                    1207,
                    1208,
                    1210,
                    1211,
                    1214,
                    1215,
                    1218,
                    1219,
                    1223,
                    1224,
                    1227,
                    1228,
                    1231,
                    1234,
                    1245,
                    1271,
                    1272,
                    1274,
                    1275,
                    1278,
                    1279,
                    1282,
                    1283,
                    1286,
                    1287,
                    1292,
                    1293,
                    1296,
                    1297,
                    1300,
                    1301,
                    1304,
                    1308,
                    1314,
                    1335,
                    1340,
                    1348,
                    1354,
                    1357,
                    1364,
                    1368,
                    1372,
                    1373,
                    1377,
                    1385,
                    1698,
                    1699,
                    1701,
                    1709,
                    1710,
                    1715,
                    1717,
                    1718,
                    1728,
                    1751,
                    1759,
                    1761,
                    1764,
                    1766,
                    1769,
                    1772,
                    1774,
                    1778,
                    1779,
                    1780,
                    1781,
                    1782,
                    1786,
                    1790,
                    1846,
                    1850,
                    1854,
                    1880,
                    1882,
                    1885,
                    1886,
                    1887,
                    1891,
                    1893,
                    1936,
                    1937,
                    1946,
                    1947
                ],
                "NAME_SCOPE_STACK": [
                    120,
                    114,
                    126,
                    122
                ],
                "NAME_SCOPE_STACK.append": [
                    120
                ],
                "name": [
                    129,
                    2827,
                    150,
                    159,
                    2850,
                    175,
                    2873,
                    317,
                    325,
                    333,
                    120,
                    354,
                    359,
                    364,
                    248,
                    1401,
                    1403,
                    252,
                    254,
                    127
                ],
                "NAME_SCOPE_STACK.pop": [
                    122
                ],
                "contextmanager": [
                    117
                ],
                "join": [
                    126
                ],
                "default": [
                    128
                ],
                "dtype": [
                    2689,
                    2692,
                    145,
                    146,
                    157,
                    168,
                    169,
                    1065,
                    174,
                    2682,
                    315,
                    316,
                    317,
                    578,
                    323,
                    324,
                    325,
                    581,
                    582,
                    331,
                    332,
                    333,
                    337,
                    341,
                    2655,
                    2656,
                    2660,
                    359,
                    2664,
                    2665,
                    364,
                    2669,
                    239,
                    240,
                    2673,
                    2674,
                    2678,
                    378,
                    2683,
                    252,
                    254
                ],
                "_assert_sparse_module": [
                    251,
                    148
                ],
                "variable": [
                    161,
                    162,
                    163,
                    164,
                    325,
                    358,
                    363,
                    333,
                    397,
                    149,
                    317,
                    158
                ],
                "th_sparse_module.as_sparse_variable": [
                    149
                ],
                "_prepare_name": [
                    248,
                    175,
                    150,
                    159
                ],
                "theano.tensor.TensorVariable": [
                    152
                ],
                "theano.tensor": [
                    824,
                    825,
                    152,
                    153,
                    154,
                    826,
                    894
                ],
                "theano.tensor.sharedvar.TensorSharedVariable": [
                    153
                ],
                "theano.tensor.sharedvar": [
                    153
                ],
                "theano.tensor.TensorConstant": [
                    154
                ],
                "value.eval": [
                    156
                ],
                "np.asarray": [
                    1368,
                    1373,
                    157,
                    950
                ],
                "np": [
                    648,
                    157,
                    172,
                    950,
                    2744,
                    317,
                    702,
                    704,
                    325,
                    2763,
                    333,
                    1872,
                    1105,
                    1368,
                    1373,
                    1118,
                    2658,
                    358,
                    363,
                    2667,
                    2676,
                    374,
                    2685
                ],
                "theano.shared": [
                    158
                ],
                "variable._keras_shape": [
                    161
                ],
                "value.shape": [
                    161
                ],
                "variable._uses_learning_phase": [
                    162
                ],
                "variable.constraint": [
                    163
                ],
                "constraint": [
                    163
                ],
                "np_value": [
                    172,
                    173
                ],
                "np.ones": [
                    172,
                    325
                ],
                "const": [
                    176,
                    177,
                    178,
                    173
                ],
                "T.constant": [
                    173
                ],
                "const._keras_shape": [
                    176
                ],
                "const._uses_learning_phase": [
                    177
                ],
                "is_tensor": [
                    224
                ],
                "str": [
                    226,
                    475,
                    1988
                ],
                "type": [
                    226
                ],
                "T.TensorVariable": [
                    232
                ],
                "T.sharedvar.TensorSharedVariable": [
                    233
                ],
                "T.sharedvar": [
                    233
                ],
                "ndim": [
                    1673,
                    1674,
                    907,
                    814,
                    2608,
                    2611,
                    2613,
                    2615,
                    2620,
                    2624,
                    2626,
                    836,
                    2631,
                    2635,
                    2637,
                    1487,
                    1488,
                    2642,
                    1496,
                    486,
                    874,
                    876,
                    1516,
                    1517,
                    241,
                    881,
                    1521,
                    244,
                    246,
                    249
                ],
                "len": [
                    1536,
                    1794,
                    1160,
                    1416,
                    1800,
                    1554,
                    1178,
                    1179,
                    1180,
                    1565,
                    1818,
                    416,
                    1824,
                    418,
                    420,
                    1072,
                    1613,
                    1627,
                    612,
                    491,
                    494,
                    497,
                    244
                ],
                "tuple": [
                    2050,
                    2065,
                    1050,
                    925,
                    1438,
                    932,
                    422,
                    1322,
                    947,
                    2612,
                    950,
                    966,
                    1874,
                    2004,
                    472,
                    476,
                    2020,
                    1136,
                    1010,
                    499,
                    627,
                    2035,
                    246,
                    1145,
                    506,
                    1149,
                    1406
                ],
                "_": [
                    2760,
                    1517,
                    1582,
                    1677,
                    2803,
                    246,
                    2906,
                    1630
                ],
                "range": [
                    1794,
                    1677,
                    1554,
                    794,
                    1818,
                    796,
                    1322,
                    1335,
                    1598,
                    836,
                    2884,
                    1613,
                    1496,
                    2910,
                    2911,
                    1125,
                    491,
                    1517,
                    494,
                    2926,
                    1648,
                    881,
                    2927,
                    1524,
                    246
                ],
                "broadcast": [
                    249,
                    254
                ],
                "sparse": [
                    250
                ],
                "th_sparse_module.csr_matrix": [
                    252
                ],
                "T.TensorType": [
                    254
                ],
                "x._keras_shape": [
                    1282,
                    1283,
                    1286,
                    1287,
                    1274,
                    1292,
                    1293,
                    1275,
                    1168,
                    1169,
                    1170,
                    1296,
                    1297,
                    1300,
                    1301,
                    1048,
                    1304,
                    1308,
                    414,
                    1279,
                    293,
                    1072,
                    1073,
                    950,
                    1079,
                    1080,
                    1210,
                    1083,
                    1211,
                    1214,
                    1215,
                    1088,
                    962,
                    963,
                    1092,
                    1218,
                    1219,
                    1223,
                    1224,
                    715,
                    1227,
                    1228,
                    1102,
                    1231,
                    1105,
                    1234,
                    2130,
                    2137,
                    1115,
                    1116,
                    1118,
                    612,
                    618,
                    491,
                    493,
                    1134,
                    1007,
                    506,
                    1147,
                    1278,
                    255
                ],
                "x._uses_learning_phase": [
                    256,
                    1717,
                    935
                ],
                "x._theano_placeholder": [
                    257,
                    270
                ],
                "x.shape": [
                    1161,
                    1166,
                    279,
                    2074,
                    2077,
                    2592,
                    1185,
                    2593,
                    2594,
                    2089,
                    2092,
                    2095,
                    840,
                    1143,
                    1880,
                    1113,
                    1245,
                    373,
                    2550,
                    2551
                ],
                "x.ndim": [
                    864,
                    868,
                    1094,
                    874,
                    299,
                    1778,
                    1044,
                    469,
                    471,
                    794,
                    796,
                    1335
                ],
                "x.dtype": [
                    580,
                    303,
                    1880,
                    1368,
                    1882,
                    1373
                ],
                "eval": [
                    309
                ],
                "to_dense": [
                    915,
                    309
                ],
                "np.zeros": [
                    317
                ],
                "np.eye": [
                    333
                ],
                "size": [
                    333
                ],
                "T.ones_like": [
                    337,
                    1921,
                    1919
                ],
                "T.zeros_like": [
                    1913,
                    2746,
                    1915,
                    341
                ],
                "x.copy": [
                    354
                ],
                "np.random.uniform": [
                    358
                ],
                "np.random": [
                    2658,
                    358,
                    2667,
                    363,
                    1872,
                    2676,
                    2685
                ],
                "low": [
                    358
                ],
                "high": [
                    358
                ],
                "np.random.normal": [
                    363
                ],
                "scale": [
                    363
                ],
                "f": [
                    373,
                    374
                ],
                "theano.function": [
                    1418,
                    373,
                    1398
                ],
                "np.prod": [
                    1105,
                    374,
                    1118
                ],
                "T.cast": [
                    378,
                    1764,
                    2718,
                    1831
                ],
                "new_x": [
                    385
                ],
                "increment": [
                    389
                ],
                "decrement": [
                    393
                ],
                "momentum": [
                    397
                ],
                "out": [
                    485,
                    422,
                    423,
                    486,
                    487,
                    499,
                    500,
                    410,
                    412
                ],
                "th_sparse_module.basic.structured_dot": [
                    410
                ],
                "th_sparse_module.basic": [
                    410,
                    909,
                    911
                ],
                "y": [
                    520,
                    522,
                    523,
                    1045,
                    1046,
                    410,
                    1050,
                    412,
                    413,
                    1052,
                    415,
                    1309,
                    1310,
                    931,
                    933,
                    935,
                    937,
                    938,
                    1069,
                    948,
                    950,
                    951,
                    1207,
                    960,
                    962,
                    709,
                    965,
                    966,
                    967,
                    713,
                    1095,
                    1096,
                    716,
                    717,
                    1100,
                    1103,
                    1236,
                    1105,
                    722,
                    1106,
                    468,
                    469,
                    596,
                    471,
                    597,
                    598,
                    726,
                    730,
                    604,
                    605,
                    606,
                    734,
                    1113,
                    1116,
                    738,
                    1118,
                    612,
                    485,
                    742,
                    1119,
                    1235,
                    489,
                    1132,
                    494,
                    496,
                    1136,
                    1137,
                    627,
                    628,
                    1271,
                    504,
                    1145,
                    506,
                    507,
                    1149,
                    1150
                ],
                "T.dot": [
                    412
                ],
                "x_shape": [
                    416,
                    417,
                    422,
                    414
                ],
                "list": [
                    1794,
                    919,
                    1048,
                    794,
                    1818,
                    1438,
                    414,
                    415,
                    950,
                    1590,
                    1598,
                    962,
                    1873,
                    1496,
                    472,
                    477,
                    1638,
                    617,
                    618,
                    1134,
                    1007,
                    1648,
                    881,
                    1524,
                    1143,
                    1147,
                    1406
                ],
                "y_shape": [
                    418,
                    419,
                    420,
                    421,
                    422,
                    415
                ],
                "y._keras_shape": [
                    522,
                    1050,
                    1309,
                    415,
                    933,
                    950,
                    962,
                    965,
                    966,
                    1095,
                    717,
                    1103,
                    1105,
                    1235,
                    1116,
                    1118,
                    612,
                    494,
                    496,
                    1136,
                    627,
                    506,
                    1149
                ],
                "x_shape.pop": [
                    417
                ],
                "y_shape.pop": [
                    419,
                    421
                ],
                "out._keras_shape": [
                    499,
                    422
                ],
                "axes": [
                    1332,
                    1333,
                    1336,
                    464,
                    465,
                    466,
                    469,
                    471,
                    1496,
                    472,
                    1497,
                    475,
                    476,
                    477,
                    479,
                    485,
                    492,
                    495,
                    1648,
                    1649
                ],
                "y.ndim": [
                    468,
                    469,
                    471
                ],
                "a": [
                    617,
                    620,
                    621,
                    623,
                    624,
                    693,
                    472
                ],
                "T.batched_tensordot": [
                    485
                ],
                "expand_dims": [
                    2274,
                    2275,
                    2276,
                    487,
                    1678,
                    2127,
                    1518,
                    2134,
                    2143
                ],
                "axis": [
                    1795,
                    1798,
                    1802,
                    907,
                    908,
                    1803,
                    910,
                    1804,
                    1148,
                    913,
                    530,
                    915,
                    534,
                    921,
                    922,
                    1819,
                    540,
                    924,
                    1822,
                    546,
                    1314,
                    1826,
                    1827,
                    1828,
                    681,
                    559,
                    1130,
                    1131,
                    572,
                    960,
                    963,
                    836,
                    837,
                    582,
                    965,
                    840,
                    586,
                    590,
                    596,
                    597,
                    604,
                    605,
                    611,
                    1891,
                    614,
                    615,
                    1126,
                    617,
                    1128,
                    491,
                    492,
                    493,
                    494,
                    495,
                    496,
                    879,
                    880,
                    882,
                    883,
                    1135,
                    1144,
                    1778,
                    632,
                    1780,
                    1782,
                    636
                ],
                "shape.append": [
                    496,
                    498,
                    493
                ],
                "T.transpose": [
                    504
                ],
                "reversed": [
                    506
                ],
                "reference": [
                    520,
                    521,
                    522
                ],
                "indices": [
                    1600,
                    1253,
                    1605,
                    520,
                    521,
                    522,
                    1192,
                    1322,
                    1323,
                    1324,
                    1266,
                    1203,
                    1524,
                    1531,
                    1526,
                    1207,
                    1271,
                    1598
                ],
                "indices._keras_shape": [
                    522
                ],
                "reference._keras_shape": [
                    522
                ],
                "T.max": [
                    2721,
                    530,
                    2724,
                    2774
                ],
                "keepdims": [
                    546,
                    612,
                    582,
                    681,
                    586,
                    619,
                    590,
                    530,
                    596,
                    597,
                    534,
                    540,
                    605,
                    604
                ],
                "T.min": [
                    534
                ],
                "T.sum": [
                    681,
                    1891,
                    540,
                    2775
                ],
                "T.prod": [
                    1113,
                    546
                ],
                "T.extra_ops.cumsum": [
                    559
                ],
                "T.extra_ops": [
                    1832,
                    1324,
                    559,
                    1046,
                    572
                ],
                "T.extra_ops.cumprod": [
                    572
                ],
                "T.mean": [
                    582
                ],
                "T.std": [
                    586
                ],
                "T.var": [
                    590
                ],
                "T.any": [
                    596
                ],
                "_set_keras_shape_for_reduction": [
                    605,
                    597
                ],
                "T.all": [
                    604
                ],
                "axis_list": [
                    617,
                    620,
                    623,
                    615
                ],
                "set": [
                    617
                ],
                "keras_shape_list": [
                    618,
                    621,
                    624,
                    625,
                    626,
                    627
                ],
                "keras_shape_list.pop": [
                    624
                ],
                "T.argmax": [
                    632
                ],
                "T.argmin": [
                    636
                ],
                "T.sqr": [
                    640
                ],
                "T.abs_": [
                    644
                ],
                "T.clip": [
                    705,
                    2694,
                    648,
                    1769,
                    1841,
                    1813
                ],
                "np.inf": [
                    648,
                    702,
                    704
                ],
                "T.sqrt": [
                    649,
                    1892
                ],
                "T.exp": [
                    2725,
                    681,
                    653,
                    1781,
                    2775
                ],
                "T.log": [
                    681,
                    2734,
                    657,
                    2775,
                    2745
                ],
                "T.round": [
                    685
                ],
                "T.sgn": [
                    689
                ],
                "T.pow": [
                    693
                ],
                "min_value": [
                    705,
                    697,
                    699,
                    700,
                    701,
                    702
                ],
                "float": [
                    697,
                    698
                ],
                "max_value": [
                    704,
                    705,
                    1768,
                    1769,
                    698,
                    699,
                    700,
                    703
                ],
                "T.eq": [
                    709
                ],
                "z": [
                    713,
                    715,
                    717,
                    718
                ],
                "T.neq": [
                    2712,
                    713
                ],
                "z._keras_shape": [
                    715,
                    717
                ],
                "T.gt": [
                    722,
                    1764
                ],
                "T.ge": [
                    1925,
                    726
                ],
                "T.lt": [
                    730
                ],
                "T.le": [
                    734
                ],
                "T.maximum": [
                    738,
                    1892,
                    2719
                ],
                "T.minimum": [
                    2718,
                    742
                ],
                "T.sin": [
                    746
                ],
                "T.cos": [
                    750
                ],
                "T.nnet.bn": [
                    899,
                    773,
                    784,
                    759,
                    798
                ],
                "T.nnet": [
                    899,
                    2179,
                    773,
                    2307,
                    784,
                    1808,
                    1814,
                    2454,
                    798,
                    1839,
                    1842,
                    1846,
                    2231,
                    1850,
                    2363,
                    2370,
                    2506,
                    1750,
                    1751,
                    1755,
                    1759,
                    1761,
                    1766,
                    1002,
                    2417,
                    1779,
                    759,
                    1786,
                    2300
                ],
                "_old_normalize_batch_in_training": [
                    760
                ],
                "gamma": [
                    769,
                    771,
                    899,
                    774,
                    893,
                    785,
                    787,
                    788,
                    799,
                    808,
                    809,
                    819,
                    846,
                    859,
                    860,
                    872,
                    886,
                    761,
                    763,
                    765,
                    767
                ],
                "beta": [
                    768,
                    770,
                    771,
                    899,
                    774,
                    785,
                    789,
                    790,
                    799,
                    810,
                    811,
                    818,
                    845,
                    861,
                    862,
                    871,
                    887,
                    761,
                    764,
                    893,
                    767
                ],
                "reduction_axes": [
                    832,
                    833,
                    837,
                    774,
                    815,
                    761,
                    794,
                    796,
                    799
                ],
                "epsilon": [
                    899,
                    1892,
                    774,
                    785,
                    849,
                    1841,
                    1813,
                    822,
                    761,
                    890,
                    893,
                    799
                ],
                "ones_like": [
                    809,
                    788,
                    860,
                    765,
                    767
                ],
                "zeros_like": [
                    1537,
                    770,
                    771,
                    811,
                    790,
                    862
                ],
                "normed": [
                    773,
                    776,
                    847,
                    850,
                    823,
                    824,
                    828
                ],
                "mean": [
                    2689,
                    899,
                    2692,
                    773,
                    2694,
                    776,
                    785,
                    790,
                    792,
                    796,
                    799,
                    823,
                    825,
                    828,
                    833,
                    843,
                    850,
                    862,
                    864,
                    2660,
                    869,
                    879,
                    888,
                    893
                ],
                "stdinv": [
                    773,
                    776,
                    823,
                    826,
                    827
                ],
                "T.nnet.bn.batch_normalization_train": [
                    773
                ],
                "T.inv": [
                    776,
                    827
                ],
                "_old_batch_normalization": [
                    785
                ],
                "var": [
                    832,
                    899,
                    870,
                    860,
                    844,
                    785,
                    850,
                    788,
                    889,
                    827,
                    828,
                    893,
                    799
                ],
                "mean.ndim": [
                    792,
                    864
                ],
                "i": [
                    1541,
                    1545,
                    1554,
                    1557,
                    2074,
                    2075,
                    796,
                    2077,
                    2078,
                    2089,
                    1322,
                    2090,
                    2092,
                    2093,
                    2095,
                    2096,
                    1073,
                    1074,
                    1077,
                    1335,
                    1336,
                    2884,
                    1605,
                    1606,
                    2885,
                    2886,
                    1613,
                    1615,
                    2910,
                    2912,
                    2913,
                    1125,
                    2919,
                    2926,
                    2928,
                    2929,
                    1531,
                    1532
                ],
                "mean.broadcastable": [
                    796,
                    879
                ],
                "T.nnet.bn.batch_normalization_test": [
                    798
                ],
                "dev": [
                    816,
                    875,
                    876,
                    813
                ],
                "theano.config.device": [
                    875,
                    813
                ],
                "use_cudnn": [
                    817,
                    876,
                    877,
                    814
                ],
                "dev.startswith": [
                    816,
                    876
                ],
                "broadcast_beta": [
                    848,
                    818,
                    845,
                    822
                ],
                "beta.dimshuffle": [
                    818,
                    887,
                    871
                ],
                "broadcast_gamma": [
                    848,
                    846,
                    819,
                    822
                ],
                "gamma.dimshuffle": [
                    872,
                    819,
                    886
                ],
                "trained": [
                    821,
                    823
                ],
                "theano.sandbox.cuda.dnn.dnn_batch_normalization_train": [
                    821
                ],
                "theano.sandbox.cuda.dnn": [
                    892,
                    884,
                    821
                ],
                "theano.sandbox.cuda": [
                    892,
                    884,
                    821
                ],
                "theano.sandbox": [
                    892,
                    884,
                    821
                ],
                "theano.tensor.as_tensor_variable": [
                    824,
                    825,
                    826,
                    894
                ],
                "T.flatten": [
                    1323,
                    828,
                    1831,
                    1100
                ],
                "AttributeError": [
                    829,
                    2119,
                    895
                ],
                "x.var": [
                    832
                ],
                "x.mean": [
                    833
                ],
                "target_shape": [
                    835,
                    838,
                    840,
                    841,
                    843,
                    844,
                    845,
                    846
                ],
                "target_shape.append": [
                    840,
                    838
                ],
                "T.stack": [
                    1314,
                    841,
                    1611,
                    1614,
                    1552,
                    1558
                ],
                "broadcast_mean": [
                    843,
                    847
                ],
                "T.reshape": [
                    1145,
                    931,
                    843,
                    844,
                    845,
                    846,
                    1325,
                    1113
                ],
                "broadcast_var": [
                    844,
                    847
                ],
                "batch_normalization": [
                    847
                ],
                "shuffle_pattern": [
                    868,
                    869,
                    870,
                    871,
                    872,
                    881,
                    882,
                    883,
                    885,
                    886,
                    887,
                    888,
                    889,
                    890
                ],
                "mean.dimshuffle": [
                    888,
                    869
                ],
                "var.dimshuffle": [
                    889,
                    870
                ],
                "mean.broadcastable.index": [
                    879
                ],
                "result": [
                    1166,
                    1168,
                    1171,
                    884,
                    892,
                    894
                ],
                "dimshuffle": [
                    884,
                    2765
                ],
                "theano.sandbox.cuda.dnn.dnn_batch_normalization_test": [
                    892,
                    884
                ],
                "x.dimshuffle": [
                    2535,
                    1132,
                    1936,
                    2576,
                    948,
                    1045,
                    885,
                    1946
                ],
                "T.nnet.bn.batch_normalization": [
                    899
                ],
                "sqrt": [
                    899
                ],
                "tensors": [
                    906,
                    907,
                    909,
                    911,
                    915,
                    917,
                    918
                ],
                "output": [
                    1025,
                    1026,
                    1027,
                    1028,
                    1537,
                    1030,
                    1031,
                    1032,
                    1033,
                    1541,
                    1542,
                    1800,
                    909,
                    1165,
                    911,
                    1166,
                    1549,
                    1805,
                    915,
                    1808,
                    1811,
                    1813,
                    1814,
                    1818,
                    925,
                    927,
                    1824,
                    1829,
                    1191,
                    1832,
                    1833,
                    1834,
                    1839,
                    1841,
                    1202,
                    1842,
                    1207,
                    1794,
                    2940,
                    2891,
                    2892,
                    2909,
                    991,
                    992,
                    2144,
                    1252,
                    2149,
                    2918,
                    999,
                    2151,
                    1001,
                    1002,
                    2152,
                    2920,
                    1005,
                    2921,
                    1007,
                    1008,
                    1009,
                    1010,
                    1265,
                    2923,
                    1014,
                    1271,
                    2936,
                    2937,
                    2939,
                    1532,
                    1533
                ],
                "th_sparse_module.basic.vstack": [
                    909
                ],
                "th_sparse_module.basic.hstack": [
                    911
                ],
                "T.concatenate": [
                    1520,
                    2721,
                    915
                ],
                "input_shapes": [
                    920,
                    918,
                    919
                ],
                "tensor._keras_shape": [
                    918
                ],
                "output_shape": [
                    1162,
                    1165,
                    919,
                    921,
                    922,
                    924,
                    925,
                    1187,
                    2211,
                    2212,
                    2213,
                    1191,
                    2214,
                    1198,
                    1072,
                    1202,
                    1075,
                    1077,
                    2485,
                    1079,
                    2486,
                    1081,
                    2487,
                    1083,
                    2488,
                    2489,
                    2238,
                    1088,
                    1092,
                    1094,
                    1095,
                    2511,
                    2904,
                    1247,
                    1252,
                    1260,
                    1265
                ],
                "output._keras_shape": [
                    1007,
                    1008,
                    1009,
                    1010,
                    925
                ],
                "y._uses_learning_phase": [
                    937,
                    935
                ],
                "pattern": [
                    1125,
                    1131,
                    1132,
                    947,
                    948,
                    950
                ],
                "T.repeat": [
                    960
                ],
                "rep": [
                    960,
                    965
                ],
                "repeat_dim": [
                    963,
                    964,
                    965
                ],
                "data_format": [
                    1024,
                    2563,
                    1029,
                    1035,
                    2575,
                    2079,
                    2601,
                    2607,
                    2097,
                    2614,
                    2619,
                    2113,
                    2625,
                    2630,
                    2124,
                    2636,
                    2641,
                    2146,
                    2148,
                    2167,
                    2169,
                    2173,
                    2175,
                    2176,
                    2186,
                    1183,
                    2208,
                    1186,
                    2210,
                    2225,
                    2227,
                    2228,
                    1209,
                    2240,
                    2264,
                    1243,
                    1246,
                    2270,
                    2280,
                    2286,
                    2292,
                    2294,
                    2296,
                    2297,
                    1273,
                    2315,
                    2341,
                    2343,
                    2349,
                    2355,
                    2357,
                    2359,
                    2360,
                    2877,
                    2378,
                    2901,
                    2908,
                    2402,
                    2404,
                    2410,
                    2412,
                    2414,
                    2425,
                    2442,
                    1931,
                    2444,
                    2448,
                    2450,
                    2451,
                    1941,
                    2461,
                    2482,
                    2484,
                    2500,
                    2502,
                    2503,
                    1999,
                    2513,
                    981,
                    2519,
                    984,
                    988,
                    2015,
                    998,
                    2534,
                    1004,
                    2556
                ],
                "axis_1": [
                    1008,
                    985,
                    982,
                    991
                ],
                "axis_2": [
                    992,
                    1009,
                    986,
                    983
                ],
                "interpolation": [
                    993,
                    990
                ],
                "repeat_elements": [
                    992,
                    1025,
                    1026,
                    1027,
                    1030,
                    1031,
                    1032,
                    991
                ],
                "height_factor": [
                    1026,
                    994,
                    1031,
                    1003,
                    1008,
                    991
                ],
                "width_factor": [
                    992,
                    994,
                    1027,
                    1032,
                    1009
                ],
                "NotImplementedError": [
                    1344,
                    995,
                    2944,
                    2949
                ],
                "permute_dimensions": [
                    1829,
                    1830,
                    999,
                    2923,
                    2892,
                    1805,
                    1005,
                    1806,
                    2939
                ],
                "T.nnet.abstract_conv.bilinear_upsampling": [
                    1002
                ],
                "T.nnet.abstract_conv": [
                    2506,
                    1002,
                    2231
                ],
                "depth_factor": [
                    1025,
                    1030
                ],
                "T.extra_ops.repeat": [
                    1046
                ],
                "n": [
                    1089,
                    1091,
                    1069,
                    1071,
                    1072,
                    1073,
                    1046,
                    1078,
                    1049,
                    1083,
                    1086
                ],
                "shape.insert": [
                    1049,
                    1135
                ],
                "T.arange": [
                    1924,
                    1065,
                    2765,
                    2706,
                    2711
                ],
                "start": [
                    1065
                ],
                "stop": [
                    1065
                ],
                "step": [
                    1065,
                    2761
                ],
                "T.tile": [
                    1521,
                    1069
                ],
                "_is_explicit_shape": [
                    1071
                ],
                "j": [
                    2914,
                    2915,
                    2919,
                    2927,
                    1073,
                    2930,
                    2931,
                    1077,
                    2911
                ],
                "zip": [
                    1544,
                    1073,
                    1577
                ],
                "n.ndim": [
                    1086
                ],
                "n_size": [
                    1091,
                    1092
                ],
                "n._keras_shape": [
                    1091
                ],
                "x.type.ndim": [
                    1130,
                    1125,
                    1127
                ],
                "x.type": [
                    1130,
                    1125,
                    1127
                ],
                "pattern.insert": [
                    1131
                ],
                "shape.pop": [
                    1144
                ],
                "kshape": [
                    1147,
                    1148,
                    1149
                ],
                "kshape.pop": [
                    1148
                ],
                "padding": [
                    2177,
                    1283,
                    2565,
                    1160,
                    2185,
                    2313,
                    1163,
                    2570,
                    1293,
                    1166,
                    2573,
                    1169,
                    1297,
                    2452,
                    1301,
                    2072,
                    1178,
                    1179,
                    1180,
                    1181,
                    1182,
                    2460,
                    2591,
                    2087,
                    2220,
                    2230,
                    2361,
                    1981,
                    1983,
                    2239,
                    1985,
                    2495,
                    1988,
                    2116,
                    2376,
                    2505,
                    2122,
                    2512,
                    2523,
                    1249,
                    1250,
                    1251,
                    2145,
                    2529,
                    2532,
                    1255,
                    1256,
                    1257,
                    1261,
                    1262,
                    1263,
                    2415,
                    1267,
                    1268,
                    1269,
                    2549,
                    2425,
                    2298,
                    1275,
                    1279
                ],
                "input_shape": [
                    1161,
                    1162,
                    1163,
                    1164,
                    1185,
                    1187,
                    1188,
                    1189,
                    1190,
                    1194,
                    1195,
                    1322,
                    1325,
                    1198,
                    1199,
                    1200,
                    1201,
                    1204,
                    1205,
                    1245,
                    1247,
                    1248,
                    1249,
                    1250,
                    1251,
                    1255,
                    1256,
                    1257,
                    1260,
                    1261,
                    1262,
                    1263,
                    1264,
                    1267,
                    1268,
                    1269
                ],
                "T.zeros": [
                    1252,
                    1191,
                    1165,
                    1265,
                    1202
                ],
                "T.set_subtensor": [
                    2728,
                    1166,
                    2736,
                    2706,
                    1271,
                    1207
                ],
                "result._keras_shape": [
                    1168
                ],
                "top_pad": [
                    1189,
                    1224,
                    1194,
                    1199,
                    1204,
                    1211,
                    1181
                ],
                "bottom_pad": [
                    1189,
                    1224,
                    1199,
                    1211,
                    1181
                ],
                "left_pad": [
                    1190,
                    2120,
                    2121,
                    1195,
                    1228,
                    1200,
                    1205,
                    1182,
                    1215
                ],
                "right_pad": [
                    1190,
                    1228,
                    1200,
                    1182,
                    1215
                ],
                "normalize_data_format": [
                    2208,
                    2113,
                    2402,
                    2519,
                    2563,
                    2341,
                    2442,
                    2607,
                    2482,
                    2901,
                    2167,
                    2264,
                    1243,
                    2877,
                    1183
                ],
                "h": [
                    1220,
                    1305,
                    1224,
                    1288,
                    1226,
                    1293,
                    1275,
                    1232,
                    1295,
                    1277,
                    1211,
                    1213
                ],
                "w": [
                    1217,
                    1281,
                    1279,
                    1221,
                    1289,
                    1228,
                    1230,
                    1233,
                    1297,
                    1299,
                    1306,
                    1215
                ],
                "output_keras_shape": [
                    1218,
                    1286,
                    1231,
                    1235,
                    1304,
                    1309
                ],
                "d": [
                    1283,
                    1285,
                    1290,
                    1301,
                    1303,
                    1307
                ],
                "indices.shape": [
                    1322
                ],
                "indices.ndim": [
                    1322
                ],
                "oh": [
                    1324,
                    1325,
                    1326
                ],
                "T.extra_ops.to_one_hot": [
                    1832,
                    1324
                ],
                "num_classes": [
                    1324,
                    1325
                ],
                "slices": [
                    1337,
                    1339,
                    1340,
                    1334
                ],
                "slices.append": [
                    1337,
                    1339
                ],
                "T.patternbroadcast": [
                    1883,
                    1348
                ],
                "broadcastable": [
                    1348
                ],
                "TypeError": [
                    1920,
                    2691,
                    1355,
                    2059,
                    1997,
                    2029,
                    1914,
                    2044,
                    2013
                ],
                "x.get_value": [
                    1377,
                    1357
                ],
                "get_value": [
                    1364
                ],
                "xs": [
                    2883,
                    2887,
                    2889,
                    2925,
                    1364,
                    2932,
                    2935
                ],
                "x.set_value": [
                    1368,
                    1373
                ],
                "tuples": [
                    1372
                ],
                "p_op": [
                    1384,
                    1385
                ],
                "Print": [
                    1384
                ],
                "message": [
                    1384
                ],
                "object": [
                    1390
                ],
                "unique_variables_to_update": [
                    1393,
                    1395,
                    1396,
                    1397
                ],
                "v": [
                    2050,
                    2020,
                    2065,
                    1394,
                    1395,
                    2004,
                    1396,
                    2035
                ],
                "nv": [
                    1394,
                    1396
                ],
                "updates": [
                    1394,
                    1420,
                    1397,
                    1398
                ],
                "unique_variables_to_update.items": [
                    1397
                ],
                "self.function": [
                    1398,
                    1407
                ],
                "self": [
                    1403,
                    1398,
                    1407
                ],
                "inputs": [
                    1632,
                    1569,
                    2916,
                    1606,
                    2887,
                    1420,
                    1561,
                    1487,
                    1584,
                    1619,
                    2932,
                    1398,
                    1497,
                    1532,
                    1406,
                    1407
                ],
                "outputs": [
                    1420,
                    1552,
                    1569,
                    1570,
                    1574,
                    1575,
                    1580,
                    1591,
                    1594,
                    1606,
                    1607,
                    1609,
                    1611,
                    1619,
                    1620,
                    1623,
                    1639,
                    1642,
                    1645,
                    1646,
                    1648,
                    1649,
                    1652,
                    1398
                ],
                "kwargs": [
                    1416,
                    1417,
                    1402,
                    1420
                ],
                "self.name": [
                    1403
                ],
                "msg": [
                    1411,
                    1412
                ],
                "key": [
                    1417,
                    1418,
                    1411,
                    1419
                ],
                "kwargs.keys": [
                    1417
                ],
                "has_arg": [
                    1418
                ],
                "_raise_invalid_arg": [
                    1419
                ],
                "Function": [
                    1420
                ],
                "T.grad": [
                    1424
                ],
                "loss": [
                    1424
                ],
                "variables": [
                    1424,
                    1441,
                    1438,
                    1439
                ],
                "map": [
                    1439
                ],
                "theano.gradient.disconnected_grad": [
                    1441,
                    1439
                ],
                "theano.gradient": [
                    1441,
                    1439
                ],
                "inputs.ndim": [
                    1487
                ],
                "unroll": [
                    1490,
                    1523,
                    1597
                ],
                "input_length": [
                    2806,
                    1491,
                    1524,
                    1598
                ],
                "inputs.dimshuffle": [
                    1497
                ],
                "constants": [
                    1634,
                    1606,
                    1532,
                    1586,
                    1561,
                    1499,
                    1500
                ],
                "uses_learning_phase": [
                    1572,
                    1608,
                    1695,
                    1651,
                    1716,
                    1622,
                    1693,
                    1534,
                    1503
                ],
                "mask": [
                    1505,
                    1506,
                    1541,
                    1509,
                    1510,
                    1574,
                    1545,
                    1578,
                    2766,
                    1584,
                    2769,
                    2773,
                    2775
                ],
                "mask.ndim": [
                    1506
                ],
                "format": [
                    1824,
                    1508,
                    1797,
                    1798,
                    1800,
                    1821,
                    1822
                ],
                "mask.shape": [
                    1509
                ],
                "mask.dimshuffle": [
                    1510
                ],
                "ref_tensor_t.ndim": [
                    1516
                ],
                "ref_tensor_t": [
                    1516,
                    1519
                ],
                "mask_t": [
                    1521,
                    1518
                ],
                "add_shape": [
                    1520,
                    1519
                ],
                "ref_tensor_t.shape": [
                    1519
                ],
                "reps": [
                    1520,
                    1521
                ],
                "go_backwards": [
                    1635,
                    1587,
                    1525,
                    1599
                ],
                "successive_outputs": [
                    1536,
                    1602,
                    1539,
                    1609,
                    1611,
                    1549,
                    1552,
                    1528
                ],
                "successive_states": [
                    1603,
                    1610,
                    1613,
                    1550,
                    1615,
                    1554,
                    1556,
                    1529
                ],
                "states": [
                    1544,
                    1547,
                    1550,
                    1553,
                    1558,
                    1569,
                    1577,
                    1592,
                    1595,
                    1604,
                    1606,
                    1610,
                    1612,
                    1614,
                    1619,
                    1640,
                    1643,
                    1650,
                    1652,
                    1530,
                    1532
                ],
                "initial_states": [
                    1633,
                    1604,
                    1585,
                    1561,
                    1530,
                    1627,
                    1628,
                    1565,
                    1566
                ],
                "new_states": [
                    1569,
                    1544,
                    1577,
                    1555,
                    1619,
                    1557,
                    1558,
                    1623,
                    1532
                ],
                "step_function": [
                    1569,
                    1606,
                    1619,
                    1561,
                    1532
                ],
                "getattr": [
                    1570,
                    1620,
                    1533,
                    1607
                ],
                "prev_output": [
                    1537,
                    1539,
                    1542
                ],
                "output_mask": [
                    1574,
                    1541,
                    1542,
                    1575
                ],
                "get_matching_mask": [
                    1545,
                    1578,
                    1541,
                    1574
                ],
                "T.switch": [
                    1542,
                    1575,
                    1546,
                    1579,
                    1679
                ],
                "kept_states": [
                    1546,
                    1547,
                    1543
                ],
                "state": [
                    1544,
                    1545,
                    1546,
                    1578,
                    1577,
                    1579,
                    1650
                ],
                "new_state": [
                    1544,
                    1577,
                    1546,
                    1579
                ],
                "state_mask": [
                    1545,
                    1546,
                    1579,
                    1578
                ],
                "kept_states.append": [
                    1546
                ],
                "successive_outputs.append": [
                    1609,
                    1549
                ],
                "successive_states.append": [
                    1610,
                    1550
                ],
                "states_at_step": [
                    1556,
                    1557,
                    1615
                ],
                "new_states.append": [
                    1557
                ],
                "states.append": [
                    1614,
                    1558
                ],
                "initial_output": [
                    1561,
                    1562,
                    1564,
                    1585
                ],
                "T.unbroadcast": [
                    1564,
                    1566,
                    1628
                ],
                "output_tm1": [
                    1575
                ],
                "return_states": [
                    1576,
                    1579,
                    1580
                ],
                "return_states.append": [
                    1579
                ],
                "results": [
                    1638,
                    1639,
                    1640,
                    1642,
                    1582,
                    1590,
                    1591,
                    1592,
                    1594,
                    1630
                ],
                "theano.scan": [
                    2760,
                    1630,
                    2803,
                    1582
                ],
                "_step": [
                    1631,
                    1583
                ],
                "T.squeeze": [
                    1650,
                    1645
                ],
                "last_output": [
                    1651,
                    1652,
                    1646
                ],
                "outputs.ndim": [
                    1648
                ],
                "outputs.dimshuffle": [
                    1649
                ],
                "last_output._uses_learning_phase": [
                    1651
                ],
                "callable": [
                    1698,
                    1669,
                    1671,
                    1704,
                    1709,
                    1711
                ],
                "then_expression": [
                    1674,
                    1669,
                    1670,
                    1679
                ],
                "else_expression": [
                    1672,
                    1679,
                    1671
                ],
                "cond_ndim": [
                    1673,
                    1675,
                    1676
                ],
                "condition": [
                    1673,
                    1678,
                    1679
                ],
                "expr_ndim": [
                    1674,
                    1675,
                    1676
                ],
                "ndim_diff": [
                    1676,
                    1677
                ],
                "training": [
                    1728,
                    1697,
                    1703,
                    1715,
                    1691,
                    1692
                ],
                "learning_phase": [
                    1692
                ],
                "alt": [
                    1728,
                    1704,
                    1705,
                    1707,
                    1711,
                    1712,
                    1715
                ],
                "ifelse": [
                    1715
                ],
                "in_train_phase": [
                    1728
                ],
                "module": [
                    1734
                ],
                "func": [
                    1734
                ],
                "EnvironmentError": [
                    1735
                ],
                "_assert_has_capability": [
                    1755,
                    1750
                ],
                "T.nnet.elu": [
                    1751
                ],
                "alpha": [
                    1771,
                    1772,
                    1751,
                    2744,
                    1757
                ],
                "threshold": [
                    1763,
                    1764,
                    1758,
                    1759
                ],
                "negative_part": [
                    1761,
                    1772,
                    1759
                ],
                "T.nnet.relu": [
                    1761,
                    1766,
                    1759
                ],
                "T.nnet.softmax": [
                    1808,
                    1779
                ],
                "xm": [
                    1780,
                    1781,
                    1782
                ],
                "x.max": [
                    1780
                ],
                "T.nnet.softplus": [
                    1786
                ],
                "T_softsign": [
                    1790
                ],
                "output_dimensions": [
                    1794,
                    1795,
                    1826,
                    1827,
                    1828,
                    1802,
                    1803,
                    1804,
                    1818,
                    1819
                ],
                "int_shape": [
                    1794,
                    1800,
                    2444,
                    2445,
                    1818,
                    1824,
                    2343,
                    2216,
                    2344,
                    2350,
                    2491,
                    2880,
                    2115,
                    2123,
                    2905,
                    2404,
                    2405,
                    2280,
                    2281,
                    2287,
                    2169,
                    2170,
                    1917
                ],
                "permutation": [
                    1827,
                    1828,
                    1829,
                    1830,
                    1803,
                    1804,
                    1805,
                    1806
                ],
                "target": [
                    1830,
                    1831,
                    1832,
                    1833,
                    1834,
                    1806,
                    1842,
                    1814
                ],
                "from_logits": [
                    1834,
                    1838,
                    1807
                ],
                "output.sum": [
                    1811
                ],
                "T.nnet.categorical_crossentropy": [
                    1814
                ],
                "output.shape": [
                    1832
                ],
                "reshape": [
                    1833,
                    1967,
                    2616,
                    2618,
                    2621,
                    2623,
                    2627,
                    2629,
                    2887,
                    2632,
                    2634,
                    2638,
                    2640,
                    2643,
                    2645,
                    2916,
                    2921,
                    2932,
                    2937
                ],
                "categorical_crossentropy": [
                    1834
                ],
                "T.nnet.sigmoid": [
                    1846,
                    1839
                ],
                "T.nnet.binary_crossentropy": [
                    1842
                ],
                "T.nnet.hard_sigmoid": [
                    1850
                ],
                "T.tanh": [
                    1854
                ],
                "level": [
                    1877,
                    1869
                ],
                "seed": [
                    2657,
                    2658,
                    2659,
                    2666,
                    2667,
                    2668,
                    1871,
                    1872,
                    2675,
                    1876,
                    2676,
                    2677,
                    2684,
                    2685,
                    2686
                ],
                "np.random.randint": [
                    2658,
                    2667,
                    1872,
                    2676,
                    2685
                ],
                "noise_shape": [
                    1873,
                    1874,
                    1879,
                    1882,
                    1884
                ],
                "rng": [
                    2689,
                    2659,
                    2660,
                    2692,
                    2668,
                    2669,
                    1876,
                    2677,
                    2678,
                    1880,
                    1882,
                    2686
                ],
                "RandomStreams": [
                    2659,
                    2668,
                    1876,
                    2677,
                    2686
                ],
                "retain_prob": [
                    1880,
                    1882,
                    1877,
                    1886
                ],
                "random_tensor": [
                    1880,
                    1882,
                    1883,
                    1885
                ],
                "rng.binomial": [
                    1880,
                    1882,
                    2678
                ],
                "dim": [
                    1884
                ],
                "square_sum": [
                    1891,
                    1892
                ],
                "T.square": [
                    1891
                ],
                "norm": [
                    1892,
                    1893
                ],
                "k": [
                    1923,
                    1917,
                    1910
                ],
                "targets": [
                    1921,
                    1924,
                    1913,
                    1915,
                    1919
                ],
                "predictions": [
                    1923,
                    1924,
                    1917
                ],
                "predictions_k": [
                    1923,
                    1925
                ],
                "T.sort": [
                    1923
                ],
                "targets_values": [
                    1924,
                    1925
                ],
                "targets.shape": [
                    1924
                ],
                "kernel": [
                    2176,
                    2179,
                    2445,
                    2447,
                    2451,
                    2454,
                    1955,
                    1956,
                    2216,
                    2218,
                    1965,
                    1966,
                    1967,
                    1968,
                    2228,
                    1976,
                    1977,
                    2491,
                    2493,
                    2238,
                    2880,
                    2115,
                    2503,
                    2891,
                    2511,
                    2905,
                    2143,
                    2144,
                    2919,
                    2936,
                    2170,
                    2172
                ],
                "kernel.dimshuffle": [
                    1976,
                    1955,
                    1966
                ],
                "kernel_shape": [
                    2183,
                    2186,
                    2445,
                    2446,
                    2447,
                    2448,
                    2073,
                    2458,
                    2076,
                    2461,
                    2088,
                    2216,
                    2217,
                    2091,
                    2218,
                    2220,
                    2094,
                    1967,
                    2225,
                    2233,
                    2491,
                    2492,
                    2493,
                    2495,
                    2240,
                    2880,
                    2881,
                    2115,
                    2500,
                    2118,
                    2120,
                    2507,
                    2513,
                    2905,
                    2906,
                    2170,
                    2171,
                    2172,
                    2173
                ],
                "th_padding": [
                    2177,
                    2180,
                    2308,
                    2452,
                    2455,
                    2230,
                    2361,
                    2235,
                    2364,
                    1982,
                    1984,
                    1986,
                    2371,
                    1989,
                    2505,
                    2509,
                    2415,
                    2418,
                    2298,
                    2301
                ],
                "image_shape": [
                    2369,
                    2306,
                    2404,
                    2182,
                    2343,
                    2280,
                    2000,
                    2001,
                    2003,
                    2004,
                    2005,
                    2420,
                    2423,
                    2169,
                    2366,
                    2303
                ],
                "transpose_shape": [
                    2001
                ],
                "int_or_none": [
                    2050,
                    2020,
                    2065,
                    2035,
                    2004
                ],
                "volume_shape": [
                    2016,
                    2017,
                    2018,
                    2019,
                    2020,
                    2021,
                    2444,
                    2457
                ],
                "filter_shape": [
                    2048,
                    2049,
                    2050,
                    2051,
                    2061,
                    2062,
                    2031,
                    2032,
                    2033,
                    2034,
                    2035,
                    2036,
                    2063,
                    2064,
                    2065,
                    2066,
                    2046,
                    2047
                ],
                "strides": [
                    2181,
                    2186,
                    2315,
                    2579,
                    2456,
                    2584,
                    2074,
                    2077,
                    2461,
                    2592,
                    2593,
                    2594,
                    2089,
                    2092,
                    2095,
                    2234,
                    2365,
                    2879,
                    2240,
                    2378,
                    2508,
                    2513,
                    2903,
                    2265,
                    2266,
                    2140,
                    2145,
                    2277,
                    2538,
                    2543,
                    2419,
                    2550,
                    2551,
                    2425,
                    2302
                ],
                "conv_out": [
                    2179,
                    2307,
                    2185,
                    2313,
                    2187,
                    2316,
                    2317,
                    2454,
                    2075,
                    2460,
                    2078,
                    2462,
                    2080,
                    2081,
                    2090,
                    2093,
                    2096,
                    2098,
                    2099,
                    2363,
                    2238,
                    2239,
                    2241,
                    2370,
                    2376,
                    2379,
                    2511,
                    2512,
                    2514,
                    2417,
                    2424,
                    2425,
                    2426,
                    2300
                ],
                "conv_out.dimshuffle": [
                    2080,
                    2098
                ],
                "dilation_rate": [
                    2368,
                    2305,
                    2146,
                    2278,
                    2375,
                    2120,
                    2184,
                    2312,
                    2267,
                    2459,
                    2422,
                    2139,
                    2268,
                    2237
                ],
                "temporal_padding": [
                    2121
                ],
                "conv2d": [
                    2144
                ],
                "squeeze": [
                    2316,
                    2149,
                    2151
                ],
                "_preprocess_conv2d_image_shape": [
                    2280,
                    2169,
                    2404,
                    2343
                ],
                "kernel.eval": [
                    2218,
                    2172,
                    2493,
                    2447
                ],
                "_preprocess_conv2d_filter_shape": [
                    2225,
                    2354,
                    2291,
                    2173
                ],
                "_preprocess_conv2d_input": [
                    2412,
                    2227,
                    2357,
                    2294,
                    2175
                ],
                "_preprocess_conv2d_kernel": [
                    2176,
                    2297,
                    2360,
                    2228
                ],
                "_preprocess_padding": [
                    2177,
                    2505,
                    2415,
                    2452,
                    2230,
                    2361,
                    2298
                ],
                "T.nnet.conv2d": [
                    2370,
                    2307,
                    2179,
                    2417,
                    2363,
                    2300
                ],
                "_postprocess_conv2d_output": [
                    2376,
                    2185,
                    2313,
                    2424,
                    2239
                ],
                "flip_filters": [
                    2481,
                    2236,
                    2510,
                    2207
                ],
                "op": [
                    2506,
                    2511,
                    2238,
                    2231
                ],
                "T.nnet.abstract_conv.AbstractConv2d_gradInputs": [
                    2231
                ],
                "spatial_start_dim": [
                    2273,
                    2274,
                    2316,
                    2271
                ],
                "depthwise_kernel": [
                    2275,
                    2405,
                    2344,
                    2281,
                    2408,
                    2347,
                    2284,
                    2413,
                    2414,
                    2417,
                    2358,
                    2295,
                    2296,
                    2363,
                    2300,
                    2359
                ],
                "pointwise_kernel": [
                    2370,
                    2307,
                    2276,
                    2350,
                    2287,
                    2353,
                    2290,
                    2360,
                    2297
                ],
                "depthwise_kernel_shape": [
                    2304,
                    2344,
                    2345,
                    2347,
                    2348,
                    2349,
                    2359,
                    2367,
                    2405,
                    2406,
                    2408,
                    2281,
                    2282,
                    2409,
                    2284,
                    2285,
                    2286,
                    2410,
                    2414,
                    2421,
                    2296,
                    2425
                ],
                "depthwise_kernel.eval": [
                    2408,
                    2347,
                    2284
                ],
                "_preprocess_conv2d_depthwise_filter_shape": [
                    2409,
                    2348,
                    2285
                ],
                "pointwise_kernel_shape": [
                    2374,
                    2311,
                    2377,
                    2314,
                    2350,
                    2287,
                    2288,
                    2351,
                    2290,
                    2291,
                    2292,
                    2353,
                    2354,
                    2355
                ],
                "pointwise_kernel.eval": [
                    2353,
                    2290
                ],
                "_preprocess_conv2d_depthwise_kernel": [
                    2413,
                    2358,
                    2295
                ],
                "_preprocess_conv3d_volume_shape": [
                    2444
                ],
                "_preprocess_conv3d_filter_shape": [
                    2448,
                    2500
                ],
                "_preprocess_conv3d_input": [
                    2450,
                    2502
                ],
                "_preprocess_conv3d_kernel": [
                    2451,
                    2503
                ],
                "T.nnet.conv3d": [
                    2454
                ],
                "_postprocess_conv3d_output": [
                    2512,
                    2460
                ],
                "T.nnet.abstract_conv.AbstractConv3d_gradInputs": [
                    2506
                ],
                "pool_size": [
                    2566,
                    2567,
                    2568,
                    2538,
                    2543,
                    2579,
                    2584,
                    2521,
                    2524,
                    2525,
                    2526,
                    2527
                ],
                "odd_pad_w": [
                    2524,
                    2525
                ],
                "w_pad": [
                    2528,
                    2569,
                    2525,
                    2566
                ],
                "odd_pad_h": [
                    2526,
                    2527
                ],
                "h_pad": [
                    2528,
                    2569,
                    2567,
                    2527
                ],
                "pad": [
                    2528,
                    2530,
                    2569,
                    2571,
                    2540,
                    2545,
                    2581,
                    2586
                ],
                "pool_mode": [
                    2537,
                    2542,
                    2578,
                    2548,
                    2583,
                    2589
                ],
                "pool_out": [
                    2596,
                    2584,
                    2538,
                    2602,
                    2603,
                    2543,
                    2579,
                    2552,
                    2557,
                    2558
                ],
                "pool.pool_2d": [
                    2538,
                    2543
                ],
                "pool": [
                    2584,
                    2538,
                    2579,
                    2543
                ],
                "expected_width": [
                    2592,
                    2553,
                    2597,
                    2550
                ],
                "expected_height": [
                    2593,
                    2554,
                    2598,
                    2551
                ],
                "pool_out.dimshuffle": [
                    2602,
                    2557
                ],
                "d_pad": [
                    2568,
                    2569
                ],
                "pool.pool_3d": [
                    2584,
                    2579
                ],
                "expected_depth": [
                    2594,
                    2599
                ],
                "bias": [
                    2608,
                    2611,
                    2612,
                    2615,
                    2616,
                    2618,
                    2620,
                    2621,
                    2623,
                    2626,
                    2627,
                    2629,
                    2631,
                    2632,
                    2634,
                    2637,
                    2638,
                    2640,
                    2642,
                    2643,
                    2645,
                    2647
                ],
                "bias_shape": [
                    2627,
                    2629,
                    2632,
                    2634,
                    2638,
                    2640,
                    2643,
                    2612,
                    2645,
                    2616,
                    2618,
                    2621,
                    2623
                ],
                "bias.shape": [
                    2612
                ],
                "rng.normal": [
                    2692,
                    2689,
                    2660
                ],
                "stddev": [
                    2692,
                    2689,
                    2660,
                    2694
                ],
                "rng.uniform": [
                    2669
                ],
                "minval": [
                    2669
                ],
                "maxval": [
                    2669
                ],
                "p": [
                    2678
                ],
                "normal_t": [
                    2692,
                    2694
                ],
                "Y_": [
                    2705,
                    2706,
                    2707
                ],
                "T.alloc": [
                    2705
                ],
                "Y.shape": [
                    2744,
                    2705,
                    2706,
                    2711
                ],
                "Y": [
                    2744,
                    2705,
                    2706,
                    2773,
                    2711,
                    2712,
                    2749,
                    2751
                ],
                "skip_idxs": [
                    2712,
                    2713,
                    2717,
                    2711
                ],
                "non_repeats": [
                    2712,
                    2713
                ],
                "non_repeats.nonzero": [
                    2713
                ],
                "active_skip_idxs": [
                    2721,
                    2717,
                    2733
                ],
                "nonzero": [
                    2717
                ],
                "active": [
                    2720,
                    2724,
                    2725,
                    2728,
                    2717
                ],
                "active_next": [
                    2726,
                    2737,
                    2738,
                    2740,
                    2718
                ],
                "log_p_curr.shape": [
                    2722
                ],
                "log_p_curr": [
                    2738,
                    2722
                ],
                "common_factor": [
                    2724,
                    2725,
                    2734,
                    2774,
                    2775,
                    2776
                ],
                "log_p_prev": [
                    2724,
                    2725
                ],
                "p_prev": [
                    2728,
                    2725,
                    2733
                ],
                "_p_prev": [
                    2726,
                    2728,
                    2730,
                    2732,
                    2733,
                    2734
                ],
                "zeros": [
                    2755,
                    2757,
                    2726,
                    2737,
                    2746,
                    2747
                ],
                "T.inc_subtensor": [
                    2730,
                    2732
                ],
                "updated_log_p_prev": [
                    2738,
                    2734
                ],
                "log_p_next": [
                    2736,
                    2740
                ],
                "smoothed": [
                    2744,
                    2745
                ],
                "predict": [
                    2744,
                    2773
                ],
                "np.float32": [
                    2744
                ],
                "L": [
                    2762,
                    2765,
                    2768,
                    2745,
                    2746
                ],
                "log_first": [
                    2747,
                    2763
                ],
                "f_skip_idxs": [
                    2755,
                    2749
                ],
                "ctc_create_skip_idxs": [
                    2749,
                    2751
                ],
                "b_skip_idxs": [
                    2757,
                    2751
                ],
                "f_active_next": [
                    2754,
                    2758
                ],
                "log_f_next": [
                    2754,
                    2758
                ],
                "ctc_update_log_p": [
                    2754,
                    2756
                ],
                "f_active": [
                    2760,
                    2755,
                    2766
                ],
                "log_f_curr": [
                    2755
                ],
                "log_f_prev": [
                    2755
                ],
                "b_active_next": [
                    2756,
                    2758
                ],
                "log_b_next": [
                    2756,
                    2758
                ],
                "b_active": [
                    2760,
                    2757,
                    2767
                ],
                "log_b_curr": [
                    2757
                ],
                "log_b_prev": [
                    2757
                ],
                "log_f_probs": [
                    2760,
                    2768
                ],
                "log_b_probs": [
                    2760,
                    2768
                ],
                "np.int32": [
                    2763
                ],
                "idxs": [
                    2765,
                    2766,
                    2767
                ],
                "L.shape": [
                    2765
                ],
                "f_active.dimshuffle": [
                    2766
                ],
                "b_active.dimshuffle": [
                    2767
                ],
                "log_probs": [
                    2768,
                    2769,
                    2773,
                    2774,
                    2775
                ],
                "ctc_path_probs": [
                    2773
                ],
                "ctc_interleave_blanks": [
                    2773
                ],
                "total_log_prob": [
                    2776,
                    2777,
                    2775
                ],
                "mask.nonzero": [
                    2775
                ],
                "y_pred_step": [
                    2801,
                    2799
                ],
                "input_length_step": [
                    2799
                ],
                "y_true_step": [
                    2800,
                    2801
                ],
                "label_length_step": [
                    2800
                ],
                "ctc_cost": [
                    2801
                ],
                "ret": [
                    2809,
                    2810,
                    2803
                ],
                "ctc_step": [
                    2804
                ],
                "y_true": [
                    2806
                ],
                "y_pred": [
                    2806
                ],
                "label_length": [
                    2806
                ],
                "ret.dimshuffle": [
                    2809
                ],
                "theano.map": [
                    2827
                ],
                "fn": [
                    2872,
                    2849,
                    2827
                ],
                "elems": [
                    2850,
                    2827,
                    2867,
                    2868,
                    2873,
                    2844,
                    2845
                ],
                "initializer": [
                    2850,
                    2866,
                    2867,
                    2873,
                    2843,
                    2844
                ],
                "theano.foldl": [
                    2849
                ],
                "acc": [
                    2872,
                    2849
                ],
                "theano.foldr": [
                    2872
                ],
                "stride": [
                    2885,
                    2886,
                    2879
                ],
                "output_length": [
                    2881,
                    2884
                ],
                "feature_dim": [
                    2881,
                    2917,
                    2888,
                    2933,
                    2906
                ],
                "filters": [
                    2881,
                    2906,
                    2922,
                    2938
                ],
                "slice_length": [
                    2885,
                    2887
                ],
                "kernel_size": [
                    2913,
                    2915,
                    2886,
                    2929,
                    2931
                ],
                "xs.append": [
                    2932,
                    2887
                ],
                "x_aggregate": [
                    2936,
                    2889,
                    2891,
                    2935
                ],
                "concatenate": [
                    2920,
                    2889,
                    2935
                ],
                "batch_dot": [
                    2936,
                    2891
                ],
                "stride_row": [
                    2912,
                    2913,
                    2928,
                    2929,
                    2903
                ],
                "stride_col": [
                    2914,
                    2915,
                    2930,
                    2931,
                    2903
                ],
                "output_row": [
                    2922,
                    2926,
                    2904,
                    2938,
                    2910
                ],
                "output_col": [
                    2919,
                    2922,
                    2927,
                    2904,
                    2938,
                    2911
                ],
                "slice_row": [
                    2912,
                    2932,
                    2916,
                    2928
                ],
                "slice_col": [
                    2932,
                    2914,
                    2916,
                    2930
                ],
                "x_flatten": [
                    2916,
                    2918
                ],
                "output.append": [
                    2918
                ],
                "dot": [
                    2918
                ]
            },
            "filtered_variables_in_file": {
                "th_sparse_module": [
                    99,
                    909,
                    911,
                    16,
                    149,
                    88,
                    410,
                    252,
                    94
                ],
                "py_all": [
                    906,
                    917,
                    31
                ],
                "py_any": [
                    32,
                    472
                ],
                "py_sum": [
                    33,
                    1169
                ],
                "py_slice": [
                    34,
                    1192,
                    1193,
                    1194,
                    1195,
                    1203,
                    1204,
                    1205,
                    1206,
                    1337,
                    1339,
                    2885,
                    2912,
                    2914,
                    1253,
                    1254,
                    1255,
                    1256,
                    1257,
                    2928,
                    1266,
                    1267,
                    1268,
                    1269,
                    1270,
                    2930
                ],
                "theano.config.floatX": [
                    38
                ],
                "theano.config": [
                    875,
                    813,
                    38
                ],
                "theano": [
                    1418,
                    2827,
                    152,
                    153,
                    154,
                    158,
                    1439,
                    1441,
                    2849,
                    38,
                    813,
                    1582,
                    821,
                    824,
                    825,
                    826,
                    2872,
                    2760,
                    1630,
                    875,
                    2803,
                    884,
                    373,
                    1398,
                    892,
                    894
                ],
                "floatx": [
                    2656,
                    324,
                    581,
                    38,
                    1764,
                    169,
                    2665,
                    332,
                    240,
                    146,
                    2674,
                    2683,
                    316
                ],
                "_LEARNING_PHASE": [
                    40,
                    54,
                    46
                ],
                "T.scalar": [
                    40
                ],
                "T": [
                    1542,
                    1546,
                    1552,
                    530,
                    534,
                    1046,
                    1558,
                    540,
                    1564,
                    1566,
                    546,
                    1575,
                    40,
                    1065,
                    1579,
                    1069,
                    559,
                    572,
                    582,
                    586,
                    1611,
                    1100,
                    590,
                    1614,
                    596,
                    1113,
                    604,
                    1628,
                    1645,
                    1650,
                    632,
                    1145,
                    636,
                    640,
                    2179,
                    644,
                    2694,
                    648,
                    649,
                    653,
                    1165,
                    1166,
                    1679,
                    657,
                    2705,
                    2706,
                    2711,
                    2712,
                    2718,
                    2719,
                    2721,
                    2724,
                    2725,
                    1191,
                    2728,
                    681,
                    2730,
                    2732,
                    173,
                    685,
                    2734,
                    2736,
                    689,
                    1202,
                    693,
                    1207,
                    2231,
                    2745,
                    2746,
                    705,
                    709,
                    713,
                    2765,
                    722,
                    726,
                    1750,
                    1751,
                    2774,
                    730,
                    1755,
                    2775,
                    734,
                    1759,
                    1761,
                    738,
                    1252,
                    1764,
                    742,
                    1766,
                    232,
                    233,
                    746,
                    1769,
                    750,
                    1265,
                    1779,
                    1781,
                    759,
                    1271,
                    1786,
                    2300,
                    254,
                    2307,
                    773,
                    776,
                    784,
                    1808,
                    1813,
                    1814,
                    798,
                    1314,
                    1831,
                    1832,
                    1323,
                    1324,
                    1325,
                    1839,
                    1841,
                    1842,
                    1846,
                    1850,
                    827,
                    828,
                    2363,
                    1854,
                    2370,
                    1348,
                    841,
                    843,
                    844,
                    845,
                    846,
                    337,
                    341,
                    1883,
                    1891,
                    1892,
                    2417,
                    1913,
                    378,
                    1915,
                    1919,
                    1921,
                    899,
                    1923,
                    1924,
                    1925,
                    1424,
                    915,
                    2454,
                    412,
                    931,
                    960,
                    2506,
                    485,
                    1002,
                    1520,
                    1521,
                    504
                ],
                "_UID_PREFIXES": [
                    81,
                    41,
                    75,
                    76
                ],
                "defaultdict": [
                    81,
                    41
                ],
                "value": [
                    2058,
                    397,
                    147,
                    150,
                    152,
                    156,
                    157,
                    158,
                    161,
                    172,
                    51,
                    54,
                    1996,
                    1368,
                    2012,
                    1373,
                    1372,
                    2028,
                    2043
                ],
                "prefix": [
                    128,
                    129,
                    75,
                    76,
                    126
                ],
                "tensor.type": [
                    94
                ],
                "tensor": [
                    98,
                    99,
                    101,
                    917,
                    918,
                    94
                ],
                "th_sparse_module.SparseType": [
                    94
                ],
                "is_sparse": [
                    409,
                    98,
                    906
                ],
                "th_sparse_module.dense_from_sparse": [
                    99
                ],
                "shape": [
                    2689,
                    2692,
                    2447,
                    920,
                    921,
                    1048,
                    1049,
                    924,
                    1050,
                    931,
                    932,
                    933,
                    1833,
                    170,
                    171,
                    172,
                    2218,
                    2347,
                    176,
                    2172,
                    2353,
                    317,
                    2493,
                    325,
                    2123,
                    2129,
                    2130,
                    1143,
                    2136,
                    2137,
                    1377,
                    2284,
                    2660,
                    358,
                    2408,
                    105,
                    106,
                    363,
                    490,
                    493,
                    1134,
                    1135,
                    496,
                    241,
                    497,
                    243,
                    244,
                    498,
                    246,
                    499,
                    1136,
                    1145,
                    2290,
                    2669,
                    1144,
                    2678,
                    255
                ],
                "x": [
                    2074,
                    2077,
                    2089,
                    2092,
                    2095,
                    2121,
                    2123,
                    2127,
                    2130,
                    2134,
                    2137,
                    2144,
                    106,
                    107,
                    108,
                    2169,
                    2175,
                    2179,
                    2185,
                    2227,
                    2238,
                    2239,
                    224,
                    226,
                    2274,
                    228,
                    232,
                    2280,
                    2294,
                    252,
                    2300,
                    254,
                    255,
                    256,
                    257,
                    258,
                    2313,
                    270,
                    279,
                    292,
                    293,
                    2343,
                    299,
                    303,
                    309,
                    2357,
                    2363,
                    2376,
                    337,
                    341,
                    354,
                    2404,
                    2412,
                    2417,
                    373,
                    2425,
                    378,
                    385,
                    389,
                    393,
                    2444,
                    2450,
                    2454,
                    409,
                    410,
                    412,
                    413,
                    414,
                    2460,
                    2502,
                    2511,
                    2512,
                    469,
                    471,
                    485,
                    2535,
                    489,
                    2538,
                    491,
                    493,
                    2543,
                    2550,
                    2551,
                    504,
                    505,
                    506,
                    2576,
                    530,
                    2579,
                    534,
                    2584,
                    540,
                    2592,
                    2593,
                    546,
                    2594,
                    559,
                    2608,
                    2611,
                    2613,
                    2616,
                    2618,
                    572,
                    2621,
                    2623,
                    2624,
                    2627,
                    580,
                    2629,
                    582,
                    2632,
                    586,
                    2634,
                    2635,
                    590,
                    2638,
                    2640,
                    2643,
                    596,
                    597,
                    2645,
                    2647,
                    2648,
                    604,
                    605,
                    610,
                    612,
                    618,
                    632,
                    636,
                    640,
                    644,
                    648,
                    649,
                    653,
                    657,
                    681,
                    685,
                    689,
                    693,
                    705,
                    709,
                    713,
                    714,
                    715,
                    722,
                    726,
                    730,
                    734,
                    738,
                    742,
                    746,
                    750,
                    761,
                    765,
                    770,
                    774,
                    785,
                    794,
                    796,
                    799,
                    2849,
                    809,
                    811,
                    814,
                    822,
                    2872,
                    832,
                    833,
                    836,
                    840,
                    847,
                    864,
                    868,
                    874,
                    885,
                    893,
                    899,
                    906,
                    915,
                    931,
                    932,
                    934,
                    935,
                    948,
                    949,
                    950,
                    960,
                    961,
                    962,
                    963,
                    991,
                    999,
                    1001,
                    1006,
                    1007,
                    1025,
                    1030,
                    1044,
                    1045,
                    1047,
                    1048,
                    1069,
                    1070,
                    1072,
                    1073,
                    1079,
                    1080,
                    1083,
                    1088,
                    1092,
                    1094,
                    1100,
                    1101,
                    1102,
                    1105,
                    1113,
                    1114,
                    1115,
                    1116,
                    1118,
                    1125,
                    1127,
                    1130,
                    1132,
                    1133,
                    1134,
                    1143,
                    1145,
                    1146,
                    1147,
                    1161,
                    1166,
                    1167,
                    1168,
                    1169,
                    1170,
                    1185,
                    1207,
                    1208,
                    1210,
                    1211,
                    1214,
                    1215,
                    1218,
                    1219,
                    1223,
                    1224,
                    1227,
                    1228,
                    1231,
                    1234,
                    1245,
                    1271,
                    1272,
                    1274,
                    1275,
                    1278,
                    1279,
                    1282,
                    1283,
                    1286,
                    1287,
                    1292,
                    1293,
                    1296,
                    1297,
                    1300,
                    1301,
                    1304,
                    1308,
                    1314,
                    1335,
                    1340,
                    1348,
                    1354,
                    1357,
                    1364,
                    1368,
                    1372,
                    1373,
                    1377,
                    1385,
                    1698,
                    1699,
                    1701,
                    1709,
                    1710,
                    1715,
                    1717,
                    1718,
                    1728,
                    1751,
                    1759,
                    1761,
                    1764,
                    1766,
                    1769,
                    1772,
                    1774,
                    1778,
                    1779,
                    1780,
                    1781,
                    1782,
                    1786,
                    1790,
                    1846,
                    1850,
                    1854,
                    1880,
                    1882,
                    1885,
                    1886,
                    1887,
                    1891,
                    1893,
                    1936,
                    1937,
                    1946,
                    1947
                ],
                "NAME_SCOPE_STACK": [
                    120,
                    114,
                    126,
                    122
                ],
                "NAME_SCOPE_STACK.append": [
                    120
                ],
                "name": [
                    129,
                    2827,
                    150,
                    159,
                    2850,
                    175,
                    2873,
                    317,
                    325,
                    333,
                    120,
                    354,
                    359,
                    364,
                    248,
                    1401,
                    1403,
                    252,
                    254,
                    127
                ],
                "NAME_SCOPE_STACK.pop": [
                    122
                ],
                "contextmanager": [
                    117
                ],
                "join": [
                    126
                ],
                "default": [
                    128
                ],
                "dtype": [
                    2689,
                    2692,
                    145,
                    146,
                    157,
                    168,
                    169,
                    1065,
                    174,
                    2682,
                    315,
                    316,
                    317,
                    578,
                    323,
                    324,
                    325,
                    581,
                    582,
                    331,
                    332,
                    333,
                    337,
                    341,
                    2655,
                    2656,
                    2660,
                    359,
                    2664,
                    2665,
                    364,
                    2669,
                    239,
                    240,
                    2673,
                    2674,
                    2678,
                    378,
                    2683,
                    252,
                    254
                ],
                "_assert_sparse_module": [
                    251,
                    148
                ],
                "variable": [
                    161,
                    162,
                    163,
                    164,
                    325,
                    358,
                    363,
                    333,
                    397,
                    149,
                    317,
                    158
                ],
                "th_sparse_module.as_sparse_variable": [
                    149
                ],
                "_prepare_name": [
                    248,
                    175,
                    150,
                    159
                ],
                "theano.tensor.TensorVariable": [
                    152
                ],
                "theano.tensor": [
                    824,
                    825,
                    152,
                    153,
                    154,
                    826,
                    894
                ],
                "theano.tensor.sharedvar.TensorSharedVariable": [
                    153
                ],
                "theano.tensor.sharedvar": [
                    153
                ],
                "theano.tensor.TensorConstant": [
                    154
                ],
                "value.eval": [
                    156
                ],
                "np.asarray": [
                    1368,
                    1373,
                    157,
                    950
                ],
                "np": [
                    648,
                    157,
                    172,
                    950,
                    2744,
                    317,
                    702,
                    704,
                    325,
                    2763,
                    333,
                    1872,
                    1105,
                    1368,
                    1373,
                    1118,
                    2658,
                    358,
                    363,
                    2667,
                    2676,
                    374,
                    2685
                ],
                "theano.shared": [
                    158
                ],
                "variable._keras_shape": [
                    161
                ],
                "value.shape": [
                    161
                ],
                "variable._uses_learning_phase": [
                    162
                ],
                "variable.constraint": [
                    163
                ],
                "constraint": [
                    163
                ],
                "np_value": [
                    172,
                    173
                ],
                "np.ones": [
                    172,
                    325
                ],
                "const": [
                    176,
                    177,
                    178,
                    173
                ],
                "T.constant": [
                    173
                ],
                "const._keras_shape": [
                    176
                ],
                "const._uses_learning_phase": [
                    177
                ],
                "is_tensor": [
                    224
                ],
                "T.TensorVariable": [
                    232
                ],
                "T.sharedvar.TensorSharedVariable": [
                    233
                ],
                "T.sharedvar": [
                    233
                ],
                "ndim": [
                    1673,
                    1674,
                    907,
                    814,
                    2608,
                    2611,
                    2613,
                    2615,
                    2620,
                    2624,
                    2626,
                    836,
                    2631,
                    2635,
                    2637,
                    1487,
                    1488,
                    2642,
                    1496,
                    486,
                    874,
                    876,
                    1516,
                    1517,
                    241,
                    881,
                    1521,
                    244,
                    246,
                    249
                ],
                "_": [
                    2760,
                    1517,
                    1582,
                    1677,
                    2803,
                    246,
                    2906,
                    1630
                ],
                "broadcast": [
                    249,
                    254
                ],
                "sparse": [
                    250
                ],
                "th_sparse_module.csr_matrix": [
                    252
                ],
                "T.TensorType": [
                    254
                ],
                "x._keras_shape": [
                    1282,
                    1283,
                    1286,
                    1287,
                    1274,
                    1292,
                    1293,
                    1275,
                    1168,
                    1169,
                    1170,
                    1296,
                    1297,
                    1300,
                    1301,
                    1048,
                    1304,
                    1308,
                    414,
                    1279,
                    293,
                    1072,
                    1073,
                    950,
                    1079,
                    1080,
                    1210,
                    1083,
                    1211,
                    1214,
                    1215,
                    1088,
                    962,
                    963,
                    1092,
                    1218,
                    1219,
                    1223,
                    1224,
                    715,
                    1227,
                    1228,
                    1102,
                    1231,
                    1105,
                    1234,
                    2130,
                    2137,
                    1115,
                    1116,
                    1118,
                    612,
                    618,
                    491,
                    493,
                    1134,
                    1007,
                    506,
                    1147,
                    1278,
                    255
                ],
                "x._uses_learning_phase": [
                    256,
                    1717,
                    935
                ],
                "x._theano_placeholder": [
                    257,
                    270
                ],
                "x.shape": [
                    1161,
                    1166,
                    279,
                    2074,
                    2077,
                    2592,
                    1185,
                    2593,
                    2594,
                    2089,
                    2092,
                    2095,
                    840,
                    1143,
                    1880,
                    1113,
                    1245,
                    373,
                    2550,
                    2551
                ],
                "x.ndim": [
                    864,
                    868,
                    1094,
                    874,
                    299,
                    1778,
                    1044,
                    469,
                    471,
                    794,
                    796,
                    1335
                ],
                "x.dtype": [
                    580,
                    303,
                    1880,
                    1368,
                    1882,
                    1373
                ],
                "to_dense": [
                    915,
                    309
                ],
                "np.zeros": [
                    317
                ],
                "np.eye": [
                    333
                ],
                "size": [
                    333
                ],
                "T.ones_like": [
                    337,
                    1921,
                    1919
                ],
                "T.zeros_like": [
                    1913,
                    2746,
                    1915,
                    341
                ],
                "x.copy": [
                    354
                ],
                "np.random.uniform": [
                    358
                ],
                "np.random": [
                    2658,
                    358,
                    2667,
                    363,
                    1872,
                    2676,
                    2685
                ],
                "low": [
                    358
                ],
                "high": [
                    358
                ],
                "np.random.normal": [
                    363
                ],
                "scale": [
                    363
                ],
                "f": [
                    373,
                    374
                ],
                "theano.function": [
                    1418,
                    373,
                    1398
                ],
                "np.prod": [
                    1105,
                    374,
                    1118
                ],
                "T.cast": [
                    378,
                    1764,
                    2718,
                    1831
                ],
                "new_x": [
                    385
                ],
                "increment": [
                    389
                ],
                "decrement": [
                    393
                ],
                "momentum": [
                    397
                ],
                "out": [
                    485,
                    422,
                    423,
                    486,
                    487,
                    499,
                    500,
                    410,
                    412
                ],
                "th_sparse_module.basic.structured_dot": [
                    410
                ],
                "th_sparse_module.basic": [
                    410,
                    909,
                    911
                ],
                "y": [
                    520,
                    522,
                    523,
                    1045,
                    1046,
                    410,
                    1050,
                    412,
                    413,
                    1052,
                    415,
                    1309,
                    1310,
                    931,
                    933,
                    935,
                    937,
                    938,
                    1069,
                    948,
                    950,
                    951,
                    1207,
                    960,
                    962,
                    709,
                    965,
                    966,
                    967,
                    713,
                    1095,
                    1096,
                    716,
                    717,
                    1100,
                    1103,
                    1236,
                    1105,
                    722,
                    1106,
                    468,
                    469,
                    596,
                    471,
                    597,
                    598,
                    726,
                    730,
                    604,
                    605,
                    606,
                    734,
                    1113,
                    1116,
                    738,
                    1118,
                    612,
                    485,
                    742,
                    1119,
                    1235,
                    489,
                    1132,
                    494,
                    496,
                    1136,
                    1137,
                    627,
                    628,
                    1271,
                    504,
                    1145,
                    506,
                    507,
                    1149,
                    1150
                ],
                "T.dot": [
                    412
                ],
                "x_shape": [
                    416,
                    417,
                    422,
                    414
                ],
                "y_shape": [
                    418,
                    419,
                    420,
                    421,
                    422,
                    415
                ],
                "y._keras_shape": [
                    522,
                    1050,
                    1309,
                    415,
                    933,
                    950,
                    962,
                    965,
                    966,
                    1095,
                    717,
                    1103,
                    1105,
                    1235,
                    1116,
                    1118,
                    612,
                    494,
                    496,
                    1136,
                    627,
                    506,
                    1149
                ],
                "x_shape.pop": [
                    417
                ],
                "y_shape.pop": [
                    419,
                    421
                ],
                "out._keras_shape": [
                    499,
                    422
                ],
                "axes": [
                    1332,
                    1333,
                    1336,
                    464,
                    465,
                    466,
                    469,
                    471,
                    1496,
                    472,
                    1497,
                    475,
                    476,
                    477,
                    479,
                    485,
                    492,
                    495,
                    1648,
                    1649
                ],
                "y.ndim": [
                    468,
                    469,
                    471
                ],
                "a": [
                    617,
                    620,
                    621,
                    623,
                    624,
                    693,
                    472
                ],
                "T.batched_tensordot": [
                    485
                ],
                "expand_dims": [
                    2274,
                    2275,
                    2276,
                    487,
                    1678,
                    2127,
                    1518,
                    2134,
                    2143
                ],
                "axis": [
                    1795,
                    1798,
                    1802,
                    907,
                    908,
                    1803,
                    910,
                    1804,
                    1148,
                    913,
                    530,
                    915,
                    534,
                    921,
                    922,
                    1819,
                    540,
                    924,
                    1822,
                    546,
                    1314,
                    1826,
                    1827,
                    1828,
                    681,
                    559,
                    1130,
                    1131,
                    572,
                    960,
                    963,
                    836,
                    837,
                    582,
                    965,
                    840,
                    586,
                    590,
                    596,
                    597,
                    604,
                    605,
                    611,
                    1891,
                    614,
                    615,
                    1126,
                    617,
                    1128,
                    491,
                    492,
                    493,
                    494,
                    495,
                    496,
                    879,
                    880,
                    882,
                    883,
                    1135,
                    1144,
                    1778,
                    632,
                    1780,
                    1782,
                    636
                ],
                "shape.append": [
                    496,
                    498,
                    493
                ],
                "T.transpose": [
                    504
                ],
                "reference": [
                    520,
                    521,
                    522
                ],
                "indices": [
                    1600,
                    1253,
                    1605,
                    520,
                    521,
                    522,
                    1192,
                    1322,
                    1323,
                    1324,
                    1266,
                    1203,
                    1524,
                    1531,
                    1526,
                    1207,
                    1271,
                    1598
                ],
                "indices._keras_shape": [
                    522
                ],
                "reference._keras_shape": [
                    522
                ],
                "T.max": [
                    2721,
                    530,
                    2724,
                    2774
                ],
                "keepdims": [
                    546,
                    612,
                    582,
                    681,
                    586,
                    619,
                    590,
                    530,
                    596,
                    597,
                    534,
                    540,
                    605,
                    604
                ],
                "T.min": [
                    534
                ],
                "T.sum": [
                    681,
                    1891,
                    540,
                    2775
                ],
                "T.prod": [
                    1113,
                    546
                ],
                "T.extra_ops.cumsum": [
                    559
                ],
                "T.extra_ops": [
                    1832,
                    1324,
                    559,
                    1046,
                    572
                ],
                "T.extra_ops.cumprod": [
                    572
                ],
                "T.mean": [
                    582
                ],
                "T.std": [
                    586
                ],
                "T.var": [
                    590
                ],
                "T.any": [
                    596
                ],
                "_set_keras_shape_for_reduction": [
                    605,
                    597
                ],
                "T.all": [
                    604
                ],
                "axis_list": [
                    617,
                    620,
                    623,
                    615
                ],
                "keras_shape_list": [
                    618,
                    621,
                    624,
                    625,
                    626,
                    627
                ],
                "keras_shape_list.pop": [
                    624
                ],
                "T.argmax": [
                    632
                ],
                "T.argmin": [
                    636
                ],
                "T.sqr": [
                    640
                ],
                "T.abs_": [
                    644
                ],
                "T.clip": [
                    705,
                    2694,
                    648,
                    1769,
                    1841,
                    1813
                ],
                "np.inf": [
                    648,
                    702,
                    704
                ],
                "T.sqrt": [
                    649,
                    1892
                ],
                "T.exp": [
                    2725,
                    681,
                    653,
                    1781,
                    2775
                ],
                "T.log": [
                    681,
                    2734,
                    657,
                    2775,
                    2745
                ],
                "T.round": [
                    685
                ],
                "T.sgn": [
                    689
                ],
                "T.pow": [
                    693
                ],
                "min_value": [
                    705,
                    697,
                    699,
                    700,
                    701,
                    702
                ],
                "max_value": [
                    704,
                    705,
                    1768,
                    1769,
                    698,
                    699,
                    700,
                    703
                ],
                "T.eq": [
                    709
                ],
                "z": [
                    713,
                    715,
                    717,
                    718
                ],
                "T.neq": [
                    2712,
                    713
                ],
                "z._keras_shape": [
                    715,
                    717
                ],
                "T.gt": [
                    722,
                    1764
                ],
                "T.ge": [
                    1925,
                    726
                ],
                "T.lt": [
                    730
                ],
                "T.le": [
                    734
                ],
                "T.maximum": [
                    738,
                    1892,
                    2719
                ],
                "T.minimum": [
                    2718,
                    742
                ],
                "T.sin": [
                    746
                ],
                "T.cos": [
                    750
                ],
                "T.nnet.bn": [
                    899,
                    773,
                    784,
                    759,
                    798
                ],
                "T.nnet": [
                    899,
                    2179,
                    773,
                    2307,
                    784,
                    1808,
                    1814,
                    2454,
                    798,
                    1839,
                    1842,
                    1846,
                    2231,
                    1850,
                    2363,
                    2370,
                    2506,
                    1750,
                    1751,
                    1755,
                    1759,
                    1761,
                    1766,
                    1002,
                    2417,
                    1779,
                    759,
                    1786,
                    2300
                ],
                "_old_normalize_batch_in_training": [
                    760
                ],
                "gamma": [
                    769,
                    771,
                    899,
                    774,
                    893,
                    785,
                    787,
                    788,
                    799,
                    808,
                    809,
                    819,
                    846,
                    859,
                    860,
                    872,
                    886,
                    761,
                    763,
                    765,
                    767
                ],
                "beta": [
                    768,
                    770,
                    771,
                    899,
                    774,
                    785,
                    789,
                    790,
                    799,
                    810,
                    811,
                    818,
                    845,
                    861,
                    862,
                    871,
                    887,
                    761,
                    764,
                    893,
                    767
                ],
                "reduction_axes": [
                    832,
                    833,
                    837,
                    774,
                    815,
                    761,
                    794,
                    796,
                    799
                ],
                "epsilon": [
                    899,
                    1892,
                    774,
                    785,
                    849,
                    1841,
                    1813,
                    822,
                    761,
                    890,
                    893,
                    799
                ],
                "ones_like": [
                    809,
                    788,
                    860,
                    765,
                    767
                ],
                "zeros_like": [
                    1537,
                    770,
                    771,
                    811,
                    790,
                    862
                ],
                "normed": [
                    773,
                    776,
                    847,
                    850,
                    823,
                    824,
                    828
                ],
                "mean": [
                    2689,
                    899,
                    2692,
                    773,
                    2694,
                    776,
                    785,
                    790,
                    792,
                    796,
                    799,
                    823,
                    825,
                    828,
                    833,
                    843,
                    850,
                    862,
                    864,
                    2660,
                    869,
                    879,
                    888,
                    893
                ],
                "stdinv": [
                    773,
                    776,
                    823,
                    826,
                    827
                ],
                "T.nnet.bn.batch_normalization_train": [
                    773
                ],
                "T.inv": [
                    776,
                    827
                ],
                "_old_batch_normalization": [
                    785
                ],
                "var": [
                    832,
                    899,
                    870,
                    860,
                    844,
                    785,
                    850,
                    788,
                    889,
                    827,
                    828,
                    893,
                    799
                ],
                "mean.ndim": [
                    792,
                    864
                ],
                "i": [
                    1541,
                    1545,
                    1554,
                    1557,
                    2074,
                    2075,
                    796,
                    2077,
                    2078,
                    2089,
                    1322,
                    2090,
                    2092,
                    2093,
                    2095,
                    2096,
                    1073,
                    1074,
                    1077,
                    1335,
                    1336,
                    2884,
                    1605,
                    1606,
                    2885,
                    2886,
                    1613,
                    1615,
                    2910,
                    2912,
                    2913,
                    1125,
                    2919,
                    2926,
                    2928,
                    2929,
                    1531,
                    1532
                ],
                "mean.broadcastable": [
                    796,
                    879
                ],
                "T.nnet.bn.batch_normalization_test": [
                    798
                ],
                "dev": [
                    816,
                    875,
                    876,
                    813
                ],
                "theano.config.device": [
                    875,
                    813
                ],
                "use_cudnn": [
                    817,
                    876,
                    877,
                    814
                ],
                "dev.startswith": [
                    816,
                    876
                ],
                "broadcast_beta": [
                    848,
                    818,
                    845,
                    822
                ],
                "beta.dimshuffle": [
                    818,
                    887,
                    871
                ],
                "broadcast_gamma": [
                    848,
                    846,
                    819,
                    822
                ],
                "gamma.dimshuffle": [
                    872,
                    819,
                    886
                ],
                "trained": [
                    821,
                    823
                ],
                "theano.sandbox.cuda.dnn.dnn_batch_normalization_train": [
                    821
                ],
                "theano.sandbox.cuda.dnn": [
                    892,
                    884,
                    821
                ],
                "theano.sandbox.cuda": [
                    892,
                    884,
                    821
                ],
                "theano.sandbox": [
                    892,
                    884,
                    821
                ],
                "theano.tensor.as_tensor_variable": [
                    824,
                    825,
                    826,
                    894
                ],
                "T.flatten": [
                    1323,
                    828,
                    1831,
                    1100
                ],
                "x.var": [
                    832
                ],
                "x.mean": [
                    833
                ],
                "target_shape": [
                    835,
                    838,
                    840,
                    841,
                    843,
                    844,
                    845,
                    846
                ],
                "target_shape.append": [
                    840,
                    838
                ],
                "T.stack": [
                    1314,
                    841,
                    1611,
                    1614,
                    1552,
                    1558
                ],
                "broadcast_mean": [
                    843,
                    847
                ],
                "T.reshape": [
                    1145,
                    931,
                    843,
                    844,
                    845,
                    846,
                    1325,
                    1113
                ],
                "broadcast_var": [
                    844,
                    847
                ],
                "batch_normalization": [
                    847
                ],
                "shuffle_pattern": [
                    868,
                    869,
                    870,
                    871,
                    872,
                    881,
                    882,
                    883,
                    885,
                    886,
                    887,
                    888,
                    889,
                    890
                ],
                "mean.dimshuffle": [
                    888,
                    869
                ],
                "var.dimshuffle": [
                    889,
                    870
                ],
                "mean.broadcastable.index": [
                    879
                ],
                "result": [
                    1166,
                    1168,
                    1171,
                    884,
                    892,
                    894
                ],
                "dimshuffle": [
                    884,
                    2765
                ],
                "theano.sandbox.cuda.dnn.dnn_batch_normalization_test": [
                    892,
                    884
                ],
                "x.dimshuffle": [
                    2535,
                    1132,
                    1936,
                    2576,
                    948,
                    1045,
                    885,
                    1946
                ],
                "T.nnet.bn.batch_normalization": [
                    899
                ],
                "sqrt": [
                    899
                ],
                "tensors": [
                    906,
                    907,
                    909,
                    911,
                    915,
                    917,
                    918
                ],
                "output": [
                    1025,
                    1026,
                    1027,
                    1028,
                    1537,
                    1030,
                    1031,
                    1032,
                    1033,
                    1541,
                    1542,
                    1800,
                    909,
                    1165,
                    911,
                    1166,
                    1549,
                    1805,
                    915,
                    1808,
                    1811,
                    1813,
                    1814,
                    1818,
                    925,
                    927,
                    1824,
                    1829,
                    1191,
                    1832,
                    1833,
                    1834,
                    1839,
                    1841,
                    1202,
                    1842,
                    1207,
                    1794,
                    2940,
                    2891,
                    2892,
                    2909,
                    991,
                    992,
                    2144,
                    1252,
                    2149,
                    2918,
                    999,
                    2151,
                    1001,
                    1002,
                    2152,
                    2920,
                    1005,
                    2921,
                    1007,
                    1008,
                    1009,
                    1010,
                    1265,
                    2923,
                    1014,
                    1271,
                    2936,
                    2937,
                    2939,
                    1532,
                    1533
                ],
                "th_sparse_module.basic.vstack": [
                    909
                ],
                "th_sparse_module.basic.hstack": [
                    911
                ],
                "T.concatenate": [
                    1520,
                    2721,
                    915
                ],
                "input_shapes": [
                    920,
                    918,
                    919
                ],
                "tensor._keras_shape": [
                    918
                ],
                "output_shape": [
                    1162,
                    1165,
                    919,
                    921,
                    922,
                    924,
                    925,
                    1187,
                    2211,
                    2212,
                    2213,
                    1191,
                    2214,
                    1198,
                    1072,
                    1202,
                    1075,
                    1077,
                    2485,
                    1079,
                    2486,
                    1081,
                    2487,
                    1083,
                    2488,
                    2489,
                    2238,
                    1088,
                    1092,
                    1094,
                    1095,
                    2511,
                    2904,
                    1247,
                    1252,
                    1260,
                    1265
                ],
                "output._keras_shape": [
                    1007,
                    1008,
                    1009,
                    1010,
                    925
                ],
                "y._uses_learning_phase": [
                    937,
                    935
                ],
                "pattern": [
                    1125,
                    1131,
                    1132,
                    947,
                    948,
                    950
                ],
                "T.repeat": [
                    960
                ],
                "rep": [
                    960,
                    965
                ],
                "repeat_dim": [
                    963,
                    964,
                    965
                ],
                "data_format": [
                    1024,
                    2563,
                    1029,
                    1035,
                    2575,
                    2079,
                    2601,
                    2607,
                    2097,
                    2614,
                    2619,
                    2113,
                    2625,
                    2630,
                    2124,
                    2636,
                    2641,
                    2146,
                    2148,
                    2167,
                    2169,
                    2173,
                    2175,
                    2176,
                    2186,
                    1183,
                    2208,
                    1186,
                    2210,
                    2225,
                    2227,
                    2228,
                    1209,
                    2240,
                    2264,
                    1243,
                    1246,
                    2270,
                    2280,
                    2286,
                    2292,
                    2294,
                    2296,
                    2297,
                    1273,
                    2315,
                    2341,
                    2343,
                    2349,
                    2355,
                    2357,
                    2359,
                    2360,
                    2877,
                    2378,
                    2901,
                    2908,
                    2402,
                    2404,
                    2410,
                    2412,
                    2414,
                    2425,
                    2442,
                    1931,
                    2444,
                    2448,
                    2450,
                    2451,
                    1941,
                    2461,
                    2482,
                    2484,
                    2500,
                    2502,
                    2503,
                    1999,
                    2513,
                    981,
                    2519,
                    984,
                    988,
                    2015,
                    998,
                    2534,
                    1004,
                    2556
                ],
                "axis_1": [
                    1008,
                    985,
                    982,
                    991
                ],
                "axis_2": [
                    992,
                    1009,
                    986,
                    983
                ],
                "interpolation": [
                    993,
                    990
                ],
                "repeat_elements": [
                    992,
                    1025,
                    1026,
                    1027,
                    1030,
                    1031,
                    1032,
                    991
                ],
                "height_factor": [
                    1026,
                    994,
                    1031,
                    1003,
                    1008,
                    991
                ],
                "width_factor": [
                    992,
                    994,
                    1027,
                    1032,
                    1009
                ],
                "permute_dimensions": [
                    1829,
                    1830,
                    999,
                    2923,
                    2892,
                    1805,
                    1005,
                    1806,
                    2939
                ],
                "T.nnet.abstract_conv.bilinear_upsampling": [
                    1002
                ],
                "T.nnet.abstract_conv": [
                    2506,
                    1002,
                    2231
                ],
                "depth_factor": [
                    1025,
                    1030
                ],
                "T.extra_ops.repeat": [
                    1046
                ],
                "n": [
                    1089,
                    1091,
                    1069,
                    1071,
                    1072,
                    1073,
                    1046,
                    1078,
                    1049,
                    1083,
                    1086
                ],
                "shape.insert": [
                    1049,
                    1135
                ],
                "T.arange": [
                    1924,
                    1065,
                    2765,
                    2706,
                    2711
                ],
                "start": [
                    1065
                ],
                "stop": [
                    1065
                ],
                "step": [
                    1065,
                    2761
                ],
                "T.tile": [
                    1521,
                    1069
                ],
                "_is_explicit_shape": [
                    1071
                ],
                "j": [
                    2914,
                    2915,
                    2919,
                    2927,
                    1073,
                    2930,
                    2931,
                    1077,
                    2911
                ],
                "n.ndim": [
                    1086
                ],
                "n_size": [
                    1091,
                    1092
                ],
                "n._keras_shape": [
                    1091
                ],
                "x.type.ndim": [
                    1130,
                    1125,
                    1127
                ],
                "x.type": [
                    1130,
                    1125,
                    1127
                ],
                "pattern.insert": [
                    1131
                ],
                "shape.pop": [
                    1144
                ],
                "kshape": [
                    1147,
                    1148,
                    1149
                ],
                "kshape.pop": [
                    1148
                ],
                "padding": [
                    2177,
                    1283,
                    2565,
                    1160,
                    2185,
                    2313,
                    1163,
                    2570,
                    1293,
                    1166,
                    2573,
                    1169,
                    1297,
                    2452,
                    1301,
                    2072,
                    1178,
                    1179,
                    1180,
                    1181,
                    1182,
                    2460,
                    2591,
                    2087,
                    2220,
                    2230,
                    2361,
                    1981,
                    1983,
                    2239,
                    1985,
                    2495,
                    1988,
                    2116,
                    2376,
                    2505,
                    2122,
                    2512,
                    2523,
                    1249,
                    1250,
                    1251,
                    2145,
                    2529,
                    2532,
                    1255,
                    1256,
                    1257,
                    1261,
                    1262,
                    1263,
                    2415,
                    1267,
                    1268,
                    1269,
                    2549,
                    2425,
                    2298,
                    1275,
                    1279
                ],
                "input_shape": [
                    1161,
                    1162,
                    1163,
                    1164,
                    1185,
                    1187,
                    1188,
                    1189,
                    1190,
                    1194,
                    1195,
                    1322,
                    1325,
                    1198,
                    1199,
                    1200,
                    1201,
                    1204,
                    1205,
                    1245,
                    1247,
                    1248,
                    1249,
                    1250,
                    1251,
                    1255,
                    1256,
                    1257,
                    1260,
                    1261,
                    1262,
                    1263,
                    1264,
                    1267,
                    1268,
                    1269
                ],
                "T.zeros": [
                    1252,
                    1191,
                    1165,
                    1265,
                    1202
                ],
                "T.set_subtensor": [
                    2728,
                    1166,
                    2736,
                    2706,
                    1271,
                    1207
                ],
                "result._keras_shape": [
                    1168
                ],
                "top_pad": [
                    1189,
                    1224,
                    1194,
                    1199,
                    1204,
                    1211,
                    1181
                ],
                "bottom_pad": [
                    1189,
                    1224,
                    1199,
                    1211,
                    1181
                ],
                "left_pad": [
                    1190,
                    2120,
                    2121,
                    1195,
                    1228,
                    1200,
                    1205,
                    1182,
                    1215
                ],
                "right_pad": [
                    1190,
                    1228,
                    1200,
                    1182,
                    1215
                ],
                "normalize_data_format": [
                    2208,
                    2113,
                    2402,
                    2519,
                    2563,
                    2341,
                    2442,
                    2607,
                    2482,
                    2901,
                    2167,
                    2264,
                    1243,
                    2877,
                    1183
                ],
                "h": [
                    1220,
                    1305,
                    1224,
                    1288,
                    1226,
                    1293,
                    1275,
                    1232,
                    1295,
                    1277,
                    1211,
                    1213
                ],
                "w": [
                    1217,
                    1281,
                    1279,
                    1221,
                    1289,
                    1228,
                    1230,
                    1233,
                    1297,
                    1299,
                    1306,
                    1215
                ],
                "output_keras_shape": [
                    1218,
                    1286,
                    1231,
                    1235,
                    1304,
                    1309
                ],
                "d": [
                    1283,
                    1285,
                    1290,
                    1301,
                    1303,
                    1307
                ],
                "indices.shape": [
                    1322
                ],
                "indices.ndim": [
                    1322
                ],
                "oh": [
                    1324,
                    1325,
                    1326
                ],
                "T.extra_ops.to_one_hot": [
                    1832,
                    1324
                ],
                "num_classes": [
                    1324,
                    1325
                ],
                "slices": [
                    1337,
                    1339,
                    1340,
                    1334
                ],
                "slices.append": [
                    1337,
                    1339
                ],
                "T.patternbroadcast": [
                    1883,
                    1348
                ],
                "broadcastable": [
                    1348
                ],
                "x.get_value": [
                    1377,
                    1357
                ],
                "get_value": [
                    1364
                ],
                "xs": [
                    2883,
                    2887,
                    2889,
                    2925,
                    1364,
                    2932,
                    2935
                ],
                "x.set_value": [
                    1368,
                    1373
                ],
                "tuples": [
                    1372
                ],
                "p_op": [
                    1384,
                    1385
                ],
                "Print": [
                    1384
                ],
                "message": [
                    1384
                ],
                "unique_variables_to_update": [
                    1393,
                    1395,
                    1396,
                    1397
                ],
                "v": [
                    2050,
                    2020,
                    2065,
                    1394,
                    1395,
                    2004,
                    1396,
                    2035
                ],
                "nv": [
                    1394,
                    1396
                ],
                "updates": [
                    1394,
                    1420,
                    1397,
                    1398
                ],
                "unique_variables_to_update.items": [
                    1397
                ],
                "self.function": [
                    1398,
                    1407
                ],
                "self": [
                    1403,
                    1398,
                    1407
                ],
                "inputs": [
                    1632,
                    1569,
                    2916,
                    1606,
                    2887,
                    1420,
                    1561,
                    1487,
                    1584,
                    1619,
                    2932,
                    1398,
                    1497,
                    1532,
                    1406,
                    1407
                ],
                "outputs": [
                    1420,
                    1552,
                    1569,
                    1570,
                    1574,
                    1575,
                    1580,
                    1591,
                    1594,
                    1606,
                    1607,
                    1609,
                    1611,
                    1619,
                    1620,
                    1623,
                    1639,
                    1642,
                    1645,
                    1646,
                    1648,
                    1649,
                    1652,
                    1398
                ],
                "kwargs": [
                    1416,
                    1417,
                    1402,
                    1420
                ],
                "self.name": [
                    1403
                ],
                "msg": [
                    1411,
                    1412
                ],
                "key": [
                    1417,
                    1418,
                    1411,
                    1419
                ],
                "kwargs.keys": [
                    1417
                ],
                "has_arg": [
                    1418
                ],
                "_raise_invalid_arg": [
                    1419
                ],
                "Function": [
                    1420
                ],
                "T.grad": [
                    1424
                ],
                "loss": [
                    1424
                ],
                "variables": [
                    1424,
                    1441,
                    1438,
                    1439
                ],
                "theano.gradient.disconnected_grad": [
                    1441,
                    1439
                ],
                "theano.gradient": [
                    1441,
                    1439
                ],
                "inputs.ndim": [
                    1487
                ],
                "unroll": [
                    1490,
                    1523,
                    1597
                ],
                "input_length": [
                    2806,
                    1491,
                    1524,
                    1598
                ],
                "inputs.dimshuffle": [
                    1497
                ],
                "constants": [
                    1634,
                    1606,
                    1532,
                    1586,
                    1561,
                    1499,
                    1500
                ],
                "uses_learning_phase": [
                    1572,
                    1608,
                    1695,
                    1651,
                    1716,
                    1622,
                    1693,
                    1534,
                    1503
                ],
                "mask": [
                    1505,
                    1506,
                    1541,
                    1509,
                    1510,
                    1574,
                    1545,
                    1578,
                    2766,
                    1584,
                    2769,
                    2773,
                    2775
                ],
                "mask.ndim": [
                    1506
                ],
                "mask.shape": [
                    1509
                ],
                "mask.dimshuffle": [
                    1510
                ],
                "ref_tensor_t.ndim": [
                    1516
                ],
                "ref_tensor_t": [
                    1516,
                    1519
                ],
                "mask_t": [
                    1521,
                    1518
                ],
                "add_shape": [
                    1520,
                    1519
                ],
                "ref_tensor_t.shape": [
                    1519
                ],
                "reps": [
                    1520,
                    1521
                ],
                "go_backwards": [
                    1635,
                    1587,
                    1525,
                    1599
                ],
                "successive_outputs": [
                    1536,
                    1602,
                    1539,
                    1609,
                    1611,
                    1549,
                    1552,
                    1528
                ],
                "successive_states": [
                    1603,
                    1610,
                    1613,
                    1550,
                    1615,
                    1554,
                    1556,
                    1529
                ],
                "states": [
                    1544,
                    1547,
                    1550,
                    1553,
                    1558,
                    1569,
                    1577,
                    1592,
                    1595,
                    1604,
                    1606,
                    1610,
                    1612,
                    1614,
                    1619,
                    1640,
                    1643,
                    1650,
                    1652,
                    1530,
                    1532
                ],
                "initial_states": [
                    1633,
                    1604,
                    1585,
                    1561,
                    1530,
                    1627,
                    1628,
                    1565,
                    1566
                ],
                "new_states": [
                    1569,
                    1544,
                    1577,
                    1555,
                    1619,
                    1557,
                    1558,
                    1623,
                    1532
                ],
                "step_function": [
                    1569,
                    1606,
                    1619,
                    1561,
                    1532
                ],
                "prev_output": [
                    1537,
                    1539,
                    1542
                ],
                "output_mask": [
                    1574,
                    1541,
                    1542,
                    1575
                ],
                "get_matching_mask": [
                    1545,
                    1578,
                    1541,
                    1574
                ],
                "T.switch": [
                    1542,
                    1575,
                    1546,
                    1579,
                    1679
                ],
                "kept_states": [
                    1546,
                    1547,
                    1543
                ],
                "state": [
                    1544,
                    1545,
                    1546,
                    1578,
                    1577,
                    1579,
                    1650
                ],
                "new_state": [
                    1544,
                    1577,
                    1546,
                    1579
                ],
                "state_mask": [
                    1545,
                    1546,
                    1579,
                    1578
                ],
                "kept_states.append": [
                    1546
                ],
                "successive_outputs.append": [
                    1609,
                    1549
                ],
                "successive_states.append": [
                    1610,
                    1550
                ],
                "states_at_step": [
                    1556,
                    1557,
                    1615
                ],
                "new_states.append": [
                    1557
                ],
                "states.append": [
                    1614,
                    1558
                ],
                "initial_output": [
                    1561,
                    1562,
                    1564,
                    1585
                ],
                "T.unbroadcast": [
                    1564,
                    1566,
                    1628
                ],
                "output_tm1": [
                    1575
                ],
                "return_states": [
                    1576,
                    1579,
                    1580
                ],
                "return_states.append": [
                    1579
                ],
                "results": [
                    1638,
                    1639,
                    1640,
                    1642,
                    1582,
                    1590,
                    1591,
                    1592,
                    1594,
                    1630
                ],
                "theano.scan": [
                    2760,
                    1630,
                    2803,
                    1582
                ],
                "_step": [
                    1631,
                    1583
                ],
                "T.squeeze": [
                    1650,
                    1645
                ],
                "last_output": [
                    1651,
                    1652,
                    1646
                ],
                "outputs.ndim": [
                    1648
                ],
                "outputs.dimshuffle": [
                    1649
                ],
                "last_output._uses_learning_phase": [
                    1651
                ],
                "then_expression": [
                    1674,
                    1669,
                    1670,
                    1679
                ],
                "else_expression": [
                    1672,
                    1679,
                    1671
                ],
                "cond_ndim": [
                    1673,
                    1675,
                    1676
                ],
                "condition": [
                    1673,
                    1678,
                    1679
                ],
                "expr_ndim": [
                    1674,
                    1675,
                    1676
                ],
                "ndim_diff": [
                    1676,
                    1677
                ],
                "training": [
                    1728,
                    1697,
                    1703,
                    1715,
                    1691,
                    1692
                ],
                "learning_phase": [
                    1692
                ],
                "alt": [
                    1728,
                    1704,
                    1705,
                    1707,
                    1711,
                    1712,
                    1715
                ],
                "ifelse": [
                    1715
                ],
                "in_train_phase": [
                    1728
                ],
                "module": [
                    1734
                ],
                "func": [
                    1734
                ],
                "_assert_has_capability": [
                    1755,
                    1750
                ],
                "T.nnet.elu": [
                    1751
                ],
                "alpha": [
                    1771,
                    1772,
                    1751,
                    2744,
                    1757
                ],
                "threshold": [
                    1763,
                    1764,
                    1758,
                    1759
                ],
                "negative_part": [
                    1761,
                    1772,
                    1759
                ],
                "T.nnet.relu": [
                    1761,
                    1766,
                    1759
                ],
                "T.nnet.softmax": [
                    1808,
                    1779
                ],
                "xm": [
                    1780,
                    1781,
                    1782
                ],
                "x.max": [
                    1780
                ],
                "T.nnet.softplus": [
                    1786
                ],
                "T_softsign": [
                    1790
                ],
                "output_dimensions": [
                    1794,
                    1795,
                    1826,
                    1827,
                    1828,
                    1802,
                    1803,
                    1804,
                    1818,
                    1819
                ],
                "int_shape": [
                    1794,
                    1800,
                    2444,
                    2445,
                    1818,
                    1824,
                    2343,
                    2216,
                    2344,
                    2350,
                    2491,
                    2880,
                    2115,
                    2123,
                    2905,
                    2404,
                    2405,
                    2280,
                    2281,
                    2287,
                    2169,
                    2170,
                    1917
                ],
                "permutation": [
                    1827,
                    1828,
                    1829,
                    1830,
                    1803,
                    1804,
                    1805,
                    1806
                ],
                "target": [
                    1830,
                    1831,
                    1832,
                    1833,
                    1834,
                    1806,
                    1842,
                    1814
                ],
                "from_logits": [
                    1834,
                    1838,
                    1807
                ],
                "output.sum": [
                    1811
                ],
                "T.nnet.categorical_crossentropy": [
                    1814
                ],
                "output.shape": [
                    1832
                ],
                "reshape": [
                    1833,
                    1967,
                    2616,
                    2618,
                    2621,
                    2623,
                    2627,
                    2629,
                    2887,
                    2632,
                    2634,
                    2638,
                    2640,
                    2643,
                    2645,
                    2916,
                    2921,
                    2932,
                    2937
                ],
                "categorical_crossentropy": [
                    1834
                ],
                "T.nnet.sigmoid": [
                    1846,
                    1839
                ],
                "T.nnet.binary_crossentropy": [
                    1842
                ],
                "T.nnet.hard_sigmoid": [
                    1850
                ],
                "T.tanh": [
                    1854
                ],
                "level": [
                    1877,
                    1869
                ],
                "seed": [
                    2657,
                    2658,
                    2659,
                    2666,
                    2667,
                    2668,
                    1871,
                    1872,
                    2675,
                    1876,
                    2676,
                    2677,
                    2684,
                    2685,
                    2686
                ],
                "np.random.randint": [
                    2658,
                    2667,
                    1872,
                    2676,
                    2685
                ],
                "noise_shape": [
                    1873,
                    1874,
                    1879,
                    1882,
                    1884
                ],
                "rng": [
                    2689,
                    2659,
                    2660,
                    2692,
                    2668,
                    2669,
                    1876,
                    2677,
                    2678,
                    1880,
                    1882,
                    2686
                ],
                "RandomStreams": [
                    2659,
                    2668,
                    1876,
                    2677,
                    2686
                ],
                "retain_prob": [
                    1880,
                    1882,
                    1877,
                    1886
                ],
                "random_tensor": [
                    1880,
                    1882,
                    1883,
                    1885
                ],
                "rng.binomial": [
                    1880,
                    1882,
                    2678
                ],
                "dim": [
                    1884
                ],
                "square_sum": [
                    1891,
                    1892
                ],
                "T.square": [
                    1891
                ],
                "norm": [
                    1892,
                    1893
                ],
                "k": [
                    1923,
                    1917,
                    1910
                ],
                "targets": [
                    1921,
                    1924,
                    1913,
                    1915,
                    1919
                ],
                "predictions": [
                    1923,
                    1924,
                    1917
                ],
                "predictions_k": [
                    1923,
                    1925
                ],
                "T.sort": [
                    1923
                ],
                "targets_values": [
                    1924,
                    1925
                ],
                "targets.shape": [
                    1924
                ],
                "kernel": [
                    2176,
                    2179,
                    2445,
                    2447,
                    2451,
                    2454,
                    1955,
                    1956,
                    2216,
                    2218,
                    1965,
                    1966,
                    1967,
                    1968,
                    2228,
                    1976,
                    1977,
                    2491,
                    2493,
                    2238,
                    2880,
                    2115,
                    2503,
                    2891,
                    2511,
                    2905,
                    2143,
                    2144,
                    2919,
                    2936,
                    2170,
                    2172
                ],
                "kernel.dimshuffle": [
                    1976,
                    1955,
                    1966
                ],
                "kernel_shape": [
                    2183,
                    2186,
                    2445,
                    2446,
                    2447,
                    2448,
                    2073,
                    2458,
                    2076,
                    2461,
                    2088,
                    2216,
                    2217,
                    2091,
                    2218,
                    2220,
                    2094,
                    1967,
                    2225,
                    2233,
                    2491,
                    2492,
                    2493,
                    2495,
                    2240,
                    2880,
                    2881,
                    2115,
                    2500,
                    2118,
                    2120,
                    2507,
                    2513,
                    2905,
                    2906,
                    2170,
                    2171,
                    2172,
                    2173
                ],
                "th_padding": [
                    2177,
                    2180,
                    2308,
                    2452,
                    2455,
                    2230,
                    2361,
                    2235,
                    2364,
                    1982,
                    1984,
                    1986,
                    2371,
                    1989,
                    2505,
                    2509,
                    2415,
                    2418,
                    2298,
                    2301
                ],
                "image_shape": [
                    2369,
                    2306,
                    2404,
                    2182,
                    2343,
                    2280,
                    2000,
                    2001,
                    2003,
                    2004,
                    2005,
                    2420,
                    2423,
                    2169,
                    2366,
                    2303
                ],
                "transpose_shape": [
                    2001
                ],
                "int_or_none": [
                    2050,
                    2020,
                    2065,
                    2035,
                    2004
                ],
                "volume_shape": [
                    2016,
                    2017,
                    2018,
                    2019,
                    2020,
                    2021,
                    2444,
                    2457
                ],
                "filter_shape": [
                    2048,
                    2049,
                    2050,
                    2051,
                    2061,
                    2062,
                    2031,
                    2032,
                    2033,
                    2034,
                    2035,
                    2036,
                    2063,
                    2064,
                    2065,
                    2066,
                    2046,
                    2047
                ],
                "strides": [
                    2181,
                    2186,
                    2315,
                    2579,
                    2456,
                    2584,
                    2074,
                    2077,
                    2461,
                    2592,
                    2593,
                    2594,
                    2089,
                    2092,
                    2095,
                    2234,
                    2365,
                    2879,
                    2240,
                    2378,
                    2508,
                    2513,
                    2903,
                    2265,
                    2266,
                    2140,
                    2145,
                    2277,
                    2538,
                    2543,
                    2419,
                    2550,
                    2551,
                    2425,
                    2302
                ],
                "conv_out": [
                    2179,
                    2307,
                    2185,
                    2313,
                    2187,
                    2316,
                    2317,
                    2454,
                    2075,
                    2460,
                    2078,
                    2462,
                    2080,
                    2081,
                    2090,
                    2093,
                    2096,
                    2098,
                    2099,
                    2363,
                    2238,
                    2239,
                    2241,
                    2370,
                    2376,
                    2379,
                    2511,
                    2512,
                    2514,
                    2417,
                    2424,
                    2425,
                    2426,
                    2300
                ],
                "conv_out.dimshuffle": [
                    2080,
                    2098
                ],
                "dilation_rate": [
                    2368,
                    2305,
                    2146,
                    2278,
                    2375,
                    2120,
                    2184,
                    2312,
                    2267,
                    2459,
                    2422,
                    2139,
                    2268,
                    2237
                ],
                "temporal_padding": [
                    2121
                ],
                "conv2d": [
                    2144
                ],
                "squeeze": [
                    2316,
                    2149,
                    2151
                ],
                "_preprocess_conv2d_image_shape": [
                    2280,
                    2169,
                    2404,
                    2343
                ],
                "kernel.eval": [
                    2218,
                    2172,
                    2493,
                    2447
                ],
                "_preprocess_conv2d_filter_shape": [
                    2225,
                    2354,
                    2291,
                    2173
                ],
                "_preprocess_conv2d_input": [
                    2412,
                    2227,
                    2357,
                    2294,
                    2175
                ],
                "_preprocess_conv2d_kernel": [
                    2176,
                    2297,
                    2360,
                    2228
                ],
                "_preprocess_padding": [
                    2177,
                    2505,
                    2415,
                    2452,
                    2230,
                    2361,
                    2298
                ],
                "T.nnet.conv2d": [
                    2370,
                    2307,
                    2179,
                    2417,
                    2363,
                    2300
                ],
                "_postprocess_conv2d_output": [
                    2376,
                    2185,
                    2313,
                    2424,
                    2239
                ],
                "flip_filters": [
                    2481,
                    2236,
                    2510,
                    2207
                ],
                "op": [
                    2506,
                    2511,
                    2238,
                    2231
                ],
                "T.nnet.abstract_conv.AbstractConv2d_gradInputs": [
                    2231
                ],
                "spatial_start_dim": [
                    2273,
                    2274,
                    2316,
                    2271
                ],
                "depthwise_kernel": [
                    2275,
                    2405,
                    2344,
                    2281,
                    2408,
                    2347,
                    2284,
                    2413,
                    2414,
                    2417,
                    2358,
                    2295,
                    2296,
                    2363,
                    2300,
                    2359
                ],
                "pointwise_kernel": [
                    2370,
                    2307,
                    2276,
                    2350,
                    2287,
                    2353,
                    2290,
                    2360,
                    2297
                ],
                "depthwise_kernel_shape": [
                    2304,
                    2344,
                    2345,
                    2347,
                    2348,
                    2349,
                    2359,
                    2367,
                    2405,
                    2406,
                    2408,
                    2281,
                    2282,
                    2409,
                    2284,
                    2285,
                    2286,
                    2410,
                    2414,
                    2421,
                    2296,
                    2425
                ],
                "depthwise_kernel.eval": [
                    2408,
                    2347,
                    2284
                ],
                "_preprocess_conv2d_depthwise_filter_shape": [
                    2409,
                    2348,
                    2285
                ],
                "pointwise_kernel_shape": [
                    2374,
                    2311,
                    2377,
                    2314,
                    2350,
                    2287,
                    2288,
                    2351,
                    2290,
                    2291,
                    2292,
                    2353,
                    2354,
                    2355
                ],
                "pointwise_kernel.eval": [
                    2353,
                    2290
                ],
                "_preprocess_conv2d_depthwise_kernel": [
                    2413,
                    2358,
                    2295
                ],
                "_preprocess_conv3d_volume_shape": [
                    2444
                ],
                "_preprocess_conv3d_filter_shape": [
                    2448,
                    2500
                ],
                "_preprocess_conv3d_input": [
                    2450,
                    2502
                ],
                "_preprocess_conv3d_kernel": [
                    2451,
                    2503
                ],
                "T.nnet.conv3d": [
                    2454
                ],
                "_postprocess_conv3d_output": [
                    2512,
                    2460
                ],
                "T.nnet.abstract_conv.AbstractConv3d_gradInputs": [
                    2506
                ],
                "pool_size": [
                    2566,
                    2567,
                    2568,
                    2538,
                    2543,
                    2579,
                    2584,
                    2521,
                    2524,
                    2525,
                    2526,
                    2527
                ],
                "odd_pad_w": [
                    2524,
                    2525
                ],
                "w_pad": [
                    2528,
                    2569,
                    2525,
                    2566
                ],
                "odd_pad_h": [
                    2526,
                    2527
                ],
                "h_pad": [
                    2528,
                    2569,
                    2567,
                    2527
                ],
                "pad": [
                    2528,
                    2530,
                    2569,
                    2571,
                    2540,
                    2545,
                    2581,
                    2586
                ],
                "pool_mode": [
                    2537,
                    2542,
                    2578,
                    2548,
                    2583,
                    2589
                ],
                "pool_out": [
                    2596,
                    2584,
                    2538,
                    2602,
                    2603,
                    2543,
                    2579,
                    2552,
                    2557,
                    2558
                ],
                "pool.pool_2d": [
                    2538,
                    2543
                ],
                "pool": [
                    2584,
                    2538,
                    2579,
                    2543
                ],
                "expected_width": [
                    2592,
                    2553,
                    2597,
                    2550
                ],
                "expected_height": [
                    2593,
                    2554,
                    2598,
                    2551
                ],
                "pool_out.dimshuffle": [
                    2602,
                    2557
                ],
                "d_pad": [
                    2568,
                    2569
                ],
                "pool.pool_3d": [
                    2584,
                    2579
                ],
                "expected_depth": [
                    2594,
                    2599
                ],
                "bias": [
                    2608,
                    2611,
                    2612,
                    2615,
                    2616,
                    2618,
                    2620,
                    2621,
                    2623,
                    2626,
                    2627,
                    2629,
                    2631,
                    2632,
                    2634,
                    2637,
                    2638,
                    2640,
                    2642,
                    2643,
                    2645,
                    2647
                ],
                "bias_shape": [
                    2627,
                    2629,
                    2632,
                    2634,
                    2638,
                    2640,
                    2643,
                    2612,
                    2645,
                    2616,
                    2618,
                    2621,
                    2623
                ],
                "bias.shape": [
                    2612
                ],
                "rng.normal": [
                    2692,
                    2689,
                    2660
                ],
                "stddev": [
                    2692,
                    2689,
                    2660,
                    2694
                ],
                "rng.uniform": [
                    2669
                ],
                "minval": [
                    2669
                ],
                "maxval": [
                    2669
                ],
                "p": [
                    2678
                ],
                "normal_t": [
                    2692,
                    2694
                ],
                "Y_": [
                    2705,
                    2706,
                    2707
                ],
                "T.alloc": [
                    2705
                ],
                "Y.shape": [
                    2744,
                    2705,
                    2706,
                    2711
                ],
                "Y": [
                    2744,
                    2705,
                    2706,
                    2773,
                    2711,
                    2712,
                    2749,
                    2751
                ],
                "skip_idxs": [
                    2712,
                    2713,
                    2717,
                    2711
                ],
                "non_repeats": [
                    2712,
                    2713
                ],
                "non_repeats.nonzero": [
                    2713
                ],
                "active_skip_idxs": [
                    2721,
                    2717,
                    2733
                ],
                "nonzero": [
                    2717
                ],
                "active": [
                    2720,
                    2724,
                    2725,
                    2728,
                    2717
                ],
                "active_next": [
                    2726,
                    2737,
                    2738,
                    2740,
                    2718
                ],
                "log_p_curr.shape": [
                    2722
                ],
                "log_p_curr": [
                    2738,
                    2722
                ],
                "common_factor": [
                    2724,
                    2725,
                    2734,
                    2774,
                    2775,
                    2776
                ],
                "log_p_prev": [
                    2724,
                    2725
                ],
                "p_prev": [
                    2728,
                    2725,
                    2733
                ],
                "_p_prev": [
                    2726,
                    2728,
                    2730,
                    2732,
                    2733,
                    2734
                ],
                "zeros": [
                    2755,
                    2757,
                    2726,
                    2737,
                    2746,
                    2747
                ],
                "T.inc_subtensor": [
                    2730,
                    2732
                ],
                "updated_log_p_prev": [
                    2738,
                    2734
                ],
                "log_p_next": [
                    2736,
                    2740
                ],
                "smoothed": [
                    2744,
                    2745
                ],
                "predict": [
                    2744,
                    2773
                ],
                "np.float32": [
                    2744
                ],
                "L": [
                    2762,
                    2765,
                    2768,
                    2745,
                    2746
                ],
                "log_first": [
                    2747,
                    2763
                ],
                "f_skip_idxs": [
                    2755,
                    2749
                ],
                "ctc_create_skip_idxs": [
                    2749,
                    2751
                ],
                "b_skip_idxs": [
                    2757,
                    2751
                ],
                "f_active_next": [
                    2754,
                    2758
                ],
                "log_f_next": [
                    2754,
                    2758
                ],
                "ctc_update_log_p": [
                    2754,
                    2756
                ],
                "f_active": [
                    2760,
                    2755,
                    2766
                ],
                "log_f_curr": [
                    2755
                ],
                "log_f_prev": [
                    2755
                ],
                "b_active_next": [
                    2756,
                    2758
                ],
                "log_b_next": [
                    2756,
                    2758
                ],
                "b_active": [
                    2760,
                    2757,
                    2767
                ],
                "log_b_curr": [
                    2757
                ],
                "log_b_prev": [
                    2757
                ],
                "log_f_probs": [
                    2760,
                    2768
                ],
                "log_b_probs": [
                    2760,
                    2768
                ],
                "np.int32": [
                    2763
                ],
                "idxs": [
                    2765,
                    2766,
                    2767
                ],
                "L.shape": [
                    2765
                ],
                "f_active.dimshuffle": [
                    2766
                ],
                "b_active.dimshuffle": [
                    2767
                ],
                "log_probs": [
                    2768,
                    2769,
                    2773,
                    2774,
                    2775
                ],
                "ctc_path_probs": [
                    2773
                ],
                "ctc_interleave_blanks": [
                    2773
                ],
                "total_log_prob": [
                    2776,
                    2777,
                    2775
                ],
                "mask.nonzero": [
                    2775
                ],
                "y_pred_step": [
                    2801,
                    2799
                ],
                "input_length_step": [
                    2799
                ],
                "y_true_step": [
                    2800,
                    2801
                ],
                "label_length_step": [
                    2800
                ],
                "ctc_cost": [
                    2801
                ],
                "ret": [
                    2809,
                    2810,
                    2803
                ],
                "ctc_step": [
                    2804
                ],
                "y_true": [
                    2806
                ],
                "y_pred": [
                    2806
                ],
                "label_length": [
                    2806
                ],
                "ret.dimshuffle": [
                    2809
                ],
                "theano.map": [
                    2827
                ],
                "fn": [
                    2872,
                    2849,
                    2827
                ],
                "elems": [
                    2850,
                    2827,
                    2867,
                    2868,
                    2873,
                    2844,
                    2845
                ],
                "initializer": [
                    2850,
                    2866,
                    2867,
                    2873,
                    2843,
                    2844
                ],
                "theano.foldl": [
                    2849
                ],
                "acc": [
                    2872,
                    2849
                ],
                "theano.foldr": [
                    2872
                ],
                "stride": [
                    2885,
                    2886,
                    2879
                ],
                "output_length": [
                    2881,
                    2884
                ],
                "feature_dim": [
                    2881,
                    2917,
                    2888,
                    2933,
                    2906
                ],
                "filters": [
                    2881,
                    2906,
                    2922,
                    2938
                ],
                "slice_length": [
                    2885,
                    2887
                ],
                "kernel_size": [
                    2913,
                    2915,
                    2886,
                    2929,
                    2931
                ],
                "xs.append": [
                    2932,
                    2887
                ],
                "x_aggregate": [
                    2936,
                    2889,
                    2891,
                    2935
                ],
                "concatenate": [
                    2920,
                    2889,
                    2935
                ],
                "batch_dot": [
                    2936,
                    2891
                ],
                "stride_row": [
                    2912,
                    2913,
                    2928,
                    2929,
                    2903
                ],
                "stride_col": [
                    2914,
                    2915,
                    2930,
                    2931,
                    2903
                ],
                "output_row": [
                    2922,
                    2926,
                    2904,
                    2938,
                    2910
                ],
                "output_col": [
                    2919,
                    2922,
                    2927,
                    2904,
                    2938,
                    2911
                ],
                "slice_row": [
                    2912,
                    2932,
                    2916,
                    2928
                ],
                "slice_col": [
                    2932,
                    2914,
                    2916,
                    2930
                ],
                "x_flatten": [
                    2916,
                    2918
                ],
                "output.append": [
                    2918
                ],
                "dot": [
                    2918
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_1/keras/initializers.py": {
            "buggy_functions": [
                {
                    "function_name": "__call__",
                    "function_code": "def __call__(self, shape, dtype=None):\n    return K.random_normal(shape, self.mean, self.stddev,\n                           dtype=dtype, seed=self.seed)\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 82,
                    "end_line": 84,
                    "variables": {
                        "K.random_normal": [
                            83
                        ],
                        "K": [
                            83
                        ],
                        "shape": [
                            83
                        ],
                        "self.mean": [
                            83
                        ],
                        "self": [
                            83,
                            84
                        ],
                        "self.stddev": [
                            83
                        ],
                        "dtype": [
                            84
                        ],
                        "self.seed": [
                            84
                        ]
                    },
                    "filtered_variables": {
                        "K.random_normal": [
                            83
                        ],
                        "K": [
                            83
                        ],
                        "shape": [
                            83
                        ],
                        "self.mean": [
                            83
                        ],
                        "self": [
                            83,
                            84
                        ],
                        "self.stddev": [
                            83
                        ],
                        "dtype": [
                            84
                        ],
                        "self.seed": [
                            84
                        ]
                    },
                    "diff_line_number": 83,
                    "class_data": {
                        "signature": "class RandomNormal(Initializer)",
                        "docstring": "Initializer that generates tensors with a normal distribution.\n\n# Arguments\n    mean: a python scalar or a scalar tensor. Mean of the random values\n      to generate.\n    stddev: a python scalar or a scalar tensor. Standard deviation of the\n      random values to generate.\n    seed: A Python integer. Used to seed the random generator.",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, mean=0.0, stddev=0.05, seed=None):\n    self.mean = mean\n    self.stddev = stddev\n    self.seed = seed",
                            "def __call__(self, shape, dtype=None):\n    return K.random_normal(shape, self.mean, self.stddev, dtype=dtype, seed=self.seed)",
                            "def get_config(self):\n    return {'mean': self.mean, 'stddev': self.stddev, 'seed': self.seed}"
                        ],
                        "constructor_variables": [
                            "seed",
                            "stddev",
                            "mean"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, mean=0.0, stddev=0.05, seed=None)",
                            "__call__(self, shape, dtype=None)",
                            "get_config(self)"
                        ]
                    },
                    "variable_values": [
                        [
                            {
                                "K.random_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.mean": {
                                    "variable_value": "0.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.RandomNormal object at 0x129e59550>",
                                    "variable_type": "RandomNormal",
                                    "variable_shape": null
                                },
                                "self.stddev": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            },
                            {
                                "K.random_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.mean": {
                                    "variable_value": "0.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.RandomNormal object at 0x129e59550>",
                                    "variable_type": "RandomNormal",
                                    "variable_shape": null
                                },
                                "self.stddev": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {
                                "K.random_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.mean": {
                                    "variable_value": "0.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.RandomNormal object at 0x129e59550>",
                                    "variable_type": "RandomNormal",
                                    "variable_shape": null
                                },
                                "self.stddev": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            },
                            {
                                "K.random_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.mean": {
                                    "variable_value": "0.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.RandomNormal object at 0x129e59550>",
                                    "variable_type": "RandomNormal",
                                    "variable_shape": null
                                },
                                "self.stddev": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            }
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {
                                "x": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.random_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.mean": {
                                    "variable_value": "0.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.RandomNormal object at 0x12a298d90>",
                                    "variable_type": "RandomNormal",
                                    "variable_shape": null
                                },
                                "self.stddev": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            },
                            {}
                        ],
                        [
                            {
                                "x": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.random_uniform": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.minval": {
                                    "variable_value": "-0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.RandomUniform object at 0x122d470d0>",
                                    "variable_type": "RandomUniform",
                                    "variable_shape": null
                                },
                                "self.maxval": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            },
                            {}
                        ],
                        [
                            {
                                "fan_in": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "fan_out": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "_compute_fans": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "scale": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.scale": {
                                    "variable_value": "1.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.VarianceScaling object at 0x1234e2090>",
                                    "variable_type": "VarianceScaling",
                                    "variable_shape": null
                                },
                                "self.mode": {
                                    "variable_value": "'fan_in'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.distribution": {
                                    "variable_value": "'normal'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "stddev": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.sqrt": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "x": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.truncated_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "limit": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.random_uniform": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            },
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "__call__",
                    "function_code": "def __call__(self, shape, dtype=None):\n    return K.random_uniform(shape, self.minval, self.maxval,\n                            dtype=dtype, seed=self.seed)\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 110,
                    "end_line": 112,
                    "variables": {
                        "K.random_uniform": [
                            111
                        ],
                        "K": [
                            111
                        ],
                        "shape": [
                            111
                        ],
                        "self.minval": [
                            111
                        ],
                        "self": [
                            112,
                            111
                        ],
                        "self.maxval": [
                            111
                        ],
                        "dtype": [
                            112
                        ],
                        "self.seed": [
                            112
                        ]
                    },
                    "filtered_variables": {
                        "K.random_uniform": [
                            111
                        ],
                        "K": [
                            111
                        ],
                        "shape": [
                            111
                        ],
                        "self.minval": [
                            111
                        ],
                        "self": [
                            112,
                            111
                        ],
                        "self.maxval": [
                            111
                        ],
                        "dtype": [
                            112
                        ],
                        "self.seed": [
                            112
                        ]
                    },
                    "diff_line_number": 111,
                    "class_data": {
                        "signature": "class RandomUniform(Initializer)",
                        "docstring": "Initializer that generates tensors with a uniform distribution.\n\n# Arguments\n    minval: A python scalar or a scalar tensor. Lower bound of the range\n      of random values to generate.\n    maxval: A python scalar or a scalar tensor. Upper bound of the range\n      of random values to generate.  Defaults to 1 for float types.\n    seed: A Python integer. Used to seed the random generator.",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n    self.minval = minval\n    self.maxval = maxval\n    self.seed = seed",
                            "def __call__(self, shape, dtype=None):\n    return K.random_uniform(shape, self.minval, self.maxval, dtype=dtype, seed=self.seed)",
                            "def get_config(self):\n    return {'minval': self.minval, 'maxval': self.maxval, 'seed': self.seed}"
                        ],
                        "constructor_variables": [
                            "seed",
                            "minval",
                            "maxval"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, minval=-0.05, maxval=0.05, seed=None)",
                            "__call__(self, shape, dtype=None)",
                            "get_config(self)"
                        ]
                    },
                    "variable_values": [
                        [
                            {
                                "K.random_uniform": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.minval": {
                                    "variable_value": "-0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.RandomUniform object at 0x125480310>",
                                    "variable_type": "RandomUniform",
                                    "variable_shape": null
                                },
                                "self.maxval": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            },
                            {
                                "K.random_uniform": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.minval": {
                                    "variable_value": "-0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.RandomUniform object at 0x125480310>",
                                    "variable_type": "RandomUniform",
                                    "variable_shape": null
                                },
                                "self.maxval": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {
                                "K.random_uniform": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.minval": {
                                    "variable_value": "-0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.RandomUniform object at 0x125480310>",
                                    "variable_type": "RandomUniform",
                                    "variable_shape": null
                                },
                                "self.maxval": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            },
                            {
                                "K.random_uniform": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.minval": {
                                    "variable_value": "-0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.RandomUniform object at 0x125480310>",
                                    "variable_type": "RandomUniform",
                                    "variable_shape": null
                                },
                                "self.maxval": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            }
                        ]
                    ]
                },
                {
                    "function_name": "__call__",
                    "function_code": "def __call__(self, shape, dtype=None):\n    return K.truncated_normal(shape, self.mean, self.stddev,\n                              dtype=dtype, seed=self.seed)\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 143,
                    "end_line": 145,
                    "variables": {
                        "K.truncated_normal": [
                            144
                        ],
                        "K": [
                            144
                        ],
                        "shape": [
                            144
                        ],
                        "self.mean": [
                            144
                        ],
                        "self": [
                            144,
                            145
                        ],
                        "self.stddev": [
                            144
                        ],
                        "dtype": [
                            145
                        ],
                        "self.seed": [
                            145
                        ]
                    },
                    "filtered_variables": {
                        "K.truncated_normal": [
                            144
                        ],
                        "K": [
                            144
                        ],
                        "shape": [
                            144
                        ],
                        "self.mean": [
                            144
                        ],
                        "self": [
                            144,
                            145
                        ],
                        "self.stddev": [
                            144
                        ],
                        "dtype": [
                            145
                        ],
                        "self.seed": [
                            145
                        ]
                    },
                    "diff_line_number": 144,
                    "class_data": {
                        "signature": "class TruncatedNormal(Initializer)",
                        "docstring": "Initializer that generates a truncated normal distribution.\n\nThese values are similar to values from a `RandomNormal`\nexcept that values more than two standard deviations from the mean\nare discarded and redrawn. This is the recommended initializer for\nneural network weights and filters.\n\n# Arguments\n    mean: a python scalar or a scalar tensor. Mean of the random values\n      to generate.\n    stddev: a python scalar or a scalar tensor. Standard deviation of the\n      random values to generate.\n    seed: A Python integer. Used to seed the random generator.",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, mean=0.0, stddev=0.05, seed=None):\n    self.mean = mean\n    self.stddev = stddev\n    self.seed = seed",
                            "def __call__(self, shape, dtype=None):\n    return K.truncated_normal(shape, self.mean, self.stddev, dtype=dtype, seed=self.seed)",
                            "def get_config(self):\n    return {'mean': self.mean, 'stddev': self.stddev, 'seed': self.seed}"
                        ],
                        "constructor_variables": [
                            "seed",
                            "stddev",
                            "mean"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, mean=0.0, stddev=0.05, seed=None)",
                            "__call__(self, shape, dtype=None)",
                            "get_config(self)"
                        ]
                    },
                    "variable_values": [
                        [
                            {
                                "K.truncated_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.mean": {
                                    "variable_value": "0.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.TruncatedNormal object at 0x12d5a44d0>",
                                    "variable_type": "TruncatedNormal",
                                    "variable_shape": null
                                },
                                "self.stddev": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            },
                            {
                                "K.truncated_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.mean": {
                                    "variable_value": "0.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.TruncatedNormal object at 0x12d5a44d0>",
                                    "variable_type": "TruncatedNormal",
                                    "variable_shape": null
                                },
                                "self.stddev": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {
                                "K.truncated_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.mean": {
                                    "variable_value": "0.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.TruncatedNormal object at 0x12d5a44d0>",
                                    "variable_type": "TruncatedNormal",
                                    "variable_shape": null
                                },
                                "self.stddev": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            },
                            {
                                "K.truncated_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "self.mean": {
                                    "variable_value": "0.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.TruncatedNormal object at 0x12d5a44d0>",
                                    "variable_type": "TruncatedNormal",
                                    "variable_shape": null
                                },
                                "self.stddev": {
                                    "variable_value": "0.05",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                }
                            }
                        ]
                    ]
                },
                {
                    "function_name": "__call__",
                    "function_code": "def __call__(self, shape, dtype=None):\n    fan_in, fan_out = _compute_fans(shape)\n    scale = self.scale\n    if self.mode == 'fan_in':\n        scale /= max(1., fan_in)\n    elif self.mode == 'fan_out':\n        scale /= max(1., fan_out)\n    else:\n        scale /= max(1., float(fan_in + fan_out) / 2)\n    if self.distribution == 'normal':\n        # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n        stddev = np.sqrt(scale) / .87962566103423978\n        return K.truncated_normal(shape, 0., stddev,\n                                  dtype=dtype, seed=self.seed)\n    else:\n        limit = np.sqrt(3. * scale)\n        return K.random_uniform(shape, -limit, limit,\n                                dtype=dtype, seed=self.seed)\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 201,
                    "end_line": 218,
                    "variables": {
                        "fan_in": [
                            209,
                            202,
                            205
                        ],
                        "fan_out": [
                            209,
                            202,
                            207
                        ],
                        "_compute_fans": [
                            202
                        ],
                        "shape": [
                            217,
                            202,
                            213
                        ],
                        "scale": [
                            203,
                            205,
                            207,
                            209,
                            212,
                            216
                        ],
                        "self.scale": [
                            203
                        ],
                        "self": [
                            203,
                            204,
                            206,
                            210,
                            214,
                            218
                        ],
                        "self.mode": [
                            204,
                            206
                        ],
                        "max": [
                            209,
                            205,
                            207
                        ],
                        "float": [
                            209
                        ],
                        "self.distribution": [
                            210
                        ],
                        "stddev": [
                            212,
                            213
                        ],
                        "np.sqrt": [
                            216,
                            212
                        ],
                        "np": [
                            216,
                            212
                        ],
                        "K.truncated_normal": [
                            213
                        ],
                        "K": [
                            217,
                            213
                        ],
                        "dtype": [
                            218,
                            214
                        ],
                        "self.seed": [
                            218,
                            214
                        ],
                        "limit": [
                            216,
                            217
                        ],
                        "K.random_uniform": [
                            217
                        ]
                    },
                    "filtered_variables": {
                        "fan_in": [
                            209,
                            202,
                            205
                        ],
                        "fan_out": [
                            209,
                            202,
                            207
                        ],
                        "_compute_fans": [
                            202
                        ],
                        "shape": [
                            217,
                            202,
                            213
                        ],
                        "scale": [
                            203,
                            205,
                            207,
                            209,
                            212,
                            216
                        ],
                        "self.scale": [
                            203
                        ],
                        "self": [
                            203,
                            204,
                            206,
                            210,
                            214,
                            218
                        ],
                        "self.mode": [
                            204,
                            206
                        ],
                        "self.distribution": [
                            210
                        ],
                        "stddev": [
                            212,
                            213
                        ],
                        "np.sqrt": [
                            216,
                            212
                        ],
                        "np": [
                            216,
                            212
                        ],
                        "K.truncated_normal": [
                            213
                        ],
                        "K": [
                            217,
                            213
                        ],
                        "dtype": [
                            218,
                            214
                        ],
                        "self.seed": [
                            218,
                            214
                        ],
                        "limit": [
                            216,
                            217
                        ],
                        "K.random_uniform": [
                            217
                        ]
                    },
                    "diff_line_number": 213,
                    "class_data": {
                        "signature": "class VarianceScaling(Initializer)",
                        "docstring": "Initializer capable of adapting its scale to the shape of weights.\n\nWith `distribution=\"normal\"`, samples are drawn from a truncated normal\ndistribution centered on zero, with `stddev = sqrt(scale / n)` where n is:\n\n    - number of input units in the weight tensor, if mode = \"fan_in\"\n    - number of output units, if mode = \"fan_out\"\n    - average of the numbers of input and output units, if mode = \"fan_avg\"\n\nWith `distribution=\"uniform\"`,\nsamples are drawn from a uniform distribution\nwithin [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n\n# Arguments\n    scale: Scaling factor (positive float).\n    mode: One of \"fan_in\", \"fan_out\", \"fan_avg\".\n    distribution: Random distribution to use. One of \"normal\", \"uniform\".\n    seed: A Python integer. Used to seed the random generator.\n\n# Raises\n    ValueError: In case of an invalid value for the \"scale\", mode\" or\n      \"distribution\" arguments.",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, scale=1.0, mode='fan_in', distribution='normal', seed=None):\n    if scale <= 0.0:\n        raise ValueError('`scale` must be a positive float. Got:', scale)\n    mode = mode.lower()\n    if mode not in {'fan_in', 'fan_out', 'fan_avg'}:\n        raise ValueError('Invalid `mode` argument: expected on of {\"fan_in\", \"fan_out\", \"fan_avg\"} but got', mode)\n    distribution = distribution.lower()\n    if distribution not in {'normal', 'uniform'}:\n        raise ValueError('Invalid `distribution` argument: expected one of {\"normal\", \"uniform\"} but got', distribution)\n    self.scale = scale\n    self.mode = mode\n    self.distribution = distribution\n    self.seed = seed",
                            "def __call__(self, shape, dtype=None):\n    fan_in, fan_out = _compute_fans(shape)\n    scale = self.scale\n    if self.mode == 'fan_in':\n        scale /= max(1.0, fan_in)\n    elif self.mode == 'fan_out':\n        scale /= max(1.0, fan_out)\n    else:\n        scale /= max(1.0, float(fan_in + fan_out) / 2)\n    if self.distribution == 'normal':\n        stddev = np.sqrt(scale) / 0.8796256610342398\n        return K.truncated_normal(shape, 0.0, stddev, dtype=dtype, seed=self.seed)\n    else:\n        limit = np.sqrt(3.0 * scale)\n        return K.random_uniform(shape, -limit, limit, dtype=dtype, seed=self.seed)",
                            "def get_config(self):\n    return {'scale': self.scale, 'mode': self.mode, 'distribution': self.distribution, 'seed': self.seed}"
                        ],
                        "constructor_variables": [
                            "scale",
                            "distribution",
                            "mode",
                            "seed"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, scale=1.0, mode='fan_in', distribution='normal', seed=None)",
                            "__call__(self, shape, dtype=None)",
                            "get_config(self)"
                        ]
                    },
                    "variable_values": [
                        [
                            {
                                "fan_in": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "fan_out": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "_compute_fans": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "scale": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.scale": {
                                    "variable_value": "1.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.VarianceScaling object at 0x12a5c6790>",
                                    "variable_type": "VarianceScaling",
                                    "variable_shape": null
                                },
                                "self.mode": {
                                    "variable_value": "'fan_in'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.distribution": {
                                    "variable_value": "'normal'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "stddev": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.sqrt": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.truncated_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "limit": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.random_uniform": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            },
                            {
                                "fan_in": {
                                    "variable_value": "2",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "fan_out": {
                                    "variable_value": "2",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "_compute_fans": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "scale": {
                                    "variable_value": "0.5",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self.scale": {
                                    "variable_value": "1.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.VarianceScaling object at 0x12a5c6790>",
                                    "variable_type": "VarianceScaling",
                                    "variable_shape": null
                                },
                                "self.mode": {
                                    "variable_value": "'fan_in'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.distribution": {
                                    "variable_value": "'normal'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "stddev": {
                                    "variable_value": "0.8038723885739654",
                                    "variable_type": "float64",
                                    "variable_shape": "()"
                                },
                                "np.sqrt": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.truncated_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "limit": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.random_uniform": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {
                                "fan_in": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "fan_out": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "_compute_fans": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "scale": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.scale": {
                                    "variable_value": "1.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.VarianceScaling object at 0x12a5c6790>",
                                    "variable_type": "VarianceScaling",
                                    "variable_shape": null
                                },
                                "self.mode": {
                                    "variable_value": "'fan_in'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.distribution": {
                                    "variable_value": "'normal'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "stddev": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.sqrt": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.truncated_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "limit": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.random_uniform": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            },
                            {
                                "fan_in": {
                                    "variable_value": "2",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "fan_out": {
                                    "variable_value": "2",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "_compute_fans": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "scale": {
                                    "variable_value": "0.5",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self.scale": {
                                    "variable_value": "1.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.VarianceScaling object at 0x12a5c6790>",
                                    "variable_type": "VarianceScaling",
                                    "variable_shape": null
                                },
                                "self.mode": {
                                    "variable_value": "'fan_in'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "self.distribution": {
                                    "variable_value": "'normal'",
                                    "variable_type": "str",
                                    "variable_shape": "6"
                                },
                                "stddev": {
                                    "variable_value": "0.8038723885739654",
                                    "variable_type": "float64",
                                    "variable_shape": "()"
                                },
                                "np.sqrt": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.truncated_normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "dtype": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "limit": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "K.random_uniform": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            }
                        ]
                    ]
                },
                {
                    "function_name": "__call__",
                    "function_code": "def __call__(self, shape, dtype=None):\n    num_rows = 1\n    for dim in shape[:-1]:\n        num_rows *= dim\n    num_cols = shape[-1]\n    flat_shape = (num_rows, num_cols)\n    rng = np.random\n    if self.seed is not None:\n        rng = np.random.RandomState(self.seed)\n    a = rng.normal(0.0, 1.0, flat_shape)\n    u, _, v = np.linalg.svd(a, full_matrices=False)\n    # Pick the one with the correct shape.\n    q = u if u.shape == flat_shape else v\n    q = q.reshape(shape)\n    return self.gain * q[:shape[0], :shape[1]]\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 245,
                    "end_line": 259,
                    "variables": {
                        "num_rows": [
                            248,
                            250,
                            246
                        ],
                        "dim": [
                            248,
                            247
                        ],
                        "shape": [
                            249,
                            258,
                            259,
                            247
                        ],
                        "num_cols": [
                            249,
                            250
                        ],
                        "flat_shape": [
                            257,
                            250,
                            254
                        ],
                        "rng": [
                            251,
                            253,
                            254
                        ],
                        "np.random": [
                            251,
                            253
                        ],
                        "np": [
                            251,
                            253,
                            255
                        ],
                        "self.seed": [
                            252,
                            253
                        ],
                        "self": [
                            259,
                            252,
                            253
                        ],
                        "np.random.RandomState": [
                            253
                        ],
                        "a": [
                            254,
                            255
                        ],
                        "rng.normal": [
                            254
                        ],
                        "u": [
                            257,
                            255
                        ],
                        "_": [
                            255
                        ],
                        "v": [
                            257,
                            255
                        ],
                        "np.linalg.svd": [
                            255
                        ],
                        "np.linalg": [
                            255
                        ],
                        "q": [
                            257,
                            258,
                            259
                        ],
                        "u.shape": [
                            257
                        ],
                        "q.reshape": [
                            258
                        ],
                        "self.gain": [
                            259
                        ]
                    },
                    "filtered_variables": {
                        "num_rows": [
                            248,
                            250,
                            246
                        ],
                        "dim": [
                            248,
                            247
                        ],
                        "shape": [
                            249,
                            258,
                            259,
                            247
                        ],
                        "num_cols": [
                            249,
                            250
                        ],
                        "flat_shape": [
                            257,
                            250,
                            254
                        ],
                        "rng": [
                            251,
                            253,
                            254
                        ],
                        "np.random": [
                            251,
                            253
                        ],
                        "np": [
                            251,
                            253,
                            255
                        ],
                        "self.seed": [
                            252,
                            253
                        ],
                        "self": [
                            259,
                            252,
                            253
                        ],
                        "np.random.RandomState": [
                            253
                        ],
                        "a": [
                            254,
                            255
                        ],
                        "rng.normal": [
                            254
                        ],
                        "u": [
                            257,
                            255
                        ],
                        "_": [
                            255
                        ],
                        "v": [
                            257,
                            255
                        ],
                        "np.linalg.svd": [
                            255
                        ],
                        "np.linalg": [
                            255
                        ],
                        "q": [
                            257,
                            258,
                            259
                        ],
                        "u.shape": [
                            257
                        ],
                        "q.reshape": [
                            258
                        ],
                        "self.gain": [
                            259
                        ]
                    },
                    "diff_line_number": 253,
                    "class_data": {
                        "signature": "class Orthogonal(Initializer)",
                        "docstring": "Initializer that generates a random orthogonal matrix.\n\n# Arguments\n    gain: Multiplicative factor to apply to the orthogonal matrix.\n    seed: A Python integer. Used to seed the random generator.\n\n# References\n    - [Exact solutions to the nonlinear dynamics of learning in deep\n       linear neural networks](http://arxiv.org/abs/1312.6120)",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, gain=1.0, seed=None):\n    self.gain = gain\n    self.seed = seed",
                            "def __call__(self, shape, dtype=None):\n    num_rows = 1\n    for dim in shape[:-1]:\n        num_rows *= dim\n    num_cols = shape[-1]\n    flat_shape = (num_rows, num_cols)\n    rng = np.random\n    if self.seed is not None:\n        rng = np.random.RandomState(self.seed)\n    a = rng.normal(0.0, 1.0, flat_shape)\n    u, _, v = np.linalg.svd(a, full_matrices=False)\n    q = u if u.shape == flat_shape else v\n    q = q.reshape(shape)\n    return self.gain * q[:shape[0], :shape[1]]",
                            "def get_config(self):\n    return {'gain': self.gain, 'seed': self.seed}"
                        ],
                        "constructor_variables": [
                            "gain",
                            "seed"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, gain=1.0, seed=None)",
                            "__call__(self, shape, dtype=None)",
                            "get_config(self)"
                        ]
                    },
                    "variable_values": [
                        [
                            {
                                "num_rows": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "dim": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "num_cols": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "flat_shape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "rng": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.random": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.Orthogonal object at 0x12962e4d0>",
                                    "variable_type": "Orthogonal",
                                    "variable_shape": null
                                },
                                "np.random.RandomState": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "a": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "rng.normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "u": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "_": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "v": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.linalg.svd": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.linalg": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "q": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "u.shape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "q.reshape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.gain": {
                                    "variable_value": "1.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                }
                            },
                            {
                                "num_rows": {
                                    "variable_value": "2",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "dim": {
                                    "variable_value": "2",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "num_cols": {
                                    "variable_value": "2",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "flat_shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "rng": {
                                    "variable_value": "RandomState(MT19937) at 0x129583270",
                                    "variable_type": "RandomState",
                                    "variable_shape": null
                                },
                                "np.random": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.Orthogonal object at 0x12962e4d0>",
                                    "variable_type": "Orthogonal",
                                    "variable_shape": null
                                },
                                "np.random.RandomState": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "a": {
                                    "variable_value": "array([[-0.70318731, -0.49028236],\n       [-0.32181433, -1.75507872]])",
                                    "variable_type": "ndarray",
                                    "variable_shape": "2"
                                },
                                "rng.normal": {
                                    "variable_value": "<built-in method normal of numpy.random.mtrand.RandomState object at 0x129583270>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                },
                                "u": {
                                    "variable_value": "array([[ 0.35502382,  0.93485726],\n       [ 0.93485726, -0.35502382]])",
                                    "variable_type": "ndarray",
                                    "variable_shape": "2"
                                },
                                "_": {
                                    "variable_value": "array([1.89646622, 0.56756571])",
                                    "variable_type": "ndarray",
                                    "variable_shape": "2"
                                },
                                "v": {
                                    "variable_value": "array([[-0.29027604, -0.95694296],\n       [-0.95694296,  0.29027604]])",
                                    "variable_type": "ndarray",
                                    "variable_shape": "2"
                                },
                                "np.linalg.svd": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.linalg": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "q": {
                                    "variable_value": "array([[ 0.35502382,  0.93485726],\n       [ 0.93485726, -0.35502382]])",
                                    "variable_type": "ndarray",
                                    "variable_shape": "2"
                                },
                                "u.shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "q.reshape": {
                                    "variable_value": "<built-in method reshape of numpy.ndarray object at 0x129636da0>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                },
                                "self.gain": {
                                    "variable_value": "1.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {
                                "num_rows": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "dim": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "num_cols": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "flat_shape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "rng": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.random": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.Orthogonal object at 0x12962e4d0>",
                                    "variable_type": "Orthogonal",
                                    "variable_shape": null
                                },
                                "np.random.RandomState": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "a": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "rng.normal": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "u": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "_": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "v": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.linalg.svd": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.linalg": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "q": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "u.shape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "q.reshape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.gain": {
                                    "variable_value": "1.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                }
                            },
                            {
                                "num_rows": {
                                    "variable_value": "2",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "dim": {
                                    "variable_value": "2",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "num_cols": {
                                    "variable_value": "2",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "flat_shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "rng": {
                                    "variable_value": "RandomState(MT19937) at 0x129583270",
                                    "variable_type": "RandomState",
                                    "variable_shape": null
                                },
                                "np.random": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.seed": {
                                    "variable_value": "1337",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.initializers.Orthogonal object at 0x12962e4d0>",
                                    "variable_type": "Orthogonal",
                                    "variable_shape": null
                                },
                                "np.random.RandomState": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "a": {
                                    "variable_value": "array([[-0.70318731, -0.49028236],\n       [-0.32181433, -1.75507872]])",
                                    "variable_type": "ndarray",
                                    "variable_shape": "2"
                                },
                                "rng.normal": {
                                    "variable_value": "<built-in method normal of numpy.random.mtrand.RandomState object at 0x129583270>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                },
                                "u": {
                                    "variable_value": "array([[ 0.35502382,  0.93485726],\n       [ 0.93485726, -0.35502382]])",
                                    "variable_type": "ndarray",
                                    "variable_shape": "2"
                                },
                                "_": {
                                    "variable_value": "array([1.89646622, 0.56756571])",
                                    "variable_type": "ndarray",
                                    "variable_shape": "2"
                                },
                                "v": {
                                    "variable_value": "array([[-0.29027604, -0.95694296],\n       [-0.95694296,  0.29027604]])",
                                    "variable_type": "ndarray",
                                    "variable_shape": "2"
                                },
                                "np.linalg.svd": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "np.linalg": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "q": {
                                    "variable_value": "array([[ 0.35502382,  0.93485726],\n       [ 0.93485726, -0.35502382]])",
                                    "variable_type": "ndarray",
                                    "variable_shape": "2"
                                },
                                "u.shape": {
                                    "variable_value": "(2, 2)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "q.reshape": {
                                    "variable_value": "<built-in method reshape of numpy.ndarray object at 0x12963c170>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                },
                                "self.gain": {
                                    "variable_value": "1.0",
                                    "variable_type": "float",
                                    "variable_shape": null
                                }
                            }
                        ]
                    ]
                }
            ],
            "inscope_functions": [
                "def lecun_uniform(seed=None):\n    \"\"\"LeCun uniform initializer.\n\n    It draws samples from a uniform distribution within [-limit, limit]\n    where `limit` is `sqrt(3 / fan_in)`\n    where `fan_in` is the number of input units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        - [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n    \"\"\"\n    return VarianceScaling(scale=1.,\n                           mode='fan_in',\n                           distribution='uniform',\n                           seed=seed)",
                "def glorot_normal(seed=None):\n    \"\"\"Glorot normal initializer, also called Xavier normal initializer.\n\n    It draws samples from a truncated normal distribution centered on 0\n    with `stddev = sqrt(2 / (fan_in + fan_out))`\n    where `fan_in` is the number of input units in the weight tensor\n    and `fan_out` is the number of output units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        - [Understanding the difficulty of training deep feedforward neural\n           networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n    \"\"\"\n    return VarianceScaling(scale=1.,\n                           mode='fan_avg',\n                           distribution='normal',\n                           seed=seed)",
                "def glorot_uniform(seed=None):\n    \"\"\"Glorot uniform initializer, also called Xavier uniform initializer.\n\n    It draws samples from a uniform distribution within [-limit, limit]\n    where `limit` is `sqrt(6 / (fan_in + fan_out))`\n    where `fan_in` is the number of input units in the weight tensor\n    and `fan_out` is the number of output units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        - [Understanding the difficulty of training deep feedforward neural\n           networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n    \"\"\"\n    return VarianceScaling(scale=1.,\n                           mode='fan_avg',\n                           distribution='uniform',\n                           seed=seed)",
                "def he_normal(seed=None):\n    \"\"\"He normal initializer.\n\n    It draws samples from a truncated normal distribution centered on 0\n    with `stddev = sqrt(2 / fan_in)`\n    where `fan_in` is the number of input units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n           ImageNet Classification](http://arxiv.org/abs/1502.01852)\n    \"\"\"\n    return VarianceScaling(scale=2.,\n                           mode='fan_in',\n                           distribution='normal',\n                           seed=seed)",
                "def lecun_normal(seed=None):\n    \"\"\"LeCun normal initializer.\n\n    It draws samples from a truncated normal distribution centered on 0\n    with `stddev = sqrt(1 / fan_in)`\n    where `fan_in` is the number of input units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n        - [Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n    \"\"\"\n    return VarianceScaling(scale=1.,\n                           mode='fan_in',\n                           distribution='normal',\n                           seed=seed)",
                "def he_uniform(seed=None):\n    \"\"\"He uniform variance scaling initializer.\n\n    It draws samples from a uniform distribution within [-limit, limit]\n    where `limit` is `sqrt(6 / fan_in)`\n    where `fan_in` is the number of input units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n           ImageNet Classification](http://arxiv.org/abs/1502.01852)\n    \"\"\"\n    return VarianceScaling(scale=2.,\n                           mode='fan_in',\n                           distribution='uniform',\n                           seed=seed)",
                "def _compute_fans(shape, data_format='channels_last'):\n    \"\"\"Computes the number of input and output units for a weight shape.\n\n    # Arguments\n        shape: Integer shape tuple.\n        data_format: Image data format to use for convolution kernels.\n            Note that all kernels in Keras are standardized on the\n            `channels_last` ordering (even when inputs are set\n            to `channels_first`).\n\n    # Returns\n        A tuple of scalars, `(fan_in, fan_out)`.\n\n    # Raises\n        ValueError: in case of invalid `data_format` argument.\n    \"\"\"\n    if len(shape) == 2:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    elif len(shape) in {3, 4, 5}:\n        # Assuming convolution kernels (1D, 2D or 3D).\n        # TH kernel shape: (depth, input_depth, ...)\n        # TF kernel shape: (..., input_depth, depth)\n        if data_format == 'channels_first':\n            receptive_field_size = np.prod(shape[2:])\n            fan_in = shape[1] * receptive_field_size\n            fan_out = shape[0] * receptive_field_size\n        elif data_format == 'channels_last':\n            receptive_field_size = np.prod(shape[:-2])\n            fan_in = shape[-2] * receptive_field_size\n            fan_out = shape[-1] * receptive_field_size\n        else:\n            raise ValueError('Invalid data_format: ' + data_format)\n    else:\n        # No specific assumptions.\n        fan_in = np.sqrt(np.prod(shape))\n        fan_out = np.sqrt(np.prod(shape))\n    return fan_in, fan_out",
                "def serialize(initializer):\n    return serialize_keras_object(initializer)",
                "def deserialize(config, custom_objects=None):\n    return deserialize_keras_object(config,\n                                    module_objects=globals(),\n                                    custom_objects=custom_objects,\n                                    printable_module_name='initializer')",
                "def get(identifier):\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif isinstance(identifier, six.string_types):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret initializer identifier: ' +\n                         str(identifier))",
                "def __call__(self, shape, dtype=None):\n    raise NotImplementedError",
                "def get_config(self):\n    return {}",
                "@classmethod\ndef from_config(cls, config):\n    if 'dtype' in config:\n        # Initializers saved from `tf.keras`\n        # may contain an unused `dtype` argument.\n        config.pop('dtype')\n    return cls(**config)",
                "def __call__(self, shape, dtype=None):\n    return K.constant(0, shape=shape, dtype=dtype)",
                "def __call__(self, shape, dtype=None):\n    return K.constant(1, shape=shape, dtype=dtype)",
                "def __init__(self, value=0):\n    self.value = value",
                "def __call__(self, shape, dtype=None):\n    return K.constant(self.value, shape=shape, dtype=dtype)",
                "def get_config(self):\n    return {'value': self.value}",
                "def __init__(self, mean=0., stddev=0.05, seed=None):\n    self.mean = mean\n    self.stddev = stddev\n    self.seed = seed",
                "def __call__(self, shape, dtype=None):\n    return K.random_normal(shape, self.mean, self.stddev,\n                           dtype=dtype, seed=self.seed)",
                "def get_config(self):\n    return {\n        'mean': self.mean,\n        'stddev': self.stddev,\n        'seed': self.seed\n    }",
                "def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n    self.minval = minval\n    self.maxval = maxval\n    self.seed = seed",
                "def __call__(self, shape, dtype=None):\n    return K.random_uniform(shape, self.minval, self.maxval,\n                            dtype=dtype, seed=self.seed)",
                "def get_config(self):\n    return {\n        'minval': self.minval,\n        'maxval': self.maxval,\n        'seed': self.seed,\n    }",
                "def __init__(self, mean=0., stddev=0.05, seed=None):\n    self.mean = mean\n    self.stddev = stddev\n    self.seed = seed",
                "def __call__(self, shape, dtype=None):\n    return K.truncated_normal(shape, self.mean, self.stddev,\n                              dtype=dtype, seed=self.seed)",
                "def get_config(self):\n    return {\n        'mean': self.mean,\n        'stddev': self.stddev,\n        'seed': self.seed\n    }",
                "def __init__(self, scale=1.0,\n             mode='fan_in',\n             distribution='normal',\n             seed=None):\n    if scale <= 0.:\n        raise ValueError('`scale` must be a positive float. Got:', scale)\n    mode = mode.lower()\n    if mode not in {'fan_in', 'fan_out', 'fan_avg'}:\n        raise ValueError('Invalid `mode` argument: '\n                         'expected on of {\"fan_in\", \"fan_out\", \"fan_avg\"} '\n                         'but got', mode)\n    distribution = distribution.lower()\n    if distribution not in {'normal', 'uniform'}:\n        raise ValueError('Invalid `distribution` argument: '\n                         'expected one of {\"normal\", \"uniform\"} '\n                         'but got', distribution)\n    self.scale = scale\n    self.mode = mode\n    self.distribution = distribution\n    self.seed = seed",
                "def __call__(self, shape, dtype=None):\n    fan_in, fan_out = _compute_fans(shape)\n    scale = self.scale\n    if self.mode == 'fan_in':\n        scale /= max(1., fan_in)\n    elif self.mode == 'fan_out':\n        scale /= max(1., fan_out)\n    else:\n        scale /= max(1., float(fan_in + fan_out) / 2)\n    if self.distribution == 'normal':\n        # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n        stddev = np.sqrt(scale) / .87962566103423978\n        return K.truncated_normal(shape, 0., stddev,\n                                  dtype=dtype, seed=self.seed)\n    else:\n        limit = np.sqrt(3. * scale)\n        return K.random_uniform(shape, -limit, limit,\n                                dtype=dtype, seed=self.seed)",
                "def get_config(self):\n    return {\n        'scale': self.scale,\n        'mode': self.mode,\n        'distribution': self.distribution,\n        'seed': self.seed\n    }",
                "def __init__(self, gain=1., seed=None):\n    self.gain = gain\n    self.seed = seed",
                "def __call__(self, shape, dtype=None):\n    num_rows = 1\n    for dim in shape[:-1]:\n        num_rows *= dim\n    num_cols = shape[-1]\n    flat_shape = (num_rows, num_cols)\n    rng = np.random\n    if self.seed is not None:\n        rng = np.random.RandomState(self.seed)\n    a = rng.normal(0.0, 1.0, flat_shape)\n    u, _, v = np.linalg.svd(a, full_matrices=False)\n    # Pick the one with the correct shape.\n    q = u if u.shape == flat_shape else v\n    q = q.reshape(shape)\n    return self.gain * q[:shape[0], :shape[1]]",
                "def get_config(self):\n    return {\n        'gain': self.gain,\n        'seed': self.seed\n    }",
                "def __init__(self, gain=1.):\n    self.gain = gain",
                "def __call__(self, shape, dtype=None):\n    if len(shape) != 2:\n        raise ValueError(\n            'Identity matrix initializer can only be used for 2D matrices.')\n\n    return self.gain * np.eye(shape[0], shape[1])",
                "def get_config(self):\n    return {\n        'gain': self.gain\n    }"
            ],
            "inscope_function_signatures": [
                "lecun_uniform(seed=None)",
                "glorot_normal(seed=None)",
                "glorot_uniform(seed=None)",
                "he_normal(seed=None)",
                "lecun_normal(seed=None)",
                "he_uniform(seed=None)",
                "_compute_fans(shape, data_format='channels_last')",
                "serialize(initializer)",
                "deserialize(config, custom_objects=None)",
                "get(identifier)",
                "__call__(self, shape, dtype=None)",
                "get_config(self)",
                "from_config(cls, config)",
                "__call__(self, shape, dtype=None)",
                "__call__(self, shape, dtype=None)",
                "__init__(self, value=0)",
                "__call__(self, shape, dtype=None)",
                "get_config(self)",
                "__init__(self, mean=0.0, stddev=0.05, seed=None)",
                "__call__(self, shape, dtype=None)",
                "get_config(self)",
                "__init__(self, minval=-0.05, maxval=0.05, seed=None)",
                "__call__(self, shape, dtype=None)",
                "get_config(self)",
                "__init__(self, mean=0.0, stddev=0.05, seed=None)",
                "__call__(self, shape, dtype=None)",
                "get_config(self)",
                "__init__(self, scale=1.0, mode='fan_in', distribution='normal', seed=None)",
                "__call__(self, shape, dtype=None)",
                "get_config(self)",
                "__init__(self, gain=1.0, seed=None)",
                "__call__(self, shape, dtype=None)",
                "get_config(self)",
                "__init__(self, gain=1.0)",
                "__call__(self, shape, dtype=None)",
                "get_config(self)"
            ],
            "variables_in_file": {
                "object": [
                    14
                ],
                "NotImplementedError": [
                    19
                ],
                "config": [
                    493,
                    503,
                    504,
                    26,
                    29,
                    30
                ],
                "config.pop": [
                    29
                ],
                "cls": [
                    30
                ],
                "classmethod": [
                    24
                ],
                "Initializer": [
                    33,
                    66,
                    229,
                    41,
                    268,
                    49,
                    122,
                    155,
                    94
                ],
                "K.constant": [
                    46,
                    60,
                    38
                ],
                "K": [
                    38,
                    46,
                    111,
                    144,
                    83,
                    213,
                    217,
                    60
                ],
                "shape": [
                    258,
                    259,
                    144,
                    283,
                    287,
                    38,
                    46,
                    60,
                    202,
                    464,
                    465,
                    466,
                    83,
                    467,
                    213,
                    472,
                    217,
                    473,
                    474,
                    476,
                    477,
                    478,
                    483,
                    484,
                    111,
                    247,
                    249
                ],
                "dtype": [
                    38,
                    46,
                    112,
                    145,
                    84,
                    214,
                    218,
                    60
                ],
                "self.value": [
                    57,
                    60,
                    63
                ],
                "self": [
                    259,
                    263,
                    264,
                    139,
                    140,
                    141,
                    144,
                    145,
                    149,
                    150,
                    151,
                    280,
                    287,
                    291,
                    57,
                    60,
                    63,
                    196,
                    197,
                    198,
                    199,
                    203,
                    204,
                    78,
                    79,
                    80,
                    206,
                    210,
                    83,
                    84,
                    214,
                    88,
                    89,
                    90,
                    218,
                    222,
                    223,
                    224,
                    225,
                    106,
                    107,
                    108,
                    111,
                    112,
                    242,
                    243,
                    116,
                    117,
                    118,
                    252,
                    253
                ],
                "value": [
                    57
                ],
                "self.mean": [
                    139,
                    78,
                    144,
                    83,
                    149,
                    88
                ],
                "mean": [
                    139,
                    78
                ],
                "self.stddev": [
                    140,
                    79,
                    144,
                    83,
                    150,
                    89
                ],
                "stddev": [
                    212,
                    140,
                    213,
                    79
                ],
                "self.seed": [
                    225,
                    199,
                    264,
                    218,
                    108,
                    141,
                    80,
                    112,
                    145,
                    243,
                    84,
                    118,
                    151,
                    214,
                    90,
                    252,
                    253
                ],
                "seed": [
                    385,
                    199,
                    362,
                    108,
                    141,
                    431,
                    80,
                    338,
                    243,
                    408,
                    314
                ],
                "K.random_normal": [
                    83
                ],
                "self.minval": [
                    106,
                    116,
                    111
                ],
                "minval": [
                    106
                ],
                "self.maxval": [
                    107,
                    117,
                    111
                ],
                "maxval": [
                    107
                ],
                "K.random_uniform": [
                    217,
                    111
                ],
                "K.truncated_normal": [
                    144,
                    213
                ],
                "scale": [
                    196,
                    203,
                    205,
                    207,
                    209,
                    212,
                    184,
                    185,
                    216
                ],
                "ValueError": [
                    480,
                    193,
                    508,
                    188,
                    185,
                    284
                ],
                "mode": [
                    186,
                    187,
                    197,
                    190
                ],
                "mode.lower": [
                    186
                ],
                "distribution": [
                    192,
                    195,
                    198,
                    191
                ],
                "distribution.lower": [
                    191
                ],
                "self.scale": [
                    203,
                    196,
                    222
                ],
                "self.mode": [
                    204,
                    197,
                    206,
                    223
                ],
                "self.distribution": [
                    224,
                    210,
                    198
                ],
                "fan_in": [
                    483,
                    485,
                    202,
                    205,
                    209,
                    465,
                    473,
                    477
                ],
                "fan_out": [
                    484,
                    485,
                    202,
                    207,
                    209,
                    466,
                    474,
                    478
                ],
                "_compute_fans": [
                    202
                ],
                "max": [
                    209,
                    205,
                    207
                ],
                "float": [
                    209
                ],
                "np.sqrt": [
                    216,
                    483,
                    212,
                    484
                ],
                "np": [
                    483,
                    287,
                    484,
                    472,
                    212,
                    216,
                    251,
                    476,
                    253,
                    255
                ],
                "limit": [
                    216,
                    217
                ],
                "self.gain": [
                    259,
                    291,
                    263,
                    242,
                    280,
                    287
                ],
                "gain": [
                    280,
                    242
                ],
                "num_rows": [
                    248,
                    250,
                    246
                ],
                "dim": [
                    248,
                    247
                ],
                "num_cols": [
                    249,
                    250
                ],
                "flat_shape": [
                    257,
                    250,
                    254
                ],
                "rng": [
                    251,
                    253,
                    254
                ],
                "np.random": [
                    251,
                    253
                ],
                "np.random.RandomState": [
                    253
                ],
                "a": [
                    254,
                    255
                ],
                "rng.normal": [
                    254
                ],
                "u": [
                    257,
                    255
                ],
                "_": [
                    255
                ],
                "v": [
                    257,
                    255
                ],
                "np.linalg.svd": [
                    255
                ],
                "np.linalg": [
                    255
                ],
                "q": [
                    257,
                    258,
                    259
                ],
                "u.shape": [
                    257
                ],
                "q.reshape": [
                    258
                ],
                "len": [
                    464,
                    283,
                    467
                ],
                "np.eye": [
                    287
                ],
                "VarianceScaling": [
                    359,
                    428,
                    335,
                    405,
                    311,
                    382
                ],
                "zero": [
                    436
                ],
                "zeros": [
                    436
                ],
                "Zeros": [
                    436
                ],
                "one": [
                    437
                ],
                "ones": [
                    437
                ],
                "Ones": [
                    437
                ],
                "constant": [
                    438
                ],
                "Constant": [
                    438
                ],
                "uniform": [
                    439
                ],
                "random_uniform": [
                    439
                ],
                "RandomUniform": [
                    439
                ],
                "normal": [
                    440
                ],
                "random_normal": [
                    440
                ],
                "RandomNormal": [
                    440
                ],
                "truncated_normal": [
                    441
                ],
                "TruncatedNormal": [
                    441
                ],
                "identity": [
                    442
                ],
                "Identity": [
                    442
                ],
                "orthogonal": [
                    443
                ],
                "Orthogonal": [
                    443
                ],
                "data_format": [
                    480,
                    475,
                    471
                ],
                "receptive_field_size": [
                    472,
                    473,
                    474,
                    476,
                    477,
                    478
                ],
                "np.prod": [
                    472,
                    483,
                    476,
                    484
                ],
                "serialize_keras_object": [
                    489
                ],
                "initializer": [
                    489
                ],
                "deserialize_keras_object": [
                    493
                ],
                "globals": [
                    494
                ],
                "custom_objects": [
                    495
                ],
                "isinstance": [
                    500,
                    502
                ],
                "identifier": [
                    500,
                    501,
                    502,
                    503,
                    505,
                    506,
                    509
                ],
                "dict": [
                    500
                ],
                "deserialize": [
                    504,
                    501
                ],
                "six.string_types": [
                    502
                ],
                "six": [
                    502
                ],
                "str": [
                    509,
                    503
                ],
                "callable": [
                    505
                ]
            },
            "filtered_variables_in_file": {
                "config": [
                    493,
                    503,
                    504,
                    26,
                    29,
                    30
                ],
                "config.pop": [
                    29
                ],
                "cls": [
                    30
                ],
                "Initializer": [
                    33,
                    66,
                    229,
                    41,
                    268,
                    49,
                    122,
                    155,
                    94
                ],
                "K.constant": [
                    46,
                    60,
                    38
                ],
                "K": [
                    38,
                    46,
                    111,
                    144,
                    83,
                    213,
                    217,
                    60
                ],
                "shape": [
                    258,
                    259,
                    144,
                    283,
                    287,
                    38,
                    46,
                    60,
                    202,
                    464,
                    465,
                    466,
                    83,
                    467,
                    213,
                    472,
                    217,
                    473,
                    474,
                    476,
                    477,
                    478,
                    483,
                    484,
                    111,
                    247,
                    249
                ],
                "dtype": [
                    38,
                    46,
                    112,
                    145,
                    84,
                    214,
                    218,
                    60
                ],
                "self.value": [
                    57,
                    60,
                    63
                ],
                "self": [
                    259,
                    263,
                    264,
                    139,
                    140,
                    141,
                    144,
                    145,
                    149,
                    150,
                    151,
                    280,
                    287,
                    291,
                    57,
                    60,
                    63,
                    196,
                    197,
                    198,
                    199,
                    203,
                    204,
                    78,
                    79,
                    80,
                    206,
                    210,
                    83,
                    84,
                    214,
                    88,
                    89,
                    90,
                    218,
                    222,
                    223,
                    224,
                    225,
                    106,
                    107,
                    108,
                    111,
                    112,
                    242,
                    243,
                    116,
                    117,
                    118,
                    252,
                    253
                ],
                "value": [
                    57
                ],
                "self.mean": [
                    139,
                    78,
                    144,
                    83,
                    149,
                    88
                ],
                "mean": [
                    139,
                    78
                ],
                "self.stddev": [
                    140,
                    79,
                    144,
                    83,
                    150,
                    89
                ],
                "stddev": [
                    212,
                    140,
                    213,
                    79
                ],
                "self.seed": [
                    225,
                    199,
                    264,
                    218,
                    108,
                    141,
                    80,
                    112,
                    145,
                    243,
                    84,
                    118,
                    151,
                    214,
                    90,
                    252,
                    253
                ],
                "seed": [
                    385,
                    199,
                    362,
                    108,
                    141,
                    431,
                    80,
                    338,
                    243,
                    408,
                    314
                ],
                "K.random_normal": [
                    83
                ],
                "self.minval": [
                    106,
                    116,
                    111
                ],
                "minval": [
                    106
                ],
                "self.maxval": [
                    107,
                    117,
                    111
                ],
                "maxval": [
                    107
                ],
                "K.random_uniform": [
                    217,
                    111
                ],
                "K.truncated_normal": [
                    144,
                    213
                ],
                "scale": [
                    196,
                    203,
                    205,
                    207,
                    209,
                    212,
                    184,
                    185,
                    216
                ],
                "mode": [
                    186,
                    187,
                    197,
                    190
                ],
                "mode.lower": [
                    186
                ],
                "distribution": [
                    192,
                    195,
                    198,
                    191
                ],
                "distribution.lower": [
                    191
                ],
                "self.scale": [
                    203,
                    196,
                    222
                ],
                "self.mode": [
                    204,
                    197,
                    206,
                    223
                ],
                "self.distribution": [
                    224,
                    210,
                    198
                ],
                "fan_in": [
                    483,
                    485,
                    202,
                    205,
                    209,
                    465,
                    473,
                    477
                ],
                "fan_out": [
                    484,
                    485,
                    202,
                    207,
                    209,
                    466,
                    474,
                    478
                ],
                "_compute_fans": [
                    202
                ],
                "np.sqrt": [
                    216,
                    483,
                    212,
                    484
                ],
                "np": [
                    483,
                    287,
                    484,
                    472,
                    212,
                    216,
                    251,
                    476,
                    253,
                    255
                ],
                "limit": [
                    216,
                    217
                ],
                "self.gain": [
                    259,
                    291,
                    263,
                    242,
                    280,
                    287
                ],
                "gain": [
                    280,
                    242
                ],
                "num_rows": [
                    248,
                    250,
                    246
                ],
                "dim": [
                    248,
                    247
                ],
                "num_cols": [
                    249,
                    250
                ],
                "flat_shape": [
                    257,
                    250,
                    254
                ],
                "rng": [
                    251,
                    253,
                    254
                ],
                "np.random": [
                    251,
                    253
                ],
                "np.random.RandomState": [
                    253
                ],
                "a": [
                    254,
                    255
                ],
                "rng.normal": [
                    254
                ],
                "u": [
                    257,
                    255
                ],
                "_": [
                    255
                ],
                "v": [
                    257,
                    255
                ],
                "np.linalg.svd": [
                    255
                ],
                "np.linalg": [
                    255
                ],
                "q": [
                    257,
                    258,
                    259
                ],
                "u.shape": [
                    257
                ],
                "q.reshape": [
                    258
                ],
                "np.eye": [
                    287
                ],
                "VarianceScaling": [
                    359,
                    428,
                    335,
                    405,
                    311,
                    382
                ],
                "zero": [
                    436
                ],
                "zeros": [
                    436
                ],
                "Zeros": [
                    436
                ],
                "one": [
                    437
                ],
                "ones": [
                    437
                ],
                "Ones": [
                    437
                ],
                "constant": [
                    438
                ],
                "Constant": [
                    438
                ],
                "uniform": [
                    439
                ],
                "random_uniform": [
                    439
                ],
                "RandomUniform": [
                    439
                ],
                "normal": [
                    440
                ],
                "random_normal": [
                    440
                ],
                "RandomNormal": [
                    440
                ],
                "truncated_normal": [
                    441
                ],
                "TruncatedNormal": [
                    441
                ],
                "identity": [
                    442
                ],
                "Identity": [
                    442
                ],
                "orthogonal": [
                    443
                ],
                "Orthogonal": [
                    443
                ],
                "data_format": [
                    480,
                    475,
                    471
                ],
                "receptive_field_size": [
                    472,
                    473,
                    474,
                    476,
                    477,
                    478
                ],
                "np.prod": [
                    472,
                    483,
                    476,
                    484
                ],
                "serialize_keras_object": [
                    489
                ],
                "initializer": [
                    489
                ],
                "deserialize_keras_object": [
                    493
                ],
                "custom_objects": [
                    495
                ],
                "identifier": [
                    500,
                    501,
                    502,
                    503,
                    505,
                    506,
                    509
                ],
                "deserialize": [
                    504,
                    501
                ],
                "six.string_types": [
                    502
                ],
                "six": [
                    502
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_1/tests/keras/backend/backend_test.py": {
            "buggy_functions": [
                {
                    "function_name": "test_print_tensor",
                    "function_code": "def test_print_tensor(self):\n    check_single_tensor_operation('print_tensor', (), WITH_NP)\n    check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n    check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n    check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 483,
                    "end_line": 487,
                    "variables": {
                        "check_single_tensor_operation": [
                            484,
                            485,
                            486,
                            487
                        ],
                        "WITH_NP": [
                            484,
                            485,
                            486,
                            487
                        ]
                    },
                    "filtered_variables": {
                        "check_single_tensor_operation": [
                            484,
                            485,
                            486,
                            487
                        ],
                        "WITH_NP": [
                            484,
                            485,
                            486,
                            487
                        ]
                    },
                    "diff_line_number": 483,
                    "class_data": {
                        "signature": "class TestBackend(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def test_is_keras_tensor(self):\n    np_var = np.array([1, 2])\n    with pytest.raises(ValueError):\n        K.is_keras_tensor(np_var)\n    keras_var = K.variable(np_var)\n    assert K.is_keras_tensor(keras_var) is False\n    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n    assert K.is_keras_tensor(keras_placeholder) is False",
                            "def test_set_learning_phase(self):\n    with pytest.raises(ValueError):\n        K.set_learning_phase(2)",
                            "def test_eye(self):\n    check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)",
                            "def test_ones(self):\n    check_single_tensor_operation('ones', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_zeros(self):\n    check_single_tensor_operation('zeros', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_ones_like(self):\n    check_single_tensor_operation('ones_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_zeros_like(self):\n    check_single_tensor_operation('zeros_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_linear_operations(self):\n    check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n    check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=(1, 1))\n    check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n    check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n    if K.backend() != 'cntk':\n        check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=(1, 2))",
                            "def test_random_variables(self):\n    check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP, low=0.0, high=1.0, shape_or_val=False, assert_value_equality=False)\n    check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP, mean=0.0, scale=1.0, shape_or_val=False, assert_value_equality=False)",
                            "def test_batch_dot_shape(self):\n    test_cases = []\n    test_cases.append([(None, 3, 4, 5), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 3, 4, 5), (None, 2, 4), 2])\n    test_cases.append([(None, 3, 4), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 4, 3), (None, 3, 5), (2, 1)])\n    test_cases.append([(None, 4), (None, 3, 4), (1, 2)])\n    test_cases.append([(None, 4), (None, 4), None])\n    batch_size = 7\n\n    def batch_shape(shape):\n        return (batch_size,) + shape[1:]\n\n    def random(shape):\n        return np.random.random(batch_shape(shape))\n    for x_shape, y_shape, axes in test_cases:\n        x_np = random(x_shape)\n        y_np = random(y_shape)\n        z_np = KNP.batch_dot(x_np, y_np, axes)\n        x = K.placeholder(shape=x_shape)\n        y = K.placeholder(shape=y_shape)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        f = K.function([x, y], [z])\n        assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        if K.backend() != 'cntk':\n            x = K.placeholder(ndim=len(x_shape))\n            y = K.placeholder(ndim=len(y_shape))\n            z = K.batch_dot(x, y, axes)\n            z_shape = K.int_shape(z)\n            if z_shape is not None:\n                assert len(z_shape) == z_np.ndim\n                assert set(z_shape) <= set((None, 1))\n            f = K.function([x, y], [z])\n            assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        x = K.variable(x_np)\n        y = K.variable(y_np)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        z = K.eval(z)\n        assert_allclose(z, z_np, atol=1e-05)",
                            "def test_shape_operations(self):\n    check_two_tensor_operation('concatenate', (4, 3), (4, 2), WITH_NP, axis=-1, concat_args=True)\n    check_single_tensor_operation('reshape', (4, 2), WITH_NP, shape=(8, 1))\n    check_single_tensor_operation('permute_dimensions', (4, 2, 3), WITH_NP, pattern=(2, 0, 1))\n    check_single_tensor_operation('repeat', (4, 1), WITH_NP, n=3)\n    check_single_tensor_operation('flatten', (4, 1), WITH_NP)\n    check_single_tensor_operation('batch_flatten', (20, 2, 5), WITH_NP, cntk_dynamicity=True)\n    check_single_tensor_operation('expand_dims', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('expand_dims', (4, 3, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('squeeze', (4, 3, 1), WITH_NP, axis=2)\n    check_single_tensor_operation('squeeze', (4, 1, 1), WITH_NP, axis=1)\n    check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)}, 'squeeze', {'axis': 2}, (4, 3, 1, 1), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='We only test the shape inference of the theano backend.')\ndef test_none_shape_operations(self):\n    x = K.placeholder((3, None, 4))\n    y = K.batch_flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (3, None)\n    y = K.flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (None,)",
                            "def test_repeat_elements(self):\n    reps = 3\n    for ndims in [1, 2, 3]:\n        shape = np.arange(2, 2 + ndims)\n        arr = np.arange(np.prod(shape)).reshape(shape)\n        for rep_axis in range(ndims):\n            check_single_tensor_operation('repeat_elements', arr, WITH_NP, rep=reps, axis=rep_axis)\n            if K.backend() != 'cntk':\n                shape = list(shape)\n                shape[rep_axis] = None\n                x = K.placeholder(shape=shape)\n                y = K.repeat_elements(x, reps, axis=rep_axis)\n                assert y._keras_shape == tuple(shape)\n                assert y._keras_shape == K.int_shape(y)",
                            "def test_tile(self):\n    shape = (3, 4)\n    arr = np.arange(np.prod(shape)).reshape(shape)\n    check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n    check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 4))\n        n = 2\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 8)\n        n = (4, 3)\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 12)",
                            "def test_gather(self):\n    shape = (10, 2, 3)\n    ref = np.arange(np.prod(shape)).reshape(shape)\n    inds = [1, 3, 7, 9]\n    t_list = [k.gather(k.variable(ref), k.variable(inds, dtype='int32')) for k in WITH_NP]\n    z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32'))) for k in WITH_NP]\n    assert_list_pairwise(z_list)\n    assert_list_keras_shape(t_list, z_list)\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 3, 4))\n        indices = K.placeholder(shape=(5, 6), dtype='int32')\n        y = K.gather(x, indices)\n        assert y._keras_shape == (5, 6, 3, 4)",
                            "@pytest.mark.parametrize('function_name', ['get_value', 'count_params', 'int_shape', 'get_variable_shape'])\ndef test_value_manipulation(self, function_name):\n    val = np.random.random((4, 2))\n    v_list = [getattr(k, function_name)(k.variable(val)) for k in WITH_NP]\n    if function_name == 'get_value':\n        assert_list_pairwise(v_list)\n    else:\n        assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)",
                            "def test_print_tensor(self):\n    check_single_tensor_operation('print_tensor', (), WITH_NP)\n    check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n    check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n    check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)",
                            "def test_elementwise_operations(self):\n    check_single_tensor_operation('max', (4, 2), WITH_NP)\n    check_single_tensor_operation('max', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('max', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('min', (4, 2), WITH_NP)\n    check_single_tensor_operation('min', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('min', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('mean', (4, 2), WITH_NP)\n    check_single_tensor_operation('mean', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=-1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('var', (4, 2), WITH_NP)\n    check_single_tensor_operation('var', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('var', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('std', (4, 2), WITH_NP)\n    check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('prod', (4, 2), WITH_NP)\n    check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('any', (4, 2), WITH_NP)\n    check_single_tensor_operation('any', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('any', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('all', (4, 2), WITH_NP)\n    check_single_tensor_operation('all', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('all', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('square', (4, 2), WITH_NP)\n    check_single_tensor_operation('abs', (4, 2), WITH_NP)\n    check_single_tensor_operation('sqrt', (4, 2), WITH_NP)\n    check_single_tensor_operation('exp', (4, 2), WITH_NP)\n    check_single_tensor_operation('round', (4, 2), WITH_NP)\n    check_single_tensor_operation('sign', (4, 2), WITH_NP)\n    check_single_tensor_operation('pow', (4, 2), WITH_NP, a=3)\n    check_single_tensor_operation('clip', (4, 2), WITH_NP, min_value=0.4, max_value=0.6)\n    check_single_tensor_operation('cos', (4, 2), WITH_NP)\n    check_single_tensor_operation('sin', (4, 2), WITH_NP)\n    check_two_tensor_operation('equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('not_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('maximum', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('minimum', (4, 2), (4, 2), WITH_NP)",
                            "def test_reset_uids(self):\n    first = K.get_uid()\n    K.get_uid()\n    K.reset_uids()\n    assert K.get_uid() == first",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support cumsum and cumprod yet')\ndef test_cumsum_cumprod(self):\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP, axis=1)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk return -85.1 for zero or negative number, not nan, so can't compare with other backend.\")\ndef test_log(self):\n    check_single_tensor_operation('log', (4, 2), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_add(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    increment = np.random.randn(3, 4)\n    x += increment\n    K.eval(K.update_add(x_var, increment))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_sub(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    decrement = np.random.randn(3, 4)\n    x -= decrement\n    K.eval(K.update_sub(x_var, decrement))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk doesn't support gradient in this way.\")\ndef test_gradient(self):\n    val = np.random.random((4, 2))\n    x_list = [k.variable(val) for k in [KTH, KTF]]\n    z_list = []\n    zero_list = []\n    for x, k in zip(x_list, [KTH, KTF]):\n        exp = x * k.exp(x)\n        loss = k.sum(exp)\n        zero_loss = k.stop_gradient(loss)\n        grad = k.gradients(loss, [exp])\n        zero_grad = k.gradients(loss + zero_loss, [exp])\n        z_list.append(k.eval(grad[0]))\n        zero_list.append(k.eval(zero_grad[0]))\n    assert_list_pairwise(z_list)\n    assert_list_pairwise(zero_list)\n    for i in range(len(z_list)):\n        assert_allclose(zero_list[i], z_list[i], atol=1e-05)",
                            "def test_stop_gradient(self):\n    val = np.random.random((4, 2))\n    a = K.variable(val)\n    b = K.square(a)\n    c, d = K.stop_gradient([a, b])\n    e = K.stop_gradient(b)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk currently not support function in this way, so can't test as this.\")\ndef test_function(self):\n    test_backend = [KTH, KTF]\n    val = np.random.random((4, 2))\n    input_val = np.random.random((4, 2))\n    f_list = []\n    x_list = []\n    for k in test_backend:\n        x = k.variable(val)\n        x_list.append(x)\n        y = k.placeholder(ndim=2)\n        exp = k.square(x) + y\n        update = x * 2\n        f = k.function([y], [exp], updates=[(x, update)])\n        f_list.append(f)\n    function_outputs_list = [f([input_val])[0] for f in f_list]\n    assert_list_pairwise(function_outputs_list)\n    new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n    assert_list_pairwise(new_val_list)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `fetches` argument.')\ndef test_function_tf_fetches(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], updates=[(x, x_placeholder + 1.0)], fetches=[K.update(y, 5.0)])\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert K.get_session().run(fetches=[x, y]) == [11.0, 5.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `feed_dict` argument.')\ndef test_function_tf_feed_dict(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    feed_dict = {y_placeholder: 3.0}\n    f = K.function(inputs=[x_placeholder], outputs=[x_placeholder + 1.0], updates=[(x, x_placeholder + 10.0)], feed_dict=feed_dict, fetches=[K.update(y, y_placeholder * 10.0)])\n    output = f([10.0])\n    assert output == [11.0]\n    assert K.get_session().run(fetches=[x, y]) == [20.0, 30.0]\n    feed_dict[y_placeholder] = 4.0\n    output = f([20.0])\n    assert output == [21.0]\n    assert K.get_session().run(fetches=[x, y]) == [30.0, 40.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `options` and `run_metadata` arguments.')\ndef test_function_tf_run_options_with_run_metadata(self):\n    from tensorflow.core.protobuf import config_pb2\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    run_options = config_pb2.RunOptions(output_partition_graphs=True)\n    run_metadata = config_pb2.RunMetadata()\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], options=run_options, run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) > 0\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) == 0",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `string` type for a tensor.')\ndef test_function_tf_string_input(self):\n    x_placeholder = K.placeholder(shape=(), dtype='string')\n    x_identity = K.identity(x_placeholder)\n    f = K.function(inputs=[x_placeholder], outputs=[x_identity])\n    output = f([b'test'])\n    assert output == [b'test']",
                            "def test_rnn(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn(inputs, states):\n            assert len(states) == 1\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y])\n        return simple_rnn\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_additional_states(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    h1 = np.concatenate([h0, h0], axis=-1)\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_with_extra_mock_state(inputs, states):\n            assert len(states) == 2\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y, backend.concatenate([y, y], axis=-1)])\n        return simple_rnn_with_extra_mock_state\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0, h1], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_no_states(self):\n    num_samples = 3\n    input_dim = 8\n    output_dim = 4\n    timesteps = 5\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n\n    def get_step_function(backend, w_i):\n\n        def simple_no_states(inputs, states):\n            assert len(states) == 0\n            y = backend.dot(inputs, w_i)\n            return (y, [])\n        return simple_no_states\n    kwargs_list = [{'go_backwards': False}, {'go_backwards': True}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k), step_function_np=get_step_function(KNP, wi), inputs_np=x, initial_states_np=[], mask_np=None, **kwargs)",
                            "def test_rnn_constants(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, c = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_add_constant(inputs, states_and_constants):\n            [h, c] = states_and_constants\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n            return (y, [y])\n        return simple_rnn_add_constant\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), constants_np=[c], **kwargs)",
                            "def test_rnn_output_and_state_masking_independent(self):\n    num_samples = 2\n    num_timesteps = 4\n    state_and_io_size = 5\n    mask_last_num_timesteps = 2\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, state_and_io_size))\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[1, -mask_last_num_timesteps:] = 0\n    expected_outputs = inputs_vals.copy()\n    expected_outputs[1, -mask_last_num_timesteps:] = expected_outputs[1, -(mask_last_num_timesteps + 1)]\n    expected_state = initial_state_vals.copy()\n    expected_state[0] += num_timesteps\n    expected_state[1] += num_timesteps - mask_last_num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)\n        assert_allclose(K.eval(last_states[0]), expected_state)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_output_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n    num_features = 5\n\n    def step_function(inputs, states):\n        outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n        return (outputs, states)\n    inputs_vals = np.random.random((num_samples, num_timesteps, num_features))\n    initial_state_vals = np.random.random((num_samples, 6))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[-1, -1] = 0\n    expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n    expected_outputs[-1, -1] = expected_outputs[-1, -2]\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_state_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n    initial_state_vals = np.random.random((num_samples, 6, 7))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[0, -2:] = 0\n    expected_last_state = initial_state_vals.copy()\n    expected_last_state[0] += num_timesteps - 2\n    expected_last_state[1:] += num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(last_states[0]), expected_last_state)",
                            "@pytest.mark.parametrize('x_np,axis,keepdims', [(np.array([1.1, 0.8, 0.9]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 1, False), (np.array([[1.1, 0.8, 0.9]]), -1, False), (np.array([[1.1, 0.8, 0.9]]), 1, True), (np.array([[1.1], [1.2]]), 0, False), (np.array([[1.1], [1.2]]), 1, False), (np.array([[1.1], [1.2]]), -1, False), (np.array([[1.1], [1.2]]), -1, True), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), None, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 0, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False)])\ndef test_logsumexp(self, x_np, axis, keepdims):\n    \"\"\"\n    Check if K.logsumexp works properly for values close to one.\n    \"\"\"\n    x = K.variable(x_np)\n    assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)), np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)), rtol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='The optimization is applied only with TensorFlow.')\ndef test_logsumexp_optim(self):\n    \"\"\"\n    Check if optimization works.\n    \"\"\"\n    x_np = np.array([10000.0, 0.0001])\n    result = K.eval(K.logsumexp(K.variable(x_np), axis=0))\n    assert_allclose(result, 10000.0, rtol=1e-05)",
                            "def test_switch(self):\n    val = np.random.random()\n    z_list = []\n    for k in WITH_NP:\n        x = k.variable(val)\n        x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n        z_list.append(k.eval(x))\n    assert_list_pairwise(z_list)\n    shapes = []\n    shapes.append([(4, 3, 2), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4, 3), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4,), (4, 3, 2), (4, 3, 2)])\n    for s in shapes:\n        z_list = []\n        arrays = list(map(np.random.random, s))\n        for k in WITH_NP:\n            x, then_expr, else_expr = map(k.variable, arrays)\n            cond = k.greater_equal(x, 0.5)\n            z_list.append(k.eval(k.switch(cond, then_expr, else_expr)))\n        assert_list_pairwise(z_list)",
                            "def test_dropout(self):\n    val = np.random.random((100, 100))\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2)) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2, noise_shape=list(val.shape))) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    with pytest.raises(ValueError):\n        z = K.dropout(K.variable(val), level=-0.5)",
                            "@pytest.mark.parametrize('alpha,max_value,threshold', [(0.0, None, 0.0), (0.1, None, 0.0), (0.0, 5.0, 0.0), (0.0, None, 0.8), (0.1, 5.0, 0.0), (0.1, None, 0.8), (0.0, 5.0, 0.8), (0.1, 5.0, 0.8), (0.1, 0.0, 0.8), (0.1, 5.0, -2.8), (0.1, 9.0, 0.8)])\ndef test_relu(self, alpha, max_value, threshold):\n    check_single_tensor_operation('relu', (4, 2), WITH_NP, alpha=alpha, max_value=max_value, threshold=threshold)",
                            "def test_nn_operations(self):\n    check_single_tensor_operation('softsign', (4, 10), WITH_NP)\n    check_single_tensor_operation('softplus', (4, 10), WITH_NP)\n    check_single_tensor_operation('elu', (4, 10), WITH_NP, alpha=0.5)\n    check_single_tensor_operation('sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('hard_sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('tanh', (4, 2), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 10), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 5, 3), WITH_NP, axis=1)\n    check_single_tensor_operation('softmax', (4, 5, 3, 10), WITH_NP, axis=2)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    if K.backend() != 'cntk':\n        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841], [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n    yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016], [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n    check_two_tensor_operation('categorical_crossentropy', yval, xval, WITH_NP, cntk_two_dynamicity=True, from_logits=True)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)",
                            "def test_in_top_k(self):\n    batch_size = 20\n    num_classes = 10\n    predictions = np.random.random((batch_size, num_classes)).astype('float32')\n    targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n    for k in range(num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)\n    num_identical = num_classes // 2\n    for i in range(batch_size):\n        idx_identical = np.random.choice(num_classes, size=num_identical, replace=False)\n        predictions[i, idx_identical] = predictions[i, 0]\n    targets = np.zeros(batch_size, dtype='int32')\n    for k in range(1, num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('conv1d', (2, 8, 2), (3, 2, 3), 'same', 'channels_last'), ('conv1d', (1, 8, 2), (3, 2, 3), 'valid', 'channels_last'), ('conv1d', (1, 2, 8), (3, 2, 3), 'valid', 'channels_first'), ('conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'), ('conv3d', (2, 3, 4, 5, 4), (3, 3, 3, 3, 4), 'same', 'channels_first'), ('conv3d', (2, 3, 5, 4, 6), (3, 2, 4, 3, 4), 'valid', 'channels_first'), ('conv3d', (1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'valid', 'channels_last'), ('conv3d', (1, 3, 5, 4, 2), (3, 3, 3, 2, 3), 'same', 'channels_last')])\ndef test_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last'), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first')])\ndef test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format,dilation_rate', [('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2), ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2), ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2), 'same', 'channels_last', (2, 2)), ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4), 'valid', 'channels_first', (2, 2)), ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4), 'valid', 'channels_last', (2, 2, 2)), ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4), 'same', 'channels_first', (2, 2, 2))])\ndef test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv transpose on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last', (2, 2)), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first', (2, 2))])\ndef test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('depthwise_conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('depthwise_conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('depthwise_conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('depthwise_conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last')])\ndef test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,pool_size,strides,padding,data_format,pool_mode', [('pool2d', (2, 3, 7, 7), (3, 3), (1, 1), 'same', 'channels_first', 'avg'), ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1), 'valid', 'channels_first', 'max'), ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1), 'valid', 'channels_last', 'avg'), ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1), 'same', 'channels_last', 'max'), ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1), 'same', 'channels_first', 'avg'), ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1), 'valid', 'channels_first', 'max'), ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1), 'valid', 'channels_last', 'avg'), ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1), 'same', 'channels_last', 'max')])\ndef test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n    check_single_tensor_operation(op, input_shape, WITH_NP, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, pool_mode=pool_mode, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [('separable_conv1d', (2, 8, 2), (3,), 1, 'same', 'channels_last'), ('separable_conv1d', (1, 8, 2), (3,), 2, 'valid', 'channels_last'), ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'), ('separable_conv2d', (2, 3, 5, 6), (4, 3), 2, 'valid', 'channels_first'), ('separable_conv2d', (1, 6, 5, 3), (3, 4), 1, 'valid', 'channels_last'), ('separable_conv2d', (1, 7, 6, 3), (3, 3), 2, 'same', 'channels_last')])\ndef test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format):\n    if data_format == 'channels_first':\n        input_depth = input_shape[1]\n    else:\n        input_depth = input_shape[-1]\n    _, x = parse_shape_or_val(input_shape)\n    _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n    _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) + (input_depth * depth_multiplier, 7))\n    y1 = KNP.separable_conv(x, depthwise, pointwise, padding=padding, data_format=data_format)\n    if K.backend() == 'cntk':\n        _, cntk_func = cntk_func_tensors(op, [input_shape, depthwise, pointwise], padding=padding, data_format=data_format)\n        y2 = cntk_func([x])[0]\n    else:\n        y2 = K.eval(getattr(K, op)(K.variable(x), K.variable(depthwise), K.variable(pointwise), padding=padding, data_format=data_format))\n    assert_allclose(y1, y2, atol=1e-05)",
                            "def test_random_normal(self):\n    for mean, std in [(0.0, 1.0), (-10.0, 5.0)]:\n        rand = K.eval(K.random_normal((300, 200), mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n        assert np.abs(np.mean(rand) - mean) < std * 0.015\n        assert np.abs(np.std(rand) - std) < std * 0.015\n        r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - mean) < std * 0.015\n        assert np.abs(np.std(samples) - std) < std * 0.015",
                            "def test_random_uniform(self):\n    min_val = -1.0\n    max_val = 1.0\n    rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand)) < 0.015\n    assert max_val - 0.015 < np.max(rand) <= max_val\n    assert min_val + 0.015 > np.min(rand) >= min_val\n    r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples)) < 0.015\n    assert max_val - 0.015 < np.max(samples) <= max_val\n    assert min_val + 0.015 > np.min(samples) >= min_val",
                            "def test_random_binomial(self):\n    p = 0.5\n    rand = K.eval(K.random_binomial((200, 100), p))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand) - p) < 0.015\n    assert np.max(rand) == 1\n    assert np.min(rand) == 0\n    r = K.random_binomial((10, 10), p)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples) - p) < 0.015\n    assert np.max(samples) == 1\n    assert np.min(samples) == 0",
                            "def test_truncated_normal(self):\n    mean = 0.0\n    std = 1.0\n    min_val = -2.0\n    max_val = 2.0\n    rand = K.eval(K.truncated_normal((300, 200), mean=mean, stddev=std, seed=1337))\n    assert rand.shape == (300, 200)\n    assert np.abs(np.mean(rand) - mean) < 0.015\n    assert np.max(rand) <= max_val\n    assert np.min(rand) >= min_val\n    assert np.abs(np.std(rand) - std * 0.87962) < 0.015",
                            "def test_conv_invalid_use(self):\n    dummy_x_1d = K.variable(np.ones((4, 8, 2)))\n    dummy_w_1d = K.variable(np.ones((3, 2, 3)))\n    dummy_x_2d = K.variable(np.ones((2, 3, 4, 5)))\n    dummy_w_2d = K.variable(np.ones((2, 2, 3, 4)))\n    dummy_x_3d = K.variable(np.ones((2, 3, 4, 5, 4)))\n    dummy_w_3d = K.variable(np.ones((2, 2, 2, 3, 4)))\n    dummy_w1x1_2d = K.variable(np.ones((1, 1, 12, 7)))\n    with pytest.raises(ValueError):\n        K.conv1d(dummy_x_1d, dummy_w_1d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv3d(dummy_x_3d, dummy_w_3d, data_format='channels_middle')\n    if K.backend() != 'theano':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    if K.backend() == 'cntk':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, strides=(2, 2), dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, strides=(2, 2), dilation_rate=(1, 2))",
                            "def test_pooling_invalid_use(self):\n    for input_shape, pool_size in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n        x = K.variable(np.random.random(input_shape))\n        if len(pool_size) == 2:\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, pool_mode='median')\n        else:\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, pool_mode='median')",
                            "def test_resize_images(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_images', x_shape, WITH_NP, cntk_dynamicity=True, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_images(K.variable(xval), 2, 2, data_format='channels_middle')",
                            "@staticmethod\ndef _helper_bilinear(data_format, height_factor, width_factor):\n    x_shape = (2, 3, 4, 5)\n    check_single_tensor_operation('resize_images', x_shape, [KTF, KTH], height_factor=height_factor, width_factor=width_factor, data_format=data_format, interpolation='bilinear')",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\ndef test_resize_images_bilinear(self, data_format):\n    self._helper_bilinear(data_format, 2, 2)\n    with pytest.raises(NotImplementedError):\n        self._helper_bilinear(data_format, 4, 4)",
                            "def test_resize_volumes(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_volumes', x_shape, WITH_NP, cntk_dynamicity=True, depth_factor=2, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_volumes(K.variable(xval), 2, 2, 2, data_format='channels_middle')",
                            "def test_temporal_padding(self):\n    check_single_tensor_operation('temporal_padding', (4, 3, 3), WITH_NP)\n    check_single_tensor_operation('temporal_padding', (2, 3, 4), WITH_NP, padding=(1, 2))",
                            "def test_spatial_2d_padding(self):\n    padding = ((1, 2), (2, 1))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, 1))\n        y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_2d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_spatial_3d_padding(self):\n    padding = ((1, 2), (2, 1), (1, 2))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, None, 1))\n        y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_3d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_bias_add(self):\n    for data_format in ['channels_first', 'channels_last']:\n        for shape in [(), (3,), (2, 3), (5, 3, 2)]:\n            if data_format == 'channels_first':\n                x_shape = (1, 4) + shape\n            else:\n                x_shape = (1,) + shape + (4,)\n            bias_shape = (4,)\n            check_two_tensor_operation('bias_add', x_shape, bias_shape, WITH_NP, cntk_dynamicity=True, data_format=data_format)\n        if data_format == 'channels_first':\n            x_shape = (20, 6, 10)\n        else:\n            x_shape = (20, 10, 6)\n        check_two_tensor_operation('bias_add', x_shape, (10, 6), WITH_NP, cntk_dynamicity=True, data_format=data_format)\n    x = K.variable(np.random.random(x_shape))\n    b = K.variable(np.random.random(bias_shape))\n    with pytest.raises(ValueError):\n        K.bias_add(x, b, data_format='channels_middle')",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='Specific to Theano.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_th(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes='per-activation')\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Specific to Tensorflow.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_tf(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'cntk', reason='Specific to CNTK.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_cntk(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.placeholder(x_shape)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.function([x], [z])([x_val])[0]\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_ctc(self):\n    if K.backend() == 'theano':\n        ref = [1.73308, 3.81351]\n    else:\n        ref = [3.34211, 5.42262]\n    label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n    input_lens = np.expand_dims(np.asarray([5, 5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]], [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508], [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549], [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456], [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345], [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)\n    if K.backend() == 'theano':\n        ref = [1.73308]\n    else:\n        ref = [3.34211]\n    input_lens = np.expand_dims(np.asarray([5]), 1)\n    label_lens = np.expand_dims(np.asarray([5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Test adapted from tensorflow.')\ndef test_ctc_decode_greedy(self):\n    \"\"\"Test two batch entries - best path decoder.\"\"\"\n    max_time_steps = 6\n    seq_len_0 = 4\n    input_prob_matrix_0 = np.asarray([[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.4, 0.6], [0.0, 0.0, 0.4, 0.6], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    seq_len_1 = 5\n    input_prob_matrix_1 = np.asarray([[0.1, 0.9, 0.0, 0.0], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.1, 0.9], [0.0, 0.9, 0.1, 0.1], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    inputs = [np.vstack([input_prob_matrix_0[t, :], input_prob_matrix_1[t, :]]) for t in range(max_time_steps)]\n    inputs = np.asarray(inputs).transpose((1, 0, 2))\n    input_length = np.array([seq_len_0, seq_len_1], dtype=np.int32)\n    decode_pred_np, log_prob_pred_np = KNP.ctc_decode(inputs, input_length, greedy=True)\n    inputs = K.variable(inputs)\n    input_length = K.variable(input_length)\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=True)\n    assert len(decode_pred_tf) == 1\n    decode_pred = K.eval(decode_pred_tf[0])\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    assert np.alltrue(decode_pred_np == decode_pred)\n    assert np.allclose(log_prob_pred_np, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='tensorflow-way slice is only supported in tensorflow.')\n@pytest.mark.parametrize('x_size', [[1, 1, 3], [1, 2, 3], [2, 1, 3]])\ndef test_slice(self, x_size):\n    npt = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])\n    x_start = [1, 0, 0]\n    tft = K.constant(npt)\n    test_input = K.eval(K.slice(tft, x_start, x_size))\n    expected = KNP.slice(npt, x_start, x_size)\n    assert np.allclose(test_input, expected)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search(self):\n    \"\"\"Test one batch, two beams - hibernating beam search.\"\"\"\n    depth = 6\n    seq_len_0 = 5\n    input_prob_matrix_0 = np.asarray([[0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908], [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517], [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763], [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655], [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878], [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]], dtype=np.float32)\n    input_prob_matrix_0 = input_prob_matrix_0 + 2.0\n    inputs = [input_prob_matrix_0[t, :][np.newaxis, :] for t in range(seq_len_0)] + 2 * [np.zeros((1, depth), dtype=np.float32)]\n    inputs = np.exp(inputs)\n    inputs = K.variable(inputs.transpose((1, 0, 2)))\n    input_length = K.variable(np.array([seq_len_0], dtype=np.int32))\n    log_prob_truth = np.array([-5.811451, -6.63339], np.float32)[np.newaxis, :]\n    decode_truth = [np.array([1, 0]), np.array([[1]])]\n    beam_width = 2\n    top_paths = 2\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=False, beam_width=beam_width, top_paths=top_paths)\n    assert len(decode_pred_tf) == top_paths\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    for i in range(top_paths):\n        assert np.alltrue(decode_truth[i] == K.eval(decode_pred_tf[i]))\n    assert np.allclose(log_prob_truth, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search_no_merge(self):\n    input_prob = np.array([[[0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]])\n    input_len = np.array(input_prob.shape[0] * [input_prob.shape[1]])\n\n    def decode(merge_repeated):\n        input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n        input_len_tensor = K.placeholder(shape=None, dtype='int64')\n        paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n        decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n        paths = decode_func([input_prob, input_len])\n        return paths\n    assert np.allclose(decode(merge_repeated=True), [np.array([[0, 1]])])\n    assert np.allclose(decode(merge_repeated=False), [np.array([[0, 0, 1, 1]])])",
                            "def test_one_hot(self):\n    input_length = 10\n    num_classes = 20\n    batch_size = 30\n    indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n    oh = KNP.one_hot(np.int32(indices), num_classes)\n    koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n    assert np.all(koh == oh)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_dot(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense = x_sparse.toarray()\n    W = np.random.random((5, 4))\n    t_W = K.variable(W)\n    k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n    k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n    assert k_s.shape == k_d.shape\n    assert_allclose(k_s, k_d, atol=1e-05)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_concat(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_1 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_2 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense_1 = x_sparse_1.toarray()\n    x_dense_2 = x_sparse_2.toarray()\n    k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n    assert K.is_sparse(k_s)\n    k_s_d = K.eval(k_s)\n    k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n    assert k_s_d.shape == k_d.shape\n    assert_allclose(k_s_d, k_d, atol=1e-05)",
                            "def test_stack(self):\n    tensor_list = [np.random.randn(5, 4, 6, 10) for _ in range(5)]\n    stack_axis = 3\n    results = []\n    if WITH_NP[0] == KC:\n        check_two_tensor_operation('stack', (5, 4, 6, 10), (5, 4, 6, 10), WITH_NP, axis=stack_axis, concat_args=True)\n    else:\n        for k in WITH_NP:\n            tensor_list_var = [k.variable(tensor) for tensor in tensor_list]\n            out = k.eval(k.stack(tensor_list_var, axis=stack_axis))\n            results.append(out)\n        assert_list_pairwise(results)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_map(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    vx = K.variable(x)\n    kx = K.eval(K.map_fn(K.sum, vx))\n    kx2 = K.eval(K.map_fn(lambda i: K.sum(vx[i]), K.arange(10), dtype=K.floatx()))\n    assert (10,) == kx.shape\n    assert (10,) == kx2.shape\n    assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n    assert_allclose(kx, kx2, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldl(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n    assert (3,) == kx.shape\n    assert_allclose(x.sum(axis=0), kx, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldr(self):\n    x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n    vx = K.variable(x)\n    p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n    p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n    assert p1 < p2\n    assert 9e-38 < p2 <= 1e-37",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk has issues with negative number.')\ndef test_arange(self):\n    for test_value in (-20, 0, 1, 10):\n        a_list = []\n        dtype_list = []\n        for k in WITH_NP:\n            t = k.arange(test_value)\n            a = k.eval(t)\n            assert np.array_equal(a, np.arange(test_value))\n            dtype_list.append(k.dtype(t))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for start, stop, step in ((0, 5, 1), (-5, 5, 2), (0, 1, 2)):\n        a_list = []\n        for k in WITH_NP:\n            a = k.eval(k.arange(start, stop, step))\n            assert np.array_equal(a, np.arange(start, stop, step))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for dtype in ('int32', 'int64', 'float32', 'float64'):\n        for k in WITH_NP:\n            t = k.arange(10, dtype=dtype)\n            assert k.dtype(t) == dtype\n    start = K.constant(1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 1\n    start = K.constant(-1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 0",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_train_phase(self, training):\n    check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_test_phase(self, training):\n    check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "def test_setfloatx_incorrect_values(self):\n    old_floatx = floatx()\n    initial = floatx()\n    for value in ['', 'beerfloat', 123]:\n        with pytest.raises(ValueError):\n            set_floatx(value)\n    assert floatx() == initial\n    set_floatx(old_floatx)",
                            "def DISABLED_test_setfloatx_correct_values(self):\n    \"\"\"Disabled because it is not thread safe at this time.\"\"\"\n    old_floatx = floatx()\n    for value in ['float16', 'float32', 'float64']:\n        set_floatx(value)\n        assert floatx() == value\n    set_floatx(old_floatx)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support float16')\ndef DISABLED_test_set_floatx(self):\n    \"\"\"Disabled because it is not thread safe at this time.\n\n    Make sure that changes to the global floatx are effectively\n    taken into account by the backend.\n    \"\"\"\n    old_floatx = floatx()\n    set_floatx('float16')\n    var = variable([10])\n    check_dtype(var, 'float16')\n    set_floatx('float64')\n    var = variable([10])\n    check_dtype(var, 'float64')\n    set_floatx(old_floatx)",
                            "def test_dtype(self):\n    assert K.dtype(K.variable(1, dtype='float64')) == 'float64'\n    assert K.dtype(K.variable(1, dtype='float32')) == 'float32'\n    assert K.dtype(K.variable(1, dtype='float16')) == 'float16'",
                            "def test_variable_support_bool_dtype(self):\n    if K.backend() == 'tensorflow':\n        assert K.dtype(K.variable(1, dtype='int16')) == 'int16'\n        assert K.dtype(K.variable(False, dtype='bool')) == 'bool'\n        with pytest.raises(TypeError):\n            K.variable('', dtype='unsupported')",
                            "def test_clip_supports_tensor_arguments(self):\n    x = K.variable([-10.0, -5.0, 0.0, 5.0, 10.0])\n    min_value = K.variable([-5.0, -4.0, 0.0, 3.0, 5.0])\n    max_value = K.variable([5.0, 4.0, 1.0, 4.0, 9.0])\n    assert np.allclose(K.eval(K.clip(x, min_value, max_value)), np.asarray([-5.0, -4.0, 0.0, 4.0, 9.0], dtype=np.float32))",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(), reason='This test is for tensorflow parallelism.')\ndef test_tensorflow_session_parallelism_settings(self, monkeypatch):\n    for threads in [1, 2]:\n        K.clear_session()\n        monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n        cfg = K.get_session()._config\n        assert cfg.intra_op_parallelism_threads == threads\n        assert cfg.inter_op_parallelism_threads == threads",
                            "def batch_shape(shape):\n    return (batch_size,) + shape[1:]",
                            "def random(shape):\n    return np.random.random(batch_shape(shape))",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn(inputs, states):\n        assert len(states) == 1\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y])\n    return simple_rnn",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_with_extra_mock_state(inputs, states):\n        assert len(states) == 2\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y, backend.concatenate([y, y], axis=-1)])\n    return simple_rnn_with_extra_mock_state",
                            "def get_step_function(backend, w_i):\n\n    def simple_no_states(inputs, states):\n        assert len(states) == 0\n        y = backend.dot(inputs, w_i)\n        return (y, [])\n    return simple_no_states",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_add_constant(inputs, states_and_constants):\n        [h, c] = states_and_constants\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n        return (y, [y])\n    return simple_rnn_add_constant",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def step_function(inputs, states):\n    outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n    return (outputs, states)",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def decode(merge_repeated):\n    input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n    input_len_tensor = K.placeholder(shape=None, dtype='int64')\n    paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n    decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n    paths = decode_func([input_prob, input_len])\n    return paths",
                            "def simple_rnn(inputs, states):\n    assert len(states) == 1\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y])",
                            "def simple_rnn_with_extra_mock_state(inputs, states):\n    assert len(states) == 2\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y, backend.concatenate([y, y], axis=-1)])",
                            "def simple_no_states(inputs, states):\n    assert len(states) == 0\n    y = backend.dot(inputs, w_i)\n    return (y, [])",
                            "def simple_rnn_add_constant(inputs, states_and_constants):\n    [h, c] = states_and_constants\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n    return (y, [y])"
                        ],
                        "constructor_variables": [],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "test_is_keras_tensor(self)",
                            "test_set_learning_phase(self)",
                            "test_eye(self)",
                            "test_ones(self)",
                            "test_zeros(self)",
                            "test_ones_like(self)",
                            "test_zeros_like(self)",
                            "test_linear_operations(self)",
                            "test_random_variables(self)",
                            "test_batch_dot_shape(self)",
                            "test_shape_operations(self)",
                            "test_none_shape_operations(self)",
                            "test_repeat_elements(self)",
                            "test_tile(self)",
                            "test_gather(self)",
                            "test_value_manipulation(self, function_name)",
                            "test_print_tensor(self)",
                            "test_elementwise_operations(self)",
                            "test_reset_uids(self)",
                            "test_cumsum_cumprod(self)",
                            "test_log(self)",
                            "test_update_add(self)",
                            "test_update_sub(self)",
                            "test_gradient(self)",
                            "test_stop_gradient(self)",
                            "test_function(self)",
                            "test_function_tf_fetches(self)",
                            "test_function_tf_feed_dict(self)",
                            "test_function_tf_run_options_with_run_metadata(self)",
                            "test_function_tf_string_input(self)",
                            "test_rnn(self)",
                            "test_rnn_additional_states(self)",
                            "test_rnn_no_states(self)",
                            "test_rnn_constants(self)",
                            "test_rnn_output_and_state_masking_independent(self)",
                            "test_rnn_output_num_dim_larger_than_2_masking(self)",
                            "test_rnn_state_num_dim_larger_than_2_masking(self)",
                            "test_logsumexp(self, x_np, axis, keepdims)",
                            "test_logsumexp_optim(self)",
                            "test_switch(self)",
                            "test_dropout(self)",
                            "test_relu(self, alpha, max_value, threshold)",
                            "test_nn_operations(self)",
                            "test_in_top_k(self)",
                            "test_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format)",
                            "test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate)",
                            "test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate)",
                            "test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode)",
                            "test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format)",
                            "test_random_normal(self)",
                            "test_random_uniform(self)",
                            "test_random_binomial(self)",
                            "test_truncated_normal(self)",
                            "test_conv_invalid_use(self)",
                            "test_pooling_invalid_use(self)",
                            "test_resize_images(self)",
                            "_helper_bilinear(data_format, height_factor, width_factor)",
                            "test_resize_images_bilinear(self, data_format)",
                            "test_resize_volumes(self)",
                            "test_temporal_padding(self)",
                            "test_spatial_2d_padding(self)",
                            "test_spatial_3d_padding(self)",
                            "test_bias_add(self)",
                            "test_batchnorm_th(self, x_shape)",
                            "test_batchnorm_tf(self, x_shape)",
                            "test_batchnorm_cntk(self, x_shape)",
                            "test_ctc(self)",
                            "test_ctc_decode_greedy(self)",
                            "test_slice(self, x_size)",
                            "test_ctc_decode_beam_search(self)",
                            "test_ctc_decode_beam_search_no_merge(self)",
                            "test_one_hot(self)",
                            "test_sparse_dot(self)",
                            "test_sparse_concat(self)",
                            "test_stack(self)",
                            "test_map(self)",
                            "test_foldl(self)",
                            "test_foldr(self)",
                            "test_arange(self)",
                            "test_in_train_phase(self, training)",
                            "test_in_test_phase(self, training)",
                            "test_setfloatx_incorrect_values(self)",
                            "DISABLED_test_setfloatx_correct_values(self)",
                            "DISABLED_test_set_floatx(self)",
                            "test_dtype(self)",
                            "test_variable_support_bool_dtype(self)",
                            "test_clip_supports_tensor_arguments(self)",
                            "test_tensorflow_session_parallelism_settings(self, monkeypatch)",
                            "batch_shape(shape)",
                            "random(shape)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i)",
                            "get_step_function(backend, w_i, w_h)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "decode(merge_repeated)",
                            "simple_rnn(inputs, states)",
                            "simple_rnn_with_extra_mock_state(inputs, states)",
                            "simple_no_states(inputs, states)",
                            "simple_rnn_add_constant(inputs, states_and_constants)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "test_update_add",
                    "function_code": "@pytest.mark.skipif(K.backend() == 'theano',\n                    reason='theano returns tuples for update ops')\ndef test_update_add(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    increment = np.random.randn(3, 4)\n\n    x += increment\n    K.eval(K.update_add(x_var, increment))\n\n    assert_allclose(x, K.eval(x_var), atol=1e-05)\n",
                    "decorators": [
                        "pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')"
                    ],
                    "docstring": null,
                    "start_line": 582,
                    "end_line": 592,
                    "variables": {
                        "x": [
                            592,
                            585,
                            586,
                            589
                        ],
                        "np.random.randn": [
                            585,
                            587
                        ],
                        "np.random": [
                            585,
                            587
                        ],
                        "np": [
                            585,
                            587
                        ],
                        "x_var": [
                            592,
                            586,
                            590
                        ],
                        "K.variable": [
                            586
                        ],
                        "K": [
                            592,
                            582,
                            586,
                            590
                        ],
                        "increment": [
                            587,
                            589,
                            590
                        ],
                        "K.eval": [
                            592,
                            590
                        ],
                        "K.update_add": [
                            590
                        ],
                        "assert_allclose": [
                            592
                        ],
                        "pytest.mark.skipif": [
                            582
                        ],
                        "pytest.mark": [
                            582
                        ],
                        "pytest": [
                            582
                        ],
                        "K.backend": [
                            582
                        ]
                    },
                    "filtered_variables": {
                        "x": [
                            592,
                            585,
                            586,
                            589
                        ],
                        "np.random.randn": [
                            585,
                            587
                        ],
                        "np.random": [
                            585,
                            587
                        ],
                        "np": [
                            585,
                            587
                        ],
                        "x_var": [
                            592,
                            586,
                            590
                        ],
                        "K.variable": [
                            586
                        ],
                        "K": [
                            592,
                            582,
                            586,
                            590
                        ],
                        "increment": [
                            587,
                            589,
                            590
                        ],
                        "K.eval": [
                            592,
                            590
                        ],
                        "K.update_add": [
                            590
                        ],
                        "assert_allclose": [
                            592
                        ],
                        "pytest.mark.skipif": [
                            582
                        ],
                        "pytest.mark": [
                            582
                        ],
                        "pytest": [
                            582
                        ],
                        "K.backend": [
                            582
                        ]
                    },
                    "diff_line_number": 585,
                    "class_data": {
                        "signature": "class TestBackend(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def test_is_keras_tensor(self):\n    np_var = np.array([1, 2])\n    with pytest.raises(ValueError):\n        K.is_keras_tensor(np_var)\n    keras_var = K.variable(np_var)\n    assert K.is_keras_tensor(keras_var) is False\n    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n    assert K.is_keras_tensor(keras_placeholder) is False",
                            "def test_set_learning_phase(self):\n    with pytest.raises(ValueError):\n        K.set_learning_phase(2)",
                            "def test_eye(self):\n    check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)",
                            "def test_ones(self):\n    check_single_tensor_operation('ones', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_zeros(self):\n    check_single_tensor_operation('zeros', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_ones_like(self):\n    check_single_tensor_operation('ones_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_zeros_like(self):\n    check_single_tensor_operation('zeros_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_linear_operations(self):\n    check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n    check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=(1, 1))\n    check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n    check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n    if K.backend() != 'cntk':\n        check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=(1, 2))",
                            "def test_random_variables(self):\n    check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP, low=0.0, high=1.0, shape_or_val=False, assert_value_equality=False)\n    check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP, mean=0.0, scale=1.0, shape_or_val=False, assert_value_equality=False)",
                            "def test_batch_dot_shape(self):\n    test_cases = []\n    test_cases.append([(None, 3, 4, 5), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 3, 4, 5), (None, 2, 4), 2])\n    test_cases.append([(None, 3, 4), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 4, 3), (None, 3, 5), (2, 1)])\n    test_cases.append([(None, 4), (None, 3, 4), (1, 2)])\n    test_cases.append([(None, 4), (None, 4), None])\n    batch_size = 7\n\n    def batch_shape(shape):\n        return (batch_size,) + shape[1:]\n\n    def random(shape):\n        return np.random.random(batch_shape(shape))\n    for x_shape, y_shape, axes in test_cases:\n        x_np = random(x_shape)\n        y_np = random(y_shape)\n        z_np = KNP.batch_dot(x_np, y_np, axes)\n        x = K.placeholder(shape=x_shape)\n        y = K.placeholder(shape=y_shape)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        f = K.function([x, y], [z])\n        assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        if K.backend() != 'cntk':\n            x = K.placeholder(ndim=len(x_shape))\n            y = K.placeholder(ndim=len(y_shape))\n            z = K.batch_dot(x, y, axes)\n            z_shape = K.int_shape(z)\n            if z_shape is not None:\n                assert len(z_shape) == z_np.ndim\n                assert set(z_shape) <= set((None, 1))\n            f = K.function([x, y], [z])\n            assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        x = K.variable(x_np)\n        y = K.variable(y_np)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        z = K.eval(z)\n        assert_allclose(z, z_np, atol=1e-05)",
                            "def test_shape_operations(self):\n    check_two_tensor_operation('concatenate', (4, 3), (4, 2), WITH_NP, axis=-1, concat_args=True)\n    check_single_tensor_operation('reshape', (4, 2), WITH_NP, shape=(8, 1))\n    check_single_tensor_operation('permute_dimensions', (4, 2, 3), WITH_NP, pattern=(2, 0, 1))\n    check_single_tensor_operation('repeat', (4, 1), WITH_NP, n=3)\n    check_single_tensor_operation('flatten', (4, 1), WITH_NP)\n    check_single_tensor_operation('batch_flatten', (20, 2, 5), WITH_NP, cntk_dynamicity=True)\n    check_single_tensor_operation('expand_dims', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('expand_dims', (4, 3, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('squeeze', (4, 3, 1), WITH_NP, axis=2)\n    check_single_tensor_operation('squeeze', (4, 1, 1), WITH_NP, axis=1)\n    check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)}, 'squeeze', {'axis': 2}, (4, 3, 1, 1), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='We only test the shape inference of the theano backend.')\ndef test_none_shape_operations(self):\n    x = K.placeholder((3, None, 4))\n    y = K.batch_flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (3, None)\n    y = K.flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (None,)",
                            "def test_repeat_elements(self):\n    reps = 3\n    for ndims in [1, 2, 3]:\n        shape = np.arange(2, 2 + ndims)\n        arr = np.arange(np.prod(shape)).reshape(shape)\n        for rep_axis in range(ndims):\n            check_single_tensor_operation('repeat_elements', arr, WITH_NP, rep=reps, axis=rep_axis)\n            if K.backend() != 'cntk':\n                shape = list(shape)\n                shape[rep_axis] = None\n                x = K.placeholder(shape=shape)\n                y = K.repeat_elements(x, reps, axis=rep_axis)\n                assert y._keras_shape == tuple(shape)\n                assert y._keras_shape == K.int_shape(y)",
                            "def test_tile(self):\n    shape = (3, 4)\n    arr = np.arange(np.prod(shape)).reshape(shape)\n    check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n    check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 4))\n        n = 2\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 8)\n        n = (4, 3)\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 12)",
                            "def test_gather(self):\n    shape = (10, 2, 3)\n    ref = np.arange(np.prod(shape)).reshape(shape)\n    inds = [1, 3, 7, 9]\n    t_list = [k.gather(k.variable(ref), k.variable(inds, dtype='int32')) for k in WITH_NP]\n    z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32'))) for k in WITH_NP]\n    assert_list_pairwise(z_list)\n    assert_list_keras_shape(t_list, z_list)\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 3, 4))\n        indices = K.placeholder(shape=(5, 6), dtype='int32')\n        y = K.gather(x, indices)\n        assert y._keras_shape == (5, 6, 3, 4)",
                            "@pytest.mark.parametrize('function_name', ['get_value', 'count_params', 'int_shape', 'get_variable_shape'])\ndef test_value_manipulation(self, function_name):\n    val = np.random.random((4, 2))\n    v_list = [getattr(k, function_name)(k.variable(val)) for k in WITH_NP]\n    if function_name == 'get_value':\n        assert_list_pairwise(v_list)\n    else:\n        assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)",
                            "def test_print_tensor(self):\n    check_single_tensor_operation('print_tensor', (), WITH_NP)\n    check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n    check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n    check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)",
                            "def test_elementwise_operations(self):\n    check_single_tensor_operation('max', (4, 2), WITH_NP)\n    check_single_tensor_operation('max', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('max', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('min', (4, 2), WITH_NP)\n    check_single_tensor_operation('min', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('min', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('mean', (4, 2), WITH_NP)\n    check_single_tensor_operation('mean', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=-1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('var', (4, 2), WITH_NP)\n    check_single_tensor_operation('var', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('var', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('std', (4, 2), WITH_NP)\n    check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('prod', (4, 2), WITH_NP)\n    check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('any', (4, 2), WITH_NP)\n    check_single_tensor_operation('any', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('any', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('all', (4, 2), WITH_NP)\n    check_single_tensor_operation('all', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('all', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('square', (4, 2), WITH_NP)\n    check_single_tensor_operation('abs', (4, 2), WITH_NP)\n    check_single_tensor_operation('sqrt', (4, 2), WITH_NP)\n    check_single_tensor_operation('exp', (4, 2), WITH_NP)\n    check_single_tensor_operation('round', (4, 2), WITH_NP)\n    check_single_tensor_operation('sign', (4, 2), WITH_NP)\n    check_single_tensor_operation('pow', (4, 2), WITH_NP, a=3)\n    check_single_tensor_operation('clip', (4, 2), WITH_NP, min_value=0.4, max_value=0.6)\n    check_single_tensor_operation('cos', (4, 2), WITH_NP)\n    check_single_tensor_operation('sin', (4, 2), WITH_NP)\n    check_two_tensor_operation('equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('not_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('maximum', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('minimum', (4, 2), (4, 2), WITH_NP)",
                            "def test_reset_uids(self):\n    first = K.get_uid()\n    K.get_uid()\n    K.reset_uids()\n    assert K.get_uid() == first",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support cumsum and cumprod yet')\ndef test_cumsum_cumprod(self):\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP, axis=1)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk return -85.1 for zero or negative number, not nan, so can't compare with other backend.\")\ndef test_log(self):\n    check_single_tensor_operation('log', (4, 2), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_add(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    increment = np.random.randn(3, 4)\n    x += increment\n    K.eval(K.update_add(x_var, increment))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_sub(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    decrement = np.random.randn(3, 4)\n    x -= decrement\n    K.eval(K.update_sub(x_var, decrement))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk doesn't support gradient in this way.\")\ndef test_gradient(self):\n    val = np.random.random((4, 2))\n    x_list = [k.variable(val) for k in [KTH, KTF]]\n    z_list = []\n    zero_list = []\n    for x, k in zip(x_list, [KTH, KTF]):\n        exp = x * k.exp(x)\n        loss = k.sum(exp)\n        zero_loss = k.stop_gradient(loss)\n        grad = k.gradients(loss, [exp])\n        zero_grad = k.gradients(loss + zero_loss, [exp])\n        z_list.append(k.eval(grad[0]))\n        zero_list.append(k.eval(zero_grad[0]))\n    assert_list_pairwise(z_list)\n    assert_list_pairwise(zero_list)\n    for i in range(len(z_list)):\n        assert_allclose(zero_list[i], z_list[i], atol=1e-05)",
                            "def test_stop_gradient(self):\n    val = np.random.random((4, 2))\n    a = K.variable(val)\n    b = K.square(a)\n    c, d = K.stop_gradient([a, b])\n    e = K.stop_gradient(b)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk currently not support function in this way, so can't test as this.\")\ndef test_function(self):\n    test_backend = [KTH, KTF]\n    val = np.random.random((4, 2))\n    input_val = np.random.random((4, 2))\n    f_list = []\n    x_list = []\n    for k in test_backend:\n        x = k.variable(val)\n        x_list.append(x)\n        y = k.placeholder(ndim=2)\n        exp = k.square(x) + y\n        update = x * 2\n        f = k.function([y], [exp], updates=[(x, update)])\n        f_list.append(f)\n    function_outputs_list = [f([input_val])[0] for f in f_list]\n    assert_list_pairwise(function_outputs_list)\n    new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n    assert_list_pairwise(new_val_list)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `fetches` argument.')\ndef test_function_tf_fetches(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], updates=[(x, x_placeholder + 1.0)], fetches=[K.update(y, 5.0)])\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert K.get_session().run(fetches=[x, y]) == [11.0, 5.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `feed_dict` argument.')\ndef test_function_tf_feed_dict(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    feed_dict = {y_placeholder: 3.0}\n    f = K.function(inputs=[x_placeholder], outputs=[x_placeholder + 1.0], updates=[(x, x_placeholder + 10.0)], feed_dict=feed_dict, fetches=[K.update(y, y_placeholder * 10.0)])\n    output = f([10.0])\n    assert output == [11.0]\n    assert K.get_session().run(fetches=[x, y]) == [20.0, 30.0]\n    feed_dict[y_placeholder] = 4.0\n    output = f([20.0])\n    assert output == [21.0]\n    assert K.get_session().run(fetches=[x, y]) == [30.0, 40.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `options` and `run_metadata` arguments.')\ndef test_function_tf_run_options_with_run_metadata(self):\n    from tensorflow.core.protobuf import config_pb2\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    run_options = config_pb2.RunOptions(output_partition_graphs=True)\n    run_metadata = config_pb2.RunMetadata()\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], options=run_options, run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) > 0\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) == 0",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `string` type for a tensor.')\ndef test_function_tf_string_input(self):\n    x_placeholder = K.placeholder(shape=(), dtype='string')\n    x_identity = K.identity(x_placeholder)\n    f = K.function(inputs=[x_placeholder], outputs=[x_identity])\n    output = f([b'test'])\n    assert output == [b'test']",
                            "def test_rnn(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn(inputs, states):\n            assert len(states) == 1\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y])\n        return simple_rnn\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_additional_states(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    h1 = np.concatenate([h0, h0], axis=-1)\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_with_extra_mock_state(inputs, states):\n            assert len(states) == 2\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y, backend.concatenate([y, y], axis=-1)])\n        return simple_rnn_with_extra_mock_state\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0, h1], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_no_states(self):\n    num_samples = 3\n    input_dim = 8\n    output_dim = 4\n    timesteps = 5\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n\n    def get_step_function(backend, w_i):\n\n        def simple_no_states(inputs, states):\n            assert len(states) == 0\n            y = backend.dot(inputs, w_i)\n            return (y, [])\n        return simple_no_states\n    kwargs_list = [{'go_backwards': False}, {'go_backwards': True}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k), step_function_np=get_step_function(KNP, wi), inputs_np=x, initial_states_np=[], mask_np=None, **kwargs)",
                            "def test_rnn_constants(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, c = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_add_constant(inputs, states_and_constants):\n            [h, c] = states_and_constants\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n            return (y, [y])\n        return simple_rnn_add_constant\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), constants_np=[c], **kwargs)",
                            "def test_rnn_output_and_state_masking_independent(self):\n    num_samples = 2\n    num_timesteps = 4\n    state_and_io_size = 5\n    mask_last_num_timesteps = 2\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, state_and_io_size))\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[1, -mask_last_num_timesteps:] = 0\n    expected_outputs = inputs_vals.copy()\n    expected_outputs[1, -mask_last_num_timesteps:] = expected_outputs[1, -(mask_last_num_timesteps + 1)]\n    expected_state = initial_state_vals.copy()\n    expected_state[0] += num_timesteps\n    expected_state[1] += num_timesteps - mask_last_num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)\n        assert_allclose(K.eval(last_states[0]), expected_state)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_output_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n    num_features = 5\n\n    def step_function(inputs, states):\n        outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n        return (outputs, states)\n    inputs_vals = np.random.random((num_samples, num_timesteps, num_features))\n    initial_state_vals = np.random.random((num_samples, 6))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[-1, -1] = 0\n    expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n    expected_outputs[-1, -1] = expected_outputs[-1, -2]\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_state_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n    initial_state_vals = np.random.random((num_samples, 6, 7))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[0, -2:] = 0\n    expected_last_state = initial_state_vals.copy()\n    expected_last_state[0] += num_timesteps - 2\n    expected_last_state[1:] += num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(last_states[0]), expected_last_state)",
                            "@pytest.mark.parametrize('x_np,axis,keepdims', [(np.array([1.1, 0.8, 0.9]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 1, False), (np.array([[1.1, 0.8, 0.9]]), -1, False), (np.array([[1.1, 0.8, 0.9]]), 1, True), (np.array([[1.1], [1.2]]), 0, False), (np.array([[1.1], [1.2]]), 1, False), (np.array([[1.1], [1.2]]), -1, False), (np.array([[1.1], [1.2]]), -1, True), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), None, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 0, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False)])\ndef test_logsumexp(self, x_np, axis, keepdims):\n    \"\"\"\n    Check if K.logsumexp works properly for values close to one.\n    \"\"\"\n    x = K.variable(x_np)\n    assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)), np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)), rtol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='The optimization is applied only with TensorFlow.')\ndef test_logsumexp_optim(self):\n    \"\"\"\n    Check if optimization works.\n    \"\"\"\n    x_np = np.array([10000.0, 0.0001])\n    result = K.eval(K.logsumexp(K.variable(x_np), axis=0))\n    assert_allclose(result, 10000.0, rtol=1e-05)",
                            "def test_switch(self):\n    val = np.random.random()\n    z_list = []\n    for k in WITH_NP:\n        x = k.variable(val)\n        x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n        z_list.append(k.eval(x))\n    assert_list_pairwise(z_list)\n    shapes = []\n    shapes.append([(4, 3, 2), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4, 3), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4,), (4, 3, 2), (4, 3, 2)])\n    for s in shapes:\n        z_list = []\n        arrays = list(map(np.random.random, s))\n        for k in WITH_NP:\n            x, then_expr, else_expr = map(k.variable, arrays)\n            cond = k.greater_equal(x, 0.5)\n            z_list.append(k.eval(k.switch(cond, then_expr, else_expr)))\n        assert_list_pairwise(z_list)",
                            "def test_dropout(self):\n    val = np.random.random((100, 100))\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2)) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2, noise_shape=list(val.shape))) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    with pytest.raises(ValueError):\n        z = K.dropout(K.variable(val), level=-0.5)",
                            "@pytest.mark.parametrize('alpha,max_value,threshold', [(0.0, None, 0.0), (0.1, None, 0.0), (0.0, 5.0, 0.0), (0.0, None, 0.8), (0.1, 5.0, 0.0), (0.1, None, 0.8), (0.0, 5.0, 0.8), (0.1, 5.0, 0.8), (0.1, 0.0, 0.8), (0.1, 5.0, -2.8), (0.1, 9.0, 0.8)])\ndef test_relu(self, alpha, max_value, threshold):\n    check_single_tensor_operation('relu', (4, 2), WITH_NP, alpha=alpha, max_value=max_value, threshold=threshold)",
                            "def test_nn_operations(self):\n    check_single_tensor_operation('softsign', (4, 10), WITH_NP)\n    check_single_tensor_operation('softplus', (4, 10), WITH_NP)\n    check_single_tensor_operation('elu', (4, 10), WITH_NP, alpha=0.5)\n    check_single_tensor_operation('sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('hard_sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('tanh', (4, 2), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 10), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 5, 3), WITH_NP, axis=1)\n    check_single_tensor_operation('softmax', (4, 5, 3, 10), WITH_NP, axis=2)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    if K.backend() != 'cntk':\n        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841], [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n    yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016], [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n    check_two_tensor_operation('categorical_crossentropy', yval, xval, WITH_NP, cntk_two_dynamicity=True, from_logits=True)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)",
                            "def test_in_top_k(self):\n    batch_size = 20\n    num_classes = 10\n    predictions = np.random.random((batch_size, num_classes)).astype('float32')\n    targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n    for k in range(num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)\n    num_identical = num_classes // 2\n    for i in range(batch_size):\n        idx_identical = np.random.choice(num_classes, size=num_identical, replace=False)\n        predictions[i, idx_identical] = predictions[i, 0]\n    targets = np.zeros(batch_size, dtype='int32')\n    for k in range(1, num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('conv1d', (2, 8, 2), (3, 2, 3), 'same', 'channels_last'), ('conv1d', (1, 8, 2), (3, 2, 3), 'valid', 'channels_last'), ('conv1d', (1, 2, 8), (3, 2, 3), 'valid', 'channels_first'), ('conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'), ('conv3d', (2, 3, 4, 5, 4), (3, 3, 3, 3, 4), 'same', 'channels_first'), ('conv3d', (2, 3, 5, 4, 6), (3, 2, 4, 3, 4), 'valid', 'channels_first'), ('conv3d', (1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'valid', 'channels_last'), ('conv3d', (1, 3, 5, 4, 2), (3, 3, 3, 2, 3), 'same', 'channels_last')])\ndef test_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last'), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first')])\ndef test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format,dilation_rate', [('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2), ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2), ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2), 'same', 'channels_last', (2, 2)), ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4), 'valid', 'channels_first', (2, 2)), ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4), 'valid', 'channels_last', (2, 2, 2)), ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4), 'same', 'channels_first', (2, 2, 2))])\ndef test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv transpose on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last', (2, 2)), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first', (2, 2))])\ndef test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('depthwise_conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('depthwise_conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('depthwise_conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('depthwise_conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last')])\ndef test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,pool_size,strides,padding,data_format,pool_mode', [('pool2d', (2, 3, 7, 7), (3, 3), (1, 1), 'same', 'channels_first', 'avg'), ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1), 'valid', 'channels_first', 'max'), ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1), 'valid', 'channels_last', 'avg'), ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1), 'same', 'channels_last', 'max'), ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1), 'same', 'channels_first', 'avg'), ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1), 'valid', 'channels_first', 'max'), ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1), 'valid', 'channels_last', 'avg'), ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1), 'same', 'channels_last', 'max')])\ndef test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n    check_single_tensor_operation(op, input_shape, WITH_NP, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, pool_mode=pool_mode, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [('separable_conv1d', (2, 8, 2), (3,), 1, 'same', 'channels_last'), ('separable_conv1d', (1, 8, 2), (3,), 2, 'valid', 'channels_last'), ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'), ('separable_conv2d', (2, 3, 5, 6), (4, 3), 2, 'valid', 'channels_first'), ('separable_conv2d', (1, 6, 5, 3), (3, 4), 1, 'valid', 'channels_last'), ('separable_conv2d', (1, 7, 6, 3), (3, 3), 2, 'same', 'channels_last')])\ndef test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format):\n    if data_format == 'channels_first':\n        input_depth = input_shape[1]\n    else:\n        input_depth = input_shape[-1]\n    _, x = parse_shape_or_val(input_shape)\n    _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n    _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) + (input_depth * depth_multiplier, 7))\n    y1 = KNP.separable_conv(x, depthwise, pointwise, padding=padding, data_format=data_format)\n    if K.backend() == 'cntk':\n        _, cntk_func = cntk_func_tensors(op, [input_shape, depthwise, pointwise], padding=padding, data_format=data_format)\n        y2 = cntk_func([x])[0]\n    else:\n        y2 = K.eval(getattr(K, op)(K.variable(x), K.variable(depthwise), K.variable(pointwise), padding=padding, data_format=data_format))\n    assert_allclose(y1, y2, atol=1e-05)",
                            "def test_random_normal(self):\n    for mean, std in [(0.0, 1.0), (-10.0, 5.0)]:\n        rand = K.eval(K.random_normal((300, 200), mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n        assert np.abs(np.mean(rand) - mean) < std * 0.015\n        assert np.abs(np.std(rand) - std) < std * 0.015\n        r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - mean) < std * 0.015\n        assert np.abs(np.std(samples) - std) < std * 0.015",
                            "def test_random_uniform(self):\n    min_val = -1.0\n    max_val = 1.0\n    rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand)) < 0.015\n    assert max_val - 0.015 < np.max(rand) <= max_val\n    assert min_val + 0.015 > np.min(rand) >= min_val\n    r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples)) < 0.015\n    assert max_val - 0.015 < np.max(samples) <= max_val\n    assert min_val + 0.015 > np.min(samples) >= min_val",
                            "def test_random_binomial(self):\n    p = 0.5\n    rand = K.eval(K.random_binomial((200, 100), p))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand) - p) < 0.015\n    assert np.max(rand) == 1\n    assert np.min(rand) == 0\n    r = K.random_binomial((10, 10), p)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples) - p) < 0.015\n    assert np.max(samples) == 1\n    assert np.min(samples) == 0",
                            "def test_truncated_normal(self):\n    mean = 0.0\n    std = 1.0\n    min_val = -2.0\n    max_val = 2.0\n    rand = K.eval(K.truncated_normal((300, 200), mean=mean, stddev=std, seed=1337))\n    assert rand.shape == (300, 200)\n    assert np.abs(np.mean(rand) - mean) < 0.015\n    assert np.max(rand) <= max_val\n    assert np.min(rand) >= min_val\n    assert np.abs(np.std(rand) - std * 0.87962) < 0.015",
                            "def test_conv_invalid_use(self):\n    dummy_x_1d = K.variable(np.ones((4, 8, 2)))\n    dummy_w_1d = K.variable(np.ones((3, 2, 3)))\n    dummy_x_2d = K.variable(np.ones((2, 3, 4, 5)))\n    dummy_w_2d = K.variable(np.ones((2, 2, 3, 4)))\n    dummy_x_3d = K.variable(np.ones((2, 3, 4, 5, 4)))\n    dummy_w_3d = K.variable(np.ones((2, 2, 2, 3, 4)))\n    dummy_w1x1_2d = K.variable(np.ones((1, 1, 12, 7)))\n    with pytest.raises(ValueError):\n        K.conv1d(dummy_x_1d, dummy_w_1d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv3d(dummy_x_3d, dummy_w_3d, data_format='channels_middle')\n    if K.backend() != 'theano':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    if K.backend() == 'cntk':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, strides=(2, 2), dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, strides=(2, 2), dilation_rate=(1, 2))",
                            "def test_pooling_invalid_use(self):\n    for input_shape, pool_size in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n        x = K.variable(np.random.random(input_shape))\n        if len(pool_size) == 2:\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, pool_mode='median')\n        else:\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, pool_mode='median')",
                            "def test_resize_images(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_images', x_shape, WITH_NP, cntk_dynamicity=True, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_images(K.variable(xval), 2, 2, data_format='channels_middle')",
                            "@staticmethod\ndef _helper_bilinear(data_format, height_factor, width_factor):\n    x_shape = (2, 3, 4, 5)\n    check_single_tensor_operation('resize_images', x_shape, [KTF, KTH], height_factor=height_factor, width_factor=width_factor, data_format=data_format, interpolation='bilinear')",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\ndef test_resize_images_bilinear(self, data_format):\n    self._helper_bilinear(data_format, 2, 2)\n    with pytest.raises(NotImplementedError):\n        self._helper_bilinear(data_format, 4, 4)",
                            "def test_resize_volumes(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_volumes', x_shape, WITH_NP, cntk_dynamicity=True, depth_factor=2, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_volumes(K.variable(xval), 2, 2, 2, data_format='channels_middle')",
                            "def test_temporal_padding(self):\n    check_single_tensor_operation('temporal_padding', (4, 3, 3), WITH_NP)\n    check_single_tensor_operation('temporal_padding', (2, 3, 4), WITH_NP, padding=(1, 2))",
                            "def test_spatial_2d_padding(self):\n    padding = ((1, 2), (2, 1))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, 1))\n        y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_2d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_spatial_3d_padding(self):\n    padding = ((1, 2), (2, 1), (1, 2))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, None, 1))\n        y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_3d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_bias_add(self):\n    for data_format in ['channels_first', 'channels_last']:\n        for shape in [(), (3,), (2, 3), (5, 3, 2)]:\n            if data_format == 'channels_first':\n                x_shape = (1, 4) + shape\n            else:\n                x_shape = (1,) + shape + (4,)\n            bias_shape = (4,)\n            check_two_tensor_operation('bias_add', x_shape, bias_shape, WITH_NP, cntk_dynamicity=True, data_format=data_format)\n        if data_format == 'channels_first':\n            x_shape = (20, 6, 10)\n        else:\n            x_shape = (20, 10, 6)\n        check_two_tensor_operation('bias_add', x_shape, (10, 6), WITH_NP, cntk_dynamicity=True, data_format=data_format)\n    x = K.variable(np.random.random(x_shape))\n    b = K.variable(np.random.random(bias_shape))\n    with pytest.raises(ValueError):\n        K.bias_add(x, b, data_format='channels_middle')",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='Specific to Theano.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_th(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes='per-activation')\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Specific to Tensorflow.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_tf(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'cntk', reason='Specific to CNTK.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_cntk(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.placeholder(x_shape)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.function([x], [z])([x_val])[0]\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_ctc(self):\n    if K.backend() == 'theano':\n        ref = [1.73308, 3.81351]\n    else:\n        ref = [3.34211, 5.42262]\n    label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n    input_lens = np.expand_dims(np.asarray([5, 5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]], [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508], [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549], [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456], [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345], [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)\n    if K.backend() == 'theano':\n        ref = [1.73308]\n    else:\n        ref = [3.34211]\n    input_lens = np.expand_dims(np.asarray([5]), 1)\n    label_lens = np.expand_dims(np.asarray([5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Test adapted from tensorflow.')\ndef test_ctc_decode_greedy(self):\n    \"\"\"Test two batch entries - best path decoder.\"\"\"\n    max_time_steps = 6\n    seq_len_0 = 4\n    input_prob_matrix_0 = np.asarray([[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.4, 0.6], [0.0, 0.0, 0.4, 0.6], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    seq_len_1 = 5\n    input_prob_matrix_1 = np.asarray([[0.1, 0.9, 0.0, 0.0], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.1, 0.9], [0.0, 0.9, 0.1, 0.1], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    inputs = [np.vstack([input_prob_matrix_0[t, :], input_prob_matrix_1[t, :]]) for t in range(max_time_steps)]\n    inputs = np.asarray(inputs).transpose((1, 0, 2))\n    input_length = np.array([seq_len_0, seq_len_1], dtype=np.int32)\n    decode_pred_np, log_prob_pred_np = KNP.ctc_decode(inputs, input_length, greedy=True)\n    inputs = K.variable(inputs)\n    input_length = K.variable(input_length)\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=True)\n    assert len(decode_pred_tf) == 1\n    decode_pred = K.eval(decode_pred_tf[0])\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    assert np.alltrue(decode_pred_np == decode_pred)\n    assert np.allclose(log_prob_pred_np, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='tensorflow-way slice is only supported in tensorflow.')\n@pytest.mark.parametrize('x_size', [[1, 1, 3], [1, 2, 3], [2, 1, 3]])\ndef test_slice(self, x_size):\n    npt = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])\n    x_start = [1, 0, 0]\n    tft = K.constant(npt)\n    test_input = K.eval(K.slice(tft, x_start, x_size))\n    expected = KNP.slice(npt, x_start, x_size)\n    assert np.allclose(test_input, expected)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search(self):\n    \"\"\"Test one batch, two beams - hibernating beam search.\"\"\"\n    depth = 6\n    seq_len_0 = 5\n    input_prob_matrix_0 = np.asarray([[0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908], [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517], [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763], [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655], [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878], [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]], dtype=np.float32)\n    input_prob_matrix_0 = input_prob_matrix_0 + 2.0\n    inputs = [input_prob_matrix_0[t, :][np.newaxis, :] for t in range(seq_len_0)] + 2 * [np.zeros((1, depth), dtype=np.float32)]\n    inputs = np.exp(inputs)\n    inputs = K.variable(inputs.transpose((1, 0, 2)))\n    input_length = K.variable(np.array([seq_len_0], dtype=np.int32))\n    log_prob_truth = np.array([-5.811451, -6.63339], np.float32)[np.newaxis, :]\n    decode_truth = [np.array([1, 0]), np.array([[1]])]\n    beam_width = 2\n    top_paths = 2\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=False, beam_width=beam_width, top_paths=top_paths)\n    assert len(decode_pred_tf) == top_paths\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    for i in range(top_paths):\n        assert np.alltrue(decode_truth[i] == K.eval(decode_pred_tf[i]))\n    assert np.allclose(log_prob_truth, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search_no_merge(self):\n    input_prob = np.array([[[0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]])\n    input_len = np.array(input_prob.shape[0] * [input_prob.shape[1]])\n\n    def decode(merge_repeated):\n        input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n        input_len_tensor = K.placeholder(shape=None, dtype='int64')\n        paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n        decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n        paths = decode_func([input_prob, input_len])\n        return paths\n    assert np.allclose(decode(merge_repeated=True), [np.array([[0, 1]])])\n    assert np.allclose(decode(merge_repeated=False), [np.array([[0, 0, 1, 1]])])",
                            "def test_one_hot(self):\n    input_length = 10\n    num_classes = 20\n    batch_size = 30\n    indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n    oh = KNP.one_hot(np.int32(indices), num_classes)\n    koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n    assert np.all(koh == oh)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_dot(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense = x_sparse.toarray()\n    W = np.random.random((5, 4))\n    t_W = K.variable(W)\n    k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n    k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n    assert k_s.shape == k_d.shape\n    assert_allclose(k_s, k_d, atol=1e-05)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_concat(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_1 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_2 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense_1 = x_sparse_1.toarray()\n    x_dense_2 = x_sparse_2.toarray()\n    k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n    assert K.is_sparse(k_s)\n    k_s_d = K.eval(k_s)\n    k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n    assert k_s_d.shape == k_d.shape\n    assert_allclose(k_s_d, k_d, atol=1e-05)",
                            "def test_stack(self):\n    tensor_list = [np.random.randn(5, 4, 6, 10) for _ in range(5)]\n    stack_axis = 3\n    results = []\n    if WITH_NP[0] == KC:\n        check_two_tensor_operation('stack', (5, 4, 6, 10), (5, 4, 6, 10), WITH_NP, axis=stack_axis, concat_args=True)\n    else:\n        for k in WITH_NP:\n            tensor_list_var = [k.variable(tensor) for tensor in tensor_list]\n            out = k.eval(k.stack(tensor_list_var, axis=stack_axis))\n            results.append(out)\n        assert_list_pairwise(results)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_map(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    vx = K.variable(x)\n    kx = K.eval(K.map_fn(K.sum, vx))\n    kx2 = K.eval(K.map_fn(lambda i: K.sum(vx[i]), K.arange(10), dtype=K.floatx()))\n    assert (10,) == kx.shape\n    assert (10,) == kx2.shape\n    assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n    assert_allclose(kx, kx2, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldl(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n    assert (3,) == kx.shape\n    assert_allclose(x.sum(axis=0), kx, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldr(self):\n    x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n    vx = K.variable(x)\n    p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n    p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n    assert p1 < p2\n    assert 9e-38 < p2 <= 1e-37",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk has issues with negative number.')\ndef test_arange(self):\n    for test_value in (-20, 0, 1, 10):\n        a_list = []\n        dtype_list = []\n        for k in WITH_NP:\n            t = k.arange(test_value)\n            a = k.eval(t)\n            assert np.array_equal(a, np.arange(test_value))\n            dtype_list.append(k.dtype(t))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for start, stop, step in ((0, 5, 1), (-5, 5, 2), (0, 1, 2)):\n        a_list = []\n        for k in WITH_NP:\n            a = k.eval(k.arange(start, stop, step))\n            assert np.array_equal(a, np.arange(start, stop, step))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for dtype in ('int32', 'int64', 'float32', 'float64'):\n        for k in WITH_NP:\n            t = k.arange(10, dtype=dtype)\n            assert k.dtype(t) == dtype\n    start = K.constant(1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 1\n    start = K.constant(-1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 0",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_train_phase(self, training):\n    check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_test_phase(self, training):\n    check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "def test_setfloatx_incorrect_values(self):\n    old_floatx = floatx()\n    initial = floatx()\n    for value in ['', 'beerfloat', 123]:\n        with pytest.raises(ValueError):\n            set_floatx(value)\n    assert floatx() == initial\n    set_floatx(old_floatx)",
                            "def DISABLED_test_setfloatx_correct_values(self):\n    \"\"\"Disabled because it is not thread safe at this time.\"\"\"\n    old_floatx = floatx()\n    for value in ['float16', 'float32', 'float64']:\n        set_floatx(value)\n        assert floatx() == value\n    set_floatx(old_floatx)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support float16')\ndef DISABLED_test_set_floatx(self):\n    \"\"\"Disabled because it is not thread safe at this time.\n\n    Make sure that changes to the global floatx are effectively\n    taken into account by the backend.\n    \"\"\"\n    old_floatx = floatx()\n    set_floatx('float16')\n    var = variable([10])\n    check_dtype(var, 'float16')\n    set_floatx('float64')\n    var = variable([10])\n    check_dtype(var, 'float64')\n    set_floatx(old_floatx)",
                            "def test_dtype(self):\n    assert K.dtype(K.variable(1, dtype='float64')) == 'float64'\n    assert K.dtype(K.variable(1, dtype='float32')) == 'float32'\n    assert K.dtype(K.variable(1, dtype='float16')) == 'float16'",
                            "def test_variable_support_bool_dtype(self):\n    if K.backend() == 'tensorflow':\n        assert K.dtype(K.variable(1, dtype='int16')) == 'int16'\n        assert K.dtype(K.variable(False, dtype='bool')) == 'bool'\n        with pytest.raises(TypeError):\n            K.variable('', dtype='unsupported')",
                            "def test_clip_supports_tensor_arguments(self):\n    x = K.variable([-10.0, -5.0, 0.0, 5.0, 10.0])\n    min_value = K.variable([-5.0, -4.0, 0.0, 3.0, 5.0])\n    max_value = K.variable([5.0, 4.0, 1.0, 4.0, 9.0])\n    assert np.allclose(K.eval(K.clip(x, min_value, max_value)), np.asarray([-5.0, -4.0, 0.0, 4.0, 9.0], dtype=np.float32))",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(), reason='This test is for tensorflow parallelism.')\ndef test_tensorflow_session_parallelism_settings(self, monkeypatch):\n    for threads in [1, 2]:\n        K.clear_session()\n        monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n        cfg = K.get_session()._config\n        assert cfg.intra_op_parallelism_threads == threads\n        assert cfg.inter_op_parallelism_threads == threads",
                            "def batch_shape(shape):\n    return (batch_size,) + shape[1:]",
                            "def random(shape):\n    return np.random.random(batch_shape(shape))",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn(inputs, states):\n        assert len(states) == 1\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y])\n    return simple_rnn",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_with_extra_mock_state(inputs, states):\n        assert len(states) == 2\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y, backend.concatenate([y, y], axis=-1)])\n    return simple_rnn_with_extra_mock_state",
                            "def get_step_function(backend, w_i):\n\n    def simple_no_states(inputs, states):\n        assert len(states) == 0\n        y = backend.dot(inputs, w_i)\n        return (y, [])\n    return simple_no_states",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_add_constant(inputs, states_and_constants):\n        [h, c] = states_and_constants\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n        return (y, [y])\n    return simple_rnn_add_constant",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def step_function(inputs, states):\n    outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n    return (outputs, states)",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def decode(merge_repeated):\n    input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n    input_len_tensor = K.placeholder(shape=None, dtype='int64')\n    paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n    decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n    paths = decode_func([input_prob, input_len])\n    return paths",
                            "def simple_rnn(inputs, states):\n    assert len(states) == 1\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y])",
                            "def simple_rnn_with_extra_mock_state(inputs, states):\n    assert len(states) == 2\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y, backend.concatenate([y, y], axis=-1)])",
                            "def simple_no_states(inputs, states):\n    assert len(states) == 0\n    y = backend.dot(inputs, w_i)\n    return (y, [])",
                            "def simple_rnn_add_constant(inputs, states_and_constants):\n    [h, c] = states_and_constants\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n    return (y, [y])"
                        ],
                        "constructor_variables": [],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "test_is_keras_tensor(self)",
                            "test_set_learning_phase(self)",
                            "test_eye(self)",
                            "test_ones(self)",
                            "test_zeros(self)",
                            "test_ones_like(self)",
                            "test_zeros_like(self)",
                            "test_linear_operations(self)",
                            "test_random_variables(self)",
                            "test_batch_dot_shape(self)",
                            "test_shape_operations(self)",
                            "test_none_shape_operations(self)",
                            "test_repeat_elements(self)",
                            "test_tile(self)",
                            "test_gather(self)",
                            "test_value_manipulation(self, function_name)",
                            "test_print_tensor(self)",
                            "test_elementwise_operations(self)",
                            "test_reset_uids(self)",
                            "test_cumsum_cumprod(self)",
                            "test_log(self)",
                            "test_update_add(self)",
                            "test_update_sub(self)",
                            "test_gradient(self)",
                            "test_stop_gradient(self)",
                            "test_function(self)",
                            "test_function_tf_fetches(self)",
                            "test_function_tf_feed_dict(self)",
                            "test_function_tf_run_options_with_run_metadata(self)",
                            "test_function_tf_string_input(self)",
                            "test_rnn(self)",
                            "test_rnn_additional_states(self)",
                            "test_rnn_no_states(self)",
                            "test_rnn_constants(self)",
                            "test_rnn_output_and_state_masking_independent(self)",
                            "test_rnn_output_num_dim_larger_than_2_masking(self)",
                            "test_rnn_state_num_dim_larger_than_2_masking(self)",
                            "test_logsumexp(self, x_np, axis, keepdims)",
                            "test_logsumexp_optim(self)",
                            "test_switch(self)",
                            "test_dropout(self)",
                            "test_relu(self, alpha, max_value, threshold)",
                            "test_nn_operations(self)",
                            "test_in_top_k(self)",
                            "test_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format)",
                            "test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate)",
                            "test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate)",
                            "test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode)",
                            "test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format)",
                            "test_random_normal(self)",
                            "test_random_uniform(self)",
                            "test_random_binomial(self)",
                            "test_truncated_normal(self)",
                            "test_conv_invalid_use(self)",
                            "test_pooling_invalid_use(self)",
                            "test_resize_images(self)",
                            "_helper_bilinear(data_format, height_factor, width_factor)",
                            "test_resize_images_bilinear(self, data_format)",
                            "test_resize_volumes(self)",
                            "test_temporal_padding(self)",
                            "test_spatial_2d_padding(self)",
                            "test_spatial_3d_padding(self)",
                            "test_bias_add(self)",
                            "test_batchnorm_th(self, x_shape)",
                            "test_batchnorm_tf(self, x_shape)",
                            "test_batchnorm_cntk(self, x_shape)",
                            "test_ctc(self)",
                            "test_ctc_decode_greedy(self)",
                            "test_slice(self, x_size)",
                            "test_ctc_decode_beam_search(self)",
                            "test_ctc_decode_beam_search_no_merge(self)",
                            "test_one_hot(self)",
                            "test_sparse_dot(self)",
                            "test_sparse_concat(self)",
                            "test_stack(self)",
                            "test_map(self)",
                            "test_foldl(self)",
                            "test_foldr(self)",
                            "test_arange(self)",
                            "test_in_train_phase(self, training)",
                            "test_in_test_phase(self, training)",
                            "test_setfloatx_incorrect_values(self)",
                            "DISABLED_test_setfloatx_correct_values(self)",
                            "DISABLED_test_set_floatx(self)",
                            "test_dtype(self)",
                            "test_variable_support_bool_dtype(self)",
                            "test_clip_supports_tensor_arguments(self)",
                            "test_tensorflow_session_parallelism_settings(self, monkeypatch)",
                            "batch_shape(shape)",
                            "random(shape)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i)",
                            "get_step_function(backend, w_i, w_h)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "decode(merge_repeated)",
                            "simple_rnn(inputs, states)",
                            "simple_rnn_with_extra_mock_state(inputs, states)",
                            "simple_no_states(inputs, states)",
                            "simple_rnn_add_constant(inputs, states_and_constants)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "test_update_sub",
                    "function_code": "@pytest.mark.skipif(K.backend() == 'theano',\n                    reason='theano returns tuples for update ops')\ndef test_update_sub(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    decrement = np.random.randn(3, 4)\n\n    x -= decrement\n    K.eval(K.update_sub(x_var, decrement))\n\n    assert_allclose(x, K.eval(x_var), atol=1e-05)\n",
                    "decorators": [
                        "pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')"
                    ],
                    "docstring": null,
                    "start_line": 594,
                    "end_line": 604,
                    "variables": {
                        "x": [
                            601,
                            604,
                            597,
                            598
                        ],
                        "np.random.randn": [
                            597,
                            599
                        ],
                        "np.random": [
                            597,
                            599
                        ],
                        "np": [
                            597,
                            599
                        ],
                        "x_var": [
                            602,
                            604,
                            598
                        ],
                        "K.variable": [
                            598
                        ],
                        "K": [
                            602,
                            604,
                            594,
                            598
                        ],
                        "decrement": [
                            601,
                            602,
                            599
                        ],
                        "K.eval": [
                            602,
                            604
                        ],
                        "K.update_sub": [
                            602
                        ],
                        "assert_allclose": [
                            604
                        ],
                        "pytest.mark.skipif": [
                            594
                        ],
                        "pytest.mark": [
                            594
                        ],
                        "pytest": [
                            594
                        ],
                        "K.backend": [
                            594
                        ]
                    },
                    "filtered_variables": {
                        "x": [
                            601,
                            604,
                            597,
                            598
                        ],
                        "np.random.randn": [
                            597,
                            599
                        ],
                        "np.random": [
                            597,
                            599
                        ],
                        "np": [
                            597,
                            599
                        ],
                        "x_var": [
                            602,
                            604,
                            598
                        ],
                        "K.variable": [
                            598
                        ],
                        "K": [
                            602,
                            604,
                            594,
                            598
                        ],
                        "decrement": [
                            601,
                            602,
                            599
                        ],
                        "K.eval": [
                            602,
                            604
                        ],
                        "K.update_sub": [
                            602
                        ],
                        "assert_allclose": [
                            604
                        ],
                        "pytest.mark.skipif": [
                            594
                        ],
                        "pytest.mark": [
                            594
                        ],
                        "pytest": [
                            594
                        ],
                        "K.backend": [
                            594
                        ]
                    },
                    "diff_line_number": 597,
                    "class_data": {
                        "signature": "class TestBackend(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def test_is_keras_tensor(self):\n    np_var = np.array([1, 2])\n    with pytest.raises(ValueError):\n        K.is_keras_tensor(np_var)\n    keras_var = K.variable(np_var)\n    assert K.is_keras_tensor(keras_var) is False\n    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n    assert K.is_keras_tensor(keras_placeholder) is False",
                            "def test_set_learning_phase(self):\n    with pytest.raises(ValueError):\n        K.set_learning_phase(2)",
                            "def test_eye(self):\n    check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)",
                            "def test_ones(self):\n    check_single_tensor_operation('ones', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_zeros(self):\n    check_single_tensor_operation('zeros', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_ones_like(self):\n    check_single_tensor_operation('ones_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_zeros_like(self):\n    check_single_tensor_operation('zeros_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_linear_operations(self):\n    check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n    check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=(1, 1))\n    check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n    check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n    if K.backend() != 'cntk':\n        check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=(1, 2))",
                            "def test_random_variables(self):\n    check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP, low=0.0, high=1.0, shape_or_val=False, assert_value_equality=False)\n    check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP, mean=0.0, scale=1.0, shape_or_val=False, assert_value_equality=False)",
                            "def test_batch_dot_shape(self):\n    test_cases = []\n    test_cases.append([(None, 3, 4, 5), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 3, 4, 5), (None, 2, 4), 2])\n    test_cases.append([(None, 3, 4), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 4, 3), (None, 3, 5), (2, 1)])\n    test_cases.append([(None, 4), (None, 3, 4), (1, 2)])\n    test_cases.append([(None, 4), (None, 4), None])\n    batch_size = 7\n\n    def batch_shape(shape):\n        return (batch_size,) + shape[1:]\n\n    def random(shape):\n        return np.random.random(batch_shape(shape))\n    for x_shape, y_shape, axes in test_cases:\n        x_np = random(x_shape)\n        y_np = random(y_shape)\n        z_np = KNP.batch_dot(x_np, y_np, axes)\n        x = K.placeholder(shape=x_shape)\n        y = K.placeholder(shape=y_shape)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        f = K.function([x, y], [z])\n        assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        if K.backend() != 'cntk':\n            x = K.placeholder(ndim=len(x_shape))\n            y = K.placeholder(ndim=len(y_shape))\n            z = K.batch_dot(x, y, axes)\n            z_shape = K.int_shape(z)\n            if z_shape is not None:\n                assert len(z_shape) == z_np.ndim\n                assert set(z_shape) <= set((None, 1))\n            f = K.function([x, y], [z])\n            assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        x = K.variable(x_np)\n        y = K.variable(y_np)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        z = K.eval(z)\n        assert_allclose(z, z_np, atol=1e-05)",
                            "def test_shape_operations(self):\n    check_two_tensor_operation('concatenate', (4, 3), (4, 2), WITH_NP, axis=-1, concat_args=True)\n    check_single_tensor_operation('reshape', (4, 2), WITH_NP, shape=(8, 1))\n    check_single_tensor_operation('permute_dimensions', (4, 2, 3), WITH_NP, pattern=(2, 0, 1))\n    check_single_tensor_operation('repeat', (4, 1), WITH_NP, n=3)\n    check_single_tensor_operation('flatten', (4, 1), WITH_NP)\n    check_single_tensor_operation('batch_flatten', (20, 2, 5), WITH_NP, cntk_dynamicity=True)\n    check_single_tensor_operation('expand_dims', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('expand_dims', (4, 3, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('squeeze', (4, 3, 1), WITH_NP, axis=2)\n    check_single_tensor_operation('squeeze', (4, 1, 1), WITH_NP, axis=1)\n    check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)}, 'squeeze', {'axis': 2}, (4, 3, 1, 1), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='We only test the shape inference of the theano backend.')\ndef test_none_shape_operations(self):\n    x = K.placeholder((3, None, 4))\n    y = K.batch_flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (3, None)\n    y = K.flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (None,)",
                            "def test_repeat_elements(self):\n    reps = 3\n    for ndims in [1, 2, 3]:\n        shape = np.arange(2, 2 + ndims)\n        arr = np.arange(np.prod(shape)).reshape(shape)\n        for rep_axis in range(ndims):\n            check_single_tensor_operation('repeat_elements', arr, WITH_NP, rep=reps, axis=rep_axis)\n            if K.backend() != 'cntk':\n                shape = list(shape)\n                shape[rep_axis] = None\n                x = K.placeholder(shape=shape)\n                y = K.repeat_elements(x, reps, axis=rep_axis)\n                assert y._keras_shape == tuple(shape)\n                assert y._keras_shape == K.int_shape(y)",
                            "def test_tile(self):\n    shape = (3, 4)\n    arr = np.arange(np.prod(shape)).reshape(shape)\n    check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n    check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 4))\n        n = 2\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 8)\n        n = (4, 3)\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 12)",
                            "def test_gather(self):\n    shape = (10, 2, 3)\n    ref = np.arange(np.prod(shape)).reshape(shape)\n    inds = [1, 3, 7, 9]\n    t_list = [k.gather(k.variable(ref), k.variable(inds, dtype='int32')) for k in WITH_NP]\n    z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32'))) for k in WITH_NP]\n    assert_list_pairwise(z_list)\n    assert_list_keras_shape(t_list, z_list)\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 3, 4))\n        indices = K.placeholder(shape=(5, 6), dtype='int32')\n        y = K.gather(x, indices)\n        assert y._keras_shape == (5, 6, 3, 4)",
                            "@pytest.mark.parametrize('function_name', ['get_value', 'count_params', 'int_shape', 'get_variable_shape'])\ndef test_value_manipulation(self, function_name):\n    val = np.random.random((4, 2))\n    v_list = [getattr(k, function_name)(k.variable(val)) for k in WITH_NP]\n    if function_name == 'get_value':\n        assert_list_pairwise(v_list)\n    else:\n        assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)",
                            "def test_print_tensor(self):\n    check_single_tensor_operation('print_tensor', (), WITH_NP)\n    check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n    check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n    check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)",
                            "def test_elementwise_operations(self):\n    check_single_tensor_operation('max', (4, 2), WITH_NP)\n    check_single_tensor_operation('max', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('max', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('min', (4, 2), WITH_NP)\n    check_single_tensor_operation('min', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('min', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('mean', (4, 2), WITH_NP)\n    check_single_tensor_operation('mean', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=-1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('var', (4, 2), WITH_NP)\n    check_single_tensor_operation('var', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('var', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('std', (4, 2), WITH_NP)\n    check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('prod', (4, 2), WITH_NP)\n    check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('any', (4, 2), WITH_NP)\n    check_single_tensor_operation('any', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('any', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('all', (4, 2), WITH_NP)\n    check_single_tensor_operation('all', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('all', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('square', (4, 2), WITH_NP)\n    check_single_tensor_operation('abs', (4, 2), WITH_NP)\n    check_single_tensor_operation('sqrt', (4, 2), WITH_NP)\n    check_single_tensor_operation('exp', (4, 2), WITH_NP)\n    check_single_tensor_operation('round', (4, 2), WITH_NP)\n    check_single_tensor_operation('sign', (4, 2), WITH_NP)\n    check_single_tensor_operation('pow', (4, 2), WITH_NP, a=3)\n    check_single_tensor_operation('clip', (4, 2), WITH_NP, min_value=0.4, max_value=0.6)\n    check_single_tensor_operation('cos', (4, 2), WITH_NP)\n    check_single_tensor_operation('sin', (4, 2), WITH_NP)\n    check_two_tensor_operation('equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('not_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('maximum', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('minimum', (4, 2), (4, 2), WITH_NP)",
                            "def test_reset_uids(self):\n    first = K.get_uid()\n    K.get_uid()\n    K.reset_uids()\n    assert K.get_uid() == first",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support cumsum and cumprod yet')\ndef test_cumsum_cumprod(self):\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP, axis=1)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk return -85.1 for zero or negative number, not nan, so can't compare with other backend.\")\ndef test_log(self):\n    check_single_tensor_operation('log', (4, 2), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_add(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    increment = np.random.randn(3, 4)\n    x += increment\n    K.eval(K.update_add(x_var, increment))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_sub(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    decrement = np.random.randn(3, 4)\n    x -= decrement\n    K.eval(K.update_sub(x_var, decrement))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk doesn't support gradient in this way.\")\ndef test_gradient(self):\n    val = np.random.random((4, 2))\n    x_list = [k.variable(val) for k in [KTH, KTF]]\n    z_list = []\n    zero_list = []\n    for x, k in zip(x_list, [KTH, KTF]):\n        exp = x * k.exp(x)\n        loss = k.sum(exp)\n        zero_loss = k.stop_gradient(loss)\n        grad = k.gradients(loss, [exp])\n        zero_grad = k.gradients(loss + zero_loss, [exp])\n        z_list.append(k.eval(grad[0]))\n        zero_list.append(k.eval(zero_grad[0]))\n    assert_list_pairwise(z_list)\n    assert_list_pairwise(zero_list)\n    for i in range(len(z_list)):\n        assert_allclose(zero_list[i], z_list[i], atol=1e-05)",
                            "def test_stop_gradient(self):\n    val = np.random.random((4, 2))\n    a = K.variable(val)\n    b = K.square(a)\n    c, d = K.stop_gradient([a, b])\n    e = K.stop_gradient(b)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk currently not support function in this way, so can't test as this.\")\ndef test_function(self):\n    test_backend = [KTH, KTF]\n    val = np.random.random((4, 2))\n    input_val = np.random.random((4, 2))\n    f_list = []\n    x_list = []\n    for k in test_backend:\n        x = k.variable(val)\n        x_list.append(x)\n        y = k.placeholder(ndim=2)\n        exp = k.square(x) + y\n        update = x * 2\n        f = k.function([y], [exp], updates=[(x, update)])\n        f_list.append(f)\n    function_outputs_list = [f([input_val])[0] for f in f_list]\n    assert_list_pairwise(function_outputs_list)\n    new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n    assert_list_pairwise(new_val_list)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `fetches` argument.')\ndef test_function_tf_fetches(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], updates=[(x, x_placeholder + 1.0)], fetches=[K.update(y, 5.0)])\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert K.get_session().run(fetches=[x, y]) == [11.0, 5.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `feed_dict` argument.')\ndef test_function_tf_feed_dict(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    feed_dict = {y_placeholder: 3.0}\n    f = K.function(inputs=[x_placeholder], outputs=[x_placeholder + 1.0], updates=[(x, x_placeholder + 10.0)], feed_dict=feed_dict, fetches=[K.update(y, y_placeholder * 10.0)])\n    output = f([10.0])\n    assert output == [11.0]\n    assert K.get_session().run(fetches=[x, y]) == [20.0, 30.0]\n    feed_dict[y_placeholder] = 4.0\n    output = f([20.0])\n    assert output == [21.0]\n    assert K.get_session().run(fetches=[x, y]) == [30.0, 40.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `options` and `run_metadata` arguments.')\ndef test_function_tf_run_options_with_run_metadata(self):\n    from tensorflow.core.protobuf import config_pb2\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    run_options = config_pb2.RunOptions(output_partition_graphs=True)\n    run_metadata = config_pb2.RunMetadata()\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], options=run_options, run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) > 0\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) == 0",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `string` type for a tensor.')\ndef test_function_tf_string_input(self):\n    x_placeholder = K.placeholder(shape=(), dtype='string')\n    x_identity = K.identity(x_placeholder)\n    f = K.function(inputs=[x_placeholder], outputs=[x_identity])\n    output = f([b'test'])\n    assert output == [b'test']",
                            "def test_rnn(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn(inputs, states):\n            assert len(states) == 1\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y])\n        return simple_rnn\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_additional_states(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    h1 = np.concatenate([h0, h0], axis=-1)\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_with_extra_mock_state(inputs, states):\n            assert len(states) == 2\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y, backend.concatenate([y, y], axis=-1)])\n        return simple_rnn_with_extra_mock_state\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0, h1], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_no_states(self):\n    num_samples = 3\n    input_dim = 8\n    output_dim = 4\n    timesteps = 5\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n\n    def get_step_function(backend, w_i):\n\n        def simple_no_states(inputs, states):\n            assert len(states) == 0\n            y = backend.dot(inputs, w_i)\n            return (y, [])\n        return simple_no_states\n    kwargs_list = [{'go_backwards': False}, {'go_backwards': True}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k), step_function_np=get_step_function(KNP, wi), inputs_np=x, initial_states_np=[], mask_np=None, **kwargs)",
                            "def test_rnn_constants(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, c = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_add_constant(inputs, states_and_constants):\n            [h, c] = states_and_constants\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n            return (y, [y])\n        return simple_rnn_add_constant\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), constants_np=[c], **kwargs)",
                            "def test_rnn_output_and_state_masking_independent(self):\n    num_samples = 2\n    num_timesteps = 4\n    state_and_io_size = 5\n    mask_last_num_timesteps = 2\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, state_and_io_size))\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[1, -mask_last_num_timesteps:] = 0\n    expected_outputs = inputs_vals.copy()\n    expected_outputs[1, -mask_last_num_timesteps:] = expected_outputs[1, -(mask_last_num_timesteps + 1)]\n    expected_state = initial_state_vals.copy()\n    expected_state[0] += num_timesteps\n    expected_state[1] += num_timesteps - mask_last_num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)\n        assert_allclose(K.eval(last_states[0]), expected_state)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_output_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n    num_features = 5\n\n    def step_function(inputs, states):\n        outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n        return (outputs, states)\n    inputs_vals = np.random.random((num_samples, num_timesteps, num_features))\n    initial_state_vals = np.random.random((num_samples, 6))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[-1, -1] = 0\n    expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n    expected_outputs[-1, -1] = expected_outputs[-1, -2]\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_state_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n    initial_state_vals = np.random.random((num_samples, 6, 7))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[0, -2:] = 0\n    expected_last_state = initial_state_vals.copy()\n    expected_last_state[0] += num_timesteps - 2\n    expected_last_state[1:] += num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(last_states[0]), expected_last_state)",
                            "@pytest.mark.parametrize('x_np,axis,keepdims', [(np.array([1.1, 0.8, 0.9]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 1, False), (np.array([[1.1, 0.8, 0.9]]), -1, False), (np.array([[1.1, 0.8, 0.9]]), 1, True), (np.array([[1.1], [1.2]]), 0, False), (np.array([[1.1], [1.2]]), 1, False), (np.array([[1.1], [1.2]]), -1, False), (np.array([[1.1], [1.2]]), -1, True), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), None, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 0, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False)])\ndef test_logsumexp(self, x_np, axis, keepdims):\n    \"\"\"\n    Check if K.logsumexp works properly for values close to one.\n    \"\"\"\n    x = K.variable(x_np)\n    assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)), np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)), rtol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='The optimization is applied only with TensorFlow.')\ndef test_logsumexp_optim(self):\n    \"\"\"\n    Check if optimization works.\n    \"\"\"\n    x_np = np.array([10000.0, 0.0001])\n    result = K.eval(K.logsumexp(K.variable(x_np), axis=0))\n    assert_allclose(result, 10000.0, rtol=1e-05)",
                            "def test_switch(self):\n    val = np.random.random()\n    z_list = []\n    for k in WITH_NP:\n        x = k.variable(val)\n        x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n        z_list.append(k.eval(x))\n    assert_list_pairwise(z_list)\n    shapes = []\n    shapes.append([(4, 3, 2), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4, 3), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4,), (4, 3, 2), (4, 3, 2)])\n    for s in shapes:\n        z_list = []\n        arrays = list(map(np.random.random, s))\n        for k in WITH_NP:\n            x, then_expr, else_expr = map(k.variable, arrays)\n            cond = k.greater_equal(x, 0.5)\n            z_list.append(k.eval(k.switch(cond, then_expr, else_expr)))\n        assert_list_pairwise(z_list)",
                            "def test_dropout(self):\n    val = np.random.random((100, 100))\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2)) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2, noise_shape=list(val.shape))) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    with pytest.raises(ValueError):\n        z = K.dropout(K.variable(val), level=-0.5)",
                            "@pytest.mark.parametrize('alpha,max_value,threshold', [(0.0, None, 0.0), (0.1, None, 0.0), (0.0, 5.0, 0.0), (0.0, None, 0.8), (0.1, 5.0, 0.0), (0.1, None, 0.8), (0.0, 5.0, 0.8), (0.1, 5.0, 0.8), (0.1, 0.0, 0.8), (0.1, 5.0, -2.8), (0.1, 9.0, 0.8)])\ndef test_relu(self, alpha, max_value, threshold):\n    check_single_tensor_operation('relu', (4, 2), WITH_NP, alpha=alpha, max_value=max_value, threshold=threshold)",
                            "def test_nn_operations(self):\n    check_single_tensor_operation('softsign', (4, 10), WITH_NP)\n    check_single_tensor_operation('softplus', (4, 10), WITH_NP)\n    check_single_tensor_operation('elu', (4, 10), WITH_NP, alpha=0.5)\n    check_single_tensor_operation('sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('hard_sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('tanh', (4, 2), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 10), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 5, 3), WITH_NP, axis=1)\n    check_single_tensor_operation('softmax', (4, 5, 3, 10), WITH_NP, axis=2)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    if K.backend() != 'cntk':\n        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841], [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n    yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016], [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n    check_two_tensor_operation('categorical_crossentropy', yval, xval, WITH_NP, cntk_two_dynamicity=True, from_logits=True)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)",
                            "def test_in_top_k(self):\n    batch_size = 20\n    num_classes = 10\n    predictions = np.random.random((batch_size, num_classes)).astype('float32')\n    targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n    for k in range(num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)\n    num_identical = num_classes // 2\n    for i in range(batch_size):\n        idx_identical = np.random.choice(num_classes, size=num_identical, replace=False)\n        predictions[i, idx_identical] = predictions[i, 0]\n    targets = np.zeros(batch_size, dtype='int32')\n    for k in range(1, num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('conv1d', (2, 8, 2), (3, 2, 3), 'same', 'channels_last'), ('conv1d', (1, 8, 2), (3, 2, 3), 'valid', 'channels_last'), ('conv1d', (1, 2, 8), (3, 2, 3), 'valid', 'channels_first'), ('conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'), ('conv3d', (2, 3, 4, 5, 4), (3, 3, 3, 3, 4), 'same', 'channels_first'), ('conv3d', (2, 3, 5, 4, 6), (3, 2, 4, 3, 4), 'valid', 'channels_first'), ('conv3d', (1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'valid', 'channels_last'), ('conv3d', (1, 3, 5, 4, 2), (3, 3, 3, 2, 3), 'same', 'channels_last')])\ndef test_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last'), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first')])\ndef test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format,dilation_rate', [('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2), ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2), ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2), 'same', 'channels_last', (2, 2)), ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4), 'valid', 'channels_first', (2, 2)), ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4), 'valid', 'channels_last', (2, 2, 2)), ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4), 'same', 'channels_first', (2, 2, 2))])\ndef test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv transpose on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last', (2, 2)), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first', (2, 2))])\ndef test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('depthwise_conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('depthwise_conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('depthwise_conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('depthwise_conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last')])\ndef test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,pool_size,strides,padding,data_format,pool_mode', [('pool2d', (2, 3, 7, 7), (3, 3), (1, 1), 'same', 'channels_first', 'avg'), ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1), 'valid', 'channels_first', 'max'), ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1), 'valid', 'channels_last', 'avg'), ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1), 'same', 'channels_last', 'max'), ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1), 'same', 'channels_first', 'avg'), ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1), 'valid', 'channels_first', 'max'), ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1), 'valid', 'channels_last', 'avg'), ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1), 'same', 'channels_last', 'max')])\ndef test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n    check_single_tensor_operation(op, input_shape, WITH_NP, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, pool_mode=pool_mode, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [('separable_conv1d', (2, 8, 2), (3,), 1, 'same', 'channels_last'), ('separable_conv1d', (1, 8, 2), (3,), 2, 'valid', 'channels_last'), ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'), ('separable_conv2d', (2, 3, 5, 6), (4, 3), 2, 'valid', 'channels_first'), ('separable_conv2d', (1, 6, 5, 3), (3, 4), 1, 'valid', 'channels_last'), ('separable_conv2d', (1, 7, 6, 3), (3, 3), 2, 'same', 'channels_last')])\ndef test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format):\n    if data_format == 'channels_first':\n        input_depth = input_shape[1]\n    else:\n        input_depth = input_shape[-1]\n    _, x = parse_shape_or_val(input_shape)\n    _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n    _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) + (input_depth * depth_multiplier, 7))\n    y1 = KNP.separable_conv(x, depthwise, pointwise, padding=padding, data_format=data_format)\n    if K.backend() == 'cntk':\n        _, cntk_func = cntk_func_tensors(op, [input_shape, depthwise, pointwise], padding=padding, data_format=data_format)\n        y2 = cntk_func([x])[0]\n    else:\n        y2 = K.eval(getattr(K, op)(K.variable(x), K.variable(depthwise), K.variable(pointwise), padding=padding, data_format=data_format))\n    assert_allclose(y1, y2, atol=1e-05)",
                            "def test_random_normal(self):\n    for mean, std in [(0.0, 1.0), (-10.0, 5.0)]:\n        rand = K.eval(K.random_normal((300, 200), mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n        assert np.abs(np.mean(rand) - mean) < std * 0.015\n        assert np.abs(np.std(rand) - std) < std * 0.015\n        r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - mean) < std * 0.015\n        assert np.abs(np.std(samples) - std) < std * 0.015",
                            "def test_random_uniform(self):\n    min_val = -1.0\n    max_val = 1.0\n    rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand)) < 0.015\n    assert max_val - 0.015 < np.max(rand) <= max_val\n    assert min_val + 0.015 > np.min(rand) >= min_val\n    r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples)) < 0.015\n    assert max_val - 0.015 < np.max(samples) <= max_val\n    assert min_val + 0.015 > np.min(samples) >= min_val",
                            "def test_random_binomial(self):\n    p = 0.5\n    rand = K.eval(K.random_binomial((200, 100), p))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand) - p) < 0.015\n    assert np.max(rand) == 1\n    assert np.min(rand) == 0\n    r = K.random_binomial((10, 10), p)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples) - p) < 0.015\n    assert np.max(samples) == 1\n    assert np.min(samples) == 0",
                            "def test_truncated_normal(self):\n    mean = 0.0\n    std = 1.0\n    min_val = -2.0\n    max_val = 2.0\n    rand = K.eval(K.truncated_normal((300, 200), mean=mean, stddev=std, seed=1337))\n    assert rand.shape == (300, 200)\n    assert np.abs(np.mean(rand) - mean) < 0.015\n    assert np.max(rand) <= max_val\n    assert np.min(rand) >= min_val\n    assert np.abs(np.std(rand) - std * 0.87962) < 0.015",
                            "def test_conv_invalid_use(self):\n    dummy_x_1d = K.variable(np.ones((4, 8, 2)))\n    dummy_w_1d = K.variable(np.ones((3, 2, 3)))\n    dummy_x_2d = K.variable(np.ones((2, 3, 4, 5)))\n    dummy_w_2d = K.variable(np.ones((2, 2, 3, 4)))\n    dummy_x_3d = K.variable(np.ones((2, 3, 4, 5, 4)))\n    dummy_w_3d = K.variable(np.ones((2, 2, 2, 3, 4)))\n    dummy_w1x1_2d = K.variable(np.ones((1, 1, 12, 7)))\n    with pytest.raises(ValueError):\n        K.conv1d(dummy_x_1d, dummy_w_1d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv3d(dummy_x_3d, dummy_w_3d, data_format='channels_middle')\n    if K.backend() != 'theano':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    if K.backend() == 'cntk':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, strides=(2, 2), dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, strides=(2, 2), dilation_rate=(1, 2))",
                            "def test_pooling_invalid_use(self):\n    for input_shape, pool_size in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n        x = K.variable(np.random.random(input_shape))\n        if len(pool_size) == 2:\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, pool_mode='median')\n        else:\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, pool_mode='median')",
                            "def test_resize_images(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_images', x_shape, WITH_NP, cntk_dynamicity=True, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_images(K.variable(xval), 2, 2, data_format='channels_middle')",
                            "@staticmethod\ndef _helper_bilinear(data_format, height_factor, width_factor):\n    x_shape = (2, 3, 4, 5)\n    check_single_tensor_operation('resize_images', x_shape, [KTF, KTH], height_factor=height_factor, width_factor=width_factor, data_format=data_format, interpolation='bilinear')",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\ndef test_resize_images_bilinear(self, data_format):\n    self._helper_bilinear(data_format, 2, 2)\n    with pytest.raises(NotImplementedError):\n        self._helper_bilinear(data_format, 4, 4)",
                            "def test_resize_volumes(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_volumes', x_shape, WITH_NP, cntk_dynamicity=True, depth_factor=2, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_volumes(K.variable(xval), 2, 2, 2, data_format='channels_middle')",
                            "def test_temporal_padding(self):\n    check_single_tensor_operation('temporal_padding', (4, 3, 3), WITH_NP)\n    check_single_tensor_operation('temporal_padding', (2, 3, 4), WITH_NP, padding=(1, 2))",
                            "def test_spatial_2d_padding(self):\n    padding = ((1, 2), (2, 1))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, 1))\n        y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_2d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_spatial_3d_padding(self):\n    padding = ((1, 2), (2, 1), (1, 2))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, None, 1))\n        y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_3d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_bias_add(self):\n    for data_format in ['channels_first', 'channels_last']:\n        for shape in [(), (3,), (2, 3), (5, 3, 2)]:\n            if data_format == 'channels_first':\n                x_shape = (1, 4) + shape\n            else:\n                x_shape = (1,) + shape + (4,)\n            bias_shape = (4,)\n            check_two_tensor_operation('bias_add', x_shape, bias_shape, WITH_NP, cntk_dynamicity=True, data_format=data_format)\n        if data_format == 'channels_first':\n            x_shape = (20, 6, 10)\n        else:\n            x_shape = (20, 10, 6)\n        check_two_tensor_operation('bias_add', x_shape, (10, 6), WITH_NP, cntk_dynamicity=True, data_format=data_format)\n    x = K.variable(np.random.random(x_shape))\n    b = K.variable(np.random.random(bias_shape))\n    with pytest.raises(ValueError):\n        K.bias_add(x, b, data_format='channels_middle')",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='Specific to Theano.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_th(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes='per-activation')\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Specific to Tensorflow.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_tf(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'cntk', reason='Specific to CNTK.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_cntk(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.placeholder(x_shape)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.function([x], [z])([x_val])[0]\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_ctc(self):\n    if K.backend() == 'theano':\n        ref = [1.73308, 3.81351]\n    else:\n        ref = [3.34211, 5.42262]\n    label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n    input_lens = np.expand_dims(np.asarray([5, 5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]], [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508], [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549], [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456], [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345], [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)\n    if K.backend() == 'theano':\n        ref = [1.73308]\n    else:\n        ref = [3.34211]\n    input_lens = np.expand_dims(np.asarray([5]), 1)\n    label_lens = np.expand_dims(np.asarray([5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Test adapted from tensorflow.')\ndef test_ctc_decode_greedy(self):\n    \"\"\"Test two batch entries - best path decoder.\"\"\"\n    max_time_steps = 6\n    seq_len_0 = 4\n    input_prob_matrix_0 = np.asarray([[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.4, 0.6], [0.0, 0.0, 0.4, 0.6], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    seq_len_1 = 5\n    input_prob_matrix_1 = np.asarray([[0.1, 0.9, 0.0, 0.0], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.1, 0.9], [0.0, 0.9, 0.1, 0.1], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    inputs = [np.vstack([input_prob_matrix_0[t, :], input_prob_matrix_1[t, :]]) for t in range(max_time_steps)]\n    inputs = np.asarray(inputs).transpose((1, 0, 2))\n    input_length = np.array([seq_len_0, seq_len_1], dtype=np.int32)\n    decode_pred_np, log_prob_pred_np = KNP.ctc_decode(inputs, input_length, greedy=True)\n    inputs = K.variable(inputs)\n    input_length = K.variable(input_length)\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=True)\n    assert len(decode_pred_tf) == 1\n    decode_pred = K.eval(decode_pred_tf[0])\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    assert np.alltrue(decode_pred_np == decode_pred)\n    assert np.allclose(log_prob_pred_np, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='tensorflow-way slice is only supported in tensorflow.')\n@pytest.mark.parametrize('x_size', [[1, 1, 3], [1, 2, 3], [2, 1, 3]])\ndef test_slice(self, x_size):\n    npt = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])\n    x_start = [1, 0, 0]\n    tft = K.constant(npt)\n    test_input = K.eval(K.slice(tft, x_start, x_size))\n    expected = KNP.slice(npt, x_start, x_size)\n    assert np.allclose(test_input, expected)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search(self):\n    \"\"\"Test one batch, two beams - hibernating beam search.\"\"\"\n    depth = 6\n    seq_len_0 = 5\n    input_prob_matrix_0 = np.asarray([[0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908], [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517], [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763], [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655], [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878], [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]], dtype=np.float32)\n    input_prob_matrix_0 = input_prob_matrix_0 + 2.0\n    inputs = [input_prob_matrix_0[t, :][np.newaxis, :] for t in range(seq_len_0)] + 2 * [np.zeros((1, depth), dtype=np.float32)]\n    inputs = np.exp(inputs)\n    inputs = K.variable(inputs.transpose((1, 0, 2)))\n    input_length = K.variable(np.array([seq_len_0], dtype=np.int32))\n    log_prob_truth = np.array([-5.811451, -6.63339], np.float32)[np.newaxis, :]\n    decode_truth = [np.array([1, 0]), np.array([[1]])]\n    beam_width = 2\n    top_paths = 2\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=False, beam_width=beam_width, top_paths=top_paths)\n    assert len(decode_pred_tf) == top_paths\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    for i in range(top_paths):\n        assert np.alltrue(decode_truth[i] == K.eval(decode_pred_tf[i]))\n    assert np.allclose(log_prob_truth, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search_no_merge(self):\n    input_prob = np.array([[[0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]])\n    input_len = np.array(input_prob.shape[0] * [input_prob.shape[1]])\n\n    def decode(merge_repeated):\n        input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n        input_len_tensor = K.placeholder(shape=None, dtype='int64')\n        paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n        decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n        paths = decode_func([input_prob, input_len])\n        return paths\n    assert np.allclose(decode(merge_repeated=True), [np.array([[0, 1]])])\n    assert np.allclose(decode(merge_repeated=False), [np.array([[0, 0, 1, 1]])])",
                            "def test_one_hot(self):\n    input_length = 10\n    num_classes = 20\n    batch_size = 30\n    indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n    oh = KNP.one_hot(np.int32(indices), num_classes)\n    koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n    assert np.all(koh == oh)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_dot(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense = x_sparse.toarray()\n    W = np.random.random((5, 4))\n    t_W = K.variable(W)\n    k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n    k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n    assert k_s.shape == k_d.shape\n    assert_allclose(k_s, k_d, atol=1e-05)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_concat(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_1 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_2 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense_1 = x_sparse_1.toarray()\n    x_dense_2 = x_sparse_2.toarray()\n    k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n    assert K.is_sparse(k_s)\n    k_s_d = K.eval(k_s)\n    k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n    assert k_s_d.shape == k_d.shape\n    assert_allclose(k_s_d, k_d, atol=1e-05)",
                            "def test_stack(self):\n    tensor_list = [np.random.randn(5, 4, 6, 10) for _ in range(5)]\n    stack_axis = 3\n    results = []\n    if WITH_NP[0] == KC:\n        check_two_tensor_operation('stack', (5, 4, 6, 10), (5, 4, 6, 10), WITH_NP, axis=stack_axis, concat_args=True)\n    else:\n        for k in WITH_NP:\n            tensor_list_var = [k.variable(tensor) for tensor in tensor_list]\n            out = k.eval(k.stack(tensor_list_var, axis=stack_axis))\n            results.append(out)\n        assert_list_pairwise(results)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_map(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    vx = K.variable(x)\n    kx = K.eval(K.map_fn(K.sum, vx))\n    kx2 = K.eval(K.map_fn(lambda i: K.sum(vx[i]), K.arange(10), dtype=K.floatx()))\n    assert (10,) == kx.shape\n    assert (10,) == kx2.shape\n    assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n    assert_allclose(kx, kx2, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldl(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n    assert (3,) == kx.shape\n    assert_allclose(x.sum(axis=0), kx, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldr(self):\n    x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n    vx = K.variable(x)\n    p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n    p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n    assert p1 < p2\n    assert 9e-38 < p2 <= 1e-37",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk has issues with negative number.')\ndef test_arange(self):\n    for test_value in (-20, 0, 1, 10):\n        a_list = []\n        dtype_list = []\n        for k in WITH_NP:\n            t = k.arange(test_value)\n            a = k.eval(t)\n            assert np.array_equal(a, np.arange(test_value))\n            dtype_list.append(k.dtype(t))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for start, stop, step in ((0, 5, 1), (-5, 5, 2), (0, 1, 2)):\n        a_list = []\n        for k in WITH_NP:\n            a = k.eval(k.arange(start, stop, step))\n            assert np.array_equal(a, np.arange(start, stop, step))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for dtype in ('int32', 'int64', 'float32', 'float64'):\n        for k in WITH_NP:\n            t = k.arange(10, dtype=dtype)\n            assert k.dtype(t) == dtype\n    start = K.constant(1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 1\n    start = K.constant(-1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 0",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_train_phase(self, training):\n    check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_test_phase(self, training):\n    check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "def test_setfloatx_incorrect_values(self):\n    old_floatx = floatx()\n    initial = floatx()\n    for value in ['', 'beerfloat', 123]:\n        with pytest.raises(ValueError):\n            set_floatx(value)\n    assert floatx() == initial\n    set_floatx(old_floatx)",
                            "def DISABLED_test_setfloatx_correct_values(self):\n    \"\"\"Disabled because it is not thread safe at this time.\"\"\"\n    old_floatx = floatx()\n    for value in ['float16', 'float32', 'float64']:\n        set_floatx(value)\n        assert floatx() == value\n    set_floatx(old_floatx)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support float16')\ndef DISABLED_test_set_floatx(self):\n    \"\"\"Disabled because it is not thread safe at this time.\n\n    Make sure that changes to the global floatx are effectively\n    taken into account by the backend.\n    \"\"\"\n    old_floatx = floatx()\n    set_floatx('float16')\n    var = variable([10])\n    check_dtype(var, 'float16')\n    set_floatx('float64')\n    var = variable([10])\n    check_dtype(var, 'float64')\n    set_floatx(old_floatx)",
                            "def test_dtype(self):\n    assert K.dtype(K.variable(1, dtype='float64')) == 'float64'\n    assert K.dtype(K.variable(1, dtype='float32')) == 'float32'\n    assert K.dtype(K.variable(1, dtype='float16')) == 'float16'",
                            "def test_variable_support_bool_dtype(self):\n    if K.backend() == 'tensorflow':\n        assert K.dtype(K.variable(1, dtype='int16')) == 'int16'\n        assert K.dtype(K.variable(False, dtype='bool')) == 'bool'\n        with pytest.raises(TypeError):\n            K.variable('', dtype='unsupported')",
                            "def test_clip_supports_tensor_arguments(self):\n    x = K.variable([-10.0, -5.0, 0.0, 5.0, 10.0])\n    min_value = K.variable([-5.0, -4.0, 0.0, 3.0, 5.0])\n    max_value = K.variable([5.0, 4.0, 1.0, 4.0, 9.0])\n    assert np.allclose(K.eval(K.clip(x, min_value, max_value)), np.asarray([-5.0, -4.0, 0.0, 4.0, 9.0], dtype=np.float32))",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(), reason='This test is for tensorflow parallelism.')\ndef test_tensorflow_session_parallelism_settings(self, monkeypatch):\n    for threads in [1, 2]:\n        K.clear_session()\n        monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n        cfg = K.get_session()._config\n        assert cfg.intra_op_parallelism_threads == threads\n        assert cfg.inter_op_parallelism_threads == threads",
                            "def batch_shape(shape):\n    return (batch_size,) + shape[1:]",
                            "def random(shape):\n    return np.random.random(batch_shape(shape))",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn(inputs, states):\n        assert len(states) == 1\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y])\n    return simple_rnn",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_with_extra_mock_state(inputs, states):\n        assert len(states) == 2\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y, backend.concatenate([y, y], axis=-1)])\n    return simple_rnn_with_extra_mock_state",
                            "def get_step_function(backend, w_i):\n\n    def simple_no_states(inputs, states):\n        assert len(states) == 0\n        y = backend.dot(inputs, w_i)\n        return (y, [])\n    return simple_no_states",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_add_constant(inputs, states_and_constants):\n        [h, c] = states_and_constants\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n        return (y, [y])\n    return simple_rnn_add_constant",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def step_function(inputs, states):\n    outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n    return (outputs, states)",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def decode(merge_repeated):\n    input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n    input_len_tensor = K.placeholder(shape=None, dtype='int64')\n    paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n    decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n    paths = decode_func([input_prob, input_len])\n    return paths",
                            "def simple_rnn(inputs, states):\n    assert len(states) == 1\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y])",
                            "def simple_rnn_with_extra_mock_state(inputs, states):\n    assert len(states) == 2\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y, backend.concatenate([y, y], axis=-1)])",
                            "def simple_no_states(inputs, states):\n    assert len(states) == 0\n    y = backend.dot(inputs, w_i)\n    return (y, [])",
                            "def simple_rnn_add_constant(inputs, states_and_constants):\n    [h, c] = states_and_constants\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n    return (y, [y])"
                        ],
                        "constructor_variables": [],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "test_is_keras_tensor(self)",
                            "test_set_learning_phase(self)",
                            "test_eye(self)",
                            "test_ones(self)",
                            "test_zeros(self)",
                            "test_ones_like(self)",
                            "test_zeros_like(self)",
                            "test_linear_operations(self)",
                            "test_random_variables(self)",
                            "test_batch_dot_shape(self)",
                            "test_shape_operations(self)",
                            "test_none_shape_operations(self)",
                            "test_repeat_elements(self)",
                            "test_tile(self)",
                            "test_gather(self)",
                            "test_value_manipulation(self, function_name)",
                            "test_print_tensor(self)",
                            "test_elementwise_operations(self)",
                            "test_reset_uids(self)",
                            "test_cumsum_cumprod(self)",
                            "test_log(self)",
                            "test_update_add(self)",
                            "test_update_sub(self)",
                            "test_gradient(self)",
                            "test_stop_gradient(self)",
                            "test_function(self)",
                            "test_function_tf_fetches(self)",
                            "test_function_tf_feed_dict(self)",
                            "test_function_tf_run_options_with_run_metadata(self)",
                            "test_function_tf_string_input(self)",
                            "test_rnn(self)",
                            "test_rnn_additional_states(self)",
                            "test_rnn_no_states(self)",
                            "test_rnn_constants(self)",
                            "test_rnn_output_and_state_masking_independent(self)",
                            "test_rnn_output_num_dim_larger_than_2_masking(self)",
                            "test_rnn_state_num_dim_larger_than_2_masking(self)",
                            "test_logsumexp(self, x_np, axis, keepdims)",
                            "test_logsumexp_optim(self)",
                            "test_switch(self)",
                            "test_dropout(self)",
                            "test_relu(self, alpha, max_value, threshold)",
                            "test_nn_operations(self)",
                            "test_in_top_k(self)",
                            "test_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format)",
                            "test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate)",
                            "test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate)",
                            "test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode)",
                            "test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format)",
                            "test_random_normal(self)",
                            "test_random_uniform(self)",
                            "test_random_binomial(self)",
                            "test_truncated_normal(self)",
                            "test_conv_invalid_use(self)",
                            "test_pooling_invalid_use(self)",
                            "test_resize_images(self)",
                            "_helper_bilinear(data_format, height_factor, width_factor)",
                            "test_resize_images_bilinear(self, data_format)",
                            "test_resize_volumes(self)",
                            "test_temporal_padding(self)",
                            "test_spatial_2d_padding(self)",
                            "test_spatial_3d_padding(self)",
                            "test_bias_add(self)",
                            "test_batchnorm_th(self, x_shape)",
                            "test_batchnorm_tf(self, x_shape)",
                            "test_batchnorm_cntk(self, x_shape)",
                            "test_ctc(self)",
                            "test_ctc_decode_greedy(self)",
                            "test_slice(self, x_size)",
                            "test_ctc_decode_beam_search(self)",
                            "test_ctc_decode_beam_search_no_merge(self)",
                            "test_one_hot(self)",
                            "test_sparse_dot(self)",
                            "test_sparse_concat(self)",
                            "test_stack(self)",
                            "test_map(self)",
                            "test_foldl(self)",
                            "test_foldr(self)",
                            "test_arange(self)",
                            "test_in_train_phase(self, training)",
                            "test_in_test_phase(self, training)",
                            "test_setfloatx_incorrect_values(self)",
                            "DISABLED_test_setfloatx_correct_values(self)",
                            "DISABLED_test_set_floatx(self)",
                            "test_dtype(self)",
                            "test_variable_support_bool_dtype(self)",
                            "test_clip_supports_tensor_arguments(self)",
                            "test_tensorflow_session_parallelism_settings(self, monkeypatch)",
                            "batch_shape(shape)",
                            "random(shape)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i)",
                            "get_step_function(backend, w_i, w_h)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "decode(merge_repeated)",
                            "simple_rnn(inputs, states)",
                            "simple_rnn_with_extra_mock_state(inputs, states)",
                            "simple_no_states(inputs, states)",
                            "simple_rnn_add_constant(inputs, states_and_constants)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "test_random_normal",
                    "function_code": "def test_random_normal(self):\n    # test standard normal as well as a normal with a different set of parameters\n    for mean, std in [(0., 1.), (-10., 5.)]:\n        rand = K.eval(K.random_normal((300, 200),\n                                      mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n        assert np.abs(np.mean(rand) - mean) < std * 0.015\n        assert np.abs(np.std(rand) - std) < std * 0.015\n\n        # test that random_normal also generates different values when used\n        # within a function\n        r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - mean) < std * 0.015\n        assert np.abs(np.std(samples) - std) < std * 0.015\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 1365,
                    "end_line": 1379,
                    "variables": {
                        "mean": [
                            1376,
                            1378,
                            1367,
                            1369,
                            1371
                        ],
                        "std": [
                            1376,
                            1378,
                            1379,
                            1367,
                            1369,
                            1371,
                            1372
                        ],
                        "rand": [
                            1368,
                            1370,
                            1371,
                            1372
                        ],
                        "K.eval": [
                            1368,
                            1377
                        ],
                        "K": [
                            1368,
                            1377,
                            1376
                        ],
                        "K.random_normal": [
                            1368,
                            1376
                        ],
                        "rand.shape": [
                            1370
                        ],
                        "np.abs": [
                            1379,
                            1378,
                            1371,
                            1372
                        ],
                        "np": [
                            1377,
                            1378,
                            1379,
                            1371,
                            1372
                        ],
                        "np.mean": [
                            1378,
                            1371
                        ],
                        "np.std": [
                            1379,
                            1372
                        ],
                        "r": [
                            1376,
                            1377
                        ],
                        "samples": [
                            1377,
                            1378,
                            1379
                        ],
                        "np.array": [
                            1377
                        ],
                        "_": [
                            1377
                        ],
                        "range": [
                            1377
                        ]
                    },
                    "filtered_variables": {
                        "mean": [
                            1376,
                            1378,
                            1367,
                            1369,
                            1371
                        ],
                        "std": [
                            1376,
                            1378,
                            1379,
                            1367,
                            1369,
                            1371,
                            1372
                        ],
                        "rand": [
                            1368,
                            1370,
                            1371,
                            1372
                        ],
                        "K.eval": [
                            1368,
                            1377
                        ],
                        "K": [
                            1368,
                            1377,
                            1376
                        ],
                        "K.random_normal": [
                            1368,
                            1376
                        ],
                        "rand.shape": [
                            1370
                        ],
                        "np.abs": [
                            1379,
                            1378,
                            1371,
                            1372
                        ],
                        "np": [
                            1377,
                            1378,
                            1379,
                            1371,
                            1372
                        ],
                        "np.mean": [
                            1378,
                            1371
                        ],
                        "np.std": [
                            1379,
                            1372
                        ],
                        "r": [
                            1376,
                            1377
                        ],
                        "samples": [
                            1377,
                            1378,
                            1379
                        ],
                        "np.array": [
                            1377
                        ],
                        "_": [
                            1377
                        ]
                    },
                    "diff_line_number": 1366,
                    "class_data": {
                        "signature": "class TestBackend(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def test_is_keras_tensor(self):\n    np_var = np.array([1, 2])\n    with pytest.raises(ValueError):\n        K.is_keras_tensor(np_var)\n    keras_var = K.variable(np_var)\n    assert K.is_keras_tensor(keras_var) is False\n    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n    assert K.is_keras_tensor(keras_placeholder) is False",
                            "def test_set_learning_phase(self):\n    with pytest.raises(ValueError):\n        K.set_learning_phase(2)",
                            "def test_eye(self):\n    check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)",
                            "def test_ones(self):\n    check_single_tensor_operation('ones', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_zeros(self):\n    check_single_tensor_operation('zeros', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_ones_like(self):\n    check_single_tensor_operation('ones_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_zeros_like(self):\n    check_single_tensor_operation('zeros_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_linear_operations(self):\n    check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n    check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=(1, 1))\n    check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n    check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n    if K.backend() != 'cntk':\n        check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=(1, 2))",
                            "def test_random_variables(self):\n    check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP, low=0.0, high=1.0, shape_or_val=False, assert_value_equality=False)\n    check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP, mean=0.0, scale=1.0, shape_or_val=False, assert_value_equality=False)",
                            "def test_batch_dot_shape(self):\n    test_cases = []\n    test_cases.append([(None, 3, 4, 5), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 3, 4, 5), (None, 2, 4), 2])\n    test_cases.append([(None, 3, 4), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 4, 3), (None, 3, 5), (2, 1)])\n    test_cases.append([(None, 4), (None, 3, 4), (1, 2)])\n    test_cases.append([(None, 4), (None, 4), None])\n    batch_size = 7\n\n    def batch_shape(shape):\n        return (batch_size,) + shape[1:]\n\n    def random(shape):\n        return np.random.random(batch_shape(shape))\n    for x_shape, y_shape, axes in test_cases:\n        x_np = random(x_shape)\n        y_np = random(y_shape)\n        z_np = KNP.batch_dot(x_np, y_np, axes)\n        x = K.placeholder(shape=x_shape)\n        y = K.placeholder(shape=y_shape)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        f = K.function([x, y], [z])\n        assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        if K.backend() != 'cntk':\n            x = K.placeholder(ndim=len(x_shape))\n            y = K.placeholder(ndim=len(y_shape))\n            z = K.batch_dot(x, y, axes)\n            z_shape = K.int_shape(z)\n            if z_shape is not None:\n                assert len(z_shape) == z_np.ndim\n                assert set(z_shape) <= set((None, 1))\n            f = K.function([x, y], [z])\n            assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        x = K.variable(x_np)\n        y = K.variable(y_np)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        z = K.eval(z)\n        assert_allclose(z, z_np, atol=1e-05)",
                            "def test_shape_operations(self):\n    check_two_tensor_operation('concatenate', (4, 3), (4, 2), WITH_NP, axis=-1, concat_args=True)\n    check_single_tensor_operation('reshape', (4, 2), WITH_NP, shape=(8, 1))\n    check_single_tensor_operation('permute_dimensions', (4, 2, 3), WITH_NP, pattern=(2, 0, 1))\n    check_single_tensor_operation('repeat', (4, 1), WITH_NP, n=3)\n    check_single_tensor_operation('flatten', (4, 1), WITH_NP)\n    check_single_tensor_operation('batch_flatten', (20, 2, 5), WITH_NP, cntk_dynamicity=True)\n    check_single_tensor_operation('expand_dims', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('expand_dims', (4, 3, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('squeeze', (4, 3, 1), WITH_NP, axis=2)\n    check_single_tensor_operation('squeeze', (4, 1, 1), WITH_NP, axis=1)\n    check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)}, 'squeeze', {'axis': 2}, (4, 3, 1, 1), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='We only test the shape inference of the theano backend.')\ndef test_none_shape_operations(self):\n    x = K.placeholder((3, None, 4))\n    y = K.batch_flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (3, None)\n    y = K.flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (None,)",
                            "def test_repeat_elements(self):\n    reps = 3\n    for ndims in [1, 2, 3]:\n        shape = np.arange(2, 2 + ndims)\n        arr = np.arange(np.prod(shape)).reshape(shape)\n        for rep_axis in range(ndims):\n            check_single_tensor_operation('repeat_elements', arr, WITH_NP, rep=reps, axis=rep_axis)\n            if K.backend() != 'cntk':\n                shape = list(shape)\n                shape[rep_axis] = None\n                x = K.placeholder(shape=shape)\n                y = K.repeat_elements(x, reps, axis=rep_axis)\n                assert y._keras_shape == tuple(shape)\n                assert y._keras_shape == K.int_shape(y)",
                            "def test_tile(self):\n    shape = (3, 4)\n    arr = np.arange(np.prod(shape)).reshape(shape)\n    check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n    check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 4))\n        n = 2\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 8)\n        n = (4, 3)\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 12)",
                            "def test_gather(self):\n    shape = (10, 2, 3)\n    ref = np.arange(np.prod(shape)).reshape(shape)\n    inds = [1, 3, 7, 9]\n    t_list = [k.gather(k.variable(ref), k.variable(inds, dtype='int32')) for k in WITH_NP]\n    z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32'))) for k in WITH_NP]\n    assert_list_pairwise(z_list)\n    assert_list_keras_shape(t_list, z_list)\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 3, 4))\n        indices = K.placeholder(shape=(5, 6), dtype='int32')\n        y = K.gather(x, indices)\n        assert y._keras_shape == (5, 6, 3, 4)",
                            "@pytest.mark.parametrize('function_name', ['get_value', 'count_params', 'int_shape', 'get_variable_shape'])\ndef test_value_manipulation(self, function_name):\n    val = np.random.random((4, 2))\n    v_list = [getattr(k, function_name)(k.variable(val)) for k in WITH_NP]\n    if function_name == 'get_value':\n        assert_list_pairwise(v_list)\n    else:\n        assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)",
                            "def test_print_tensor(self):\n    check_single_tensor_operation('print_tensor', (), WITH_NP)\n    check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n    check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n    check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)",
                            "def test_elementwise_operations(self):\n    check_single_tensor_operation('max', (4, 2), WITH_NP)\n    check_single_tensor_operation('max', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('max', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('min', (4, 2), WITH_NP)\n    check_single_tensor_operation('min', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('min', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('mean', (4, 2), WITH_NP)\n    check_single_tensor_operation('mean', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=-1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('var', (4, 2), WITH_NP)\n    check_single_tensor_operation('var', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('var', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('std', (4, 2), WITH_NP)\n    check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('prod', (4, 2), WITH_NP)\n    check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('any', (4, 2), WITH_NP)\n    check_single_tensor_operation('any', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('any', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('all', (4, 2), WITH_NP)\n    check_single_tensor_operation('all', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('all', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('square', (4, 2), WITH_NP)\n    check_single_tensor_operation('abs', (4, 2), WITH_NP)\n    check_single_tensor_operation('sqrt', (4, 2), WITH_NP)\n    check_single_tensor_operation('exp', (4, 2), WITH_NP)\n    check_single_tensor_operation('round', (4, 2), WITH_NP)\n    check_single_tensor_operation('sign', (4, 2), WITH_NP)\n    check_single_tensor_operation('pow', (4, 2), WITH_NP, a=3)\n    check_single_tensor_operation('clip', (4, 2), WITH_NP, min_value=0.4, max_value=0.6)\n    check_single_tensor_operation('cos', (4, 2), WITH_NP)\n    check_single_tensor_operation('sin', (4, 2), WITH_NP)\n    check_two_tensor_operation('equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('not_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('maximum', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('minimum', (4, 2), (4, 2), WITH_NP)",
                            "def test_reset_uids(self):\n    first = K.get_uid()\n    K.get_uid()\n    K.reset_uids()\n    assert K.get_uid() == first",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support cumsum and cumprod yet')\ndef test_cumsum_cumprod(self):\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP, axis=1)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk return -85.1 for zero or negative number, not nan, so can't compare with other backend.\")\ndef test_log(self):\n    check_single_tensor_operation('log', (4, 2), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_add(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    increment = np.random.randn(3, 4)\n    x += increment\n    K.eval(K.update_add(x_var, increment))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_sub(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    decrement = np.random.randn(3, 4)\n    x -= decrement\n    K.eval(K.update_sub(x_var, decrement))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk doesn't support gradient in this way.\")\ndef test_gradient(self):\n    val = np.random.random((4, 2))\n    x_list = [k.variable(val) for k in [KTH, KTF]]\n    z_list = []\n    zero_list = []\n    for x, k in zip(x_list, [KTH, KTF]):\n        exp = x * k.exp(x)\n        loss = k.sum(exp)\n        zero_loss = k.stop_gradient(loss)\n        grad = k.gradients(loss, [exp])\n        zero_grad = k.gradients(loss + zero_loss, [exp])\n        z_list.append(k.eval(grad[0]))\n        zero_list.append(k.eval(zero_grad[0]))\n    assert_list_pairwise(z_list)\n    assert_list_pairwise(zero_list)\n    for i in range(len(z_list)):\n        assert_allclose(zero_list[i], z_list[i], atol=1e-05)",
                            "def test_stop_gradient(self):\n    val = np.random.random((4, 2))\n    a = K.variable(val)\n    b = K.square(a)\n    c, d = K.stop_gradient([a, b])\n    e = K.stop_gradient(b)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk currently not support function in this way, so can't test as this.\")\ndef test_function(self):\n    test_backend = [KTH, KTF]\n    val = np.random.random((4, 2))\n    input_val = np.random.random((4, 2))\n    f_list = []\n    x_list = []\n    for k in test_backend:\n        x = k.variable(val)\n        x_list.append(x)\n        y = k.placeholder(ndim=2)\n        exp = k.square(x) + y\n        update = x * 2\n        f = k.function([y], [exp], updates=[(x, update)])\n        f_list.append(f)\n    function_outputs_list = [f([input_val])[0] for f in f_list]\n    assert_list_pairwise(function_outputs_list)\n    new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n    assert_list_pairwise(new_val_list)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `fetches` argument.')\ndef test_function_tf_fetches(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], updates=[(x, x_placeholder + 1.0)], fetches=[K.update(y, 5.0)])\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert K.get_session().run(fetches=[x, y]) == [11.0, 5.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `feed_dict` argument.')\ndef test_function_tf_feed_dict(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    feed_dict = {y_placeholder: 3.0}\n    f = K.function(inputs=[x_placeholder], outputs=[x_placeholder + 1.0], updates=[(x, x_placeholder + 10.0)], feed_dict=feed_dict, fetches=[K.update(y, y_placeholder * 10.0)])\n    output = f([10.0])\n    assert output == [11.0]\n    assert K.get_session().run(fetches=[x, y]) == [20.0, 30.0]\n    feed_dict[y_placeholder] = 4.0\n    output = f([20.0])\n    assert output == [21.0]\n    assert K.get_session().run(fetches=[x, y]) == [30.0, 40.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `options` and `run_metadata` arguments.')\ndef test_function_tf_run_options_with_run_metadata(self):\n    from tensorflow.core.protobuf import config_pb2\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    run_options = config_pb2.RunOptions(output_partition_graphs=True)\n    run_metadata = config_pb2.RunMetadata()\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], options=run_options, run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) > 0\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) == 0",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `string` type for a tensor.')\ndef test_function_tf_string_input(self):\n    x_placeholder = K.placeholder(shape=(), dtype='string')\n    x_identity = K.identity(x_placeholder)\n    f = K.function(inputs=[x_placeholder], outputs=[x_identity])\n    output = f([b'test'])\n    assert output == [b'test']",
                            "def test_rnn(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn(inputs, states):\n            assert len(states) == 1\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y])\n        return simple_rnn\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_additional_states(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    h1 = np.concatenate([h0, h0], axis=-1)\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_with_extra_mock_state(inputs, states):\n            assert len(states) == 2\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y, backend.concatenate([y, y], axis=-1)])\n        return simple_rnn_with_extra_mock_state\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0, h1], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_no_states(self):\n    num_samples = 3\n    input_dim = 8\n    output_dim = 4\n    timesteps = 5\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n\n    def get_step_function(backend, w_i):\n\n        def simple_no_states(inputs, states):\n            assert len(states) == 0\n            y = backend.dot(inputs, w_i)\n            return (y, [])\n        return simple_no_states\n    kwargs_list = [{'go_backwards': False}, {'go_backwards': True}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k), step_function_np=get_step_function(KNP, wi), inputs_np=x, initial_states_np=[], mask_np=None, **kwargs)",
                            "def test_rnn_constants(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, c = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_add_constant(inputs, states_and_constants):\n            [h, c] = states_and_constants\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n            return (y, [y])\n        return simple_rnn_add_constant\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), constants_np=[c], **kwargs)",
                            "def test_rnn_output_and_state_masking_independent(self):\n    num_samples = 2\n    num_timesteps = 4\n    state_and_io_size = 5\n    mask_last_num_timesteps = 2\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, state_and_io_size))\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[1, -mask_last_num_timesteps:] = 0\n    expected_outputs = inputs_vals.copy()\n    expected_outputs[1, -mask_last_num_timesteps:] = expected_outputs[1, -(mask_last_num_timesteps + 1)]\n    expected_state = initial_state_vals.copy()\n    expected_state[0] += num_timesteps\n    expected_state[1] += num_timesteps - mask_last_num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)\n        assert_allclose(K.eval(last_states[0]), expected_state)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_output_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n    num_features = 5\n\n    def step_function(inputs, states):\n        outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n        return (outputs, states)\n    inputs_vals = np.random.random((num_samples, num_timesteps, num_features))\n    initial_state_vals = np.random.random((num_samples, 6))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[-1, -1] = 0\n    expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n    expected_outputs[-1, -1] = expected_outputs[-1, -2]\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_state_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n    initial_state_vals = np.random.random((num_samples, 6, 7))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[0, -2:] = 0\n    expected_last_state = initial_state_vals.copy()\n    expected_last_state[0] += num_timesteps - 2\n    expected_last_state[1:] += num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(last_states[0]), expected_last_state)",
                            "@pytest.mark.parametrize('x_np,axis,keepdims', [(np.array([1.1, 0.8, 0.9]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 1, False), (np.array([[1.1, 0.8, 0.9]]), -1, False), (np.array([[1.1, 0.8, 0.9]]), 1, True), (np.array([[1.1], [1.2]]), 0, False), (np.array([[1.1], [1.2]]), 1, False), (np.array([[1.1], [1.2]]), -1, False), (np.array([[1.1], [1.2]]), -1, True), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), None, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 0, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False)])\ndef test_logsumexp(self, x_np, axis, keepdims):\n    \"\"\"\n    Check if K.logsumexp works properly for values close to one.\n    \"\"\"\n    x = K.variable(x_np)\n    assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)), np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)), rtol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='The optimization is applied only with TensorFlow.')\ndef test_logsumexp_optim(self):\n    \"\"\"\n    Check if optimization works.\n    \"\"\"\n    x_np = np.array([10000.0, 0.0001])\n    result = K.eval(K.logsumexp(K.variable(x_np), axis=0))\n    assert_allclose(result, 10000.0, rtol=1e-05)",
                            "def test_switch(self):\n    val = np.random.random()\n    z_list = []\n    for k in WITH_NP:\n        x = k.variable(val)\n        x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n        z_list.append(k.eval(x))\n    assert_list_pairwise(z_list)\n    shapes = []\n    shapes.append([(4, 3, 2), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4, 3), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4,), (4, 3, 2), (4, 3, 2)])\n    for s in shapes:\n        z_list = []\n        arrays = list(map(np.random.random, s))\n        for k in WITH_NP:\n            x, then_expr, else_expr = map(k.variable, arrays)\n            cond = k.greater_equal(x, 0.5)\n            z_list.append(k.eval(k.switch(cond, then_expr, else_expr)))\n        assert_list_pairwise(z_list)",
                            "def test_dropout(self):\n    val = np.random.random((100, 100))\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2)) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2, noise_shape=list(val.shape))) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    with pytest.raises(ValueError):\n        z = K.dropout(K.variable(val), level=-0.5)",
                            "@pytest.mark.parametrize('alpha,max_value,threshold', [(0.0, None, 0.0), (0.1, None, 0.0), (0.0, 5.0, 0.0), (0.0, None, 0.8), (0.1, 5.0, 0.0), (0.1, None, 0.8), (0.0, 5.0, 0.8), (0.1, 5.0, 0.8), (0.1, 0.0, 0.8), (0.1, 5.0, -2.8), (0.1, 9.0, 0.8)])\ndef test_relu(self, alpha, max_value, threshold):\n    check_single_tensor_operation('relu', (4, 2), WITH_NP, alpha=alpha, max_value=max_value, threshold=threshold)",
                            "def test_nn_operations(self):\n    check_single_tensor_operation('softsign', (4, 10), WITH_NP)\n    check_single_tensor_operation('softplus', (4, 10), WITH_NP)\n    check_single_tensor_operation('elu', (4, 10), WITH_NP, alpha=0.5)\n    check_single_tensor_operation('sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('hard_sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('tanh', (4, 2), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 10), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 5, 3), WITH_NP, axis=1)\n    check_single_tensor_operation('softmax', (4, 5, 3, 10), WITH_NP, axis=2)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    if K.backend() != 'cntk':\n        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841], [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n    yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016], [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n    check_two_tensor_operation('categorical_crossentropy', yval, xval, WITH_NP, cntk_two_dynamicity=True, from_logits=True)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)",
                            "def test_in_top_k(self):\n    batch_size = 20\n    num_classes = 10\n    predictions = np.random.random((batch_size, num_classes)).astype('float32')\n    targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n    for k in range(num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)\n    num_identical = num_classes // 2\n    for i in range(batch_size):\n        idx_identical = np.random.choice(num_classes, size=num_identical, replace=False)\n        predictions[i, idx_identical] = predictions[i, 0]\n    targets = np.zeros(batch_size, dtype='int32')\n    for k in range(1, num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('conv1d', (2, 8, 2), (3, 2, 3), 'same', 'channels_last'), ('conv1d', (1, 8, 2), (3, 2, 3), 'valid', 'channels_last'), ('conv1d', (1, 2, 8), (3, 2, 3), 'valid', 'channels_first'), ('conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'), ('conv3d', (2, 3, 4, 5, 4), (3, 3, 3, 3, 4), 'same', 'channels_first'), ('conv3d', (2, 3, 5, 4, 6), (3, 2, 4, 3, 4), 'valid', 'channels_first'), ('conv3d', (1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'valid', 'channels_last'), ('conv3d', (1, 3, 5, 4, 2), (3, 3, 3, 2, 3), 'same', 'channels_last')])\ndef test_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last'), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first')])\ndef test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format,dilation_rate', [('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2), ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2), ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2), 'same', 'channels_last', (2, 2)), ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4), 'valid', 'channels_first', (2, 2)), ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4), 'valid', 'channels_last', (2, 2, 2)), ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4), 'same', 'channels_first', (2, 2, 2))])\ndef test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv transpose on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last', (2, 2)), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first', (2, 2))])\ndef test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('depthwise_conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('depthwise_conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('depthwise_conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('depthwise_conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last')])\ndef test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,pool_size,strides,padding,data_format,pool_mode', [('pool2d', (2, 3, 7, 7), (3, 3), (1, 1), 'same', 'channels_first', 'avg'), ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1), 'valid', 'channels_first', 'max'), ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1), 'valid', 'channels_last', 'avg'), ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1), 'same', 'channels_last', 'max'), ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1), 'same', 'channels_first', 'avg'), ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1), 'valid', 'channels_first', 'max'), ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1), 'valid', 'channels_last', 'avg'), ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1), 'same', 'channels_last', 'max')])\ndef test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n    check_single_tensor_operation(op, input_shape, WITH_NP, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, pool_mode=pool_mode, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [('separable_conv1d', (2, 8, 2), (3,), 1, 'same', 'channels_last'), ('separable_conv1d', (1, 8, 2), (3,), 2, 'valid', 'channels_last'), ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'), ('separable_conv2d', (2, 3, 5, 6), (4, 3), 2, 'valid', 'channels_first'), ('separable_conv2d', (1, 6, 5, 3), (3, 4), 1, 'valid', 'channels_last'), ('separable_conv2d', (1, 7, 6, 3), (3, 3), 2, 'same', 'channels_last')])\ndef test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format):\n    if data_format == 'channels_first':\n        input_depth = input_shape[1]\n    else:\n        input_depth = input_shape[-1]\n    _, x = parse_shape_or_val(input_shape)\n    _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n    _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) + (input_depth * depth_multiplier, 7))\n    y1 = KNP.separable_conv(x, depthwise, pointwise, padding=padding, data_format=data_format)\n    if K.backend() == 'cntk':\n        _, cntk_func = cntk_func_tensors(op, [input_shape, depthwise, pointwise], padding=padding, data_format=data_format)\n        y2 = cntk_func([x])[0]\n    else:\n        y2 = K.eval(getattr(K, op)(K.variable(x), K.variable(depthwise), K.variable(pointwise), padding=padding, data_format=data_format))\n    assert_allclose(y1, y2, atol=1e-05)",
                            "def test_random_normal(self):\n    for mean, std in [(0.0, 1.0), (-10.0, 5.0)]:\n        rand = K.eval(K.random_normal((300, 200), mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n        assert np.abs(np.mean(rand) - mean) < std * 0.015\n        assert np.abs(np.std(rand) - std) < std * 0.015\n        r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - mean) < std * 0.015\n        assert np.abs(np.std(samples) - std) < std * 0.015",
                            "def test_random_uniform(self):\n    min_val = -1.0\n    max_val = 1.0\n    rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand)) < 0.015\n    assert max_val - 0.015 < np.max(rand) <= max_val\n    assert min_val + 0.015 > np.min(rand) >= min_val\n    r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples)) < 0.015\n    assert max_val - 0.015 < np.max(samples) <= max_val\n    assert min_val + 0.015 > np.min(samples) >= min_val",
                            "def test_random_binomial(self):\n    p = 0.5\n    rand = K.eval(K.random_binomial((200, 100), p))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand) - p) < 0.015\n    assert np.max(rand) == 1\n    assert np.min(rand) == 0\n    r = K.random_binomial((10, 10), p)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples) - p) < 0.015\n    assert np.max(samples) == 1\n    assert np.min(samples) == 0",
                            "def test_truncated_normal(self):\n    mean = 0.0\n    std = 1.0\n    min_val = -2.0\n    max_val = 2.0\n    rand = K.eval(K.truncated_normal((300, 200), mean=mean, stddev=std, seed=1337))\n    assert rand.shape == (300, 200)\n    assert np.abs(np.mean(rand) - mean) < 0.015\n    assert np.max(rand) <= max_val\n    assert np.min(rand) >= min_val\n    assert np.abs(np.std(rand) - std * 0.87962) < 0.015",
                            "def test_conv_invalid_use(self):\n    dummy_x_1d = K.variable(np.ones((4, 8, 2)))\n    dummy_w_1d = K.variable(np.ones((3, 2, 3)))\n    dummy_x_2d = K.variable(np.ones((2, 3, 4, 5)))\n    dummy_w_2d = K.variable(np.ones((2, 2, 3, 4)))\n    dummy_x_3d = K.variable(np.ones((2, 3, 4, 5, 4)))\n    dummy_w_3d = K.variable(np.ones((2, 2, 2, 3, 4)))\n    dummy_w1x1_2d = K.variable(np.ones((1, 1, 12, 7)))\n    with pytest.raises(ValueError):\n        K.conv1d(dummy_x_1d, dummy_w_1d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv3d(dummy_x_3d, dummy_w_3d, data_format='channels_middle')\n    if K.backend() != 'theano':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    if K.backend() == 'cntk':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, strides=(2, 2), dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, strides=(2, 2), dilation_rate=(1, 2))",
                            "def test_pooling_invalid_use(self):\n    for input_shape, pool_size in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n        x = K.variable(np.random.random(input_shape))\n        if len(pool_size) == 2:\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, pool_mode='median')\n        else:\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, pool_mode='median')",
                            "def test_resize_images(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_images', x_shape, WITH_NP, cntk_dynamicity=True, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_images(K.variable(xval), 2, 2, data_format='channels_middle')",
                            "@staticmethod\ndef _helper_bilinear(data_format, height_factor, width_factor):\n    x_shape = (2, 3, 4, 5)\n    check_single_tensor_operation('resize_images', x_shape, [KTF, KTH], height_factor=height_factor, width_factor=width_factor, data_format=data_format, interpolation='bilinear')",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\ndef test_resize_images_bilinear(self, data_format):\n    self._helper_bilinear(data_format, 2, 2)\n    with pytest.raises(NotImplementedError):\n        self._helper_bilinear(data_format, 4, 4)",
                            "def test_resize_volumes(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_volumes', x_shape, WITH_NP, cntk_dynamicity=True, depth_factor=2, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_volumes(K.variable(xval), 2, 2, 2, data_format='channels_middle')",
                            "def test_temporal_padding(self):\n    check_single_tensor_operation('temporal_padding', (4, 3, 3), WITH_NP)\n    check_single_tensor_operation('temporal_padding', (2, 3, 4), WITH_NP, padding=(1, 2))",
                            "def test_spatial_2d_padding(self):\n    padding = ((1, 2), (2, 1))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, 1))\n        y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_2d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_spatial_3d_padding(self):\n    padding = ((1, 2), (2, 1), (1, 2))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, None, 1))\n        y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_3d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_bias_add(self):\n    for data_format in ['channels_first', 'channels_last']:\n        for shape in [(), (3,), (2, 3), (5, 3, 2)]:\n            if data_format == 'channels_first':\n                x_shape = (1, 4) + shape\n            else:\n                x_shape = (1,) + shape + (4,)\n            bias_shape = (4,)\n            check_two_tensor_operation('bias_add', x_shape, bias_shape, WITH_NP, cntk_dynamicity=True, data_format=data_format)\n        if data_format == 'channels_first':\n            x_shape = (20, 6, 10)\n        else:\n            x_shape = (20, 10, 6)\n        check_two_tensor_operation('bias_add', x_shape, (10, 6), WITH_NP, cntk_dynamicity=True, data_format=data_format)\n    x = K.variable(np.random.random(x_shape))\n    b = K.variable(np.random.random(bias_shape))\n    with pytest.raises(ValueError):\n        K.bias_add(x, b, data_format='channels_middle')",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='Specific to Theano.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_th(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes='per-activation')\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Specific to Tensorflow.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_tf(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'cntk', reason='Specific to CNTK.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_cntk(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.placeholder(x_shape)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.function([x], [z])([x_val])[0]\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_ctc(self):\n    if K.backend() == 'theano':\n        ref = [1.73308, 3.81351]\n    else:\n        ref = [3.34211, 5.42262]\n    label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n    input_lens = np.expand_dims(np.asarray([5, 5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]], [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508], [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549], [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456], [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345], [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)\n    if K.backend() == 'theano':\n        ref = [1.73308]\n    else:\n        ref = [3.34211]\n    input_lens = np.expand_dims(np.asarray([5]), 1)\n    label_lens = np.expand_dims(np.asarray([5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Test adapted from tensorflow.')\ndef test_ctc_decode_greedy(self):\n    \"\"\"Test two batch entries - best path decoder.\"\"\"\n    max_time_steps = 6\n    seq_len_0 = 4\n    input_prob_matrix_0 = np.asarray([[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.4, 0.6], [0.0, 0.0, 0.4, 0.6], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    seq_len_1 = 5\n    input_prob_matrix_1 = np.asarray([[0.1, 0.9, 0.0, 0.0], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.1, 0.9], [0.0, 0.9, 0.1, 0.1], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    inputs = [np.vstack([input_prob_matrix_0[t, :], input_prob_matrix_1[t, :]]) for t in range(max_time_steps)]\n    inputs = np.asarray(inputs).transpose((1, 0, 2))\n    input_length = np.array([seq_len_0, seq_len_1], dtype=np.int32)\n    decode_pred_np, log_prob_pred_np = KNP.ctc_decode(inputs, input_length, greedy=True)\n    inputs = K.variable(inputs)\n    input_length = K.variable(input_length)\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=True)\n    assert len(decode_pred_tf) == 1\n    decode_pred = K.eval(decode_pred_tf[0])\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    assert np.alltrue(decode_pred_np == decode_pred)\n    assert np.allclose(log_prob_pred_np, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='tensorflow-way slice is only supported in tensorflow.')\n@pytest.mark.parametrize('x_size', [[1, 1, 3], [1, 2, 3], [2, 1, 3]])\ndef test_slice(self, x_size):\n    npt = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])\n    x_start = [1, 0, 0]\n    tft = K.constant(npt)\n    test_input = K.eval(K.slice(tft, x_start, x_size))\n    expected = KNP.slice(npt, x_start, x_size)\n    assert np.allclose(test_input, expected)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search(self):\n    \"\"\"Test one batch, two beams - hibernating beam search.\"\"\"\n    depth = 6\n    seq_len_0 = 5\n    input_prob_matrix_0 = np.asarray([[0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908], [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517], [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763], [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655], [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878], [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]], dtype=np.float32)\n    input_prob_matrix_0 = input_prob_matrix_0 + 2.0\n    inputs = [input_prob_matrix_0[t, :][np.newaxis, :] for t in range(seq_len_0)] + 2 * [np.zeros((1, depth), dtype=np.float32)]\n    inputs = np.exp(inputs)\n    inputs = K.variable(inputs.transpose((1, 0, 2)))\n    input_length = K.variable(np.array([seq_len_0], dtype=np.int32))\n    log_prob_truth = np.array([-5.811451, -6.63339], np.float32)[np.newaxis, :]\n    decode_truth = [np.array([1, 0]), np.array([[1]])]\n    beam_width = 2\n    top_paths = 2\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=False, beam_width=beam_width, top_paths=top_paths)\n    assert len(decode_pred_tf) == top_paths\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    for i in range(top_paths):\n        assert np.alltrue(decode_truth[i] == K.eval(decode_pred_tf[i]))\n    assert np.allclose(log_prob_truth, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search_no_merge(self):\n    input_prob = np.array([[[0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]])\n    input_len = np.array(input_prob.shape[0] * [input_prob.shape[1]])\n\n    def decode(merge_repeated):\n        input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n        input_len_tensor = K.placeholder(shape=None, dtype='int64')\n        paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n        decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n        paths = decode_func([input_prob, input_len])\n        return paths\n    assert np.allclose(decode(merge_repeated=True), [np.array([[0, 1]])])\n    assert np.allclose(decode(merge_repeated=False), [np.array([[0, 0, 1, 1]])])",
                            "def test_one_hot(self):\n    input_length = 10\n    num_classes = 20\n    batch_size = 30\n    indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n    oh = KNP.one_hot(np.int32(indices), num_classes)\n    koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n    assert np.all(koh == oh)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_dot(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense = x_sparse.toarray()\n    W = np.random.random((5, 4))\n    t_W = K.variable(W)\n    k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n    k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n    assert k_s.shape == k_d.shape\n    assert_allclose(k_s, k_d, atol=1e-05)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_concat(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_1 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_2 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense_1 = x_sparse_1.toarray()\n    x_dense_2 = x_sparse_2.toarray()\n    k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n    assert K.is_sparse(k_s)\n    k_s_d = K.eval(k_s)\n    k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n    assert k_s_d.shape == k_d.shape\n    assert_allclose(k_s_d, k_d, atol=1e-05)",
                            "def test_stack(self):\n    tensor_list = [np.random.randn(5, 4, 6, 10) for _ in range(5)]\n    stack_axis = 3\n    results = []\n    if WITH_NP[0] == KC:\n        check_two_tensor_operation('stack', (5, 4, 6, 10), (5, 4, 6, 10), WITH_NP, axis=stack_axis, concat_args=True)\n    else:\n        for k in WITH_NP:\n            tensor_list_var = [k.variable(tensor) for tensor in tensor_list]\n            out = k.eval(k.stack(tensor_list_var, axis=stack_axis))\n            results.append(out)\n        assert_list_pairwise(results)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_map(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    vx = K.variable(x)\n    kx = K.eval(K.map_fn(K.sum, vx))\n    kx2 = K.eval(K.map_fn(lambda i: K.sum(vx[i]), K.arange(10), dtype=K.floatx()))\n    assert (10,) == kx.shape\n    assert (10,) == kx2.shape\n    assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n    assert_allclose(kx, kx2, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldl(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n    assert (3,) == kx.shape\n    assert_allclose(x.sum(axis=0), kx, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldr(self):\n    x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n    vx = K.variable(x)\n    p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n    p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n    assert p1 < p2\n    assert 9e-38 < p2 <= 1e-37",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk has issues with negative number.')\ndef test_arange(self):\n    for test_value in (-20, 0, 1, 10):\n        a_list = []\n        dtype_list = []\n        for k in WITH_NP:\n            t = k.arange(test_value)\n            a = k.eval(t)\n            assert np.array_equal(a, np.arange(test_value))\n            dtype_list.append(k.dtype(t))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for start, stop, step in ((0, 5, 1), (-5, 5, 2), (0, 1, 2)):\n        a_list = []\n        for k in WITH_NP:\n            a = k.eval(k.arange(start, stop, step))\n            assert np.array_equal(a, np.arange(start, stop, step))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for dtype in ('int32', 'int64', 'float32', 'float64'):\n        for k in WITH_NP:\n            t = k.arange(10, dtype=dtype)\n            assert k.dtype(t) == dtype\n    start = K.constant(1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 1\n    start = K.constant(-1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 0",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_train_phase(self, training):\n    check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_test_phase(self, training):\n    check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "def test_setfloatx_incorrect_values(self):\n    old_floatx = floatx()\n    initial = floatx()\n    for value in ['', 'beerfloat', 123]:\n        with pytest.raises(ValueError):\n            set_floatx(value)\n    assert floatx() == initial\n    set_floatx(old_floatx)",
                            "def DISABLED_test_setfloatx_correct_values(self):\n    \"\"\"Disabled because it is not thread safe at this time.\"\"\"\n    old_floatx = floatx()\n    for value in ['float16', 'float32', 'float64']:\n        set_floatx(value)\n        assert floatx() == value\n    set_floatx(old_floatx)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support float16')\ndef DISABLED_test_set_floatx(self):\n    \"\"\"Disabled because it is not thread safe at this time.\n\n    Make sure that changes to the global floatx are effectively\n    taken into account by the backend.\n    \"\"\"\n    old_floatx = floatx()\n    set_floatx('float16')\n    var = variable([10])\n    check_dtype(var, 'float16')\n    set_floatx('float64')\n    var = variable([10])\n    check_dtype(var, 'float64')\n    set_floatx(old_floatx)",
                            "def test_dtype(self):\n    assert K.dtype(K.variable(1, dtype='float64')) == 'float64'\n    assert K.dtype(K.variable(1, dtype='float32')) == 'float32'\n    assert K.dtype(K.variable(1, dtype='float16')) == 'float16'",
                            "def test_variable_support_bool_dtype(self):\n    if K.backend() == 'tensorflow':\n        assert K.dtype(K.variable(1, dtype='int16')) == 'int16'\n        assert K.dtype(K.variable(False, dtype='bool')) == 'bool'\n        with pytest.raises(TypeError):\n            K.variable('', dtype='unsupported')",
                            "def test_clip_supports_tensor_arguments(self):\n    x = K.variable([-10.0, -5.0, 0.0, 5.0, 10.0])\n    min_value = K.variable([-5.0, -4.0, 0.0, 3.0, 5.0])\n    max_value = K.variable([5.0, 4.0, 1.0, 4.0, 9.0])\n    assert np.allclose(K.eval(K.clip(x, min_value, max_value)), np.asarray([-5.0, -4.0, 0.0, 4.0, 9.0], dtype=np.float32))",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(), reason='This test is for tensorflow parallelism.')\ndef test_tensorflow_session_parallelism_settings(self, monkeypatch):\n    for threads in [1, 2]:\n        K.clear_session()\n        monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n        cfg = K.get_session()._config\n        assert cfg.intra_op_parallelism_threads == threads\n        assert cfg.inter_op_parallelism_threads == threads",
                            "def batch_shape(shape):\n    return (batch_size,) + shape[1:]",
                            "def random(shape):\n    return np.random.random(batch_shape(shape))",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn(inputs, states):\n        assert len(states) == 1\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y])\n    return simple_rnn",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_with_extra_mock_state(inputs, states):\n        assert len(states) == 2\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y, backend.concatenate([y, y], axis=-1)])\n    return simple_rnn_with_extra_mock_state",
                            "def get_step_function(backend, w_i):\n\n    def simple_no_states(inputs, states):\n        assert len(states) == 0\n        y = backend.dot(inputs, w_i)\n        return (y, [])\n    return simple_no_states",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_add_constant(inputs, states_and_constants):\n        [h, c] = states_and_constants\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n        return (y, [y])\n    return simple_rnn_add_constant",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def step_function(inputs, states):\n    outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n    return (outputs, states)",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def decode(merge_repeated):\n    input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n    input_len_tensor = K.placeholder(shape=None, dtype='int64')\n    paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n    decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n    paths = decode_func([input_prob, input_len])\n    return paths",
                            "def simple_rnn(inputs, states):\n    assert len(states) == 1\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y])",
                            "def simple_rnn_with_extra_mock_state(inputs, states):\n    assert len(states) == 2\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y, backend.concatenate([y, y], axis=-1)])",
                            "def simple_no_states(inputs, states):\n    assert len(states) == 0\n    y = backend.dot(inputs, w_i)\n    return (y, [])",
                            "def simple_rnn_add_constant(inputs, states_and_constants):\n    [h, c] = states_and_constants\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n    return (y, [y])"
                        ],
                        "constructor_variables": [],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "test_is_keras_tensor(self)",
                            "test_set_learning_phase(self)",
                            "test_eye(self)",
                            "test_ones(self)",
                            "test_zeros(self)",
                            "test_ones_like(self)",
                            "test_zeros_like(self)",
                            "test_linear_operations(self)",
                            "test_random_variables(self)",
                            "test_batch_dot_shape(self)",
                            "test_shape_operations(self)",
                            "test_none_shape_operations(self)",
                            "test_repeat_elements(self)",
                            "test_tile(self)",
                            "test_gather(self)",
                            "test_value_manipulation(self, function_name)",
                            "test_print_tensor(self)",
                            "test_elementwise_operations(self)",
                            "test_reset_uids(self)",
                            "test_cumsum_cumprod(self)",
                            "test_log(self)",
                            "test_update_add(self)",
                            "test_update_sub(self)",
                            "test_gradient(self)",
                            "test_stop_gradient(self)",
                            "test_function(self)",
                            "test_function_tf_fetches(self)",
                            "test_function_tf_feed_dict(self)",
                            "test_function_tf_run_options_with_run_metadata(self)",
                            "test_function_tf_string_input(self)",
                            "test_rnn(self)",
                            "test_rnn_additional_states(self)",
                            "test_rnn_no_states(self)",
                            "test_rnn_constants(self)",
                            "test_rnn_output_and_state_masking_independent(self)",
                            "test_rnn_output_num_dim_larger_than_2_masking(self)",
                            "test_rnn_state_num_dim_larger_than_2_masking(self)",
                            "test_logsumexp(self, x_np, axis, keepdims)",
                            "test_logsumexp_optim(self)",
                            "test_switch(self)",
                            "test_dropout(self)",
                            "test_relu(self, alpha, max_value, threshold)",
                            "test_nn_operations(self)",
                            "test_in_top_k(self)",
                            "test_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format)",
                            "test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate)",
                            "test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate)",
                            "test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode)",
                            "test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format)",
                            "test_random_normal(self)",
                            "test_random_uniform(self)",
                            "test_random_binomial(self)",
                            "test_truncated_normal(self)",
                            "test_conv_invalid_use(self)",
                            "test_pooling_invalid_use(self)",
                            "test_resize_images(self)",
                            "_helper_bilinear(data_format, height_factor, width_factor)",
                            "test_resize_images_bilinear(self, data_format)",
                            "test_resize_volumes(self)",
                            "test_temporal_padding(self)",
                            "test_spatial_2d_padding(self)",
                            "test_spatial_3d_padding(self)",
                            "test_bias_add(self)",
                            "test_batchnorm_th(self, x_shape)",
                            "test_batchnorm_tf(self, x_shape)",
                            "test_batchnorm_cntk(self, x_shape)",
                            "test_ctc(self)",
                            "test_ctc_decode_greedy(self)",
                            "test_slice(self, x_size)",
                            "test_ctc_decode_beam_search(self)",
                            "test_ctc_decode_beam_search_no_merge(self)",
                            "test_one_hot(self)",
                            "test_sparse_dot(self)",
                            "test_sparse_concat(self)",
                            "test_stack(self)",
                            "test_map(self)",
                            "test_foldl(self)",
                            "test_foldr(self)",
                            "test_arange(self)",
                            "test_in_train_phase(self, training)",
                            "test_in_test_phase(self, training)",
                            "test_setfloatx_incorrect_values(self)",
                            "DISABLED_test_setfloatx_correct_values(self)",
                            "DISABLED_test_set_floatx(self)",
                            "test_dtype(self)",
                            "test_variable_support_bool_dtype(self)",
                            "test_clip_supports_tensor_arguments(self)",
                            "test_tensorflow_session_parallelism_settings(self, monkeypatch)",
                            "batch_shape(shape)",
                            "random(shape)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i)",
                            "get_step_function(backend, w_i, w_h)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "decode(merge_repeated)",
                            "simple_rnn(inputs, states)",
                            "simple_rnn_with_extra_mock_state(inputs, states)",
                            "simple_no_states(inputs, states)",
                            "simple_rnn_add_constant(inputs, states_and_constants)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "test_random_uniform",
                    "function_code": "def test_random_uniform(self):\n    min_val = -1.\n    max_val = 1.\n    rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand)) < 0.015\n    assert max_val - 0.015 < np.max(rand) <= max_val\n    assert min_val + 0.015 > np.min(rand) >= min_val\n\n    r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples)) < 0.015\n    assert max_val - 0.015 < np.max(samples) <= max_val\n    assert min_val + 0.015 > np.min(samples) >= min_val\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 1381,
                    "end_line": 1394,
                    "variables": {
                        "min_val": [
                            1382,
                            1384,
                            1388,
                            1390,
                            1394
                        ],
                        "max_val": [
                            1383,
                            1384,
                            1387,
                            1390,
                            1393
                        ],
                        "rand": [
                            1384,
                            1385,
                            1386,
                            1387,
                            1388
                        ],
                        "K.eval": [
                            1384,
                            1391
                        ],
                        "K": [
                            1384,
                            1390,
                            1391
                        ],
                        "K.random_uniform": [
                            1384,
                            1390
                        ],
                        "rand.shape": [
                            1385
                        ],
                        "np.abs": [
                            1392,
                            1386
                        ],
                        "np": [
                            1386,
                            1387,
                            1388,
                            1391,
                            1392,
                            1393,
                            1394
                        ],
                        "np.mean": [
                            1392,
                            1386
                        ],
                        "np.max": [
                            1393,
                            1387
                        ],
                        "np.min": [
                            1394,
                            1388
                        ],
                        "r": [
                            1390,
                            1391
                        ],
                        "samples": [
                            1392,
                            1393,
                            1394,
                            1391
                        ],
                        "np.array": [
                            1391
                        ],
                        "_": [
                            1391
                        ],
                        "range": [
                            1391
                        ]
                    },
                    "filtered_variables": {
                        "min_val": [
                            1382,
                            1384,
                            1388,
                            1390,
                            1394
                        ],
                        "max_val": [
                            1383,
                            1384,
                            1387,
                            1390,
                            1393
                        ],
                        "rand": [
                            1384,
                            1385,
                            1386,
                            1387,
                            1388
                        ],
                        "K.eval": [
                            1384,
                            1391
                        ],
                        "K": [
                            1384,
                            1390,
                            1391
                        ],
                        "K.random_uniform": [
                            1384,
                            1390
                        ],
                        "rand.shape": [
                            1385
                        ],
                        "np.abs": [
                            1392,
                            1386
                        ],
                        "np": [
                            1386,
                            1387,
                            1388,
                            1391,
                            1392,
                            1393,
                            1394
                        ],
                        "np.mean": [
                            1392,
                            1386
                        ],
                        "np.max": [
                            1393,
                            1387
                        ],
                        "np.min": [
                            1394,
                            1388
                        ],
                        "r": [
                            1390,
                            1391
                        ],
                        "samples": [
                            1392,
                            1393,
                            1394,
                            1391
                        ],
                        "np.array": [
                            1391
                        ],
                        "_": [
                            1391
                        ]
                    },
                    "diff_line_number": 1384,
                    "class_data": {
                        "signature": "class TestBackend(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def test_is_keras_tensor(self):\n    np_var = np.array([1, 2])\n    with pytest.raises(ValueError):\n        K.is_keras_tensor(np_var)\n    keras_var = K.variable(np_var)\n    assert K.is_keras_tensor(keras_var) is False\n    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n    assert K.is_keras_tensor(keras_placeholder) is False",
                            "def test_set_learning_phase(self):\n    with pytest.raises(ValueError):\n        K.set_learning_phase(2)",
                            "def test_eye(self):\n    check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)",
                            "def test_ones(self):\n    check_single_tensor_operation('ones', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_zeros(self):\n    check_single_tensor_operation('zeros', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_ones_like(self):\n    check_single_tensor_operation('ones_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_zeros_like(self):\n    check_single_tensor_operation('zeros_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_linear_operations(self):\n    check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n    check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=(1, 1))\n    check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n    check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n    if K.backend() != 'cntk':\n        check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=(1, 2))",
                            "def test_random_variables(self):\n    check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP, low=0.0, high=1.0, shape_or_val=False, assert_value_equality=False)\n    check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP, mean=0.0, scale=1.0, shape_or_val=False, assert_value_equality=False)",
                            "def test_batch_dot_shape(self):\n    test_cases = []\n    test_cases.append([(None, 3, 4, 5), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 3, 4, 5), (None, 2, 4), 2])\n    test_cases.append([(None, 3, 4), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 4, 3), (None, 3, 5), (2, 1)])\n    test_cases.append([(None, 4), (None, 3, 4), (1, 2)])\n    test_cases.append([(None, 4), (None, 4), None])\n    batch_size = 7\n\n    def batch_shape(shape):\n        return (batch_size,) + shape[1:]\n\n    def random(shape):\n        return np.random.random(batch_shape(shape))\n    for x_shape, y_shape, axes in test_cases:\n        x_np = random(x_shape)\n        y_np = random(y_shape)\n        z_np = KNP.batch_dot(x_np, y_np, axes)\n        x = K.placeholder(shape=x_shape)\n        y = K.placeholder(shape=y_shape)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        f = K.function([x, y], [z])\n        assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        if K.backend() != 'cntk':\n            x = K.placeholder(ndim=len(x_shape))\n            y = K.placeholder(ndim=len(y_shape))\n            z = K.batch_dot(x, y, axes)\n            z_shape = K.int_shape(z)\n            if z_shape is not None:\n                assert len(z_shape) == z_np.ndim\n                assert set(z_shape) <= set((None, 1))\n            f = K.function([x, y], [z])\n            assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        x = K.variable(x_np)\n        y = K.variable(y_np)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        z = K.eval(z)\n        assert_allclose(z, z_np, atol=1e-05)",
                            "def test_shape_operations(self):\n    check_two_tensor_operation('concatenate', (4, 3), (4, 2), WITH_NP, axis=-1, concat_args=True)\n    check_single_tensor_operation('reshape', (4, 2), WITH_NP, shape=(8, 1))\n    check_single_tensor_operation('permute_dimensions', (4, 2, 3), WITH_NP, pattern=(2, 0, 1))\n    check_single_tensor_operation('repeat', (4, 1), WITH_NP, n=3)\n    check_single_tensor_operation('flatten', (4, 1), WITH_NP)\n    check_single_tensor_operation('batch_flatten', (20, 2, 5), WITH_NP, cntk_dynamicity=True)\n    check_single_tensor_operation('expand_dims', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('expand_dims', (4, 3, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('squeeze', (4, 3, 1), WITH_NP, axis=2)\n    check_single_tensor_operation('squeeze', (4, 1, 1), WITH_NP, axis=1)\n    check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)}, 'squeeze', {'axis': 2}, (4, 3, 1, 1), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='We only test the shape inference of the theano backend.')\ndef test_none_shape_operations(self):\n    x = K.placeholder((3, None, 4))\n    y = K.batch_flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (3, None)\n    y = K.flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (None,)",
                            "def test_repeat_elements(self):\n    reps = 3\n    for ndims in [1, 2, 3]:\n        shape = np.arange(2, 2 + ndims)\n        arr = np.arange(np.prod(shape)).reshape(shape)\n        for rep_axis in range(ndims):\n            check_single_tensor_operation('repeat_elements', arr, WITH_NP, rep=reps, axis=rep_axis)\n            if K.backend() != 'cntk':\n                shape = list(shape)\n                shape[rep_axis] = None\n                x = K.placeholder(shape=shape)\n                y = K.repeat_elements(x, reps, axis=rep_axis)\n                assert y._keras_shape == tuple(shape)\n                assert y._keras_shape == K.int_shape(y)",
                            "def test_tile(self):\n    shape = (3, 4)\n    arr = np.arange(np.prod(shape)).reshape(shape)\n    check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n    check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 4))\n        n = 2\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 8)\n        n = (4, 3)\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 12)",
                            "def test_gather(self):\n    shape = (10, 2, 3)\n    ref = np.arange(np.prod(shape)).reshape(shape)\n    inds = [1, 3, 7, 9]\n    t_list = [k.gather(k.variable(ref), k.variable(inds, dtype='int32')) for k in WITH_NP]\n    z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32'))) for k in WITH_NP]\n    assert_list_pairwise(z_list)\n    assert_list_keras_shape(t_list, z_list)\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 3, 4))\n        indices = K.placeholder(shape=(5, 6), dtype='int32')\n        y = K.gather(x, indices)\n        assert y._keras_shape == (5, 6, 3, 4)",
                            "@pytest.mark.parametrize('function_name', ['get_value', 'count_params', 'int_shape', 'get_variable_shape'])\ndef test_value_manipulation(self, function_name):\n    val = np.random.random((4, 2))\n    v_list = [getattr(k, function_name)(k.variable(val)) for k in WITH_NP]\n    if function_name == 'get_value':\n        assert_list_pairwise(v_list)\n    else:\n        assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)",
                            "def test_print_tensor(self):\n    check_single_tensor_operation('print_tensor', (), WITH_NP)\n    check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n    check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n    check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)",
                            "def test_elementwise_operations(self):\n    check_single_tensor_operation('max', (4, 2), WITH_NP)\n    check_single_tensor_operation('max', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('max', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('min', (4, 2), WITH_NP)\n    check_single_tensor_operation('min', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('min', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('mean', (4, 2), WITH_NP)\n    check_single_tensor_operation('mean', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=-1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('var', (4, 2), WITH_NP)\n    check_single_tensor_operation('var', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('var', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('std', (4, 2), WITH_NP)\n    check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('prod', (4, 2), WITH_NP)\n    check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('any', (4, 2), WITH_NP)\n    check_single_tensor_operation('any', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('any', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('all', (4, 2), WITH_NP)\n    check_single_tensor_operation('all', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('all', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('square', (4, 2), WITH_NP)\n    check_single_tensor_operation('abs', (4, 2), WITH_NP)\n    check_single_tensor_operation('sqrt', (4, 2), WITH_NP)\n    check_single_tensor_operation('exp', (4, 2), WITH_NP)\n    check_single_tensor_operation('round', (4, 2), WITH_NP)\n    check_single_tensor_operation('sign', (4, 2), WITH_NP)\n    check_single_tensor_operation('pow', (4, 2), WITH_NP, a=3)\n    check_single_tensor_operation('clip', (4, 2), WITH_NP, min_value=0.4, max_value=0.6)\n    check_single_tensor_operation('cos', (4, 2), WITH_NP)\n    check_single_tensor_operation('sin', (4, 2), WITH_NP)\n    check_two_tensor_operation('equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('not_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('maximum', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('minimum', (4, 2), (4, 2), WITH_NP)",
                            "def test_reset_uids(self):\n    first = K.get_uid()\n    K.get_uid()\n    K.reset_uids()\n    assert K.get_uid() == first",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support cumsum and cumprod yet')\ndef test_cumsum_cumprod(self):\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP, axis=1)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk return -85.1 for zero or negative number, not nan, so can't compare with other backend.\")\ndef test_log(self):\n    check_single_tensor_operation('log', (4, 2), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_add(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    increment = np.random.randn(3, 4)\n    x += increment\n    K.eval(K.update_add(x_var, increment))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_sub(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    decrement = np.random.randn(3, 4)\n    x -= decrement\n    K.eval(K.update_sub(x_var, decrement))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk doesn't support gradient in this way.\")\ndef test_gradient(self):\n    val = np.random.random((4, 2))\n    x_list = [k.variable(val) for k in [KTH, KTF]]\n    z_list = []\n    zero_list = []\n    for x, k in zip(x_list, [KTH, KTF]):\n        exp = x * k.exp(x)\n        loss = k.sum(exp)\n        zero_loss = k.stop_gradient(loss)\n        grad = k.gradients(loss, [exp])\n        zero_grad = k.gradients(loss + zero_loss, [exp])\n        z_list.append(k.eval(grad[0]))\n        zero_list.append(k.eval(zero_grad[0]))\n    assert_list_pairwise(z_list)\n    assert_list_pairwise(zero_list)\n    for i in range(len(z_list)):\n        assert_allclose(zero_list[i], z_list[i], atol=1e-05)",
                            "def test_stop_gradient(self):\n    val = np.random.random((4, 2))\n    a = K.variable(val)\n    b = K.square(a)\n    c, d = K.stop_gradient([a, b])\n    e = K.stop_gradient(b)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk currently not support function in this way, so can't test as this.\")\ndef test_function(self):\n    test_backend = [KTH, KTF]\n    val = np.random.random((4, 2))\n    input_val = np.random.random((4, 2))\n    f_list = []\n    x_list = []\n    for k in test_backend:\n        x = k.variable(val)\n        x_list.append(x)\n        y = k.placeholder(ndim=2)\n        exp = k.square(x) + y\n        update = x * 2\n        f = k.function([y], [exp], updates=[(x, update)])\n        f_list.append(f)\n    function_outputs_list = [f([input_val])[0] for f in f_list]\n    assert_list_pairwise(function_outputs_list)\n    new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n    assert_list_pairwise(new_val_list)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `fetches` argument.')\ndef test_function_tf_fetches(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], updates=[(x, x_placeholder + 1.0)], fetches=[K.update(y, 5.0)])\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert K.get_session().run(fetches=[x, y]) == [11.0, 5.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `feed_dict` argument.')\ndef test_function_tf_feed_dict(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    feed_dict = {y_placeholder: 3.0}\n    f = K.function(inputs=[x_placeholder], outputs=[x_placeholder + 1.0], updates=[(x, x_placeholder + 10.0)], feed_dict=feed_dict, fetches=[K.update(y, y_placeholder * 10.0)])\n    output = f([10.0])\n    assert output == [11.0]\n    assert K.get_session().run(fetches=[x, y]) == [20.0, 30.0]\n    feed_dict[y_placeholder] = 4.0\n    output = f([20.0])\n    assert output == [21.0]\n    assert K.get_session().run(fetches=[x, y]) == [30.0, 40.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `options` and `run_metadata` arguments.')\ndef test_function_tf_run_options_with_run_metadata(self):\n    from tensorflow.core.protobuf import config_pb2\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    run_options = config_pb2.RunOptions(output_partition_graphs=True)\n    run_metadata = config_pb2.RunMetadata()\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], options=run_options, run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) > 0\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) == 0",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `string` type for a tensor.')\ndef test_function_tf_string_input(self):\n    x_placeholder = K.placeholder(shape=(), dtype='string')\n    x_identity = K.identity(x_placeholder)\n    f = K.function(inputs=[x_placeholder], outputs=[x_identity])\n    output = f([b'test'])\n    assert output == [b'test']",
                            "def test_rnn(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn(inputs, states):\n            assert len(states) == 1\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y])\n        return simple_rnn\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_additional_states(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    h1 = np.concatenate([h0, h0], axis=-1)\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_with_extra_mock_state(inputs, states):\n            assert len(states) == 2\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y, backend.concatenate([y, y], axis=-1)])\n        return simple_rnn_with_extra_mock_state\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0, h1], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_no_states(self):\n    num_samples = 3\n    input_dim = 8\n    output_dim = 4\n    timesteps = 5\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n\n    def get_step_function(backend, w_i):\n\n        def simple_no_states(inputs, states):\n            assert len(states) == 0\n            y = backend.dot(inputs, w_i)\n            return (y, [])\n        return simple_no_states\n    kwargs_list = [{'go_backwards': False}, {'go_backwards': True}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k), step_function_np=get_step_function(KNP, wi), inputs_np=x, initial_states_np=[], mask_np=None, **kwargs)",
                            "def test_rnn_constants(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, c = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_add_constant(inputs, states_and_constants):\n            [h, c] = states_and_constants\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n            return (y, [y])\n        return simple_rnn_add_constant\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), constants_np=[c], **kwargs)",
                            "def test_rnn_output_and_state_masking_independent(self):\n    num_samples = 2\n    num_timesteps = 4\n    state_and_io_size = 5\n    mask_last_num_timesteps = 2\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, state_and_io_size))\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[1, -mask_last_num_timesteps:] = 0\n    expected_outputs = inputs_vals.copy()\n    expected_outputs[1, -mask_last_num_timesteps:] = expected_outputs[1, -(mask_last_num_timesteps + 1)]\n    expected_state = initial_state_vals.copy()\n    expected_state[0] += num_timesteps\n    expected_state[1] += num_timesteps - mask_last_num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)\n        assert_allclose(K.eval(last_states[0]), expected_state)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_output_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n    num_features = 5\n\n    def step_function(inputs, states):\n        outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n        return (outputs, states)\n    inputs_vals = np.random.random((num_samples, num_timesteps, num_features))\n    initial_state_vals = np.random.random((num_samples, 6))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[-1, -1] = 0\n    expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n    expected_outputs[-1, -1] = expected_outputs[-1, -2]\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_state_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n    initial_state_vals = np.random.random((num_samples, 6, 7))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[0, -2:] = 0\n    expected_last_state = initial_state_vals.copy()\n    expected_last_state[0] += num_timesteps - 2\n    expected_last_state[1:] += num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(last_states[0]), expected_last_state)",
                            "@pytest.mark.parametrize('x_np,axis,keepdims', [(np.array([1.1, 0.8, 0.9]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 1, False), (np.array([[1.1, 0.8, 0.9]]), -1, False), (np.array([[1.1, 0.8, 0.9]]), 1, True), (np.array([[1.1], [1.2]]), 0, False), (np.array([[1.1], [1.2]]), 1, False), (np.array([[1.1], [1.2]]), -1, False), (np.array([[1.1], [1.2]]), -1, True), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), None, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 0, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False)])\ndef test_logsumexp(self, x_np, axis, keepdims):\n    \"\"\"\n    Check if K.logsumexp works properly for values close to one.\n    \"\"\"\n    x = K.variable(x_np)\n    assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)), np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)), rtol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='The optimization is applied only with TensorFlow.')\ndef test_logsumexp_optim(self):\n    \"\"\"\n    Check if optimization works.\n    \"\"\"\n    x_np = np.array([10000.0, 0.0001])\n    result = K.eval(K.logsumexp(K.variable(x_np), axis=0))\n    assert_allclose(result, 10000.0, rtol=1e-05)",
                            "def test_switch(self):\n    val = np.random.random()\n    z_list = []\n    for k in WITH_NP:\n        x = k.variable(val)\n        x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n        z_list.append(k.eval(x))\n    assert_list_pairwise(z_list)\n    shapes = []\n    shapes.append([(4, 3, 2), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4, 3), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4,), (4, 3, 2), (4, 3, 2)])\n    for s in shapes:\n        z_list = []\n        arrays = list(map(np.random.random, s))\n        for k in WITH_NP:\n            x, then_expr, else_expr = map(k.variable, arrays)\n            cond = k.greater_equal(x, 0.5)\n            z_list.append(k.eval(k.switch(cond, then_expr, else_expr)))\n        assert_list_pairwise(z_list)",
                            "def test_dropout(self):\n    val = np.random.random((100, 100))\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2)) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2, noise_shape=list(val.shape))) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    with pytest.raises(ValueError):\n        z = K.dropout(K.variable(val), level=-0.5)",
                            "@pytest.mark.parametrize('alpha,max_value,threshold', [(0.0, None, 0.0), (0.1, None, 0.0), (0.0, 5.0, 0.0), (0.0, None, 0.8), (0.1, 5.0, 0.0), (0.1, None, 0.8), (0.0, 5.0, 0.8), (0.1, 5.0, 0.8), (0.1, 0.0, 0.8), (0.1, 5.0, -2.8), (0.1, 9.0, 0.8)])\ndef test_relu(self, alpha, max_value, threshold):\n    check_single_tensor_operation('relu', (4, 2), WITH_NP, alpha=alpha, max_value=max_value, threshold=threshold)",
                            "def test_nn_operations(self):\n    check_single_tensor_operation('softsign', (4, 10), WITH_NP)\n    check_single_tensor_operation('softplus', (4, 10), WITH_NP)\n    check_single_tensor_operation('elu', (4, 10), WITH_NP, alpha=0.5)\n    check_single_tensor_operation('sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('hard_sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('tanh', (4, 2), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 10), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 5, 3), WITH_NP, axis=1)\n    check_single_tensor_operation('softmax', (4, 5, 3, 10), WITH_NP, axis=2)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    if K.backend() != 'cntk':\n        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841], [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n    yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016], [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n    check_two_tensor_operation('categorical_crossentropy', yval, xval, WITH_NP, cntk_two_dynamicity=True, from_logits=True)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)",
                            "def test_in_top_k(self):\n    batch_size = 20\n    num_classes = 10\n    predictions = np.random.random((batch_size, num_classes)).astype('float32')\n    targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n    for k in range(num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)\n    num_identical = num_classes // 2\n    for i in range(batch_size):\n        idx_identical = np.random.choice(num_classes, size=num_identical, replace=False)\n        predictions[i, idx_identical] = predictions[i, 0]\n    targets = np.zeros(batch_size, dtype='int32')\n    for k in range(1, num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('conv1d', (2, 8, 2), (3, 2, 3), 'same', 'channels_last'), ('conv1d', (1, 8, 2), (3, 2, 3), 'valid', 'channels_last'), ('conv1d', (1, 2, 8), (3, 2, 3), 'valid', 'channels_first'), ('conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'), ('conv3d', (2, 3, 4, 5, 4), (3, 3, 3, 3, 4), 'same', 'channels_first'), ('conv3d', (2, 3, 5, 4, 6), (3, 2, 4, 3, 4), 'valid', 'channels_first'), ('conv3d', (1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'valid', 'channels_last'), ('conv3d', (1, 3, 5, 4, 2), (3, 3, 3, 2, 3), 'same', 'channels_last')])\ndef test_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last'), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first')])\ndef test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format,dilation_rate', [('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2), ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2), ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2), 'same', 'channels_last', (2, 2)), ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4), 'valid', 'channels_first', (2, 2)), ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4), 'valid', 'channels_last', (2, 2, 2)), ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4), 'same', 'channels_first', (2, 2, 2))])\ndef test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv transpose on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last', (2, 2)), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first', (2, 2))])\ndef test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('depthwise_conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('depthwise_conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('depthwise_conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('depthwise_conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last')])\ndef test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,pool_size,strides,padding,data_format,pool_mode', [('pool2d', (2, 3, 7, 7), (3, 3), (1, 1), 'same', 'channels_first', 'avg'), ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1), 'valid', 'channels_first', 'max'), ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1), 'valid', 'channels_last', 'avg'), ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1), 'same', 'channels_last', 'max'), ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1), 'same', 'channels_first', 'avg'), ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1), 'valid', 'channels_first', 'max'), ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1), 'valid', 'channels_last', 'avg'), ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1), 'same', 'channels_last', 'max')])\ndef test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n    check_single_tensor_operation(op, input_shape, WITH_NP, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, pool_mode=pool_mode, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [('separable_conv1d', (2, 8, 2), (3,), 1, 'same', 'channels_last'), ('separable_conv1d', (1, 8, 2), (3,), 2, 'valid', 'channels_last'), ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'), ('separable_conv2d', (2, 3, 5, 6), (4, 3), 2, 'valid', 'channels_first'), ('separable_conv2d', (1, 6, 5, 3), (3, 4), 1, 'valid', 'channels_last'), ('separable_conv2d', (1, 7, 6, 3), (3, 3), 2, 'same', 'channels_last')])\ndef test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format):\n    if data_format == 'channels_first':\n        input_depth = input_shape[1]\n    else:\n        input_depth = input_shape[-1]\n    _, x = parse_shape_or_val(input_shape)\n    _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n    _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) + (input_depth * depth_multiplier, 7))\n    y1 = KNP.separable_conv(x, depthwise, pointwise, padding=padding, data_format=data_format)\n    if K.backend() == 'cntk':\n        _, cntk_func = cntk_func_tensors(op, [input_shape, depthwise, pointwise], padding=padding, data_format=data_format)\n        y2 = cntk_func([x])[0]\n    else:\n        y2 = K.eval(getattr(K, op)(K.variable(x), K.variable(depthwise), K.variable(pointwise), padding=padding, data_format=data_format))\n    assert_allclose(y1, y2, atol=1e-05)",
                            "def test_random_normal(self):\n    for mean, std in [(0.0, 1.0), (-10.0, 5.0)]:\n        rand = K.eval(K.random_normal((300, 200), mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n        assert np.abs(np.mean(rand) - mean) < std * 0.015\n        assert np.abs(np.std(rand) - std) < std * 0.015\n        r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - mean) < std * 0.015\n        assert np.abs(np.std(samples) - std) < std * 0.015",
                            "def test_random_uniform(self):\n    min_val = -1.0\n    max_val = 1.0\n    rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand)) < 0.015\n    assert max_val - 0.015 < np.max(rand) <= max_val\n    assert min_val + 0.015 > np.min(rand) >= min_val\n    r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples)) < 0.015\n    assert max_val - 0.015 < np.max(samples) <= max_val\n    assert min_val + 0.015 > np.min(samples) >= min_val",
                            "def test_random_binomial(self):\n    p = 0.5\n    rand = K.eval(K.random_binomial((200, 100), p))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand) - p) < 0.015\n    assert np.max(rand) == 1\n    assert np.min(rand) == 0\n    r = K.random_binomial((10, 10), p)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples) - p) < 0.015\n    assert np.max(samples) == 1\n    assert np.min(samples) == 0",
                            "def test_truncated_normal(self):\n    mean = 0.0\n    std = 1.0\n    min_val = -2.0\n    max_val = 2.0\n    rand = K.eval(K.truncated_normal((300, 200), mean=mean, stddev=std, seed=1337))\n    assert rand.shape == (300, 200)\n    assert np.abs(np.mean(rand) - mean) < 0.015\n    assert np.max(rand) <= max_val\n    assert np.min(rand) >= min_val\n    assert np.abs(np.std(rand) - std * 0.87962) < 0.015",
                            "def test_conv_invalid_use(self):\n    dummy_x_1d = K.variable(np.ones((4, 8, 2)))\n    dummy_w_1d = K.variable(np.ones((3, 2, 3)))\n    dummy_x_2d = K.variable(np.ones((2, 3, 4, 5)))\n    dummy_w_2d = K.variable(np.ones((2, 2, 3, 4)))\n    dummy_x_3d = K.variable(np.ones((2, 3, 4, 5, 4)))\n    dummy_w_3d = K.variable(np.ones((2, 2, 2, 3, 4)))\n    dummy_w1x1_2d = K.variable(np.ones((1, 1, 12, 7)))\n    with pytest.raises(ValueError):\n        K.conv1d(dummy_x_1d, dummy_w_1d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv3d(dummy_x_3d, dummy_w_3d, data_format='channels_middle')\n    if K.backend() != 'theano':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    if K.backend() == 'cntk':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, strides=(2, 2), dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, strides=(2, 2), dilation_rate=(1, 2))",
                            "def test_pooling_invalid_use(self):\n    for input_shape, pool_size in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n        x = K.variable(np.random.random(input_shape))\n        if len(pool_size) == 2:\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, pool_mode='median')\n        else:\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, pool_mode='median')",
                            "def test_resize_images(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_images', x_shape, WITH_NP, cntk_dynamicity=True, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_images(K.variable(xval), 2, 2, data_format='channels_middle')",
                            "@staticmethod\ndef _helper_bilinear(data_format, height_factor, width_factor):\n    x_shape = (2, 3, 4, 5)\n    check_single_tensor_operation('resize_images', x_shape, [KTF, KTH], height_factor=height_factor, width_factor=width_factor, data_format=data_format, interpolation='bilinear')",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\ndef test_resize_images_bilinear(self, data_format):\n    self._helper_bilinear(data_format, 2, 2)\n    with pytest.raises(NotImplementedError):\n        self._helper_bilinear(data_format, 4, 4)",
                            "def test_resize_volumes(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_volumes', x_shape, WITH_NP, cntk_dynamicity=True, depth_factor=2, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_volumes(K.variable(xval), 2, 2, 2, data_format='channels_middle')",
                            "def test_temporal_padding(self):\n    check_single_tensor_operation('temporal_padding', (4, 3, 3), WITH_NP)\n    check_single_tensor_operation('temporal_padding', (2, 3, 4), WITH_NP, padding=(1, 2))",
                            "def test_spatial_2d_padding(self):\n    padding = ((1, 2), (2, 1))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, 1))\n        y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_2d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_spatial_3d_padding(self):\n    padding = ((1, 2), (2, 1), (1, 2))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, None, 1))\n        y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_3d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_bias_add(self):\n    for data_format in ['channels_first', 'channels_last']:\n        for shape in [(), (3,), (2, 3), (5, 3, 2)]:\n            if data_format == 'channels_first':\n                x_shape = (1, 4) + shape\n            else:\n                x_shape = (1,) + shape + (4,)\n            bias_shape = (4,)\n            check_two_tensor_operation('bias_add', x_shape, bias_shape, WITH_NP, cntk_dynamicity=True, data_format=data_format)\n        if data_format == 'channels_first':\n            x_shape = (20, 6, 10)\n        else:\n            x_shape = (20, 10, 6)\n        check_two_tensor_operation('bias_add', x_shape, (10, 6), WITH_NP, cntk_dynamicity=True, data_format=data_format)\n    x = K.variable(np.random.random(x_shape))\n    b = K.variable(np.random.random(bias_shape))\n    with pytest.raises(ValueError):\n        K.bias_add(x, b, data_format='channels_middle')",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='Specific to Theano.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_th(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes='per-activation')\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Specific to Tensorflow.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_tf(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'cntk', reason='Specific to CNTK.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_cntk(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.placeholder(x_shape)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.function([x], [z])([x_val])[0]\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_ctc(self):\n    if K.backend() == 'theano':\n        ref = [1.73308, 3.81351]\n    else:\n        ref = [3.34211, 5.42262]\n    label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n    input_lens = np.expand_dims(np.asarray([5, 5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]], [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508], [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549], [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456], [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345], [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)\n    if K.backend() == 'theano':\n        ref = [1.73308]\n    else:\n        ref = [3.34211]\n    input_lens = np.expand_dims(np.asarray([5]), 1)\n    label_lens = np.expand_dims(np.asarray([5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Test adapted from tensorflow.')\ndef test_ctc_decode_greedy(self):\n    \"\"\"Test two batch entries - best path decoder.\"\"\"\n    max_time_steps = 6\n    seq_len_0 = 4\n    input_prob_matrix_0 = np.asarray([[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.4, 0.6], [0.0, 0.0, 0.4, 0.6], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    seq_len_1 = 5\n    input_prob_matrix_1 = np.asarray([[0.1, 0.9, 0.0, 0.0], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.1, 0.9], [0.0, 0.9, 0.1, 0.1], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    inputs = [np.vstack([input_prob_matrix_0[t, :], input_prob_matrix_1[t, :]]) for t in range(max_time_steps)]\n    inputs = np.asarray(inputs).transpose((1, 0, 2))\n    input_length = np.array([seq_len_0, seq_len_1], dtype=np.int32)\n    decode_pred_np, log_prob_pred_np = KNP.ctc_decode(inputs, input_length, greedy=True)\n    inputs = K.variable(inputs)\n    input_length = K.variable(input_length)\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=True)\n    assert len(decode_pred_tf) == 1\n    decode_pred = K.eval(decode_pred_tf[0])\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    assert np.alltrue(decode_pred_np == decode_pred)\n    assert np.allclose(log_prob_pred_np, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='tensorflow-way slice is only supported in tensorflow.')\n@pytest.mark.parametrize('x_size', [[1, 1, 3], [1, 2, 3], [2, 1, 3]])\ndef test_slice(self, x_size):\n    npt = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])\n    x_start = [1, 0, 0]\n    tft = K.constant(npt)\n    test_input = K.eval(K.slice(tft, x_start, x_size))\n    expected = KNP.slice(npt, x_start, x_size)\n    assert np.allclose(test_input, expected)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search(self):\n    \"\"\"Test one batch, two beams - hibernating beam search.\"\"\"\n    depth = 6\n    seq_len_0 = 5\n    input_prob_matrix_0 = np.asarray([[0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908], [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517], [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763], [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655], [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878], [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]], dtype=np.float32)\n    input_prob_matrix_0 = input_prob_matrix_0 + 2.0\n    inputs = [input_prob_matrix_0[t, :][np.newaxis, :] for t in range(seq_len_0)] + 2 * [np.zeros((1, depth), dtype=np.float32)]\n    inputs = np.exp(inputs)\n    inputs = K.variable(inputs.transpose((1, 0, 2)))\n    input_length = K.variable(np.array([seq_len_0], dtype=np.int32))\n    log_prob_truth = np.array([-5.811451, -6.63339], np.float32)[np.newaxis, :]\n    decode_truth = [np.array([1, 0]), np.array([[1]])]\n    beam_width = 2\n    top_paths = 2\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=False, beam_width=beam_width, top_paths=top_paths)\n    assert len(decode_pred_tf) == top_paths\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    for i in range(top_paths):\n        assert np.alltrue(decode_truth[i] == K.eval(decode_pred_tf[i]))\n    assert np.allclose(log_prob_truth, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search_no_merge(self):\n    input_prob = np.array([[[0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]])\n    input_len = np.array(input_prob.shape[0] * [input_prob.shape[1]])\n\n    def decode(merge_repeated):\n        input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n        input_len_tensor = K.placeholder(shape=None, dtype='int64')\n        paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n        decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n        paths = decode_func([input_prob, input_len])\n        return paths\n    assert np.allclose(decode(merge_repeated=True), [np.array([[0, 1]])])\n    assert np.allclose(decode(merge_repeated=False), [np.array([[0, 0, 1, 1]])])",
                            "def test_one_hot(self):\n    input_length = 10\n    num_classes = 20\n    batch_size = 30\n    indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n    oh = KNP.one_hot(np.int32(indices), num_classes)\n    koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n    assert np.all(koh == oh)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_dot(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense = x_sparse.toarray()\n    W = np.random.random((5, 4))\n    t_W = K.variable(W)\n    k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n    k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n    assert k_s.shape == k_d.shape\n    assert_allclose(k_s, k_d, atol=1e-05)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_concat(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_1 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_2 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense_1 = x_sparse_1.toarray()\n    x_dense_2 = x_sparse_2.toarray()\n    k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n    assert K.is_sparse(k_s)\n    k_s_d = K.eval(k_s)\n    k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n    assert k_s_d.shape == k_d.shape\n    assert_allclose(k_s_d, k_d, atol=1e-05)",
                            "def test_stack(self):\n    tensor_list = [np.random.randn(5, 4, 6, 10) for _ in range(5)]\n    stack_axis = 3\n    results = []\n    if WITH_NP[0] == KC:\n        check_two_tensor_operation('stack', (5, 4, 6, 10), (5, 4, 6, 10), WITH_NP, axis=stack_axis, concat_args=True)\n    else:\n        for k in WITH_NP:\n            tensor_list_var = [k.variable(tensor) for tensor in tensor_list]\n            out = k.eval(k.stack(tensor_list_var, axis=stack_axis))\n            results.append(out)\n        assert_list_pairwise(results)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_map(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    vx = K.variable(x)\n    kx = K.eval(K.map_fn(K.sum, vx))\n    kx2 = K.eval(K.map_fn(lambda i: K.sum(vx[i]), K.arange(10), dtype=K.floatx()))\n    assert (10,) == kx.shape\n    assert (10,) == kx2.shape\n    assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n    assert_allclose(kx, kx2, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldl(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n    assert (3,) == kx.shape\n    assert_allclose(x.sum(axis=0), kx, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldr(self):\n    x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n    vx = K.variable(x)\n    p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n    p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n    assert p1 < p2\n    assert 9e-38 < p2 <= 1e-37",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk has issues with negative number.')\ndef test_arange(self):\n    for test_value in (-20, 0, 1, 10):\n        a_list = []\n        dtype_list = []\n        for k in WITH_NP:\n            t = k.arange(test_value)\n            a = k.eval(t)\n            assert np.array_equal(a, np.arange(test_value))\n            dtype_list.append(k.dtype(t))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for start, stop, step in ((0, 5, 1), (-5, 5, 2), (0, 1, 2)):\n        a_list = []\n        for k in WITH_NP:\n            a = k.eval(k.arange(start, stop, step))\n            assert np.array_equal(a, np.arange(start, stop, step))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for dtype in ('int32', 'int64', 'float32', 'float64'):\n        for k in WITH_NP:\n            t = k.arange(10, dtype=dtype)\n            assert k.dtype(t) == dtype\n    start = K.constant(1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 1\n    start = K.constant(-1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 0",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_train_phase(self, training):\n    check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_test_phase(self, training):\n    check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "def test_setfloatx_incorrect_values(self):\n    old_floatx = floatx()\n    initial = floatx()\n    for value in ['', 'beerfloat', 123]:\n        with pytest.raises(ValueError):\n            set_floatx(value)\n    assert floatx() == initial\n    set_floatx(old_floatx)",
                            "def DISABLED_test_setfloatx_correct_values(self):\n    \"\"\"Disabled because it is not thread safe at this time.\"\"\"\n    old_floatx = floatx()\n    for value in ['float16', 'float32', 'float64']:\n        set_floatx(value)\n        assert floatx() == value\n    set_floatx(old_floatx)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support float16')\ndef DISABLED_test_set_floatx(self):\n    \"\"\"Disabled because it is not thread safe at this time.\n\n    Make sure that changes to the global floatx are effectively\n    taken into account by the backend.\n    \"\"\"\n    old_floatx = floatx()\n    set_floatx('float16')\n    var = variable([10])\n    check_dtype(var, 'float16')\n    set_floatx('float64')\n    var = variable([10])\n    check_dtype(var, 'float64')\n    set_floatx(old_floatx)",
                            "def test_dtype(self):\n    assert K.dtype(K.variable(1, dtype='float64')) == 'float64'\n    assert K.dtype(K.variable(1, dtype='float32')) == 'float32'\n    assert K.dtype(K.variable(1, dtype='float16')) == 'float16'",
                            "def test_variable_support_bool_dtype(self):\n    if K.backend() == 'tensorflow':\n        assert K.dtype(K.variable(1, dtype='int16')) == 'int16'\n        assert K.dtype(K.variable(False, dtype='bool')) == 'bool'\n        with pytest.raises(TypeError):\n            K.variable('', dtype='unsupported')",
                            "def test_clip_supports_tensor_arguments(self):\n    x = K.variable([-10.0, -5.0, 0.0, 5.0, 10.0])\n    min_value = K.variable([-5.0, -4.0, 0.0, 3.0, 5.0])\n    max_value = K.variable([5.0, 4.0, 1.0, 4.0, 9.0])\n    assert np.allclose(K.eval(K.clip(x, min_value, max_value)), np.asarray([-5.0, -4.0, 0.0, 4.0, 9.0], dtype=np.float32))",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(), reason='This test is for tensorflow parallelism.')\ndef test_tensorflow_session_parallelism_settings(self, monkeypatch):\n    for threads in [1, 2]:\n        K.clear_session()\n        monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n        cfg = K.get_session()._config\n        assert cfg.intra_op_parallelism_threads == threads\n        assert cfg.inter_op_parallelism_threads == threads",
                            "def batch_shape(shape):\n    return (batch_size,) + shape[1:]",
                            "def random(shape):\n    return np.random.random(batch_shape(shape))",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn(inputs, states):\n        assert len(states) == 1\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y])\n    return simple_rnn",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_with_extra_mock_state(inputs, states):\n        assert len(states) == 2\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y, backend.concatenate([y, y], axis=-1)])\n    return simple_rnn_with_extra_mock_state",
                            "def get_step_function(backend, w_i):\n\n    def simple_no_states(inputs, states):\n        assert len(states) == 0\n        y = backend.dot(inputs, w_i)\n        return (y, [])\n    return simple_no_states",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_add_constant(inputs, states_and_constants):\n        [h, c] = states_and_constants\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n        return (y, [y])\n    return simple_rnn_add_constant",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def step_function(inputs, states):\n    outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n    return (outputs, states)",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def decode(merge_repeated):\n    input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n    input_len_tensor = K.placeholder(shape=None, dtype='int64')\n    paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n    decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n    paths = decode_func([input_prob, input_len])\n    return paths",
                            "def simple_rnn(inputs, states):\n    assert len(states) == 1\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y])",
                            "def simple_rnn_with_extra_mock_state(inputs, states):\n    assert len(states) == 2\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y, backend.concatenate([y, y], axis=-1)])",
                            "def simple_no_states(inputs, states):\n    assert len(states) == 0\n    y = backend.dot(inputs, w_i)\n    return (y, [])",
                            "def simple_rnn_add_constant(inputs, states_and_constants):\n    [h, c] = states_and_constants\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n    return (y, [y])"
                        ],
                        "constructor_variables": [],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "test_is_keras_tensor(self)",
                            "test_set_learning_phase(self)",
                            "test_eye(self)",
                            "test_ones(self)",
                            "test_zeros(self)",
                            "test_ones_like(self)",
                            "test_zeros_like(self)",
                            "test_linear_operations(self)",
                            "test_random_variables(self)",
                            "test_batch_dot_shape(self)",
                            "test_shape_operations(self)",
                            "test_none_shape_operations(self)",
                            "test_repeat_elements(self)",
                            "test_tile(self)",
                            "test_gather(self)",
                            "test_value_manipulation(self, function_name)",
                            "test_print_tensor(self)",
                            "test_elementwise_operations(self)",
                            "test_reset_uids(self)",
                            "test_cumsum_cumprod(self)",
                            "test_log(self)",
                            "test_update_add(self)",
                            "test_update_sub(self)",
                            "test_gradient(self)",
                            "test_stop_gradient(self)",
                            "test_function(self)",
                            "test_function_tf_fetches(self)",
                            "test_function_tf_feed_dict(self)",
                            "test_function_tf_run_options_with_run_metadata(self)",
                            "test_function_tf_string_input(self)",
                            "test_rnn(self)",
                            "test_rnn_additional_states(self)",
                            "test_rnn_no_states(self)",
                            "test_rnn_constants(self)",
                            "test_rnn_output_and_state_masking_independent(self)",
                            "test_rnn_output_num_dim_larger_than_2_masking(self)",
                            "test_rnn_state_num_dim_larger_than_2_masking(self)",
                            "test_logsumexp(self, x_np, axis, keepdims)",
                            "test_logsumexp_optim(self)",
                            "test_switch(self)",
                            "test_dropout(self)",
                            "test_relu(self, alpha, max_value, threshold)",
                            "test_nn_operations(self)",
                            "test_in_top_k(self)",
                            "test_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format)",
                            "test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate)",
                            "test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate)",
                            "test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode)",
                            "test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format)",
                            "test_random_normal(self)",
                            "test_random_uniform(self)",
                            "test_random_binomial(self)",
                            "test_truncated_normal(self)",
                            "test_conv_invalid_use(self)",
                            "test_pooling_invalid_use(self)",
                            "test_resize_images(self)",
                            "_helper_bilinear(data_format, height_factor, width_factor)",
                            "test_resize_images_bilinear(self, data_format)",
                            "test_resize_volumes(self)",
                            "test_temporal_padding(self)",
                            "test_spatial_2d_padding(self)",
                            "test_spatial_3d_padding(self)",
                            "test_bias_add(self)",
                            "test_batchnorm_th(self, x_shape)",
                            "test_batchnorm_tf(self, x_shape)",
                            "test_batchnorm_cntk(self, x_shape)",
                            "test_ctc(self)",
                            "test_ctc_decode_greedy(self)",
                            "test_slice(self, x_size)",
                            "test_ctc_decode_beam_search(self)",
                            "test_ctc_decode_beam_search_no_merge(self)",
                            "test_one_hot(self)",
                            "test_sparse_dot(self)",
                            "test_sparse_concat(self)",
                            "test_stack(self)",
                            "test_map(self)",
                            "test_foldl(self)",
                            "test_foldr(self)",
                            "test_arange(self)",
                            "test_in_train_phase(self, training)",
                            "test_in_test_phase(self, training)",
                            "test_setfloatx_incorrect_values(self)",
                            "DISABLED_test_setfloatx_correct_values(self)",
                            "DISABLED_test_set_floatx(self)",
                            "test_dtype(self)",
                            "test_variable_support_bool_dtype(self)",
                            "test_clip_supports_tensor_arguments(self)",
                            "test_tensorflow_session_parallelism_settings(self, monkeypatch)",
                            "batch_shape(shape)",
                            "random(shape)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i)",
                            "get_step_function(backend, w_i, w_h)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "decode(merge_repeated)",
                            "simple_rnn(inputs, states)",
                            "simple_rnn_with_extra_mock_state(inputs, states)",
                            "simple_no_states(inputs, states)",
                            "simple_rnn_add_constant(inputs, states_and_constants)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "test_random_binomial",
                    "function_code": "def test_random_binomial(self):\n    p = 0.5\n    rand = K.eval(K.random_binomial((200, 100), p))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand) - p) < 0.015\n    assert np.max(rand) == 1\n    assert np.min(rand) == 0\n\n    r = K.random_binomial((10, 10), p)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples) - p) < 0.015\n    assert np.max(samples) == 1\n    assert np.min(samples) == 0\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 1396,
                    "end_line": 1408,
                    "variables": {
                        "p": [
                            1397,
                            1398,
                            1400,
                            1404,
                            1406
                        ],
                        "rand": [
                            1398,
                            1399,
                            1400,
                            1401,
                            1402
                        ],
                        "K.eval": [
                            1405,
                            1398
                        ],
                        "K": [
                            1404,
                            1405,
                            1398
                        ],
                        "K.random_binomial": [
                            1404,
                            1398
                        ],
                        "rand.shape": [
                            1399
                        ],
                        "np.abs": [
                            1400,
                            1406
                        ],
                        "np": [
                            1408,
                            1400,
                            1401,
                            1402,
                            1405,
                            1406,
                            1407
                        ],
                        "np.mean": [
                            1400,
                            1406
                        ],
                        "np.max": [
                            1401,
                            1407
                        ],
                        "np.min": [
                            1408,
                            1402
                        ],
                        "r": [
                            1404,
                            1405
                        ],
                        "samples": [
                            1408,
                            1405,
                            1406,
                            1407
                        ],
                        "np.array": [
                            1405
                        ],
                        "_": [
                            1405
                        ],
                        "range": [
                            1405
                        ]
                    },
                    "filtered_variables": {
                        "p": [
                            1397,
                            1398,
                            1400,
                            1404,
                            1406
                        ],
                        "rand": [
                            1398,
                            1399,
                            1400,
                            1401,
                            1402
                        ],
                        "K.eval": [
                            1405,
                            1398
                        ],
                        "K": [
                            1404,
                            1405,
                            1398
                        ],
                        "K.random_binomial": [
                            1404,
                            1398
                        ],
                        "rand.shape": [
                            1399
                        ],
                        "np.abs": [
                            1400,
                            1406
                        ],
                        "np": [
                            1408,
                            1400,
                            1401,
                            1402,
                            1405,
                            1406,
                            1407
                        ],
                        "np.mean": [
                            1400,
                            1406
                        ],
                        "np.max": [
                            1401,
                            1407
                        ],
                        "np.min": [
                            1408,
                            1402
                        ],
                        "r": [
                            1404,
                            1405
                        ],
                        "samples": [
                            1408,
                            1405,
                            1406,
                            1407
                        ],
                        "np.array": [
                            1405
                        ],
                        "_": [
                            1405
                        ]
                    },
                    "diff_line_number": 1398,
                    "class_data": {
                        "signature": "class TestBackend(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def test_is_keras_tensor(self):\n    np_var = np.array([1, 2])\n    with pytest.raises(ValueError):\n        K.is_keras_tensor(np_var)\n    keras_var = K.variable(np_var)\n    assert K.is_keras_tensor(keras_var) is False\n    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n    assert K.is_keras_tensor(keras_placeholder) is False",
                            "def test_set_learning_phase(self):\n    with pytest.raises(ValueError):\n        K.set_learning_phase(2)",
                            "def test_eye(self):\n    check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)",
                            "def test_ones(self):\n    check_single_tensor_operation('ones', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_zeros(self):\n    check_single_tensor_operation('zeros', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_ones_like(self):\n    check_single_tensor_operation('ones_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_zeros_like(self):\n    check_single_tensor_operation('zeros_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_linear_operations(self):\n    check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n    check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=(1, 1))\n    check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n    check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n    if K.backend() != 'cntk':\n        check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=(1, 2))",
                            "def test_random_variables(self):\n    check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP, low=0.0, high=1.0, shape_or_val=False, assert_value_equality=False)\n    check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP, mean=0.0, scale=1.0, shape_or_val=False, assert_value_equality=False)",
                            "def test_batch_dot_shape(self):\n    test_cases = []\n    test_cases.append([(None, 3, 4, 5), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 3, 4, 5), (None, 2, 4), 2])\n    test_cases.append([(None, 3, 4), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 4, 3), (None, 3, 5), (2, 1)])\n    test_cases.append([(None, 4), (None, 3, 4), (1, 2)])\n    test_cases.append([(None, 4), (None, 4), None])\n    batch_size = 7\n\n    def batch_shape(shape):\n        return (batch_size,) + shape[1:]\n\n    def random(shape):\n        return np.random.random(batch_shape(shape))\n    for x_shape, y_shape, axes in test_cases:\n        x_np = random(x_shape)\n        y_np = random(y_shape)\n        z_np = KNP.batch_dot(x_np, y_np, axes)\n        x = K.placeholder(shape=x_shape)\n        y = K.placeholder(shape=y_shape)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        f = K.function([x, y], [z])\n        assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        if K.backend() != 'cntk':\n            x = K.placeholder(ndim=len(x_shape))\n            y = K.placeholder(ndim=len(y_shape))\n            z = K.batch_dot(x, y, axes)\n            z_shape = K.int_shape(z)\n            if z_shape is not None:\n                assert len(z_shape) == z_np.ndim\n                assert set(z_shape) <= set((None, 1))\n            f = K.function([x, y], [z])\n            assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        x = K.variable(x_np)\n        y = K.variable(y_np)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        z = K.eval(z)\n        assert_allclose(z, z_np, atol=1e-05)",
                            "def test_shape_operations(self):\n    check_two_tensor_operation('concatenate', (4, 3), (4, 2), WITH_NP, axis=-1, concat_args=True)\n    check_single_tensor_operation('reshape', (4, 2), WITH_NP, shape=(8, 1))\n    check_single_tensor_operation('permute_dimensions', (4, 2, 3), WITH_NP, pattern=(2, 0, 1))\n    check_single_tensor_operation('repeat', (4, 1), WITH_NP, n=3)\n    check_single_tensor_operation('flatten', (4, 1), WITH_NP)\n    check_single_tensor_operation('batch_flatten', (20, 2, 5), WITH_NP, cntk_dynamicity=True)\n    check_single_tensor_operation('expand_dims', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('expand_dims', (4, 3, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('squeeze', (4, 3, 1), WITH_NP, axis=2)\n    check_single_tensor_operation('squeeze', (4, 1, 1), WITH_NP, axis=1)\n    check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)}, 'squeeze', {'axis': 2}, (4, 3, 1, 1), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='We only test the shape inference of the theano backend.')\ndef test_none_shape_operations(self):\n    x = K.placeholder((3, None, 4))\n    y = K.batch_flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (3, None)\n    y = K.flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (None,)",
                            "def test_repeat_elements(self):\n    reps = 3\n    for ndims in [1, 2, 3]:\n        shape = np.arange(2, 2 + ndims)\n        arr = np.arange(np.prod(shape)).reshape(shape)\n        for rep_axis in range(ndims):\n            check_single_tensor_operation('repeat_elements', arr, WITH_NP, rep=reps, axis=rep_axis)\n            if K.backend() != 'cntk':\n                shape = list(shape)\n                shape[rep_axis] = None\n                x = K.placeholder(shape=shape)\n                y = K.repeat_elements(x, reps, axis=rep_axis)\n                assert y._keras_shape == tuple(shape)\n                assert y._keras_shape == K.int_shape(y)",
                            "def test_tile(self):\n    shape = (3, 4)\n    arr = np.arange(np.prod(shape)).reshape(shape)\n    check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n    check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 4))\n        n = 2\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 8)\n        n = (4, 3)\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 12)",
                            "def test_gather(self):\n    shape = (10, 2, 3)\n    ref = np.arange(np.prod(shape)).reshape(shape)\n    inds = [1, 3, 7, 9]\n    t_list = [k.gather(k.variable(ref), k.variable(inds, dtype='int32')) for k in WITH_NP]\n    z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32'))) for k in WITH_NP]\n    assert_list_pairwise(z_list)\n    assert_list_keras_shape(t_list, z_list)\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 3, 4))\n        indices = K.placeholder(shape=(5, 6), dtype='int32')\n        y = K.gather(x, indices)\n        assert y._keras_shape == (5, 6, 3, 4)",
                            "@pytest.mark.parametrize('function_name', ['get_value', 'count_params', 'int_shape', 'get_variable_shape'])\ndef test_value_manipulation(self, function_name):\n    val = np.random.random((4, 2))\n    v_list = [getattr(k, function_name)(k.variable(val)) for k in WITH_NP]\n    if function_name == 'get_value':\n        assert_list_pairwise(v_list)\n    else:\n        assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)",
                            "def test_print_tensor(self):\n    check_single_tensor_operation('print_tensor', (), WITH_NP)\n    check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n    check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n    check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)",
                            "def test_elementwise_operations(self):\n    check_single_tensor_operation('max', (4, 2), WITH_NP)\n    check_single_tensor_operation('max', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('max', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('min', (4, 2), WITH_NP)\n    check_single_tensor_operation('min', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('min', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('mean', (4, 2), WITH_NP)\n    check_single_tensor_operation('mean', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=-1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('var', (4, 2), WITH_NP)\n    check_single_tensor_operation('var', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('var', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('std', (4, 2), WITH_NP)\n    check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('prod', (4, 2), WITH_NP)\n    check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('any', (4, 2), WITH_NP)\n    check_single_tensor_operation('any', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('any', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('all', (4, 2), WITH_NP)\n    check_single_tensor_operation('all', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('all', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('square', (4, 2), WITH_NP)\n    check_single_tensor_operation('abs', (4, 2), WITH_NP)\n    check_single_tensor_operation('sqrt', (4, 2), WITH_NP)\n    check_single_tensor_operation('exp', (4, 2), WITH_NP)\n    check_single_tensor_operation('round', (4, 2), WITH_NP)\n    check_single_tensor_operation('sign', (4, 2), WITH_NP)\n    check_single_tensor_operation('pow', (4, 2), WITH_NP, a=3)\n    check_single_tensor_operation('clip', (4, 2), WITH_NP, min_value=0.4, max_value=0.6)\n    check_single_tensor_operation('cos', (4, 2), WITH_NP)\n    check_single_tensor_operation('sin', (4, 2), WITH_NP)\n    check_two_tensor_operation('equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('not_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('maximum', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('minimum', (4, 2), (4, 2), WITH_NP)",
                            "def test_reset_uids(self):\n    first = K.get_uid()\n    K.get_uid()\n    K.reset_uids()\n    assert K.get_uid() == first",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support cumsum and cumprod yet')\ndef test_cumsum_cumprod(self):\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP, axis=1)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk return -85.1 for zero or negative number, not nan, so can't compare with other backend.\")\ndef test_log(self):\n    check_single_tensor_operation('log', (4, 2), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_add(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    increment = np.random.randn(3, 4)\n    x += increment\n    K.eval(K.update_add(x_var, increment))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_sub(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    decrement = np.random.randn(3, 4)\n    x -= decrement\n    K.eval(K.update_sub(x_var, decrement))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk doesn't support gradient in this way.\")\ndef test_gradient(self):\n    val = np.random.random((4, 2))\n    x_list = [k.variable(val) for k in [KTH, KTF]]\n    z_list = []\n    zero_list = []\n    for x, k in zip(x_list, [KTH, KTF]):\n        exp = x * k.exp(x)\n        loss = k.sum(exp)\n        zero_loss = k.stop_gradient(loss)\n        grad = k.gradients(loss, [exp])\n        zero_grad = k.gradients(loss + zero_loss, [exp])\n        z_list.append(k.eval(grad[0]))\n        zero_list.append(k.eval(zero_grad[0]))\n    assert_list_pairwise(z_list)\n    assert_list_pairwise(zero_list)\n    for i in range(len(z_list)):\n        assert_allclose(zero_list[i], z_list[i], atol=1e-05)",
                            "def test_stop_gradient(self):\n    val = np.random.random((4, 2))\n    a = K.variable(val)\n    b = K.square(a)\n    c, d = K.stop_gradient([a, b])\n    e = K.stop_gradient(b)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk currently not support function in this way, so can't test as this.\")\ndef test_function(self):\n    test_backend = [KTH, KTF]\n    val = np.random.random((4, 2))\n    input_val = np.random.random((4, 2))\n    f_list = []\n    x_list = []\n    for k in test_backend:\n        x = k.variable(val)\n        x_list.append(x)\n        y = k.placeholder(ndim=2)\n        exp = k.square(x) + y\n        update = x * 2\n        f = k.function([y], [exp], updates=[(x, update)])\n        f_list.append(f)\n    function_outputs_list = [f([input_val])[0] for f in f_list]\n    assert_list_pairwise(function_outputs_list)\n    new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n    assert_list_pairwise(new_val_list)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `fetches` argument.')\ndef test_function_tf_fetches(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], updates=[(x, x_placeholder + 1.0)], fetches=[K.update(y, 5.0)])\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert K.get_session().run(fetches=[x, y]) == [11.0, 5.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `feed_dict` argument.')\ndef test_function_tf_feed_dict(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    feed_dict = {y_placeholder: 3.0}\n    f = K.function(inputs=[x_placeholder], outputs=[x_placeholder + 1.0], updates=[(x, x_placeholder + 10.0)], feed_dict=feed_dict, fetches=[K.update(y, y_placeholder * 10.0)])\n    output = f([10.0])\n    assert output == [11.0]\n    assert K.get_session().run(fetches=[x, y]) == [20.0, 30.0]\n    feed_dict[y_placeholder] = 4.0\n    output = f([20.0])\n    assert output == [21.0]\n    assert K.get_session().run(fetches=[x, y]) == [30.0, 40.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `options` and `run_metadata` arguments.')\ndef test_function_tf_run_options_with_run_metadata(self):\n    from tensorflow.core.protobuf import config_pb2\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    run_options = config_pb2.RunOptions(output_partition_graphs=True)\n    run_metadata = config_pb2.RunMetadata()\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], options=run_options, run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) > 0\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) == 0",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `string` type for a tensor.')\ndef test_function_tf_string_input(self):\n    x_placeholder = K.placeholder(shape=(), dtype='string')\n    x_identity = K.identity(x_placeholder)\n    f = K.function(inputs=[x_placeholder], outputs=[x_identity])\n    output = f([b'test'])\n    assert output == [b'test']",
                            "def test_rnn(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn(inputs, states):\n            assert len(states) == 1\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y])\n        return simple_rnn\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_additional_states(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    h1 = np.concatenate([h0, h0], axis=-1)\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_with_extra_mock_state(inputs, states):\n            assert len(states) == 2\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y, backend.concatenate([y, y], axis=-1)])\n        return simple_rnn_with_extra_mock_state\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0, h1], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_no_states(self):\n    num_samples = 3\n    input_dim = 8\n    output_dim = 4\n    timesteps = 5\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n\n    def get_step_function(backend, w_i):\n\n        def simple_no_states(inputs, states):\n            assert len(states) == 0\n            y = backend.dot(inputs, w_i)\n            return (y, [])\n        return simple_no_states\n    kwargs_list = [{'go_backwards': False}, {'go_backwards': True}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k), step_function_np=get_step_function(KNP, wi), inputs_np=x, initial_states_np=[], mask_np=None, **kwargs)",
                            "def test_rnn_constants(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, c = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_add_constant(inputs, states_and_constants):\n            [h, c] = states_and_constants\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n            return (y, [y])\n        return simple_rnn_add_constant\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), constants_np=[c], **kwargs)",
                            "def test_rnn_output_and_state_masking_independent(self):\n    num_samples = 2\n    num_timesteps = 4\n    state_and_io_size = 5\n    mask_last_num_timesteps = 2\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, state_and_io_size))\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[1, -mask_last_num_timesteps:] = 0\n    expected_outputs = inputs_vals.copy()\n    expected_outputs[1, -mask_last_num_timesteps:] = expected_outputs[1, -(mask_last_num_timesteps + 1)]\n    expected_state = initial_state_vals.copy()\n    expected_state[0] += num_timesteps\n    expected_state[1] += num_timesteps - mask_last_num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)\n        assert_allclose(K.eval(last_states[0]), expected_state)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_output_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n    num_features = 5\n\n    def step_function(inputs, states):\n        outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n        return (outputs, states)\n    inputs_vals = np.random.random((num_samples, num_timesteps, num_features))\n    initial_state_vals = np.random.random((num_samples, 6))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[-1, -1] = 0\n    expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n    expected_outputs[-1, -1] = expected_outputs[-1, -2]\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_state_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n    initial_state_vals = np.random.random((num_samples, 6, 7))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[0, -2:] = 0\n    expected_last_state = initial_state_vals.copy()\n    expected_last_state[0] += num_timesteps - 2\n    expected_last_state[1:] += num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(last_states[0]), expected_last_state)",
                            "@pytest.mark.parametrize('x_np,axis,keepdims', [(np.array([1.1, 0.8, 0.9]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 1, False), (np.array([[1.1, 0.8, 0.9]]), -1, False), (np.array([[1.1, 0.8, 0.9]]), 1, True), (np.array([[1.1], [1.2]]), 0, False), (np.array([[1.1], [1.2]]), 1, False), (np.array([[1.1], [1.2]]), -1, False), (np.array([[1.1], [1.2]]), -1, True), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), None, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 0, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False)])\ndef test_logsumexp(self, x_np, axis, keepdims):\n    \"\"\"\n    Check if K.logsumexp works properly for values close to one.\n    \"\"\"\n    x = K.variable(x_np)\n    assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)), np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)), rtol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='The optimization is applied only with TensorFlow.')\ndef test_logsumexp_optim(self):\n    \"\"\"\n    Check if optimization works.\n    \"\"\"\n    x_np = np.array([10000.0, 0.0001])\n    result = K.eval(K.logsumexp(K.variable(x_np), axis=0))\n    assert_allclose(result, 10000.0, rtol=1e-05)",
                            "def test_switch(self):\n    val = np.random.random()\n    z_list = []\n    for k in WITH_NP:\n        x = k.variable(val)\n        x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n        z_list.append(k.eval(x))\n    assert_list_pairwise(z_list)\n    shapes = []\n    shapes.append([(4, 3, 2), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4, 3), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4,), (4, 3, 2), (4, 3, 2)])\n    for s in shapes:\n        z_list = []\n        arrays = list(map(np.random.random, s))\n        for k in WITH_NP:\n            x, then_expr, else_expr = map(k.variable, arrays)\n            cond = k.greater_equal(x, 0.5)\n            z_list.append(k.eval(k.switch(cond, then_expr, else_expr)))\n        assert_list_pairwise(z_list)",
                            "def test_dropout(self):\n    val = np.random.random((100, 100))\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2)) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2, noise_shape=list(val.shape))) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    with pytest.raises(ValueError):\n        z = K.dropout(K.variable(val), level=-0.5)",
                            "@pytest.mark.parametrize('alpha,max_value,threshold', [(0.0, None, 0.0), (0.1, None, 0.0), (0.0, 5.0, 0.0), (0.0, None, 0.8), (0.1, 5.0, 0.0), (0.1, None, 0.8), (0.0, 5.0, 0.8), (0.1, 5.0, 0.8), (0.1, 0.0, 0.8), (0.1, 5.0, -2.8), (0.1, 9.0, 0.8)])\ndef test_relu(self, alpha, max_value, threshold):\n    check_single_tensor_operation('relu', (4, 2), WITH_NP, alpha=alpha, max_value=max_value, threshold=threshold)",
                            "def test_nn_operations(self):\n    check_single_tensor_operation('softsign', (4, 10), WITH_NP)\n    check_single_tensor_operation('softplus', (4, 10), WITH_NP)\n    check_single_tensor_operation('elu', (4, 10), WITH_NP, alpha=0.5)\n    check_single_tensor_operation('sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('hard_sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('tanh', (4, 2), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 10), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 5, 3), WITH_NP, axis=1)\n    check_single_tensor_operation('softmax', (4, 5, 3, 10), WITH_NP, axis=2)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    if K.backend() != 'cntk':\n        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841], [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n    yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016], [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n    check_two_tensor_operation('categorical_crossentropy', yval, xval, WITH_NP, cntk_two_dynamicity=True, from_logits=True)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)",
                            "def test_in_top_k(self):\n    batch_size = 20\n    num_classes = 10\n    predictions = np.random.random((batch_size, num_classes)).astype('float32')\n    targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n    for k in range(num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)\n    num_identical = num_classes // 2\n    for i in range(batch_size):\n        idx_identical = np.random.choice(num_classes, size=num_identical, replace=False)\n        predictions[i, idx_identical] = predictions[i, 0]\n    targets = np.zeros(batch_size, dtype='int32')\n    for k in range(1, num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('conv1d', (2, 8, 2), (3, 2, 3), 'same', 'channels_last'), ('conv1d', (1, 8, 2), (3, 2, 3), 'valid', 'channels_last'), ('conv1d', (1, 2, 8), (3, 2, 3), 'valid', 'channels_first'), ('conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'), ('conv3d', (2, 3, 4, 5, 4), (3, 3, 3, 3, 4), 'same', 'channels_first'), ('conv3d', (2, 3, 5, 4, 6), (3, 2, 4, 3, 4), 'valid', 'channels_first'), ('conv3d', (1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'valid', 'channels_last'), ('conv3d', (1, 3, 5, 4, 2), (3, 3, 3, 2, 3), 'same', 'channels_last')])\ndef test_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last'), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first')])\ndef test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format,dilation_rate', [('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2), ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2), ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2), 'same', 'channels_last', (2, 2)), ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4), 'valid', 'channels_first', (2, 2)), ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4), 'valid', 'channels_last', (2, 2, 2)), ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4), 'same', 'channels_first', (2, 2, 2))])\ndef test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv transpose on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last', (2, 2)), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first', (2, 2))])\ndef test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('depthwise_conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('depthwise_conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('depthwise_conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('depthwise_conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last')])\ndef test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,pool_size,strides,padding,data_format,pool_mode', [('pool2d', (2, 3, 7, 7), (3, 3), (1, 1), 'same', 'channels_first', 'avg'), ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1), 'valid', 'channels_first', 'max'), ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1), 'valid', 'channels_last', 'avg'), ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1), 'same', 'channels_last', 'max'), ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1), 'same', 'channels_first', 'avg'), ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1), 'valid', 'channels_first', 'max'), ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1), 'valid', 'channels_last', 'avg'), ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1), 'same', 'channels_last', 'max')])\ndef test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n    check_single_tensor_operation(op, input_shape, WITH_NP, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, pool_mode=pool_mode, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [('separable_conv1d', (2, 8, 2), (3,), 1, 'same', 'channels_last'), ('separable_conv1d', (1, 8, 2), (3,), 2, 'valid', 'channels_last'), ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'), ('separable_conv2d', (2, 3, 5, 6), (4, 3), 2, 'valid', 'channels_first'), ('separable_conv2d', (1, 6, 5, 3), (3, 4), 1, 'valid', 'channels_last'), ('separable_conv2d', (1, 7, 6, 3), (3, 3), 2, 'same', 'channels_last')])\ndef test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format):\n    if data_format == 'channels_first':\n        input_depth = input_shape[1]\n    else:\n        input_depth = input_shape[-1]\n    _, x = parse_shape_or_val(input_shape)\n    _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n    _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) + (input_depth * depth_multiplier, 7))\n    y1 = KNP.separable_conv(x, depthwise, pointwise, padding=padding, data_format=data_format)\n    if K.backend() == 'cntk':\n        _, cntk_func = cntk_func_tensors(op, [input_shape, depthwise, pointwise], padding=padding, data_format=data_format)\n        y2 = cntk_func([x])[0]\n    else:\n        y2 = K.eval(getattr(K, op)(K.variable(x), K.variable(depthwise), K.variable(pointwise), padding=padding, data_format=data_format))\n    assert_allclose(y1, y2, atol=1e-05)",
                            "def test_random_normal(self):\n    for mean, std in [(0.0, 1.0), (-10.0, 5.0)]:\n        rand = K.eval(K.random_normal((300, 200), mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n        assert np.abs(np.mean(rand) - mean) < std * 0.015\n        assert np.abs(np.std(rand) - std) < std * 0.015\n        r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - mean) < std * 0.015\n        assert np.abs(np.std(samples) - std) < std * 0.015",
                            "def test_random_uniform(self):\n    min_val = -1.0\n    max_val = 1.0\n    rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand)) < 0.015\n    assert max_val - 0.015 < np.max(rand) <= max_val\n    assert min_val + 0.015 > np.min(rand) >= min_val\n    r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples)) < 0.015\n    assert max_val - 0.015 < np.max(samples) <= max_val\n    assert min_val + 0.015 > np.min(samples) >= min_val",
                            "def test_random_binomial(self):\n    p = 0.5\n    rand = K.eval(K.random_binomial((200, 100), p))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand) - p) < 0.015\n    assert np.max(rand) == 1\n    assert np.min(rand) == 0\n    r = K.random_binomial((10, 10), p)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples) - p) < 0.015\n    assert np.max(samples) == 1\n    assert np.min(samples) == 0",
                            "def test_truncated_normal(self):\n    mean = 0.0\n    std = 1.0\n    min_val = -2.0\n    max_val = 2.0\n    rand = K.eval(K.truncated_normal((300, 200), mean=mean, stddev=std, seed=1337))\n    assert rand.shape == (300, 200)\n    assert np.abs(np.mean(rand) - mean) < 0.015\n    assert np.max(rand) <= max_val\n    assert np.min(rand) >= min_val\n    assert np.abs(np.std(rand) - std * 0.87962) < 0.015",
                            "def test_conv_invalid_use(self):\n    dummy_x_1d = K.variable(np.ones((4, 8, 2)))\n    dummy_w_1d = K.variable(np.ones((3, 2, 3)))\n    dummy_x_2d = K.variable(np.ones((2, 3, 4, 5)))\n    dummy_w_2d = K.variable(np.ones((2, 2, 3, 4)))\n    dummy_x_3d = K.variable(np.ones((2, 3, 4, 5, 4)))\n    dummy_w_3d = K.variable(np.ones((2, 2, 2, 3, 4)))\n    dummy_w1x1_2d = K.variable(np.ones((1, 1, 12, 7)))\n    with pytest.raises(ValueError):\n        K.conv1d(dummy_x_1d, dummy_w_1d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv3d(dummy_x_3d, dummy_w_3d, data_format='channels_middle')\n    if K.backend() != 'theano':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    if K.backend() == 'cntk':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, strides=(2, 2), dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, strides=(2, 2), dilation_rate=(1, 2))",
                            "def test_pooling_invalid_use(self):\n    for input_shape, pool_size in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n        x = K.variable(np.random.random(input_shape))\n        if len(pool_size) == 2:\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, pool_mode='median')\n        else:\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, pool_mode='median')",
                            "def test_resize_images(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_images', x_shape, WITH_NP, cntk_dynamicity=True, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_images(K.variable(xval), 2, 2, data_format='channels_middle')",
                            "@staticmethod\ndef _helper_bilinear(data_format, height_factor, width_factor):\n    x_shape = (2, 3, 4, 5)\n    check_single_tensor_operation('resize_images', x_shape, [KTF, KTH], height_factor=height_factor, width_factor=width_factor, data_format=data_format, interpolation='bilinear')",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\ndef test_resize_images_bilinear(self, data_format):\n    self._helper_bilinear(data_format, 2, 2)\n    with pytest.raises(NotImplementedError):\n        self._helper_bilinear(data_format, 4, 4)",
                            "def test_resize_volumes(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_volumes', x_shape, WITH_NP, cntk_dynamicity=True, depth_factor=2, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_volumes(K.variable(xval), 2, 2, 2, data_format='channels_middle')",
                            "def test_temporal_padding(self):\n    check_single_tensor_operation('temporal_padding', (4, 3, 3), WITH_NP)\n    check_single_tensor_operation('temporal_padding', (2, 3, 4), WITH_NP, padding=(1, 2))",
                            "def test_spatial_2d_padding(self):\n    padding = ((1, 2), (2, 1))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, 1))\n        y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_2d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_spatial_3d_padding(self):\n    padding = ((1, 2), (2, 1), (1, 2))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, None, 1))\n        y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_3d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_bias_add(self):\n    for data_format in ['channels_first', 'channels_last']:\n        for shape in [(), (3,), (2, 3), (5, 3, 2)]:\n            if data_format == 'channels_first':\n                x_shape = (1, 4) + shape\n            else:\n                x_shape = (1,) + shape + (4,)\n            bias_shape = (4,)\n            check_two_tensor_operation('bias_add', x_shape, bias_shape, WITH_NP, cntk_dynamicity=True, data_format=data_format)\n        if data_format == 'channels_first':\n            x_shape = (20, 6, 10)\n        else:\n            x_shape = (20, 10, 6)\n        check_two_tensor_operation('bias_add', x_shape, (10, 6), WITH_NP, cntk_dynamicity=True, data_format=data_format)\n    x = K.variable(np.random.random(x_shape))\n    b = K.variable(np.random.random(bias_shape))\n    with pytest.raises(ValueError):\n        K.bias_add(x, b, data_format='channels_middle')",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='Specific to Theano.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_th(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes='per-activation')\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Specific to Tensorflow.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_tf(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'cntk', reason='Specific to CNTK.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_cntk(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.placeholder(x_shape)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.function([x], [z])([x_val])[0]\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_ctc(self):\n    if K.backend() == 'theano':\n        ref = [1.73308, 3.81351]\n    else:\n        ref = [3.34211, 5.42262]\n    label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n    input_lens = np.expand_dims(np.asarray([5, 5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]], [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508], [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549], [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456], [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345], [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)\n    if K.backend() == 'theano':\n        ref = [1.73308]\n    else:\n        ref = [3.34211]\n    input_lens = np.expand_dims(np.asarray([5]), 1)\n    label_lens = np.expand_dims(np.asarray([5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Test adapted from tensorflow.')\ndef test_ctc_decode_greedy(self):\n    \"\"\"Test two batch entries - best path decoder.\"\"\"\n    max_time_steps = 6\n    seq_len_0 = 4\n    input_prob_matrix_0 = np.asarray([[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.4, 0.6], [0.0, 0.0, 0.4, 0.6], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    seq_len_1 = 5\n    input_prob_matrix_1 = np.asarray([[0.1, 0.9, 0.0, 0.0], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.1, 0.9], [0.0, 0.9, 0.1, 0.1], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    inputs = [np.vstack([input_prob_matrix_0[t, :], input_prob_matrix_1[t, :]]) for t in range(max_time_steps)]\n    inputs = np.asarray(inputs).transpose((1, 0, 2))\n    input_length = np.array([seq_len_0, seq_len_1], dtype=np.int32)\n    decode_pred_np, log_prob_pred_np = KNP.ctc_decode(inputs, input_length, greedy=True)\n    inputs = K.variable(inputs)\n    input_length = K.variable(input_length)\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=True)\n    assert len(decode_pred_tf) == 1\n    decode_pred = K.eval(decode_pred_tf[0])\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    assert np.alltrue(decode_pred_np == decode_pred)\n    assert np.allclose(log_prob_pred_np, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='tensorflow-way slice is only supported in tensorflow.')\n@pytest.mark.parametrize('x_size', [[1, 1, 3], [1, 2, 3], [2, 1, 3]])\ndef test_slice(self, x_size):\n    npt = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])\n    x_start = [1, 0, 0]\n    tft = K.constant(npt)\n    test_input = K.eval(K.slice(tft, x_start, x_size))\n    expected = KNP.slice(npt, x_start, x_size)\n    assert np.allclose(test_input, expected)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search(self):\n    \"\"\"Test one batch, two beams - hibernating beam search.\"\"\"\n    depth = 6\n    seq_len_0 = 5\n    input_prob_matrix_0 = np.asarray([[0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908], [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517], [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763], [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655], [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878], [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]], dtype=np.float32)\n    input_prob_matrix_0 = input_prob_matrix_0 + 2.0\n    inputs = [input_prob_matrix_0[t, :][np.newaxis, :] for t in range(seq_len_0)] + 2 * [np.zeros((1, depth), dtype=np.float32)]\n    inputs = np.exp(inputs)\n    inputs = K.variable(inputs.transpose((1, 0, 2)))\n    input_length = K.variable(np.array([seq_len_0], dtype=np.int32))\n    log_prob_truth = np.array([-5.811451, -6.63339], np.float32)[np.newaxis, :]\n    decode_truth = [np.array([1, 0]), np.array([[1]])]\n    beam_width = 2\n    top_paths = 2\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=False, beam_width=beam_width, top_paths=top_paths)\n    assert len(decode_pred_tf) == top_paths\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    for i in range(top_paths):\n        assert np.alltrue(decode_truth[i] == K.eval(decode_pred_tf[i]))\n    assert np.allclose(log_prob_truth, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search_no_merge(self):\n    input_prob = np.array([[[0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]])\n    input_len = np.array(input_prob.shape[0] * [input_prob.shape[1]])\n\n    def decode(merge_repeated):\n        input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n        input_len_tensor = K.placeholder(shape=None, dtype='int64')\n        paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n        decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n        paths = decode_func([input_prob, input_len])\n        return paths\n    assert np.allclose(decode(merge_repeated=True), [np.array([[0, 1]])])\n    assert np.allclose(decode(merge_repeated=False), [np.array([[0, 0, 1, 1]])])",
                            "def test_one_hot(self):\n    input_length = 10\n    num_classes = 20\n    batch_size = 30\n    indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n    oh = KNP.one_hot(np.int32(indices), num_classes)\n    koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n    assert np.all(koh == oh)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_dot(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense = x_sparse.toarray()\n    W = np.random.random((5, 4))\n    t_W = K.variable(W)\n    k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n    k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n    assert k_s.shape == k_d.shape\n    assert_allclose(k_s, k_d, atol=1e-05)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_concat(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_1 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_2 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense_1 = x_sparse_1.toarray()\n    x_dense_2 = x_sparse_2.toarray()\n    k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n    assert K.is_sparse(k_s)\n    k_s_d = K.eval(k_s)\n    k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n    assert k_s_d.shape == k_d.shape\n    assert_allclose(k_s_d, k_d, atol=1e-05)",
                            "def test_stack(self):\n    tensor_list = [np.random.randn(5, 4, 6, 10) for _ in range(5)]\n    stack_axis = 3\n    results = []\n    if WITH_NP[0] == KC:\n        check_two_tensor_operation('stack', (5, 4, 6, 10), (5, 4, 6, 10), WITH_NP, axis=stack_axis, concat_args=True)\n    else:\n        for k in WITH_NP:\n            tensor_list_var = [k.variable(tensor) for tensor in tensor_list]\n            out = k.eval(k.stack(tensor_list_var, axis=stack_axis))\n            results.append(out)\n        assert_list_pairwise(results)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_map(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    vx = K.variable(x)\n    kx = K.eval(K.map_fn(K.sum, vx))\n    kx2 = K.eval(K.map_fn(lambda i: K.sum(vx[i]), K.arange(10), dtype=K.floatx()))\n    assert (10,) == kx.shape\n    assert (10,) == kx2.shape\n    assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n    assert_allclose(kx, kx2, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldl(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n    assert (3,) == kx.shape\n    assert_allclose(x.sum(axis=0), kx, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldr(self):\n    x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n    vx = K.variable(x)\n    p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n    p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n    assert p1 < p2\n    assert 9e-38 < p2 <= 1e-37",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk has issues with negative number.')\ndef test_arange(self):\n    for test_value in (-20, 0, 1, 10):\n        a_list = []\n        dtype_list = []\n        for k in WITH_NP:\n            t = k.arange(test_value)\n            a = k.eval(t)\n            assert np.array_equal(a, np.arange(test_value))\n            dtype_list.append(k.dtype(t))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for start, stop, step in ((0, 5, 1), (-5, 5, 2), (0, 1, 2)):\n        a_list = []\n        for k in WITH_NP:\n            a = k.eval(k.arange(start, stop, step))\n            assert np.array_equal(a, np.arange(start, stop, step))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for dtype in ('int32', 'int64', 'float32', 'float64'):\n        for k in WITH_NP:\n            t = k.arange(10, dtype=dtype)\n            assert k.dtype(t) == dtype\n    start = K.constant(1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 1\n    start = K.constant(-1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 0",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_train_phase(self, training):\n    check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_test_phase(self, training):\n    check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "def test_setfloatx_incorrect_values(self):\n    old_floatx = floatx()\n    initial = floatx()\n    for value in ['', 'beerfloat', 123]:\n        with pytest.raises(ValueError):\n            set_floatx(value)\n    assert floatx() == initial\n    set_floatx(old_floatx)",
                            "def DISABLED_test_setfloatx_correct_values(self):\n    \"\"\"Disabled because it is not thread safe at this time.\"\"\"\n    old_floatx = floatx()\n    for value in ['float16', 'float32', 'float64']:\n        set_floatx(value)\n        assert floatx() == value\n    set_floatx(old_floatx)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support float16')\ndef DISABLED_test_set_floatx(self):\n    \"\"\"Disabled because it is not thread safe at this time.\n\n    Make sure that changes to the global floatx are effectively\n    taken into account by the backend.\n    \"\"\"\n    old_floatx = floatx()\n    set_floatx('float16')\n    var = variable([10])\n    check_dtype(var, 'float16')\n    set_floatx('float64')\n    var = variable([10])\n    check_dtype(var, 'float64')\n    set_floatx(old_floatx)",
                            "def test_dtype(self):\n    assert K.dtype(K.variable(1, dtype='float64')) == 'float64'\n    assert K.dtype(K.variable(1, dtype='float32')) == 'float32'\n    assert K.dtype(K.variable(1, dtype='float16')) == 'float16'",
                            "def test_variable_support_bool_dtype(self):\n    if K.backend() == 'tensorflow':\n        assert K.dtype(K.variable(1, dtype='int16')) == 'int16'\n        assert K.dtype(K.variable(False, dtype='bool')) == 'bool'\n        with pytest.raises(TypeError):\n            K.variable('', dtype='unsupported')",
                            "def test_clip_supports_tensor_arguments(self):\n    x = K.variable([-10.0, -5.0, 0.0, 5.0, 10.0])\n    min_value = K.variable([-5.0, -4.0, 0.0, 3.0, 5.0])\n    max_value = K.variable([5.0, 4.0, 1.0, 4.0, 9.0])\n    assert np.allclose(K.eval(K.clip(x, min_value, max_value)), np.asarray([-5.0, -4.0, 0.0, 4.0, 9.0], dtype=np.float32))",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(), reason='This test is for tensorflow parallelism.')\ndef test_tensorflow_session_parallelism_settings(self, monkeypatch):\n    for threads in [1, 2]:\n        K.clear_session()\n        monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n        cfg = K.get_session()._config\n        assert cfg.intra_op_parallelism_threads == threads\n        assert cfg.inter_op_parallelism_threads == threads",
                            "def batch_shape(shape):\n    return (batch_size,) + shape[1:]",
                            "def random(shape):\n    return np.random.random(batch_shape(shape))",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn(inputs, states):\n        assert len(states) == 1\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y])\n    return simple_rnn",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_with_extra_mock_state(inputs, states):\n        assert len(states) == 2\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y, backend.concatenate([y, y], axis=-1)])\n    return simple_rnn_with_extra_mock_state",
                            "def get_step_function(backend, w_i):\n\n    def simple_no_states(inputs, states):\n        assert len(states) == 0\n        y = backend.dot(inputs, w_i)\n        return (y, [])\n    return simple_no_states",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_add_constant(inputs, states_and_constants):\n        [h, c] = states_and_constants\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n        return (y, [y])\n    return simple_rnn_add_constant",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def step_function(inputs, states):\n    outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n    return (outputs, states)",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def decode(merge_repeated):\n    input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n    input_len_tensor = K.placeholder(shape=None, dtype='int64')\n    paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n    decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n    paths = decode_func([input_prob, input_len])\n    return paths",
                            "def simple_rnn(inputs, states):\n    assert len(states) == 1\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y])",
                            "def simple_rnn_with_extra_mock_state(inputs, states):\n    assert len(states) == 2\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y, backend.concatenate([y, y], axis=-1)])",
                            "def simple_no_states(inputs, states):\n    assert len(states) == 0\n    y = backend.dot(inputs, w_i)\n    return (y, [])",
                            "def simple_rnn_add_constant(inputs, states_and_constants):\n    [h, c] = states_and_constants\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n    return (y, [y])"
                        ],
                        "constructor_variables": [],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "test_is_keras_tensor(self)",
                            "test_set_learning_phase(self)",
                            "test_eye(self)",
                            "test_ones(self)",
                            "test_zeros(self)",
                            "test_ones_like(self)",
                            "test_zeros_like(self)",
                            "test_linear_operations(self)",
                            "test_random_variables(self)",
                            "test_batch_dot_shape(self)",
                            "test_shape_operations(self)",
                            "test_none_shape_operations(self)",
                            "test_repeat_elements(self)",
                            "test_tile(self)",
                            "test_gather(self)",
                            "test_value_manipulation(self, function_name)",
                            "test_print_tensor(self)",
                            "test_elementwise_operations(self)",
                            "test_reset_uids(self)",
                            "test_cumsum_cumprod(self)",
                            "test_log(self)",
                            "test_update_add(self)",
                            "test_update_sub(self)",
                            "test_gradient(self)",
                            "test_stop_gradient(self)",
                            "test_function(self)",
                            "test_function_tf_fetches(self)",
                            "test_function_tf_feed_dict(self)",
                            "test_function_tf_run_options_with_run_metadata(self)",
                            "test_function_tf_string_input(self)",
                            "test_rnn(self)",
                            "test_rnn_additional_states(self)",
                            "test_rnn_no_states(self)",
                            "test_rnn_constants(self)",
                            "test_rnn_output_and_state_masking_independent(self)",
                            "test_rnn_output_num_dim_larger_than_2_masking(self)",
                            "test_rnn_state_num_dim_larger_than_2_masking(self)",
                            "test_logsumexp(self, x_np, axis, keepdims)",
                            "test_logsumexp_optim(self)",
                            "test_switch(self)",
                            "test_dropout(self)",
                            "test_relu(self, alpha, max_value, threshold)",
                            "test_nn_operations(self)",
                            "test_in_top_k(self)",
                            "test_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format)",
                            "test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate)",
                            "test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate)",
                            "test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode)",
                            "test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format)",
                            "test_random_normal(self)",
                            "test_random_uniform(self)",
                            "test_random_binomial(self)",
                            "test_truncated_normal(self)",
                            "test_conv_invalid_use(self)",
                            "test_pooling_invalid_use(self)",
                            "test_resize_images(self)",
                            "_helper_bilinear(data_format, height_factor, width_factor)",
                            "test_resize_images_bilinear(self, data_format)",
                            "test_resize_volumes(self)",
                            "test_temporal_padding(self)",
                            "test_spatial_2d_padding(self)",
                            "test_spatial_3d_padding(self)",
                            "test_bias_add(self)",
                            "test_batchnorm_th(self, x_shape)",
                            "test_batchnorm_tf(self, x_shape)",
                            "test_batchnorm_cntk(self, x_shape)",
                            "test_ctc(self)",
                            "test_ctc_decode_greedy(self)",
                            "test_slice(self, x_size)",
                            "test_ctc_decode_beam_search(self)",
                            "test_ctc_decode_beam_search_no_merge(self)",
                            "test_one_hot(self)",
                            "test_sparse_dot(self)",
                            "test_sparse_concat(self)",
                            "test_stack(self)",
                            "test_map(self)",
                            "test_foldl(self)",
                            "test_foldr(self)",
                            "test_arange(self)",
                            "test_in_train_phase(self, training)",
                            "test_in_test_phase(self, training)",
                            "test_setfloatx_incorrect_values(self)",
                            "DISABLED_test_setfloatx_correct_values(self)",
                            "DISABLED_test_set_floatx(self)",
                            "test_dtype(self)",
                            "test_variable_support_bool_dtype(self)",
                            "test_clip_supports_tensor_arguments(self)",
                            "test_tensorflow_session_parallelism_settings(self, monkeypatch)",
                            "batch_shape(shape)",
                            "random(shape)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i)",
                            "get_step_function(backend, w_i, w_h)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "decode(merge_repeated)",
                            "simple_rnn(inputs, states)",
                            "simple_rnn_with_extra_mock_state(inputs, states)",
                            "simple_no_states(inputs, states)",
                            "simple_rnn_add_constant(inputs, states_and_constants)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "test_truncated_normal",
                    "function_code": "def test_truncated_normal(self):\n    mean = 0.\n    std = 1.\n    min_val = -2.\n    max_val = 2.\n    rand = K.eval(K.truncated_normal((300, 200),\n                                     mean=mean, stddev=std, seed=1337))\n    assert rand.shape == (300, 200)\n    assert np.abs(np.mean(rand) - mean) < 0.015\n    assert np.max(rand) <= max_val\n    assert np.min(rand) >= min_val\n\n    # assumption in initializers.VarianceScaling\n    assert np.abs(np.std(rand) - std * 0.87962) < 0.015\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 1410,
                    "end_line": 1423,
                    "variables": {
                        "mean": [
                            1416,
                            1418,
                            1411
                        ],
                        "std": [
                            1416,
                            1412,
                            1423
                        ],
                        "min_val": [
                            1420,
                            1413
                        ],
                        "max_val": [
                            1419,
                            1414
                        ],
                        "rand": [
                            1415,
                            1417,
                            1418,
                            1419,
                            1420,
                            1423
                        ],
                        "K.eval": [
                            1415
                        ],
                        "K": [
                            1415
                        ],
                        "K.truncated_normal": [
                            1415
                        ],
                        "rand.shape": [
                            1417
                        ],
                        "np.abs": [
                            1418,
                            1423
                        ],
                        "np": [
                            1418,
                            1419,
                            1420,
                            1423
                        ],
                        "np.mean": [
                            1418
                        ],
                        "np.max": [
                            1419
                        ],
                        "np.min": [
                            1420
                        ],
                        "np.std": [
                            1423
                        ]
                    },
                    "filtered_variables": {
                        "mean": [
                            1416,
                            1418,
                            1411
                        ],
                        "std": [
                            1416,
                            1412,
                            1423
                        ],
                        "min_val": [
                            1420,
                            1413
                        ],
                        "max_val": [
                            1419,
                            1414
                        ],
                        "rand": [
                            1415,
                            1417,
                            1418,
                            1419,
                            1420,
                            1423
                        ],
                        "K.eval": [
                            1415
                        ],
                        "K": [
                            1415
                        ],
                        "K.truncated_normal": [
                            1415
                        ],
                        "rand.shape": [
                            1417
                        ],
                        "np.abs": [
                            1418,
                            1423
                        ],
                        "np": [
                            1418,
                            1419,
                            1420,
                            1423
                        ],
                        "np.mean": [
                            1418
                        ],
                        "np.max": [
                            1419
                        ],
                        "np.min": [
                            1420
                        ],
                        "np.std": [
                            1423
                        ]
                    },
                    "diff_line_number": 1415,
                    "class_data": {
                        "signature": "class TestBackend(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def test_is_keras_tensor(self):\n    np_var = np.array([1, 2])\n    with pytest.raises(ValueError):\n        K.is_keras_tensor(np_var)\n    keras_var = K.variable(np_var)\n    assert K.is_keras_tensor(keras_var) is False\n    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n    assert K.is_keras_tensor(keras_placeholder) is False",
                            "def test_set_learning_phase(self):\n    with pytest.raises(ValueError):\n        K.set_learning_phase(2)",
                            "def test_eye(self):\n    check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)",
                            "def test_ones(self):\n    check_single_tensor_operation('ones', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_zeros(self):\n    check_single_tensor_operation('zeros', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_ones_like(self):\n    check_single_tensor_operation('ones_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_zeros_like(self):\n    check_single_tensor_operation('zeros_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_linear_operations(self):\n    check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n    check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=(1, 1))\n    check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n    check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n    if K.backend() != 'cntk':\n        check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=(1, 2))",
                            "def test_random_variables(self):\n    check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP, low=0.0, high=1.0, shape_or_val=False, assert_value_equality=False)\n    check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP, mean=0.0, scale=1.0, shape_or_val=False, assert_value_equality=False)",
                            "def test_batch_dot_shape(self):\n    test_cases = []\n    test_cases.append([(None, 3, 4, 5), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 3, 4, 5), (None, 2, 4), 2])\n    test_cases.append([(None, 3, 4), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 4, 3), (None, 3, 5), (2, 1)])\n    test_cases.append([(None, 4), (None, 3, 4), (1, 2)])\n    test_cases.append([(None, 4), (None, 4), None])\n    batch_size = 7\n\n    def batch_shape(shape):\n        return (batch_size,) + shape[1:]\n\n    def random(shape):\n        return np.random.random(batch_shape(shape))\n    for x_shape, y_shape, axes in test_cases:\n        x_np = random(x_shape)\n        y_np = random(y_shape)\n        z_np = KNP.batch_dot(x_np, y_np, axes)\n        x = K.placeholder(shape=x_shape)\n        y = K.placeholder(shape=y_shape)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        f = K.function([x, y], [z])\n        assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        if K.backend() != 'cntk':\n            x = K.placeholder(ndim=len(x_shape))\n            y = K.placeholder(ndim=len(y_shape))\n            z = K.batch_dot(x, y, axes)\n            z_shape = K.int_shape(z)\n            if z_shape is not None:\n                assert len(z_shape) == z_np.ndim\n                assert set(z_shape) <= set((None, 1))\n            f = K.function([x, y], [z])\n            assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        x = K.variable(x_np)\n        y = K.variable(y_np)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        z = K.eval(z)\n        assert_allclose(z, z_np, atol=1e-05)",
                            "def test_shape_operations(self):\n    check_two_tensor_operation('concatenate', (4, 3), (4, 2), WITH_NP, axis=-1, concat_args=True)\n    check_single_tensor_operation('reshape', (4, 2), WITH_NP, shape=(8, 1))\n    check_single_tensor_operation('permute_dimensions', (4, 2, 3), WITH_NP, pattern=(2, 0, 1))\n    check_single_tensor_operation('repeat', (4, 1), WITH_NP, n=3)\n    check_single_tensor_operation('flatten', (4, 1), WITH_NP)\n    check_single_tensor_operation('batch_flatten', (20, 2, 5), WITH_NP, cntk_dynamicity=True)\n    check_single_tensor_operation('expand_dims', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('expand_dims', (4, 3, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('squeeze', (4, 3, 1), WITH_NP, axis=2)\n    check_single_tensor_operation('squeeze', (4, 1, 1), WITH_NP, axis=1)\n    check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)}, 'squeeze', {'axis': 2}, (4, 3, 1, 1), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='We only test the shape inference of the theano backend.')\ndef test_none_shape_operations(self):\n    x = K.placeholder((3, None, 4))\n    y = K.batch_flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (3, None)\n    y = K.flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (None,)",
                            "def test_repeat_elements(self):\n    reps = 3\n    for ndims in [1, 2, 3]:\n        shape = np.arange(2, 2 + ndims)\n        arr = np.arange(np.prod(shape)).reshape(shape)\n        for rep_axis in range(ndims):\n            check_single_tensor_operation('repeat_elements', arr, WITH_NP, rep=reps, axis=rep_axis)\n            if K.backend() != 'cntk':\n                shape = list(shape)\n                shape[rep_axis] = None\n                x = K.placeholder(shape=shape)\n                y = K.repeat_elements(x, reps, axis=rep_axis)\n                assert y._keras_shape == tuple(shape)\n                assert y._keras_shape == K.int_shape(y)",
                            "def test_tile(self):\n    shape = (3, 4)\n    arr = np.arange(np.prod(shape)).reshape(shape)\n    check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n    check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 4))\n        n = 2\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 8)\n        n = (4, 3)\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 12)",
                            "def test_gather(self):\n    shape = (10, 2, 3)\n    ref = np.arange(np.prod(shape)).reshape(shape)\n    inds = [1, 3, 7, 9]\n    t_list = [k.gather(k.variable(ref), k.variable(inds, dtype='int32')) for k in WITH_NP]\n    z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32'))) for k in WITH_NP]\n    assert_list_pairwise(z_list)\n    assert_list_keras_shape(t_list, z_list)\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 3, 4))\n        indices = K.placeholder(shape=(5, 6), dtype='int32')\n        y = K.gather(x, indices)\n        assert y._keras_shape == (5, 6, 3, 4)",
                            "@pytest.mark.parametrize('function_name', ['get_value', 'count_params', 'int_shape', 'get_variable_shape'])\ndef test_value_manipulation(self, function_name):\n    val = np.random.random((4, 2))\n    v_list = [getattr(k, function_name)(k.variable(val)) for k in WITH_NP]\n    if function_name == 'get_value':\n        assert_list_pairwise(v_list)\n    else:\n        assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)",
                            "def test_print_tensor(self):\n    check_single_tensor_operation('print_tensor', (), WITH_NP)\n    check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n    check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n    check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)",
                            "def test_elementwise_operations(self):\n    check_single_tensor_operation('max', (4, 2), WITH_NP)\n    check_single_tensor_operation('max', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('max', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('min', (4, 2), WITH_NP)\n    check_single_tensor_operation('min', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('min', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('mean', (4, 2), WITH_NP)\n    check_single_tensor_operation('mean', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=-1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('var', (4, 2), WITH_NP)\n    check_single_tensor_operation('var', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('var', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('std', (4, 2), WITH_NP)\n    check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('prod', (4, 2), WITH_NP)\n    check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('any', (4, 2), WITH_NP)\n    check_single_tensor_operation('any', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('any', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('all', (4, 2), WITH_NP)\n    check_single_tensor_operation('all', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('all', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('square', (4, 2), WITH_NP)\n    check_single_tensor_operation('abs', (4, 2), WITH_NP)\n    check_single_tensor_operation('sqrt', (4, 2), WITH_NP)\n    check_single_tensor_operation('exp', (4, 2), WITH_NP)\n    check_single_tensor_operation('round', (4, 2), WITH_NP)\n    check_single_tensor_operation('sign', (4, 2), WITH_NP)\n    check_single_tensor_operation('pow', (4, 2), WITH_NP, a=3)\n    check_single_tensor_operation('clip', (4, 2), WITH_NP, min_value=0.4, max_value=0.6)\n    check_single_tensor_operation('cos', (4, 2), WITH_NP)\n    check_single_tensor_operation('sin', (4, 2), WITH_NP)\n    check_two_tensor_operation('equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('not_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('maximum', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('minimum', (4, 2), (4, 2), WITH_NP)",
                            "def test_reset_uids(self):\n    first = K.get_uid()\n    K.get_uid()\n    K.reset_uids()\n    assert K.get_uid() == first",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support cumsum and cumprod yet')\ndef test_cumsum_cumprod(self):\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP, axis=1)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk return -85.1 for zero or negative number, not nan, so can't compare with other backend.\")\ndef test_log(self):\n    check_single_tensor_operation('log', (4, 2), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_add(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    increment = np.random.randn(3, 4)\n    x += increment\n    K.eval(K.update_add(x_var, increment))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_sub(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    decrement = np.random.randn(3, 4)\n    x -= decrement\n    K.eval(K.update_sub(x_var, decrement))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk doesn't support gradient in this way.\")\ndef test_gradient(self):\n    val = np.random.random((4, 2))\n    x_list = [k.variable(val) for k in [KTH, KTF]]\n    z_list = []\n    zero_list = []\n    for x, k in zip(x_list, [KTH, KTF]):\n        exp = x * k.exp(x)\n        loss = k.sum(exp)\n        zero_loss = k.stop_gradient(loss)\n        grad = k.gradients(loss, [exp])\n        zero_grad = k.gradients(loss + zero_loss, [exp])\n        z_list.append(k.eval(grad[0]))\n        zero_list.append(k.eval(zero_grad[0]))\n    assert_list_pairwise(z_list)\n    assert_list_pairwise(zero_list)\n    for i in range(len(z_list)):\n        assert_allclose(zero_list[i], z_list[i], atol=1e-05)",
                            "def test_stop_gradient(self):\n    val = np.random.random((4, 2))\n    a = K.variable(val)\n    b = K.square(a)\n    c, d = K.stop_gradient([a, b])\n    e = K.stop_gradient(b)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk currently not support function in this way, so can't test as this.\")\ndef test_function(self):\n    test_backend = [KTH, KTF]\n    val = np.random.random((4, 2))\n    input_val = np.random.random((4, 2))\n    f_list = []\n    x_list = []\n    for k in test_backend:\n        x = k.variable(val)\n        x_list.append(x)\n        y = k.placeholder(ndim=2)\n        exp = k.square(x) + y\n        update = x * 2\n        f = k.function([y], [exp], updates=[(x, update)])\n        f_list.append(f)\n    function_outputs_list = [f([input_val])[0] for f in f_list]\n    assert_list_pairwise(function_outputs_list)\n    new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n    assert_list_pairwise(new_val_list)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `fetches` argument.')\ndef test_function_tf_fetches(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], updates=[(x, x_placeholder + 1.0)], fetches=[K.update(y, 5.0)])\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert K.get_session().run(fetches=[x, y]) == [11.0, 5.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `feed_dict` argument.')\ndef test_function_tf_feed_dict(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    feed_dict = {y_placeholder: 3.0}\n    f = K.function(inputs=[x_placeholder], outputs=[x_placeholder + 1.0], updates=[(x, x_placeholder + 10.0)], feed_dict=feed_dict, fetches=[K.update(y, y_placeholder * 10.0)])\n    output = f([10.0])\n    assert output == [11.0]\n    assert K.get_session().run(fetches=[x, y]) == [20.0, 30.0]\n    feed_dict[y_placeholder] = 4.0\n    output = f([20.0])\n    assert output == [21.0]\n    assert K.get_session().run(fetches=[x, y]) == [30.0, 40.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `options` and `run_metadata` arguments.')\ndef test_function_tf_run_options_with_run_metadata(self):\n    from tensorflow.core.protobuf import config_pb2\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    run_options = config_pb2.RunOptions(output_partition_graphs=True)\n    run_metadata = config_pb2.RunMetadata()\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], options=run_options, run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) > 0\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) == 0",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `string` type for a tensor.')\ndef test_function_tf_string_input(self):\n    x_placeholder = K.placeholder(shape=(), dtype='string')\n    x_identity = K.identity(x_placeholder)\n    f = K.function(inputs=[x_placeholder], outputs=[x_identity])\n    output = f([b'test'])\n    assert output == [b'test']",
                            "def test_rnn(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn(inputs, states):\n            assert len(states) == 1\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y])\n        return simple_rnn\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_additional_states(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    h1 = np.concatenate([h0, h0], axis=-1)\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_with_extra_mock_state(inputs, states):\n            assert len(states) == 2\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y, backend.concatenate([y, y], axis=-1)])\n        return simple_rnn_with_extra_mock_state\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0, h1], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_no_states(self):\n    num_samples = 3\n    input_dim = 8\n    output_dim = 4\n    timesteps = 5\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n\n    def get_step_function(backend, w_i):\n\n        def simple_no_states(inputs, states):\n            assert len(states) == 0\n            y = backend.dot(inputs, w_i)\n            return (y, [])\n        return simple_no_states\n    kwargs_list = [{'go_backwards': False}, {'go_backwards': True}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k), step_function_np=get_step_function(KNP, wi), inputs_np=x, initial_states_np=[], mask_np=None, **kwargs)",
                            "def test_rnn_constants(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, c = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_add_constant(inputs, states_and_constants):\n            [h, c] = states_and_constants\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n            return (y, [y])\n        return simple_rnn_add_constant\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), constants_np=[c], **kwargs)",
                            "def test_rnn_output_and_state_masking_independent(self):\n    num_samples = 2\n    num_timesteps = 4\n    state_and_io_size = 5\n    mask_last_num_timesteps = 2\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, state_and_io_size))\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[1, -mask_last_num_timesteps:] = 0\n    expected_outputs = inputs_vals.copy()\n    expected_outputs[1, -mask_last_num_timesteps:] = expected_outputs[1, -(mask_last_num_timesteps + 1)]\n    expected_state = initial_state_vals.copy()\n    expected_state[0] += num_timesteps\n    expected_state[1] += num_timesteps - mask_last_num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)\n        assert_allclose(K.eval(last_states[0]), expected_state)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_output_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n    num_features = 5\n\n    def step_function(inputs, states):\n        outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n        return (outputs, states)\n    inputs_vals = np.random.random((num_samples, num_timesteps, num_features))\n    initial_state_vals = np.random.random((num_samples, 6))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[-1, -1] = 0\n    expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n    expected_outputs[-1, -1] = expected_outputs[-1, -2]\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_state_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n    initial_state_vals = np.random.random((num_samples, 6, 7))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[0, -2:] = 0\n    expected_last_state = initial_state_vals.copy()\n    expected_last_state[0] += num_timesteps - 2\n    expected_last_state[1:] += num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(last_states[0]), expected_last_state)",
                            "@pytest.mark.parametrize('x_np,axis,keepdims', [(np.array([1.1, 0.8, 0.9]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 1, False), (np.array([[1.1, 0.8, 0.9]]), -1, False), (np.array([[1.1, 0.8, 0.9]]), 1, True), (np.array([[1.1], [1.2]]), 0, False), (np.array([[1.1], [1.2]]), 1, False), (np.array([[1.1], [1.2]]), -1, False), (np.array([[1.1], [1.2]]), -1, True), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), None, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 0, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False)])\ndef test_logsumexp(self, x_np, axis, keepdims):\n    \"\"\"\n    Check if K.logsumexp works properly for values close to one.\n    \"\"\"\n    x = K.variable(x_np)\n    assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)), np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)), rtol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='The optimization is applied only with TensorFlow.')\ndef test_logsumexp_optim(self):\n    \"\"\"\n    Check if optimization works.\n    \"\"\"\n    x_np = np.array([10000.0, 0.0001])\n    result = K.eval(K.logsumexp(K.variable(x_np), axis=0))\n    assert_allclose(result, 10000.0, rtol=1e-05)",
                            "def test_switch(self):\n    val = np.random.random()\n    z_list = []\n    for k in WITH_NP:\n        x = k.variable(val)\n        x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n        z_list.append(k.eval(x))\n    assert_list_pairwise(z_list)\n    shapes = []\n    shapes.append([(4, 3, 2), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4, 3), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4,), (4, 3, 2), (4, 3, 2)])\n    for s in shapes:\n        z_list = []\n        arrays = list(map(np.random.random, s))\n        for k in WITH_NP:\n            x, then_expr, else_expr = map(k.variable, arrays)\n            cond = k.greater_equal(x, 0.5)\n            z_list.append(k.eval(k.switch(cond, then_expr, else_expr)))\n        assert_list_pairwise(z_list)",
                            "def test_dropout(self):\n    val = np.random.random((100, 100))\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2)) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2, noise_shape=list(val.shape))) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    with pytest.raises(ValueError):\n        z = K.dropout(K.variable(val), level=-0.5)",
                            "@pytest.mark.parametrize('alpha,max_value,threshold', [(0.0, None, 0.0), (0.1, None, 0.0), (0.0, 5.0, 0.0), (0.0, None, 0.8), (0.1, 5.0, 0.0), (0.1, None, 0.8), (0.0, 5.0, 0.8), (0.1, 5.0, 0.8), (0.1, 0.0, 0.8), (0.1, 5.0, -2.8), (0.1, 9.0, 0.8)])\ndef test_relu(self, alpha, max_value, threshold):\n    check_single_tensor_operation('relu', (4, 2), WITH_NP, alpha=alpha, max_value=max_value, threshold=threshold)",
                            "def test_nn_operations(self):\n    check_single_tensor_operation('softsign', (4, 10), WITH_NP)\n    check_single_tensor_operation('softplus', (4, 10), WITH_NP)\n    check_single_tensor_operation('elu', (4, 10), WITH_NP, alpha=0.5)\n    check_single_tensor_operation('sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('hard_sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('tanh', (4, 2), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 10), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 5, 3), WITH_NP, axis=1)\n    check_single_tensor_operation('softmax', (4, 5, 3, 10), WITH_NP, axis=2)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    if K.backend() != 'cntk':\n        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841], [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n    yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016], [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n    check_two_tensor_operation('categorical_crossentropy', yval, xval, WITH_NP, cntk_two_dynamicity=True, from_logits=True)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)",
                            "def test_in_top_k(self):\n    batch_size = 20\n    num_classes = 10\n    predictions = np.random.random((batch_size, num_classes)).astype('float32')\n    targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n    for k in range(num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)\n    num_identical = num_classes // 2\n    for i in range(batch_size):\n        idx_identical = np.random.choice(num_classes, size=num_identical, replace=False)\n        predictions[i, idx_identical] = predictions[i, 0]\n    targets = np.zeros(batch_size, dtype='int32')\n    for k in range(1, num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('conv1d', (2, 8, 2), (3, 2, 3), 'same', 'channels_last'), ('conv1d', (1, 8, 2), (3, 2, 3), 'valid', 'channels_last'), ('conv1d', (1, 2, 8), (3, 2, 3), 'valid', 'channels_first'), ('conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'), ('conv3d', (2, 3, 4, 5, 4), (3, 3, 3, 3, 4), 'same', 'channels_first'), ('conv3d', (2, 3, 5, 4, 6), (3, 2, 4, 3, 4), 'valid', 'channels_first'), ('conv3d', (1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'valid', 'channels_last'), ('conv3d', (1, 3, 5, 4, 2), (3, 3, 3, 2, 3), 'same', 'channels_last')])\ndef test_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last'), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first')])\ndef test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format,dilation_rate', [('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2), ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2), ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2), 'same', 'channels_last', (2, 2)), ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4), 'valid', 'channels_first', (2, 2)), ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4), 'valid', 'channels_last', (2, 2, 2)), ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4), 'same', 'channels_first', (2, 2, 2))])\ndef test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv transpose on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last', (2, 2)), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first', (2, 2))])\ndef test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('depthwise_conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('depthwise_conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('depthwise_conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('depthwise_conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last')])\ndef test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,pool_size,strides,padding,data_format,pool_mode', [('pool2d', (2, 3, 7, 7), (3, 3), (1, 1), 'same', 'channels_first', 'avg'), ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1), 'valid', 'channels_first', 'max'), ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1), 'valid', 'channels_last', 'avg'), ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1), 'same', 'channels_last', 'max'), ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1), 'same', 'channels_first', 'avg'), ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1), 'valid', 'channels_first', 'max'), ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1), 'valid', 'channels_last', 'avg'), ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1), 'same', 'channels_last', 'max')])\ndef test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n    check_single_tensor_operation(op, input_shape, WITH_NP, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, pool_mode=pool_mode, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [('separable_conv1d', (2, 8, 2), (3,), 1, 'same', 'channels_last'), ('separable_conv1d', (1, 8, 2), (3,), 2, 'valid', 'channels_last'), ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'), ('separable_conv2d', (2, 3, 5, 6), (4, 3), 2, 'valid', 'channels_first'), ('separable_conv2d', (1, 6, 5, 3), (3, 4), 1, 'valid', 'channels_last'), ('separable_conv2d', (1, 7, 6, 3), (3, 3), 2, 'same', 'channels_last')])\ndef test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format):\n    if data_format == 'channels_first':\n        input_depth = input_shape[1]\n    else:\n        input_depth = input_shape[-1]\n    _, x = parse_shape_or_val(input_shape)\n    _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n    _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) + (input_depth * depth_multiplier, 7))\n    y1 = KNP.separable_conv(x, depthwise, pointwise, padding=padding, data_format=data_format)\n    if K.backend() == 'cntk':\n        _, cntk_func = cntk_func_tensors(op, [input_shape, depthwise, pointwise], padding=padding, data_format=data_format)\n        y2 = cntk_func([x])[0]\n    else:\n        y2 = K.eval(getattr(K, op)(K.variable(x), K.variable(depthwise), K.variable(pointwise), padding=padding, data_format=data_format))\n    assert_allclose(y1, y2, atol=1e-05)",
                            "def test_random_normal(self):\n    for mean, std in [(0.0, 1.0), (-10.0, 5.0)]:\n        rand = K.eval(K.random_normal((300, 200), mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n        assert np.abs(np.mean(rand) - mean) < std * 0.015\n        assert np.abs(np.std(rand) - std) < std * 0.015\n        r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - mean) < std * 0.015\n        assert np.abs(np.std(samples) - std) < std * 0.015",
                            "def test_random_uniform(self):\n    min_val = -1.0\n    max_val = 1.0\n    rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand)) < 0.015\n    assert max_val - 0.015 < np.max(rand) <= max_val\n    assert min_val + 0.015 > np.min(rand) >= min_val\n    r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples)) < 0.015\n    assert max_val - 0.015 < np.max(samples) <= max_val\n    assert min_val + 0.015 > np.min(samples) >= min_val",
                            "def test_random_binomial(self):\n    p = 0.5\n    rand = K.eval(K.random_binomial((200, 100), p))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand) - p) < 0.015\n    assert np.max(rand) == 1\n    assert np.min(rand) == 0\n    r = K.random_binomial((10, 10), p)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples) - p) < 0.015\n    assert np.max(samples) == 1\n    assert np.min(samples) == 0",
                            "def test_truncated_normal(self):\n    mean = 0.0\n    std = 1.0\n    min_val = -2.0\n    max_val = 2.0\n    rand = K.eval(K.truncated_normal((300, 200), mean=mean, stddev=std, seed=1337))\n    assert rand.shape == (300, 200)\n    assert np.abs(np.mean(rand) - mean) < 0.015\n    assert np.max(rand) <= max_val\n    assert np.min(rand) >= min_val\n    assert np.abs(np.std(rand) - std * 0.87962) < 0.015",
                            "def test_conv_invalid_use(self):\n    dummy_x_1d = K.variable(np.ones((4, 8, 2)))\n    dummy_w_1d = K.variable(np.ones((3, 2, 3)))\n    dummy_x_2d = K.variable(np.ones((2, 3, 4, 5)))\n    dummy_w_2d = K.variable(np.ones((2, 2, 3, 4)))\n    dummy_x_3d = K.variable(np.ones((2, 3, 4, 5, 4)))\n    dummy_w_3d = K.variable(np.ones((2, 2, 2, 3, 4)))\n    dummy_w1x1_2d = K.variable(np.ones((1, 1, 12, 7)))\n    with pytest.raises(ValueError):\n        K.conv1d(dummy_x_1d, dummy_w_1d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv3d(dummy_x_3d, dummy_w_3d, data_format='channels_middle')\n    if K.backend() != 'theano':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    if K.backend() == 'cntk':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, strides=(2, 2), dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, strides=(2, 2), dilation_rate=(1, 2))",
                            "def test_pooling_invalid_use(self):\n    for input_shape, pool_size in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n        x = K.variable(np.random.random(input_shape))\n        if len(pool_size) == 2:\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, pool_mode='median')\n        else:\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, pool_mode='median')",
                            "def test_resize_images(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_images', x_shape, WITH_NP, cntk_dynamicity=True, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_images(K.variable(xval), 2, 2, data_format='channels_middle')",
                            "@staticmethod\ndef _helper_bilinear(data_format, height_factor, width_factor):\n    x_shape = (2, 3, 4, 5)\n    check_single_tensor_operation('resize_images', x_shape, [KTF, KTH], height_factor=height_factor, width_factor=width_factor, data_format=data_format, interpolation='bilinear')",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\ndef test_resize_images_bilinear(self, data_format):\n    self._helper_bilinear(data_format, 2, 2)\n    with pytest.raises(NotImplementedError):\n        self._helper_bilinear(data_format, 4, 4)",
                            "def test_resize_volumes(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_volumes', x_shape, WITH_NP, cntk_dynamicity=True, depth_factor=2, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_volumes(K.variable(xval), 2, 2, 2, data_format='channels_middle')",
                            "def test_temporal_padding(self):\n    check_single_tensor_operation('temporal_padding', (4, 3, 3), WITH_NP)\n    check_single_tensor_operation('temporal_padding', (2, 3, 4), WITH_NP, padding=(1, 2))",
                            "def test_spatial_2d_padding(self):\n    padding = ((1, 2), (2, 1))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, 1))\n        y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_2d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_spatial_3d_padding(self):\n    padding = ((1, 2), (2, 1), (1, 2))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, None, 1))\n        y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_3d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_bias_add(self):\n    for data_format in ['channels_first', 'channels_last']:\n        for shape in [(), (3,), (2, 3), (5, 3, 2)]:\n            if data_format == 'channels_first':\n                x_shape = (1, 4) + shape\n            else:\n                x_shape = (1,) + shape + (4,)\n            bias_shape = (4,)\n            check_two_tensor_operation('bias_add', x_shape, bias_shape, WITH_NP, cntk_dynamicity=True, data_format=data_format)\n        if data_format == 'channels_first':\n            x_shape = (20, 6, 10)\n        else:\n            x_shape = (20, 10, 6)\n        check_two_tensor_operation('bias_add', x_shape, (10, 6), WITH_NP, cntk_dynamicity=True, data_format=data_format)\n    x = K.variable(np.random.random(x_shape))\n    b = K.variable(np.random.random(bias_shape))\n    with pytest.raises(ValueError):\n        K.bias_add(x, b, data_format='channels_middle')",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='Specific to Theano.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_th(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes='per-activation')\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Specific to Tensorflow.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_tf(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'cntk', reason='Specific to CNTK.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_cntk(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.placeholder(x_shape)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.function([x], [z])([x_val])[0]\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_ctc(self):\n    if K.backend() == 'theano':\n        ref = [1.73308, 3.81351]\n    else:\n        ref = [3.34211, 5.42262]\n    label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n    input_lens = np.expand_dims(np.asarray([5, 5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]], [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508], [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549], [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456], [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345], [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)\n    if K.backend() == 'theano':\n        ref = [1.73308]\n    else:\n        ref = [3.34211]\n    input_lens = np.expand_dims(np.asarray([5]), 1)\n    label_lens = np.expand_dims(np.asarray([5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Test adapted from tensorflow.')\ndef test_ctc_decode_greedy(self):\n    \"\"\"Test two batch entries - best path decoder.\"\"\"\n    max_time_steps = 6\n    seq_len_0 = 4\n    input_prob_matrix_0 = np.asarray([[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.4, 0.6], [0.0, 0.0, 0.4, 0.6], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    seq_len_1 = 5\n    input_prob_matrix_1 = np.asarray([[0.1, 0.9, 0.0, 0.0], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.1, 0.9], [0.0, 0.9, 0.1, 0.1], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    inputs = [np.vstack([input_prob_matrix_0[t, :], input_prob_matrix_1[t, :]]) for t in range(max_time_steps)]\n    inputs = np.asarray(inputs).transpose((1, 0, 2))\n    input_length = np.array([seq_len_0, seq_len_1], dtype=np.int32)\n    decode_pred_np, log_prob_pred_np = KNP.ctc_decode(inputs, input_length, greedy=True)\n    inputs = K.variable(inputs)\n    input_length = K.variable(input_length)\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=True)\n    assert len(decode_pred_tf) == 1\n    decode_pred = K.eval(decode_pred_tf[0])\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    assert np.alltrue(decode_pred_np == decode_pred)\n    assert np.allclose(log_prob_pred_np, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='tensorflow-way slice is only supported in tensorflow.')\n@pytest.mark.parametrize('x_size', [[1, 1, 3], [1, 2, 3], [2, 1, 3]])\ndef test_slice(self, x_size):\n    npt = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])\n    x_start = [1, 0, 0]\n    tft = K.constant(npt)\n    test_input = K.eval(K.slice(tft, x_start, x_size))\n    expected = KNP.slice(npt, x_start, x_size)\n    assert np.allclose(test_input, expected)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search(self):\n    \"\"\"Test one batch, two beams - hibernating beam search.\"\"\"\n    depth = 6\n    seq_len_0 = 5\n    input_prob_matrix_0 = np.asarray([[0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908], [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517], [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763], [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655], [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878], [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]], dtype=np.float32)\n    input_prob_matrix_0 = input_prob_matrix_0 + 2.0\n    inputs = [input_prob_matrix_0[t, :][np.newaxis, :] for t in range(seq_len_0)] + 2 * [np.zeros((1, depth), dtype=np.float32)]\n    inputs = np.exp(inputs)\n    inputs = K.variable(inputs.transpose((1, 0, 2)))\n    input_length = K.variable(np.array([seq_len_0], dtype=np.int32))\n    log_prob_truth = np.array([-5.811451, -6.63339], np.float32)[np.newaxis, :]\n    decode_truth = [np.array([1, 0]), np.array([[1]])]\n    beam_width = 2\n    top_paths = 2\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=False, beam_width=beam_width, top_paths=top_paths)\n    assert len(decode_pred_tf) == top_paths\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    for i in range(top_paths):\n        assert np.alltrue(decode_truth[i] == K.eval(decode_pred_tf[i]))\n    assert np.allclose(log_prob_truth, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search_no_merge(self):\n    input_prob = np.array([[[0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]])\n    input_len = np.array(input_prob.shape[0] * [input_prob.shape[1]])\n\n    def decode(merge_repeated):\n        input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n        input_len_tensor = K.placeholder(shape=None, dtype='int64')\n        paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n        decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n        paths = decode_func([input_prob, input_len])\n        return paths\n    assert np.allclose(decode(merge_repeated=True), [np.array([[0, 1]])])\n    assert np.allclose(decode(merge_repeated=False), [np.array([[0, 0, 1, 1]])])",
                            "def test_one_hot(self):\n    input_length = 10\n    num_classes = 20\n    batch_size = 30\n    indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n    oh = KNP.one_hot(np.int32(indices), num_classes)\n    koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n    assert np.all(koh == oh)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_dot(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense = x_sparse.toarray()\n    W = np.random.random((5, 4))\n    t_W = K.variable(W)\n    k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n    k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n    assert k_s.shape == k_d.shape\n    assert_allclose(k_s, k_d, atol=1e-05)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_concat(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_1 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_2 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense_1 = x_sparse_1.toarray()\n    x_dense_2 = x_sparse_2.toarray()\n    k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n    assert K.is_sparse(k_s)\n    k_s_d = K.eval(k_s)\n    k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n    assert k_s_d.shape == k_d.shape\n    assert_allclose(k_s_d, k_d, atol=1e-05)",
                            "def test_stack(self):\n    tensor_list = [np.random.randn(5, 4, 6, 10) for _ in range(5)]\n    stack_axis = 3\n    results = []\n    if WITH_NP[0] == KC:\n        check_two_tensor_operation('stack', (5, 4, 6, 10), (5, 4, 6, 10), WITH_NP, axis=stack_axis, concat_args=True)\n    else:\n        for k in WITH_NP:\n            tensor_list_var = [k.variable(tensor) for tensor in tensor_list]\n            out = k.eval(k.stack(tensor_list_var, axis=stack_axis))\n            results.append(out)\n        assert_list_pairwise(results)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_map(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    vx = K.variable(x)\n    kx = K.eval(K.map_fn(K.sum, vx))\n    kx2 = K.eval(K.map_fn(lambda i: K.sum(vx[i]), K.arange(10), dtype=K.floatx()))\n    assert (10,) == kx.shape\n    assert (10,) == kx2.shape\n    assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n    assert_allclose(kx, kx2, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldl(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n    assert (3,) == kx.shape\n    assert_allclose(x.sum(axis=0), kx, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldr(self):\n    x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n    vx = K.variable(x)\n    p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n    p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n    assert p1 < p2\n    assert 9e-38 < p2 <= 1e-37",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk has issues with negative number.')\ndef test_arange(self):\n    for test_value in (-20, 0, 1, 10):\n        a_list = []\n        dtype_list = []\n        for k in WITH_NP:\n            t = k.arange(test_value)\n            a = k.eval(t)\n            assert np.array_equal(a, np.arange(test_value))\n            dtype_list.append(k.dtype(t))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for start, stop, step in ((0, 5, 1), (-5, 5, 2), (0, 1, 2)):\n        a_list = []\n        for k in WITH_NP:\n            a = k.eval(k.arange(start, stop, step))\n            assert np.array_equal(a, np.arange(start, stop, step))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for dtype in ('int32', 'int64', 'float32', 'float64'):\n        for k in WITH_NP:\n            t = k.arange(10, dtype=dtype)\n            assert k.dtype(t) == dtype\n    start = K.constant(1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 1\n    start = K.constant(-1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 0",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_train_phase(self, training):\n    check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_test_phase(self, training):\n    check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "def test_setfloatx_incorrect_values(self):\n    old_floatx = floatx()\n    initial = floatx()\n    for value in ['', 'beerfloat', 123]:\n        with pytest.raises(ValueError):\n            set_floatx(value)\n    assert floatx() == initial\n    set_floatx(old_floatx)",
                            "def DISABLED_test_setfloatx_correct_values(self):\n    \"\"\"Disabled because it is not thread safe at this time.\"\"\"\n    old_floatx = floatx()\n    for value in ['float16', 'float32', 'float64']:\n        set_floatx(value)\n        assert floatx() == value\n    set_floatx(old_floatx)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support float16')\ndef DISABLED_test_set_floatx(self):\n    \"\"\"Disabled because it is not thread safe at this time.\n\n    Make sure that changes to the global floatx are effectively\n    taken into account by the backend.\n    \"\"\"\n    old_floatx = floatx()\n    set_floatx('float16')\n    var = variable([10])\n    check_dtype(var, 'float16')\n    set_floatx('float64')\n    var = variable([10])\n    check_dtype(var, 'float64')\n    set_floatx(old_floatx)",
                            "def test_dtype(self):\n    assert K.dtype(K.variable(1, dtype='float64')) == 'float64'\n    assert K.dtype(K.variable(1, dtype='float32')) == 'float32'\n    assert K.dtype(K.variable(1, dtype='float16')) == 'float16'",
                            "def test_variable_support_bool_dtype(self):\n    if K.backend() == 'tensorflow':\n        assert K.dtype(K.variable(1, dtype='int16')) == 'int16'\n        assert K.dtype(K.variable(False, dtype='bool')) == 'bool'\n        with pytest.raises(TypeError):\n            K.variable('', dtype='unsupported')",
                            "def test_clip_supports_tensor_arguments(self):\n    x = K.variable([-10.0, -5.0, 0.0, 5.0, 10.0])\n    min_value = K.variable([-5.0, -4.0, 0.0, 3.0, 5.0])\n    max_value = K.variable([5.0, 4.0, 1.0, 4.0, 9.0])\n    assert np.allclose(K.eval(K.clip(x, min_value, max_value)), np.asarray([-5.0, -4.0, 0.0, 4.0, 9.0], dtype=np.float32))",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(), reason='This test is for tensorflow parallelism.')\ndef test_tensorflow_session_parallelism_settings(self, monkeypatch):\n    for threads in [1, 2]:\n        K.clear_session()\n        monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n        cfg = K.get_session()._config\n        assert cfg.intra_op_parallelism_threads == threads\n        assert cfg.inter_op_parallelism_threads == threads",
                            "def batch_shape(shape):\n    return (batch_size,) + shape[1:]",
                            "def random(shape):\n    return np.random.random(batch_shape(shape))",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn(inputs, states):\n        assert len(states) == 1\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y])\n    return simple_rnn",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_with_extra_mock_state(inputs, states):\n        assert len(states) == 2\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y, backend.concatenate([y, y], axis=-1)])\n    return simple_rnn_with_extra_mock_state",
                            "def get_step_function(backend, w_i):\n\n    def simple_no_states(inputs, states):\n        assert len(states) == 0\n        y = backend.dot(inputs, w_i)\n        return (y, [])\n    return simple_no_states",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_add_constant(inputs, states_and_constants):\n        [h, c] = states_and_constants\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n        return (y, [y])\n    return simple_rnn_add_constant",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def step_function(inputs, states):\n    outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n    return (outputs, states)",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def decode(merge_repeated):\n    input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n    input_len_tensor = K.placeholder(shape=None, dtype='int64')\n    paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n    decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n    paths = decode_func([input_prob, input_len])\n    return paths",
                            "def simple_rnn(inputs, states):\n    assert len(states) == 1\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y])",
                            "def simple_rnn_with_extra_mock_state(inputs, states):\n    assert len(states) == 2\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y, backend.concatenate([y, y], axis=-1)])",
                            "def simple_no_states(inputs, states):\n    assert len(states) == 0\n    y = backend.dot(inputs, w_i)\n    return (y, [])",
                            "def simple_rnn_add_constant(inputs, states_and_constants):\n    [h, c] = states_and_constants\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n    return (y, [y])"
                        ],
                        "constructor_variables": [],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "test_is_keras_tensor(self)",
                            "test_set_learning_phase(self)",
                            "test_eye(self)",
                            "test_ones(self)",
                            "test_zeros(self)",
                            "test_ones_like(self)",
                            "test_zeros_like(self)",
                            "test_linear_operations(self)",
                            "test_random_variables(self)",
                            "test_batch_dot_shape(self)",
                            "test_shape_operations(self)",
                            "test_none_shape_operations(self)",
                            "test_repeat_elements(self)",
                            "test_tile(self)",
                            "test_gather(self)",
                            "test_value_manipulation(self, function_name)",
                            "test_print_tensor(self)",
                            "test_elementwise_operations(self)",
                            "test_reset_uids(self)",
                            "test_cumsum_cumprod(self)",
                            "test_log(self)",
                            "test_update_add(self)",
                            "test_update_sub(self)",
                            "test_gradient(self)",
                            "test_stop_gradient(self)",
                            "test_function(self)",
                            "test_function_tf_fetches(self)",
                            "test_function_tf_feed_dict(self)",
                            "test_function_tf_run_options_with_run_metadata(self)",
                            "test_function_tf_string_input(self)",
                            "test_rnn(self)",
                            "test_rnn_additional_states(self)",
                            "test_rnn_no_states(self)",
                            "test_rnn_constants(self)",
                            "test_rnn_output_and_state_masking_independent(self)",
                            "test_rnn_output_num_dim_larger_than_2_masking(self)",
                            "test_rnn_state_num_dim_larger_than_2_masking(self)",
                            "test_logsumexp(self, x_np, axis, keepdims)",
                            "test_logsumexp_optim(self)",
                            "test_switch(self)",
                            "test_dropout(self)",
                            "test_relu(self, alpha, max_value, threshold)",
                            "test_nn_operations(self)",
                            "test_in_top_k(self)",
                            "test_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format)",
                            "test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate)",
                            "test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate)",
                            "test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode)",
                            "test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format)",
                            "test_random_normal(self)",
                            "test_random_uniform(self)",
                            "test_random_binomial(self)",
                            "test_truncated_normal(self)",
                            "test_conv_invalid_use(self)",
                            "test_pooling_invalid_use(self)",
                            "test_resize_images(self)",
                            "_helper_bilinear(data_format, height_factor, width_factor)",
                            "test_resize_images_bilinear(self, data_format)",
                            "test_resize_volumes(self)",
                            "test_temporal_padding(self)",
                            "test_spatial_2d_padding(self)",
                            "test_spatial_3d_padding(self)",
                            "test_bias_add(self)",
                            "test_batchnorm_th(self, x_shape)",
                            "test_batchnorm_tf(self, x_shape)",
                            "test_batchnorm_cntk(self, x_shape)",
                            "test_ctc(self)",
                            "test_ctc_decode_greedy(self)",
                            "test_slice(self, x_size)",
                            "test_ctc_decode_beam_search(self)",
                            "test_ctc_decode_beam_search_no_merge(self)",
                            "test_one_hot(self)",
                            "test_sparse_dot(self)",
                            "test_sparse_concat(self)",
                            "test_stack(self)",
                            "test_map(self)",
                            "test_foldl(self)",
                            "test_foldr(self)",
                            "test_arange(self)",
                            "test_in_train_phase(self, training)",
                            "test_in_test_phase(self, training)",
                            "test_setfloatx_incorrect_values(self)",
                            "DISABLED_test_setfloatx_correct_values(self)",
                            "DISABLED_test_set_floatx(self)",
                            "test_dtype(self)",
                            "test_variable_support_bool_dtype(self)",
                            "test_clip_supports_tensor_arguments(self)",
                            "test_tensorflow_session_parallelism_settings(self, monkeypatch)",
                            "batch_shape(shape)",
                            "random(shape)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i)",
                            "get_step_function(backend, w_i, w_h)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "decode(merge_repeated)",
                            "simple_rnn(inputs, states)",
                            "simple_rnn_with_extra_mock_state(inputs, states)",
                            "simple_no_states(inputs, states)",
                            "simple_rnn_add_constant(inputs, states_and_constants)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "test_tensorflow_session_parallelism_settings",
                    "function_code": "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                    reason='This test is for tensorflow parallelism.')\ndef test_tensorflow_session_parallelism_settings(self, monkeypatch):\n    for threads in [1, 2]:\n        K.clear_session()\n        monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n        cfg = K.get_session()._config\n        assert cfg.intra_op_parallelism_threads == threads\n        assert cfg.inter_op_parallelism_threads == threads\n",
                    "decorators": [
                        "pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(), reason='This test is for tensorflow parallelism.')"
                    ],
                    "docstring": null,
                    "start_line": 2125,
                    "end_line": 2133,
                    "variables": {
                        "threads": [
                            2128,
                            2130,
                            2132,
                            2133
                        ],
                        "K.clear_session": [
                            2129
                        ],
                        "K": [
                            2129,
                            2131,
                            2125
                        ],
                        "monkeypatch.setenv": [
                            2130
                        ],
                        "monkeypatch": [
                            2130
                        ],
                        "str": [
                            2130
                        ],
                        "cfg": [
                            2131,
                            2132,
                            2133
                        ],
                        "_config": [
                            2131
                        ],
                        "K.get_session": [
                            2131
                        ],
                        "cfg.intra_op_parallelism_threads": [
                            2132
                        ],
                        "cfg.inter_op_parallelism_threads": [
                            2133
                        ],
                        "pytest.mark.skipif": [
                            2125
                        ],
                        "pytest.mark": [
                            2125
                        ],
                        "pytest": [
                            2125
                        ],
                        "K.backend": [
                            2125
                        ],
                        "KTF._is_tf_1": [
                            2125
                        ],
                        "KTF": [
                            2125
                        ]
                    },
                    "filtered_variables": {
                        "threads": [
                            2128,
                            2130,
                            2132,
                            2133
                        ],
                        "K.clear_session": [
                            2129
                        ],
                        "K": [
                            2129,
                            2131,
                            2125
                        ],
                        "monkeypatch.setenv": [
                            2130
                        ],
                        "monkeypatch": [
                            2130
                        ],
                        "cfg": [
                            2131,
                            2132,
                            2133
                        ],
                        "_config": [
                            2131
                        ],
                        "K.get_session": [
                            2131
                        ],
                        "cfg.intra_op_parallelism_threads": [
                            2132
                        ],
                        "cfg.inter_op_parallelism_threads": [
                            2133
                        ],
                        "pytest.mark.skipif": [
                            2125
                        ],
                        "pytest.mark": [
                            2125
                        ],
                        "pytest": [
                            2125
                        ],
                        "K.backend": [
                            2125
                        ],
                        "KTF._is_tf_1": [
                            2125
                        ],
                        "KTF": [
                            2125
                        ]
                    },
                    "diff_line_number": 2127,
                    "class_data": {
                        "signature": "class TestBackend(object)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def test_is_keras_tensor(self):\n    np_var = np.array([1, 2])\n    with pytest.raises(ValueError):\n        K.is_keras_tensor(np_var)\n    keras_var = K.variable(np_var)\n    assert K.is_keras_tensor(keras_var) is False\n    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n    assert K.is_keras_tensor(keras_placeholder) is False",
                            "def test_set_learning_phase(self):\n    with pytest.raises(ValueError):\n        K.set_learning_phase(2)",
                            "def test_eye(self):\n    check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)",
                            "def test_ones(self):\n    check_single_tensor_operation('ones', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_zeros(self):\n    check_single_tensor_operation('zeros', (3, 5, 10, 8), WITH_NP, shape_or_val=False)",
                            "def test_ones_like(self):\n    check_single_tensor_operation('ones_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_zeros_like(self):\n    check_single_tensor_operation('zeros_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)",
                            "def test_linear_operations(self):\n    check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n    check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, cntk_two_dynamicity=True, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), WITH_NP, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), WITH_NP, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), WITH_NP, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20), WITH_NP, axes=(1, 1))\n    check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n    check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n    if K.backend() != 'cntk':\n        check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=(1, 2))",
                            "def test_random_variables(self):\n    check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP, low=0.0, high=1.0, shape_or_val=False, assert_value_equality=False)\n    check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP, mean=0.0, scale=1.0, shape_or_val=False, assert_value_equality=False)",
                            "def test_batch_dot_shape(self):\n    test_cases = []\n    test_cases.append([(None, 3, 4, 5), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 3, 4, 5), (None, 2, 4), 2])\n    test_cases.append([(None, 3, 4), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 4, 3), (None, 3, 5), (2, 1)])\n    test_cases.append([(None, 4), (None, 3, 4), (1, 2)])\n    test_cases.append([(None, 4), (None, 4), None])\n    batch_size = 7\n\n    def batch_shape(shape):\n        return (batch_size,) + shape[1:]\n\n    def random(shape):\n        return np.random.random(batch_shape(shape))\n    for x_shape, y_shape, axes in test_cases:\n        x_np = random(x_shape)\n        y_np = random(y_shape)\n        z_np = KNP.batch_dot(x_np, y_np, axes)\n        x = K.placeholder(shape=x_shape)\n        y = K.placeholder(shape=y_shape)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        f = K.function([x, y], [z])\n        assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        if K.backend() != 'cntk':\n            x = K.placeholder(ndim=len(x_shape))\n            y = K.placeholder(ndim=len(y_shape))\n            z = K.batch_dot(x, y, axes)\n            z_shape = K.int_shape(z)\n            if z_shape is not None:\n                assert len(z_shape) == z_np.ndim\n                assert set(z_shape) <= set((None, 1))\n            f = K.function([x, y], [z])\n            assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n        x = K.variable(x_np)\n        y = K.variable(y_np)\n        z = K.batch_dot(x, y, axes)\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n        z = K.eval(z)\n        assert_allclose(z, z_np, atol=1e-05)",
                            "def test_shape_operations(self):\n    check_two_tensor_operation('concatenate', (4, 3), (4, 2), WITH_NP, axis=-1, concat_args=True)\n    check_single_tensor_operation('reshape', (4, 2), WITH_NP, shape=(8, 1))\n    check_single_tensor_operation('permute_dimensions', (4, 2, 3), WITH_NP, pattern=(2, 0, 1))\n    check_single_tensor_operation('repeat', (4, 1), WITH_NP, n=3)\n    check_single_tensor_operation('flatten', (4, 1), WITH_NP)\n    check_single_tensor_operation('batch_flatten', (20, 2, 5), WITH_NP, cntk_dynamicity=True)\n    check_single_tensor_operation('expand_dims', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('expand_dims', (4, 3, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('squeeze', (4, 3, 1), WITH_NP, axis=2)\n    check_single_tensor_operation('squeeze', (4, 1, 1), WITH_NP, axis=1)\n    check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)}, 'squeeze', {'axis': 2}, (4, 3, 1, 1), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='We only test the shape inference of the theano backend.')\ndef test_none_shape_operations(self):\n    x = K.placeholder((3, None, 4))\n    y = K.batch_flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (3, None)\n    y = K.flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (None,)",
                            "def test_repeat_elements(self):\n    reps = 3\n    for ndims in [1, 2, 3]:\n        shape = np.arange(2, 2 + ndims)\n        arr = np.arange(np.prod(shape)).reshape(shape)\n        for rep_axis in range(ndims):\n            check_single_tensor_operation('repeat_elements', arr, WITH_NP, rep=reps, axis=rep_axis)\n            if K.backend() != 'cntk':\n                shape = list(shape)\n                shape[rep_axis] = None\n                x = K.placeholder(shape=shape)\n                y = K.repeat_elements(x, reps, axis=rep_axis)\n                assert y._keras_shape == tuple(shape)\n                assert y._keras_shape == K.int_shape(y)",
                            "def test_tile(self):\n    shape = (3, 4)\n    arr = np.arange(np.prod(shape)).reshape(shape)\n    check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n    check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 4))\n        n = 2\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 8)\n        n = (4, 3)\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 12)",
                            "def test_gather(self):\n    shape = (10, 2, 3)\n    ref = np.arange(np.prod(shape)).reshape(shape)\n    inds = [1, 3, 7, 9]\n    t_list = [k.gather(k.variable(ref), k.variable(inds, dtype='int32')) for k in WITH_NP]\n    z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32'))) for k in WITH_NP]\n    assert_list_pairwise(z_list)\n    assert_list_keras_shape(t_list, z_list)\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 3, 4))\n        indices = K.placeholder(shape=(5, 6), dtype='int32')\n        y = K.gather(x, indices)\n        assert y._keras_shape == (5, 6, 3, 4)",
                            "@pytest.mark.parametrize('function_name', ['get_value', 'count_params', 'int_shape', 'get_variable_shape'])\ndef test_value_manipulation(self, function_name):\n    val = np.random.random((4, 2))\n    v_list = [getattr(k, function_name)(k.variable(val)) for k in WITH_NP]\n    if function_name == 'get_value':\n        assert_list_pairwise(v_list)\n    else:\n        assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)",
                            "def test_print_tensor(self):\n    check_single_tensor_operation('print_tensor', (), WITH_NP)\n    check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n    check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n    check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)",
                            "def test_elementwise_operations(self):\n    check_single_tensor_operation('max', (4, 2), WITH_NP)\n    check_single_tensor_operation('max', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('max', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('min', (4, 2), WITH_NP)\n    check_single_tensor_operation('min', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('min', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('mean', (4, 2), WITH_NP)\n    check_single_tensor_operation('mean', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=-1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('var', (4, 2), WITH_NP)\n    check_single_tensor_operation('var', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('var', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('std', (4, 2), WITH_NP)\n    check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('prod', (4, 2), WITH_NP)\n    check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('any', (4, 2), WITH_NP)\n    check_single_tensor_operation('any', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('any', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('all', (4, 2), WITH_NP)\n    check_single_tensor_operation('all', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('all', (4, 2, 3), WITH_NP, axis=[1, -1])\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('square', (4, 2), WITH_NP)\n    check_single_tensor_operation('abs', (4, 2), WITH_NP)\n    check_single_tensor_operation('sqrt', (4, 2), WITH_NP)\n    check_single_tensor_operation('exp', (4, 2), WITH_NP)\n    check_single_tensor_operation('round', (4, 2), WITH_NP)\n    check_single_tensor_operation('sign', (4, 2), WITH_NP)\n    check_single_tensor_operation('pow', (4, 2), WITH_NP, a=3)\n    check_single_tensor_operation('clip', (4, 2), WITH_NP, min_value=0.4, max_value=0.6)\n    check_single_tensor_operation('cos', (4, 2), WITH_NP)\n    check_single_tensor_operation('sin', (4, 2), WITH_NP)\n    check_two_tensor_operation('equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('not_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('maximum', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('minimum', (4, 2), (4, 2), WITH_NP)",
                            "def test_reset_uids(self):\n    first = K.get_uid()\n    K.get_uid()\n    K.reset_uids()\n    assert K.get_uid() == first",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support cumsum and cumprod yet')\ndef test_cumsum_cumprod(self):\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP, axis=1)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk return -85.1 for zero or negative number, not nan, so can't compare with other backend.\")\ndef test_log(self):\n    check_single_tensor_operation('log', (4, 2), WITH_NP)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_add(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    increment = np.random.randn(3, 4)\n    x += increment\n    K.eval(K.update_add(x_var, increment))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'theano', reason='theano returns tuples for update ops')\ndef test_update_sub(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    decrement = np.random.randn(3, 4)\n    x -= decrement\n    K.eval(K.update_sub(x_var, decrement))\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk doesn't support gradient in this way.\")\ndef test_gradient(self):\n    val = np.random.random((4, 2))\n    x_list = [k.variable(val) for k in [KTH, KTF]]\n    z_list = []\n    zero_list = []\n    for x, k in zip(x_list, [KTH, KTF]):\n        exp = x * k.exp(x)\n        loss = k.sum(exp)\n        zero_loss = k.stop_gradient(loss)\n        grad = k.gradients(loss, [exp])\n        zero_grad = k.gradients(loss + zero_loss, [exp])\n        z_list.append(k.eval(grad[0]))\n        zero_list.append(k.eval(zero_grad[0]))\n    assert_list_pairwise(z_list)\n    assert_list_pairwise(zero_list)\n    for i in range(len(z_list)):\n        assert_allclose(zero_list[i], z_list[i], atol=1e-05)",
                            "def test_stop_gradient(self):\n    val = np.random.random((4, 2))\n    a = K.variable(val)\n    b = K.square(a)\n    c, d = K.stop_gradient([a, b])\n    e = K.stop_gradient(b)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason=\"cntk currently not support function in this way, so can't test as this.\")\ndef test_function(self):\n    test_backend = [KTH, KTF]\n    val = np.random.random((4, 2))\n    input_val = np.random.random((4, 2))\n    f_list = []\n    x_list = []\n    for k in test_backend:\n        x = k.variable(val)\n        x_list.append(x)\n        y = k.placeholder(ndim=2)\n        exp = k.square(x) + y\n        update = x * 2\n        f = k.function([y], [exp], updates=[(x, update)])\n        f_list.append(f)\n    function_outputs_list = [f([input_val])[0] for f in f_list]\n    assert_list_pairwise(function_outputs_list)\n    new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n    assert_list_pairwise(new_val_list)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `fetches` argument.')\ndef test_function_tf_fetches(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], updates=[(x, x_placeholder + 1.0)], fetches=[K.update(y, 5.0)])\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert K.get_session().run(fetches=[x, y]) == [11.0, 5.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(), reason='Uses the `feed_dict` argument.')\ndef test_function_tf_feed_dict(self):\n    x = K.variable(0.0)\n    y = K.variable(0.0)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    feed_dict = {y_placeholder: 3.0}\n    f = K.function(inputs=[x_placeholder], outputs=[x_placeholder + 1.0], updates=[(x, x_placeholder + 10.0)], feed_dict=feed_dict, fetches=[K.update(y, y_placeholder * 10.0)])\n    output = f([10.0])\n    assert output == [11.0]\n    assert K.get_session().run(fetches=[x, y]) == [20.0, 30.0]\n    feed_dict[y_placeholder] = 4.0\n    output = f([20.0])\n    assert output == [21.0]\n    assert K.get_session().run(fetches=[x, y]) == [30.0, 40.0]",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `options` and `run_metadata` arguments.')\ndef test_function_tf_run_options_with_run_metadata(self):\n    from tensorflow.core.protobuf import config_pb2\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n    run_options = config_pb2.RunOptions(output_partition_graphs=True)\n    run_metadata = config_pb2.RunMetadata()\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], options=run_options, run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) > 0\n    f = K.function(inputs=[x_placeholder, y_placeholder], outputs=[x_placeholder + y_placeholder], run_metadata=run_metadata)\n    output = f([10.0, 20.0])\n    assert output == [30.0]\n    assert len(run_metadata.partition_graphs) == 0",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses the `string` type for a tensor.')\ndef test_function_tf_string_input(self):\n    x_placeholder = K.placeholder(shape=(), dtype='string')\n    x_identity = K.identity(x_placeholder)\n    f = K.function(inputs=[x_placeholder], outputs=[x_identity])\n    output = f([b'test'])\n    assert output == [b'test']",
                            "def test_rnn(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn(inputs, states):\n            assert len(states) == 1\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y])\n        return simple_rnn\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_additional_states(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    h1 = np.concatenate([h0, h0], axis=-1)\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_with_extra_mock_state(inputs, states):\n            assert len(states) == 2\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return (y, [y, backend.concatenate([y, y], axis=-1)])\n        return simple_rnn_with_extra_mock_state\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0, h1], mask_np=kwargs.pop('mask', None), **kwargs)",
                            "def test_rnn_no_states(self):\n    num_samples = 3\n    input_dim = 8\n    output_dim = 4\n    timesteps = 5\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n\n    def get_step_function(backend, w_i):\n\n        def simple_no_states(inputs, states):\n            assert len(states) == 0\n            y = backend.dot(inputs, w_i)\n            return (y, [])\n        return simple_no_states\n    kwargs_list = [{'go_backwards': False}, {'go_backwards': True}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k), step_function_np=get_step_function(KNP, wi), inputs_np=x, initial_states_np=[], mask_np=None, **kwargs)",
                            "def test_rnn_constants(self):\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, c = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_add_constant(inputs, states_and_constants):\n            [h, c] = states_and_constants\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n            return (y, [y])\n        return simple_rnn_add_constant\n    kwargs_list = [{'go_backwards': False, 'mask': None}, {'go_backwards': True, 'mask': None}, {'go_backwards': False, 'mask': mask}, {'go_backwards': True, 'mask': mask}]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k), step_function_np=get_step_function(KNP, wi, wh), inputs_np=x, initial_states_np=[h0], mask_np=kwargs.pop('mask', None), constants_np=[c], **kwargs)",
                            "def test_rnn_output_and_state_masking_independent(self):\n    num_samples = 2\n    num_timesteps = 4\n    state_and_io_size = 5\n    mask_last_num_timesteps = 2\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, state_and_io_size))\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[1, -mask_last_num_timesteps:] = 0\n    expected_outputs = inputs_vals.copy()\n    expected_outputs[1, -mask_last_num_timesteps:] = expected_outputs[1, -(mask_last_num_timesteps + 1)]\n    expected_state = initial_state_vals.copy()\n    expected_state[0] += num_timesteps\n    expected_state[1] += num_timesteps - mask_last_num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)\n        assert_allclose(K.eval(last_states[0]), expected_state)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_output_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n    num_features = 5\n\n    def step_function(inputs, states):\n        outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n        return (outputs, states)\n    inputs_vals = np.random.random((num_samples, num_timesteps, num_features))\n    initial_state_vals = np.random.random((num_samples, 6))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[-1, -1] = 0\n    expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n    expected_outputs[-1, -1] = expected_outputs[-1, -2]\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(outputs), expected_outputs)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_state_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n\n    def step_function(inputs, states):\n        return (inputs, [s + 1 for s in states])\n    inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n    initial_state_vals = np.random.random((num_samples, 6, 7))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[0, -2:] = 0\n    expected_last_state = initial_state_vals.copy()\n    expected_last_state[0] += num_timesteps - 2\n    expected_last_state[1:] += num_timesteps\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(step_function, inputs, initial_states, mask=mask, unroll=unroll, input_length=num_timesteps if unroll else None)\n        assert_allclose(K.eval(last_states[0]), expected_last_state)",
                            "@pytest.mark.parametrize('x_np,axis,keepdims', [(np.array([1.1, 0.8, 0.9]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 0, False), (np.array([[1.1, 0.8, 0.9]]), 1, False), (np.array([[1.1, 0.8, 0.9]]), -1, False), (np.array([[1.1, 0.8, 0.9]]), 1, True), (np.array([[1.1], [1.2]]), 0, False), (np.array([[1.1], [1.2]]), 1, False), (np.array([[1.1], [1.2]]), -1, False), (np.array([[1.1], [1.2]]), -1, True), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), None, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 0, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False), (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False)])\ndef test_logsumexp(self, x_np, axis, keepdims):\n    \"\"\"\n    Check if K.logsumexp works properly for values close to one.\n    \"\"\"\n    x = K.variable(x_np)\n    assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)), np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)), rtol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='The optimization is applied only with TensorFlow.')\ndef test_logsumexp_optim(self):\n    \"\"\"\n    Check if optimization works.\n    \"\"\"\n    x_np = np.array([10000.0, 0.0001])\n    result = K.eval(K.logsumexp(K.variable(x_np), axis=0))\n    assert_allclose(result, 10000.0, rtol=1e-05)",
                            "def test_switch(self):\n    val = np.random.random()\n    z_list = []\n    for k in WITH_NP:\n        x = k.variable(val)\n        x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n        z_list.append(k.eval(x))\n    assert_list_pairwise(z_list)\n    shapes = []\n    shapes.append([(4, 3, 2), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4, 3), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4,), (4, 3, 2), (4, 3, 2)])\n    for s in shapes:\n        z_list = []\n        arrays = list(map(np.random.random, s))\n        for k in WITH_NP:\n            x, then_expr, else_expr = map(k.variable, arrays)\n            cond = k.greater_equal(x, 0.5)\n            z_list.append(k.eval(k.switch(cond, then_expr, else_expr)))\n        assert_list_pairwise(z_list)",
                            "def test_dropout(self):\n    val = np.random.random((100, 100))\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2)) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2, noise_shape=list(val.shape))) for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n    with pytest.raises(ValueError):\n        z = K.dropout(K.variable(val), level=-0.5)",
                            "@pytest.mark.parametrize('alpha,max_value,threshold', [(0.0, None, 0.0), (0.1, None, 0.0), (0.0, 5.0, 0.0), (0.0, None, 0.8), (0.1, 5.0, 0.0), (0.1, None, 0.8), (0.0, 5.0, 0.8), (0.1, 5.0, 0.8), (0.1, 0.0, 0.8), (0.1, 5.0, -2.8), (0.1, 9.0, 0.8)])\ndef test_relu(self, alpha, max_value, threshold):\n    check_single_tensor_operation('relu', (4, 2), WITH_NP, alpha=alpha, max_value=max_value, threshold=threshold)",
                            "def test_nn_operations(self):\n    check_single_tensor_operation('softsign', (4, 10), WITH_NP)\n    check_single_tensor_operation('softplus', (4, 10), WITH_NP)\n    check_single_tensor_operation('elu', (4, 10), WITH_NP, alpha=0.5)\n    check_single_tensor_operation('sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('hard_sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('tanh', (4, 2), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 10), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 5, 3), WITH_NP, axis=1)\n    check_single_tensor_operation('softmax', (4, 5, 3, 10), WITH_NP, axis=2)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    if K.backend() != 'cntk':\n        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n    xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841], [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n    yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016], [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n    check_two_tensor_operation('categorical_crossentropy', yval, xval, WITH_NP, cntk_two_dynamicity=True, from_logits=True)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)",
                            "def test_in_top_k(self):\n    batch_size = 20\n    num_classes = 10\n    predictions = np.random.random((batch_size, num_classes)).astype('float32')\n    targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n    for k in range(num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)\n    num_identical = num_classes // 2\n    for i in range(batch_size):\n        idx_identical = np.random.choice(num_classes, size=num_identical, replace=False)\n        predictions[i, idx_identical] = predictions[i, 0]\n    targets = np.zeros(batch_size, dtype='int32')\n    for k in range(1, num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'), b.variable(targets, dtype='int32'), k)) for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('conv1d', (2, 8, 2), (3, 2, 3), 'same', 'channels_last'), ('conv1d', (1, 8, 2), (3, 2, 3), 'valid', 'channels_last'), ('conv1d', (1, 2, 8), (3, 2, 3), 'valid', 'channels_first'), ('conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'), ('conv3d', (2, 3, 4, 5, 4), (3, 3, 3, 3, 4), 'same', 'channels_first'), ('conv3d', (2, 3, 5, 4, 6), (3, 2, 4, 3, 4), 'valid', 'channels_first'), ('conv3d', (1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'valid', 'channels_last'), ('conv3d', (1, 3, 5, 4, 2), (3, 3, 3, 2, 3), 'same', 'channels_last')])\ndef test_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last'), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first')])\ndef test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format,dilation_rate', [('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2), ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2), ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2), 'same', 'channels_last', (2, 2)), ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4), 'valid', 'channels_first', (2, 2)), ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4), 'valid', 'channels_last', (2, 2, 2)), ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4), 'same', 'channels_first', (2, 2, 2))])\ndef test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.skipif(K.backend() == 'cntk' and KC.dev.type() == 0, reason='cntk only supports dilated conv transpose on GPU')\n@pytest.mark.parametrize('op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate', [('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2), 'same', 'channels_last', (2, 2)), ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9), 'same', 'channels_first', (2, 2))])\ndef test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape, padding=padding, data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [('depthwise_conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'), ('depthwise_conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'), ('depthwise_conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'), ('depthwise_conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last')])\ndef test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(op, input_shape, kernel_shape, WITH_NP, padding=padding, data_format=data_format, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,pool_size,strides,padding,data_format,pool_mode', [('pool2d', (2, 3, 7, 7), (3, 3), (1, 1), 'same', 'channels_first', 'avg'), ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1), 'valid', 'channels_first', 'max'), ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1), 'valid', 'channels_last', 'avg'), ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1), 'same', 'channels_last', 'max'), ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1), 'same', 'channels_first', 'avg'), ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1), 'valid', 'channels_first', 'max'), ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1), 'valid', 'channels_last', 'avg'), ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1), 'same', 'channels_last', 'max')])\ndef test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n    check_single_tensor_operation(op, input_shape, WITH_NP, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, pool_mode=pool_mode, cntk_dynamicity=True)",
                            "@pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [('separable_conv1d', (2, 8, 2), (3,), 1, 'same', 'channels_last'), ('separable_conv1d', (1, 8, 2), (3,), 2, 'valid', 'channels_last'), ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'), ('separable_conv2d', (2, 3, 5, 6), (4, 3), 2, 'valid', 'channels_first'), ('separable_conv2d', (1, 6, 5, 3), (3, 4), 1, 'valid', 'channels_last'), ('separable_conv2d', (1, 7, 6, 3), (3, 3), 2, 'same', 'channels_last')])\ndef test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format):\n    if data_format == 'channels_first':\n        input_depth = input_shape[1]\n    else:\n        input_depth = input_shape[-1]\n    _, x = parse_shape_or_val(input_shape)\n    _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n    _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) + (input_depth * depth_multiplier, 7))\n    y1 = KNP.separable_conv(x, depthwise, pointwise, padding=padding, data_format=data_format)\n    if K.backend() == 'cntk':\n        _, cntk_func = cntk_func_tensors(op, [input_shape, depthwise, pointwise], padding=padding, data_format=data_format)\n        y2 = cntk_func([x])[0]\n    else:\n        y2 = K.eval(getattr(K, op)(K.variable(x), K.variable(depthwise), K.variable(pointwise), padding=padding, data_format=data_format))\n    assert_allclose(y1, y2, atol=1e-05)",
                            "def test_random_normal(self):\n    for mean, std in [(0.0, 1.0), (-10.0, 5.0)]:\n        rand = K.eval(K.random_normal((300, 200), mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n        assert np.abs(np.mean(rand) - mean) < std * 0.015\n        assert np.abs(np.std(rand) - std) < std * 0.015\n        r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - mean) < std * 0.015\n        assert np.abs(np.std(samples) - std) < std * 0.015",
                            "def test_random_uniform(self):\n    min_val = -1.0\n    max_val = 1.0\n    rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand)) < 0.015\n    assert max_val - 0.015 < np.max(rand) <= max_val\n    assert min_val + 0.015 > np.min(rand) >= min_val\n    r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples)) < 0.015\n    assert max_val - 0.015 < np.max(samples) <= max_val\n    assert min_val + 0.015 > np.min(samples) >= min_val",
                            "def test_random_binomial(self):\n    p = 0.5\n    rand = K.eval(K.random_binomial((200, 100), p))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand) - p) < 0.015\n    assert np.max(rand) == 1\n    assert np.min(rand) == 0\n    r = K.random_binomial((10, 10), p)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples) - p) < 0.015\n    assert np.max(samples) == 1\n    assert np.min(samples) == 0",
                            "def test_truncated_normal(self):\n    mean = 0.0\n    std = 1.0\n    min_val = -2.0\n    max_val = 2.0\n    rand = K.eval(K.truncated_normal((300, 200), mean=mean, stddev=std, seed=1337))\n    assert rand.shape == (300, 200)\n    assert np.abs(np.mean(rand) - mean) < 0.015\n    assert np.max(rand) <= max_val\n    assert np.min(rand) >= min_val\n    assert np.abs(np.std(rand) - std * 0.87962) < 0.015",
                            "def test_conv_invalid_use(self):\n    dummy_x_1d = K.variable(np.ones((4, 8, 2)))\n    dummy_w_1d = K.variable(np.ones((3, 2, 3)))\n    dummy_x_2d = K.variable(np.ones((2, 3, 4, 5)))\n    dummy_w_2d = K.variable(np.ones((2, 2, 3, 4)))\n    dummy_x_3d = K.variable(np.ones((2, 3, 4, 5, 4)))\n    dummy_w_3d = K.variable(np.ones((2, 2, 2, 3, 4)))\n    dummy_w1x1_2d = K.variable(np.ones((1, 1, 12, 7)))\n    with pytest.raises(ValueError):\n        K.conv1d(dummy_x_1d, dummy_w_1d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.conv3d(dummy_x_3d, dummy_w_3d, data_format='channels_middle')\n    if K.backend() != 'theano':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, data_format='channels_middle')\n    with pytest.raises(ValueError):\n        K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n    if K.backend() == 'cntk':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d, strides=(2, 2), dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d, strides=(2, 2), dilation_rate=(1, 2))",
                            "def test_pooling_invalid_use(self):\n    for input_shape, pool_size in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n        x = K.variable(np.random.random(input_shape))\n        if len(pool_size) == 2:\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, pool_mode='median')\n        else:\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, pool_mode='median')",
                            "def test_resize_images(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_images', x_shape, WITH_NP, cntk_dynamicity=True, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_images(K.variable(xval), 2, 2, data_format='channels_middle')",
                            "@staticmethod\ndef _helper_bilinear(data_format, height_factor, width_factor):\n    x_shape = (2, 3, 4, 5)\n    check_single_tensor_operation('resize_images', x_shape, [KTF, KTH], height_factor=height_factor, width_factor=width_factor, data_format=data_format, interpolation='bilinear')",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\ndef test_resize_images_bilinear(self, data_format):\n    self._helper_bilinear(data_format, 2, 2)\n    with pytest.raises(NotImplementedError):\n        self._helper_bilinear(data_format, 4, 4)",
                            "def test_resize_volumes(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_volumes', x_shape, WITH_NP, cntk_dynamicity=True, depth_factor=2, height_factor=2, width_factor=2, data_format=data_format)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_volumes(K.variable(xval), 2, 2, 2, data_format='channels_middle')",
                            "def test_temporal_padding(self):\n    check_single_tensor_operation('temporal_padding', (4, 3, 3), WITH_NP)\n    check_single_tensor_operation('temporal_padding', (2, 3, 4), WITH_NP, padding=(1, 2))",
                            "def test_spatial_2d_padding(self):\n    padding = ((1, 2), (2, 1))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, 1))\n        y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_2d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_spatial_3d_padding(self):\n    padding = ((1, 2), (2, 1), (1, 2))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP, padding=padding, data_format=data_format)\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, None, 1))\n        y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, None, 1)\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_3d_padding(K.variable(xval), padding=padding, data_format='channels_middle')",
                            "def test_bias_add(self):\n    for data_format in ['channels_first', 'channels_last']:\n        for shape in [(), (3,), (2, 3), (5, 3, 2)]:\n            if data_format == 'channels_first':\n                x_shape = (1, 4) + shape\n            else:\n                x_shape = (1,) + shape + (4,)\n            bias_shape = (4,)\n            check_two_tensor_operation('bias_add', x_shape, bias_shape, WITH_NP, cntk_dynamicity=True, data_format=data_format)\n        if data_format == 'channels_first':\n            x_shape = (20, 6, 10)\n        else:\n            x_shape = (20, 10, 6)\n        check_two_tensor_operation('bias_add', x_shape, (10, 6), WITH_NP, cntk_dynamicity=True, data_format=data_format)\n    x = K.variable(np.random.random(x_shape))\n    b = K.variable(np.random.random(bias_shape))\n    with pytest.raises(ValueError):\n        K.bias_add(x, b, data_format='channels_middle')",
                            "@pytest.mark.skipif(K.backend() != 'theano', reason='Specific to Theano.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_th(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes='per-activation')\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Specific to Tensorflow.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_tf(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.eval(z)\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() != 'cntk', reason='Specific to CNTK.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_cntk(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.placeholder(x_shape)\n    z, _, _ = K.normalize_batch_in_training(x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.function([x], [z])([x_val])[0]\n    assert z.shape == x_shape",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_ctc(self):\n    if K.backend() == 'theano':\n        ref = [1.73308, 3.81351]\n    else:\n        ref = [3.34211, 5.42262]\n    label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n    input_lens = np.expand_dims(np.asarray([5, 5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]], [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508], [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549], [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456], [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345], [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)\n    if K.backend() == 'theano':\n        ref = [1.73308]\n    else:\n        ref = [3.34211]\n    input_lens = np.expand_dims(np.asarray([5]), 1)\n    label_lens = np.expand_dims(np.asarray([5]), 1)\n    labels = np.asarray([[0, 1, 2, 1, 0]])\n    inputs = np.asarray([[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688], [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533], [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]], dtype=np.float32)\n    k_labels = K.variable(labels, dtype='int32')\n    k_inputs = K.variable(inputs, dtype='float32')\n    k_input_lens = K.variable(input_lens, dtype='int32')\n    k_label_lens = K.variable(label_lens, dtype='int32')\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Test adapted from tensorflow.')\ndef test_ctc_decode_greedy(self):\n    \"\"\"Test two batch entries - best path decoder.\"\"\"\n    max_time_steps = 6\n    seq_len_0 = 4\n    input_prob_matrix_0 = np.asarray([[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.4, 0.6], [0.0, 0.0, 0.4, 0.6], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    seq_len_1 = 5\n    input_prob_matrix_1 = np.asarray([[0.1, 0.9, 0.0, 0.0], [0.0, 0.9, 0.1, 0.0], [0.0, 0.0, 0.1, 0.9], [0.0, 0.9, 0.1, 0.1], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], dtype=np.float32)\n    inputs = [np.vstack([input_prob_matrix_0[t, :], input_prob_matrix_1[t, :]]) for t in range(max_time_steps)]\n    inputs = np.asarray(inputs).transpose((1, 0, 2))\n    input_length = np.array([seq_len_0, seq_len_1], dtype=np.int32)\n    decode_pred_np, log_prob_pred_np = KNP.ctc_decode(inputs, input_length, greedy=True)\n    inputs = K.variable(inputs)\n    input_length = K.variable(input_length)\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=True)\n    assert len(decode_pred_tf) == 1\n    decode_pred = K.eval(decode_pred_tf[0])\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    assert np.alltrue(decode_pred_np == decode_pred)\n    assert np.allclose(log_prob_pred_np, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='tensorflow-way slice is only supported in tensorflow.')\n@pytest.mark.parametrize('x_size', [[1, 1, 3], [1, 2, 3], [2, 1, 3]])\ndef test_slice(self, x_size):\n    npt = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])\n    x_start = [1, 0, 0]\n    tft = K.constant(npt)\n    test_input = K.eval(K.slice(tft, x_start, x_size))\n    expected = KNP.slice(npt, x_start, x_size)\n    assert np.allclose(test_input, expected)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search(self):\n    \"\"\"Test one batch, two beams - hibernating beam search.\"\"\"\n    depth = 6\n    seq_len_0 = 5\n    input_prob_matrix_0 = np.asarray([[0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908], [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517], [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763], [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655], [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878], [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]], dtype=np.float32)\n    input_prob_matrix_0 = input_prob_matrix_0 + 2.0\n    inputs = [input_prob_matrix_0[t, :][np.newaxis, :] for t in range(seq_len_0)] + 2 * [np.zeros((1, depth), dtype=np.float32)]\n    inputs = np.exp(inputs)\n    inputs = K.variable(inputs.transpose((1, 0, 2)))\n    input_length = K.variable(np.array([seq_len_0], dtype=np.int32))\n    log_prob_truth = np.array([-5.811451, -6.63339], np.float32)[np.newaxis, :]\n    decode_truth = [np.array([1, 0]), np.array([[1]])]\n    beam_width = 2\n    top_paths = 2\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs, input_length, greedy=False, beam_width=beam_width, top_paths=top_paths)\n    assert len(decode_pred_tf) == top_paths\n    log_prob_pred = K.eval(log_prob_pred_tf)\n    for i in range(top_paths):\n        assert np.alltrue(decode_truth[i] == K.eval(decode_pred_tf[i]))\n    assert np.allclose(log_prob_truth, log_prob_pred)",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Beam search is only implemented with the TensorFlow backend.')\ndef test_ctc_decode_beam_search_no_merge(self):\n    input_prob = np.array([[[0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]])\n    input_len = np.array(input_prob.shape[0] * [input_prob.shape[1]])\n\n    def decode(merge_repeated):\n        input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n        input_len_tensor = K.placeholder(shape=None, dtype='int64')\n        paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n        decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n        paths = decode_func([input_prob, input_len])\n        return paths\n    assert np.allclose(decode(merge_repeated=True), [np.array([[0, 1]])])\n    assert np.allclose(decode(merge_repeated=False), [np.array([[0, 0, 1, 1]])])",
                            "def test_one_hot(self):\n    input_length = 10\n    num_classes = 20\n    batch_size = 30\n    indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n    oh = KNP.one_hot(np.int32(indices), num_classes)\n    koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n    assert np.all(koh == oh)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_dot(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense = x_sparse.toarray()\n    W = np.random.random((5, 4))\n    t_W = K.variable(W)\n    k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n    k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n    assert k_s.shape == k_d.shape\n    assert_allclose(k_s, k_d, atol=1e-05)",
                            "@pytest.mark.skipif(not supports_sparse, reason='Sparse tensors are not supported in cntk and Theano has some dependency issues for sparse.')\ndef test_sparse_concat(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_1 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n    x_sparse_2 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense_1 = x_sparse_1.toarray()\n    x_dense_2 = x_sparse_2.toarray()\n    k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n    assert K.is_sparse(k_s)\n    k_s_d = K.eval(k_s)\n    k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n    assert k_s_d.shape == k_d.shape\n    assert_allclose(k_s_d, k_d, atol=1e-05)",
                            "def test_stack(self):\n    tensor_list = [np.random.randn(5, 4, 6, 10) for _ in range(5)]\n    stack_axis = 3\n    results = []\n    if WITH_NP[0] == KC:\n        check_two_tensor_operation('stack', (5, 4, 6, 10), (5, 4, 6, 10), WITH_NP, axis=stack_axis, concat_args=True)\n    else:\n        for k in WITH_NP:\n            tensor_list_var = [k.variable(tensor) for tensor in tensor_list]\n            out = k.eval(k.stack(tensor_list_var, axis=stack_axis))\n            results.append(out)\n        assert_list_pairwise(results)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_map(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    vx = K.variable(x)\n    kx = K.eval(K.map_fn(K.sum, vx))\n    kx2 = K.eval(K.map_fn(lambda i: K.sum(vx[i]), K.arange(10), dtype=K.floatx()))\n    assert (10,) == kx.shape\n    assert (10,) == kx2.shape\n    assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n    assert_allclose(kx, kx2, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldl(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n    assert (3,) == kx.shape\n    assert_allclose(x.sum(axis=0), kx, atol=1e-05)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldr(self):\n    x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n    vx = K.variable(x)\n    p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n    p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n    assert p1 < p2\n    assert 9e-38 < p2 <= 1e-37",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk has issues with negative number.')\ndef test_arange(self):\n    for test_value in (-20, 0, 1, 10):\n        a_list = []\n        dtype_list = []\n        for k in WITH_NP:\n            t = k.arange(test_value)\n            a = k.eval(t)\n            assert np.array_equal(a, np.arange(test_value))\n            dtype_list.append(k.dtype(t))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for start, stop, step in ((0, 5, 1), (-5, 5, 2), (0, 1, 2)):\n        a_list = []\n        for k in WITH_NP:\n            a = k.eval(k.arange(start, stop, step))\n            assert np.array_equal(a, np.arange(start, stop, step))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n    for dtype in ('int32', 'int64', 'float32', 'float64'):\n        for k in WITH_NP:\n            t = k.arange(10, dtype=dtype)\n            assert k.dtype(t) == dtype\n    start = K.constant(1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 1\n    start = K.constant(-1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 0",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_train_phase(self, training):\n    check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "@pytest.mark.parametrize('training', [True, False])\ndef test_in_test_phase(self, training):\n    check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), WITH_NP, training=training)\n    check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), WITH_NP, training=training)",
                            "def test_setfloatx_incorrect_values(self):\n    old_floatx = floatx()\n    initial = floatx()\n    for value in ['', 'beerfloat', 123]:\n        with pytest.raises(ValueError):\n            set_floatx(value)\n    assert floatx() == initial\n    set_floatx(old_floatx)",
                            "def DISABLED_test_setfloatx_correct_values(self):\n    \"\"\"Disabled because it is not thread safe at this time.\"\"\"\n    old_floatx = floatx()\n    for value in ['float16', 'float32', 'float64']:\n        set_floatx(value)\n        assert floatx() == value\n    set_floatx(old_floatx)",
                            "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support float16')\ndef DISABLED_test_set_floatx(self):\n    \"\"\"Disabled because it is not thread safe at this time.\n\n    Make sure that changes to the global floatx are effectively\n    taken into account by the backend.\n    \"\"\"\n    old_floatx = floatx()\n    set_floatx('float16')\n    var = variable([10])\n    check_dtype(var, 'float16')\n    set_floatx('float64')\n    var = variable([10])\n    check_dtype(var, 'float64')\n    set_floatx(old_floatx)",
                            "def test_dtype(self):\n    assert K.dtype(K.variable(1, dtype='float64')) == 'float64'\n    assert K.dtype(K.variable(1, dtype='float32')) == 'float32'\n    assert K.dtype(K.variable(1, dtype='float16')) == 'float16'",
                            "def test_variable_support_bool_dtype(self):\n    if K.backend() == 'tensorflow':\n        assert K.dtype(K.variable(1, dtype='int16')) == 'int16'\n        assert K.dtype(K.variable(False, dtype='bool')) == 'bool'\n        with pytest.raises(TypeError):\n            K.variable('', dtype='unsupported')",
                            "def test_clip_supports_tensor_arguments(self):\n    x = K.variable([-10.0, -5.0, 0.0, 5.0, 10.0])\n    min_value = K.variable([-5.0, -4.0, 0.0, 3.0, 5.0])\n    max_value = K.variable([5.0, 4.0, 1.0, 4.0, 9.0])\n    assert np.allclose(K.eval(K.clip(x, min_value, max_value)), np.asarray([-5.0, -4.0, 0.0, 4.0, 9.0], dtype=np.float32))",
                            "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(), reason='This test is for tensorflow parallelism.')\ndef test_tensorflow_session_parallelism_settings(self, monkeypatch):\n    for threads in [1, 2]:\n        K.clear_session()\n        monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n        cfg = K.get_session()._config\n        assert cfg.intra_op_parallelism_threads == threads\n        assert cfg.inter_op_parallelism_threads == threads",
                            "def batch_shape(shape):\n    return (batch_size,) + shape[1:]",
                            "def random(shape):\n    return np.random.random(batch_shape(shape))",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn(inputs, states):\n        assert len(states) == 1\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y])\n    return simple_rnn",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_with_extra_mock_state(inputs, states):\n        assert len(states) == 2\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return (y, [y, backend.concatenate([y, y], axis=-1)])\n    return simple_rnn_with_extra_mock_state",
                            "def get_step_function(backend, w_i):\n\n    def simple_no_states(inputs, states):\n        assert len(states) == 0\n        y = backend.dot(inputs, w_i)\n        return (y, [])\n    return simple_no_states",
                            "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_add_constant(inputs, states_and_constants):\n        [h, c] = states_and_constants\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n        return (y, [y])\n    return simple_rnn_add_constant",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def step_function(inputs, states):\n    outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n    return (outputs, states)",
                            "def step_function(inputs, states):\n    return (inputs, [s + 1 for s in states])",
                            "def decode(merge_repeated):\n    input_prob_tensor = K.placeholder(shape=(None, None, None), dtype='float32')\n    input_len_tensor = K.placeholder(shape=None, dtype='int64')\n    paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor, greedy=False, beam_width=1, top_paths=1, merge_repeated=merge_repeated)\n    decode_func = K.function([input_prob_tensor, input_len_tensor], paths_tensors)\n    paths = decode_func([input_prob, input_len])\n    return paths",
                            "def simple_rnn(inputs, states):\n    assert len(states) == 1\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y])",
                            "def simple_rnn_with_extra_mock_state(inputs, states):\n    assert len(states) == 2\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return (y, [y, backend.concatenate([y, y], axis=-1)])",
                            "def simple_no_states(inputs, states):\n    assert len(states) == 0\n    y = backend.dot(inputs, w_i)\n    return (y, [])",
                            "def simple_rnn_add_constant(inputs, states_and_constants):\n    [h, c] = states_and_constants\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n    return (y, [y])"
                        ],
                        "constructor_variables": [],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "test_is_keras_tensor(self)",
                            "test_set_learning_phase(self)",
                            "test_eye(self)",
                            "test_ones(self)",
                            "test_zeros(self)",
                            "test_ones_like(self)",
                            "test_zeros_like(self)",
                            "test_linear_operations(self)",
                            "test_random_variables(self)",
                            "test_batch_dot_shape(self)",
                            "test_shape_operations(self)",
                            "test_none_shape_operations(self)",
                            "test_repeat_elements(self)",
                            "test_tile(self)",
                            "test_gather(self)",
                            "test_value_manipulation(self, function_name)",
                            "test_print_tensor(self)",
                            "test_elementwise_operations(self)",
                            "test_reset_uids(self)",
                            "test_cumsum_cumprod(self)",
                            "test_log(self)",
                            "test_update_add(self)",
                            "test_update_sub(self)",
                            "test_gradient(self)",
                            "test_stop_gradient(self)",
                            "test_function(self)",
                            "test_function_tf_fetches(self)",
                            "test_function_tf_feed_dict(self)",
                            "test_function_tf_run_options_with_run_metadata(self)",
                            "test_function_tf_string_input(self)",
                            "test_rnn(self)",
                            "test_rnn_additional_states(self)",
                            "test_rnn_no_states(self)",
                            "test_rnn_constants(self)",
                            "test_rnn_output_and_state_masking_independent(self)",
                            "test_rnn_output_num_dim_larger_than_2_masking(self)",
                            "test_rnn_state_num_dim_larger_than_2_masking(self)",
                            "test_logsumexp(self, x_np, axis, keepdims)",
                            "test_logsumexp_optim(self)",
                            "test_switch(self)",
                            "test_dropout(self)",
                            "test_relu(self, alpha, max_value, threshold)",
                            "test_nn_operations(self)",
                            "test_in_top_k(self)",
                            "test_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format)",
                            "test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate)",
                            "test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate)",
                            "test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                            "test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode)",
                            "test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format)",
                            "test_random_normal(self)",
                            "test_random_uniform(self)",
                            "test_random_binomial(self)",
                            "test_truncated_normal(self)",
                            "test_conv_invalid_use(self)",
                            "test_pooling_invalid_use(self)",
                            "test_resize_images(self)",
                            "_helper_bilinear(data_format, height_factor, width_factor)",
                            "test_resize_images_bilinear(self, data_format)",
                            "test_resize_volumes(self)",
                            "test_temporal_padding(self)",
                            "test_spatial_2d_padding(self)",
                            "test_spatial_3d_padding(self)",
                            "test_bias_add(self)",
                            "test_batchnorm_th(self, x_shape)",
                            "test_batchnorm_tf(self, x_shape)",
                            "test_batchnorm_cntk(self, x_shape)",
                            "test_ctc(self)",
                            "test_ctc_decode_greedy(self)",
                            "test_slice(self, x_size)",
                            "test_ctc_decode_beam_search(self)",
                            "test_ctc_decode_beam_search_no_merge(self)",
                            "test_one_hot(self)",
                            "test_sparse_dot(self)",
                            "test_sparse_concat(self)",
                            "test_stack(self)",
                            "test_map(self)",
                            "test_foldl(self)",
                            "test_foldr(self)",
                            "test_arange(self)",
                            "test_in_train_phase(self, training)",
                            "test_in_test_phase(self, training)",
                            "test_setfloatx_incorrect_values(self)",
                            "DISABLED_test_setfloatx_correct_values(self)",
                            "DISABLED_test_set_floatx(self)",
                            "test_dtype(self)",
                            "test_variable_support_bool_dtype(self)",
                            "test_clip_supports_tensor_arguments(self)",
                            "test_tensorflow_session_parallelism_settings(self, monkeypatch)",
                            "batch_shape(shape)",
                            "random(shape)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i, w_h)",
                            "get_step_function(backend, w_i)",
                            "get_step_function(backend, w_i, w_h)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "step_function(inputs, states)",
                            "decode(merge_repeated)",
                            "simple_rnn(inputs, states)",
                            "simple_rnn_with_extra_mock_state(inputs, states)",
                            "simple_no_states(inputs, states)",
                            "simple_rnn_add_constant(inputs, states_and_constants)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "@pytest.mark.skipif(K.backend() == 'theano',",
                    "start_line": 581,
                    "end_line": 582
                },
                {
                    "snippet_code": "@pytest.mark.skipif(K.backend() != 'tensorflow',\n                        reason='Uses the `options` and `run_metadata` arguments.')",
                    "start_line": 715,
                    "end_line": 716
                },
                {
                    "snippet_code": "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                        reason='This test is for tensorflow parallelism.')",
                    "start_line": 2125,
                    "end_line": 2126
                }
            ],
            "inscope_functions": [
                "def check_dtype(var, dtype):\n    if K.backend() == 'theano':\n        assert var.dtype == dtype\n    else:\n        assert var.dtype.name == '%s_ref' % dtype",
                "def cntk_func_tensors(function_name, shapes_or_vals, **kwargs):\n    placeholders = []\n    variables = []\n    for shape_or_val in shapes_or_vals:\n        if isinstance(shape_or_val, tuple):\n            shape = shape_or_val\n            placeholders.append(KC.placeholder(shape))\n        else:\n            value = shape_or_val\n            variables.append(KC.variable(value))\n\n    output_cntk = getattr(KC, function_name)(*(placeholders + variables), **kwargs)\n    cntk_func = KC.function(placeholders, [output_cntk])\n    return output_cntk, cntk_func",
                "def parse_shape_or_val(shape_or_val):\n    if isinstance(shape_or_val, np.ndarray):\n        return shape_or_val.shape, shape_or_val\n    else:\n        return shape_or_val, np.random.random(shape_or_val).astype(np.float32) - 0.5",
                "def assert_list_pairwise(z_list,\n                         shape=True,\n                         allclose=True,\n                         itself=False,\n                         atol=1e-05):\n    for (z1, z2) in zip(z_list[1:], z_list[:-1]):\n        if shape:\n            assert z1.shape == z2.shape\n        if allclose:\n            assert_allclose(z1, z2, atol=atol)\n        if itself:\n            assert z1 == z2",
                "def assert_list_keras_shape(t_list, z_list):\n    for t, z in zip(t_list, z_list):\n        if hasattr(t, '_keras_shape') and len(t._keras_shape) > 1:\n            for i, s in enumerate(t._keras_shape):\n                if s:\n                    assert t._keras_shape[i] == z.shape[i]",
                "def check_single_tensor_operation(function_name,\n                                  x_shape_or_val,\n                                  backend_list,\n                                  **kwargs):\n    shape_or_val = kwargs.pop('shape_or_val', True)\n    assert_value_equality = kwargs.pop('assert_value_equality', True)\n    cntk_dynamicity = kwargs.pop('cntk_dynamicity', False)\n\n    if shape_or_val:\n        x_shape, x_val = parse_shape_or_val(x_shape_or_val)\n\n    t_list = []\n    z_list = []\n    for k in backend_list:\n        if shape_or_val:\n            if (k == KC) & (cntk_dynamicity):\n                t, f = cntk_func_tensors(function_name, [x_shape], **kwargs)\n                z = f([x_val])[0]\n            else:\n                t = getattr(k, function_name)(k.variable(x_val), **kwargs)\n                z = k.eval(t)\n        else:\n            t = getattr(k, function_name)(x_shape_or_val, **kwargs)\n            z = k.eval(t)\n        t_list += [t]\n        z_list += [z]\n\n    assert_list_pairwise(z_list, allclose=assert_value_equality)\n    assert_list_keras_shape(t_list, z_list)",
                "def check_two_tensor_operation(function_name,\n                               x_shape_or_val,\n                               y_shape_or_val,\n                               backend_list,\n                               **kwargs):\n    concat_args = kwargs.pop('concat_args', False)\n    cntk_dynamicity = kwargs.pop('cntk_dynamicity', False)\n    cntk_two_dynamicity = kwargs.pop('cntk_two_dynamicity', False)\n\n    x_shape, x_val = parse_shape_or_val(x_shape_or_val)\n    y_shape, y_val = parse_shape_or_val(y_shape_or_val)\n\n    t_list = []\n    z_list = []\n    for k in backend_list:\n        if (k == KC) & (cntk_dynamicity):\n            t, f = cntk_func_tensors(function_name, [x_shape, y_val], **kwargs)\n            z = f([x_val])[0]\n        elif (k == KC) & (cntk_two_dynamicity):\n            t, f = cntk_func_tensors(function_name, [x_shape, y_shape], **kwargs)\n            z = f([x_val, y_val])[0]\n        elif (k == KTH) & (function_name[:4] == 'conv'):\n            t = getattr(k, function_name)(\n                k.variable(x_val), k.variable(convert_kernel(y_val)), **kwargs)\n            z = k.eval(t)\n        elif concat_args:\n            t = getattr(k, function_name)(\n                [k.variable(x_val), k.variable(y_val)], **kwargs)\n            z = k.eval(t)\n        else:\n            t = getattr(k, function_name)(\n                k.variable(x_val), k.variable(y_val), **kwargs)\n            z = k.eval(t)\n        t_list += [t]\n        z_list += [z]\n\n    assert_list_pairwise(z_list)\n    assert_list_keras_shape(t_list, z_list)",
                "def check_composed_tensor_operations(first_function_name,\n                                     first_function_args,\n                                     second_function_name,\n                                     second_function_args,\n                                     input_shape,\n                                     backend_list):\n    val = np.random.random(input_shape) - 0.5\n\n    z_list = []\n    for k in backend_list:\n        x = k.variable(val)\n        y = getattr(k, first_function_name)(x, **first_function_args)\n        z = k.eval(getattr(k, second_function_name)(y, **second_function_args))\n        z_list += [z]\n\n    assert_list_pairwise(z_list)",
                "def check_rnn_operation(step_function_k,\n                        step_function_np,\n                        inputs_np,\n                        initial_states_np,\n                        mask_np=None,\n                        constants_np=None,\n                        **kwargs):\n    inputs_k = K.variable(inputs_np)\n    initial_states_k = [K.variable(s) for s in initial_states_np]\n    if mask_np is not None:\n        mask_k = K.variable(mask_np)\n    else:\n        mask_k = None\n    if constants_np is not None:\n        constants_k = [K.variable(c) for c in constants_np]\n    else:\n        constants_k = None\n\n    last_output_np, output_np, last_states_np = KNP.rnn(\n        step_function_np,\n        inputs_np,\n        initial_states_np,\n        mask=mask_np,\n        constants=constants_np,\n        **kwargs)\n    # note that numpy reference implementation is independent of `unroll` argument\n\n    for unroll in [True, False]:\n        last_output_k, output_k, last_states_k = K.rnn(\n            step_function_k,\n            inputs_k,\n            initial_states_k,\n            mask=mask_k,\n            constants=constants_k,\n            unroll=unroll,\n            input_length=inputs_np.shape[1] if unroll else None,\n            **kwargs)\n\n        last_states_k = [K.eval(s) for s in last_states_k]\n        last_output_k = K.eval(last_output_k)\n        output_k = K.eval(output_k)\n\n        assert_allclose(last_output_k, last_output_np, atol=1e-05)\n        assert_allclose(output_k, output_np, atol=1e-05)\n        assert len(last_states_k) == len(last_states_np)\n        for s_k, s_np in zip(last_states_k, last_states_np):\n            assert_allclose(s_k, s_np, atol=1e-05)",
                "def test_is_keras_tensor(self):\n    np_var = np.array([1, 2])\n    with pytest.raises(ValueError):\n        K.is_keras_tensor(np_var)\n\n    keras_var = K.variable(np_var)\n    assert K.is_keras_tensor(keras_var) is False\n    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n    assert K.is_keras_tensor(keras_placeholder) is False",
                "def test_set_learning_phase(self):\n    # not supported learning_phase\n    with pytest.raises(ValueError):\n        K.set_learning_phase(2)",
                "def test_eye(self):\n    check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)",
                "def test_ones(self):\n    check_single_tensor_operation('ones', (3, 5, 10, 8),\n                                  WITH_NP, shape_or_val=False)",
                "def test_zeros(self):\n    check_single_tensor_operation('zeros', (3, 5, 10, 8),\n                                  WITH_NP, shape_or_val=False)",
                "def test_ones_like(self):\n    check_single_tensor_operation('ones_like', (3, 5, 10, 8),\n                                  WITH_NP, shape_or_val=True)",
                "def test_zeros_like(self):\n    check_single_tensor_operation('zeros_like', (3, 5, 10, 8),\n                                  WITH_NP, shape_or_val=True)",
                "def test_linear_operations(self):\n    check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n    check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n                               WITH_NP, cntk_two_dynamicity=True, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3),\n                               WITH_NP, cntk_two_dynamicity=True, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3),\n                               WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n                               WITH_NP, cntk_two_dynamicity=True, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n                               WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n                               WITH_NP, axes=(2, 2))\n    check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3),\n                               WITH_NP, axes=(2, 1))\n    check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3),\n                               WITH_NP, axes=(1, 1))\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n                               WITH_NP, axes=1)\n    check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n                               WITH_NP, axes=(1, 1))\n\n    check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n    check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n    if K.backend() != 'cntk':\n        check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=(1, 2))",
                "def test_random_variables(self):\n    check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP,\n                                  low=0., high=1.,\n                                  shape_or_val=False,\n                                  assert_value_equality=False)\n    check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP,\n                                  mean=0., scale=1.,\n                                  shape_or_val=False,\n                                  assert_value_equality=False)",
                "def test_batch_dot_shape(self):\n    # Note : batch_dot implementation is different for\n    # placeholders and variables in CNTK backend\n\n    test_cases = []\n    test_cases.append([(None, 3, 4, 5), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 3, 4, 5), (None, 2, 4), 2])\n    test_cases.append([(None, 3, 4), (None, 2, 3, 4), (2, 3)])\n    test_cases.append([(None, 4, 3), (None, 3, 5), (2, 1)])\n    test_cases.append([(None, 4), (None, 3, 4), (1, 2)])\n    test_cases.append([(None, 4), (None, 4), None])\n\n    batch_size = 7\n\n    def batch_shape(shape):\n        return (batch_size, ) + shape[1:]\n\n    def random(shape):\n        return np.random.random(batch_shape(shape))\n\n    for x_shape, y_shape, axes in test_cases:\n        x_np = random(x_shape)\n        y_np = random(y_shape)\n        z_np = KNP.batch_dot(x_np, y_np, axes)\n\n        # test with placeholders\n        x = K.placeholder(shape=x_shape)\n        y = K.placeholder(shape=y_shape)\n        z = K.batch_dot(x, y, axes)\n\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n\n        f = K.function([x, y], [z])\n\n        assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n\n        # test with placeholders (no shape info)\n        if K.backend() != 'cntk':\n            x = K.placeholder(ndim=len(x_shape))\n            y = K.placeholder(ndim=len(y_shape))\n            z = K.batch_dot(x, y, axes)\n\n            z_shape = K.int_shape(z)\n            if z_shape is not None:\n                assert len(z_shape) == z_np.ndim\n                assert set(z_shape) <= set((None, 1))\n\n            f = K.function([x, y], [z])\n\n            assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n\n        # test with variables\n        x = K.variable(x_np)\n        y = K.variable(y_np)\n        z = K.batch_dot(x, y, axes)\n\n        z_shape = K.int_shape(z)\n        if z_shape is not None:\n            assert z_shape[1:] == z_np.shape[1:]\n\n        z = K.eval(z)\n        assert_allclose(z, z_np, atol=1e-05)",
                "def test_shape_operations(self):\n    check_two_tensor_operation('concatenate', (4, 3), (4, 2), WITH_NP,\n                               axis=-1, concat_args=True)\n\n    check_single_tensor_operation('reshape', (4, 2), WITH_NP, shape=(8, 1))\n    check_single_tensor_operation('permute_dimensions', (4, 2, 3), WITH_NP,\n                                  pattern=(2, 0, 1))\n    check_single_tensor_operation('repeat', (4, 1), WITH_NP, n=3)\n    check_single_tensor_operation('flatten', (4, 1), WITH_NP)\n    check_single_tensor_operation('batch_flatten', (20, 2, 5), WITH_NP,\n                                  cntk_dynamicity=True)\n    check_single_tensor_operation('expand_dims', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('expand_dims', (4, 3, 2), WITH_NP, axis=1)\n    check_single_tensor_operation('squeeze', (4, 3, 1), WITH_NP, axis=2)\n    check_single_tensor_operation('squeeze', (4, 1, 1), WITH_NP, axis=1)\n    check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)},\n                                     'squeeze', {'axis': 2},\n                                     (4, 3, 1, 1), WITH_NP)",
                "@pytest.mark.skipif(K.backend() != 'theano',\n                    reason='We only test the shape inference of the '\n                           'theano backend.')\ndef test_none_shape_operations(self):\n    # Test shape inference when input\n    # shape has `None` entries\n    x = K.placeholder((3, None, 4))\n\n    y = K.batch_flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (3, None)\n\n    y = K.flatten(x)\n    if hasattr(y, '_keras_shape'):\n        assert y._keras_shape == (None,)",
                "def test_repeat_elements(self):\n    reps = 3\n    for ndims in [1, 2, 3]:\n        shape = np.arange(2, 2 + ndims)\n        arr = np.arange(np.prod(shape)).reshape(shape)\n\n        for rep_axis in range(ndims):\n            check_single_tensor_operation('repeat_elements', arr, WITH_NP,\n                                          rep=reps, axis=rep_axis)\n\n            if K.backend() != 'cntk':\n                shape = list(shape)\n                shape[rep_axis] = None\n                x = K.placeholder(shape=shape)\n                y = K.repeat_elements(x, reps, axis=rep_axis)\n                assert y._keras_shape == tuple(shape)\n                assert y._keras_shape == K.int_shape(y)",
                "def test_tile(self):\n    shape = (3, 4)\n    arr = np.arange(np.prod(shape)).reshape(shape)\n    check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n    check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n\n    # test theano shape inference when\n    # input shape has None entries\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 4))\n        n = 2\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 8)\n        n = (4, 3)\n        y = K.tile(x, n)\n        assert y._keras_shape == (None, 12)",
                "def test_gather(self):\n    shape = (10, 2, 3)\n    ref = np.arange(np.prod(shape)).reshape(shape)\n    inds = [1, 3, 7, 9]\n    t_list = [k.gather(k.variable(ref), k.variable(inds, dtype='int32'))\n              for k in WITH_NP]\n    z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32')))\n              for k in WITH_NP]\n\n    assert_list_pairwise(z_list)\n    assert_list_keras_shape(t_list, z_list)\n\n    # test theano shape inference when\n    # input shape has None entries\n    if K.backend() == 'theano':\n        x = K.placeholder(shape=(None, 3, 4))\n        indices = K.placeholder(shape=(5, 6), dtype='int32')\n        y = K.gather(x, indices)\n        assert y._keras_shape == (5, 6, 3, 4)",
                "@pytest.mark.parametrize('function_name',\n                         ['get_value', 'count_params',\n                          'int_shape', 'get_variable_shape'])\ndef test_value_manipulation(self, function_name):\n    val = np.random.random((4, 2))\n    v_list = [getattr(k, function_name)(k.variable(val))\n              for k in WITH_NP]\n\n    if function_name == 'get_value':\n        assert_list_pairwise(v_list)\n    else:\n        assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)",
                "def test_print_tensor(self):\n    check_single_tensor_operation('print_tensor', (), WITH_NP)\n    check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n    check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n    check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)",
                "def test_elementwise_operations(self):\n    check_single_tensor_operation('max', (4, 2), WITH_NP)\n    check_single_tensor_operation('max', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('max', (4, 2, 3), WITH_NP, axis=[1, -1])\n\n    check_single_tensor_operation('min', (4, 2), WITH_NP)\n    check_single_tensor_operation('min', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('min', (4, 2, 3), WITH_NP, axis=[1, -1])\n\n    check_single_tensor_operation('mean', (4, 2), WITH_NP)\n    check_single_tensor_operation('mean', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3),\n                                  WITH_NP, axis=-1, keepdims=True)\n    check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n\n    check_single_tensor_operation('var', (4, 2), WITH_NP)\n    check_single_tensor_operation('var', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('var', (4, 2, 3), WITH_NP, axis=[1, -1])\n\n    check_single_tensor_operation('std', (4, 2), WITH_NP)\n    check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n\n    check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n    check_single_tensor_operation('logsumexp', (4, 2),\n                                  WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n\n    check_single_tensor_operation('prod', (4, 2), WITH_NP)\n    check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n\n    check_single_tensor_operation('any', (4, 2), WITH_NP)\n    check_single_tensor_operation('any', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('any', (4, 2, 3), WITH_NP, axis=[1, -1])\n\n    check_single_tensor_operation('all', (4, 2), WITH_NP)\n    check_single_tensor_operation('all', (4, 2), WITH_NP, axis=1, keepdims=True)\n    check_single_tensor_operation('all', (4, 2, 3), WITH_NP, axis=[1, -1])\n\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmax', (4, 2), WITH_NP, axis=1)\n\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP)\n    check_single_tensor_operation('argmin', (4, 2), WITH_NP, axis=1)\n\n    check_single_tensor_operation('square', (4, 2), WITH_NP)\n    check_single_tensor_operation('abs', (4, 2), WITH_NP)\n    check_single_tensor_operation('sqrt', (4, 2), WITH_NP)\n    check_single_tensor_operation('exp', (4, 2), WITH_NP)\n\n    check_single_tensor_operation('round', (4, 2), WITH_NP)\n    check_single_tensor_operation('sign', (4, 2), WITH_NP)\n    check_single_tensor_operation('pow', (4, 2), WITH_NP, a=3)\n    check_single_tensor_operation('clip', (4, 2), WITH_NP, min_value=0.4,\n                                  max_value=0.6)\n\n    check_single_tensor_operation('cos', (4, 2), WITH_NP)\n    check_single_tensor_operation('sin', (4, 2), WITH_NP)\n\n    # two-tensor ops\n    check_two_tensor_operation('equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('not_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('greater_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('less_equal', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('maximum', (4, 2), (4, 2), WITH_NP)\n    check_two_tensor_operation('minimum', (4, 2), (4, 2), WITH_NP)",
                "def test_reset_uids(self):\n    first = K.get_uid()\n    K.get_uid()\n    K.reset_uids()\n    assert K.get_uid() == first",
                "@pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support '\n                                                  'cumsum and cumprod yet')\ndef test_cumsum_cumprod(self):\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumsum', (4, 2), WITH_NP, axis=1)\n\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP)\n    check_single_tensor_operation('cumprod', (4, 2), WITH_NP, axis=1)",
                "@pytest.mark.skipif(K.backend() == 'cntk',\n                    reason='cntk return -85.1 for zero or '\n                           'negative number, not nan, so can\\'t '\n                           'compare with other backend.')\ndef test_log(self):\n    check_single_tensor_operation('log', (4, 2), WITH_NP)",
                "@pytest.mark.skipif(K.backend() == 'theano',\n                    reason='theano returns tuples for update ops')\ndef test_update_add(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    increment = np.random.randn(3, 4)\n\n    x += increment\n    K.eval(K.update_add(x_var, increment))\n\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                "@pytest.mark.skipif(K.backend() == 'theano',\n                    reason='theano returns tuples for update ops')\ndef test_update_sub(self):\n    x = np.random.randn(3, 4)\n    x_var = K.variable(x)\n    decrement = np.random.randn(3, 4)\n\n    x -= decrement\n    K.eval(K.update_sub(x_var, decrement))\n\n    assert_allclose(x, K.eval(x_var), atol=1e-05)",
                "@pytest.mark.skipif(K.backend() == 'cntk',\n                    reason='cntk doesn\\'t support gradient in this way.')\ndef test_gradient(self):\n    val = np.random.random((4, 2))\n    x_list = [k.variable(val) for k in [KTH, KTF]]\n    z_list = []\n    zero_list = []\n    for x, k in zip(x_list, [KTH, KTF]):\n        exp = x * k.exp(x)\n        loss = k.sum(exp)\n        zero_loss = k.stop_gradient(loss)\n        grad = k.gradients(loss, [exp])\n        zero_grad = k.gradients(loss + zero_loss, [exp])\n        z_list.append(k.eval(grad[0]))\n        zero_list.append(k.eval(zero_grad[0]))\n\n    assert_list_pairwise(z_list)\n    assert_list_pairwise(zero_list)\n    for i in range(len(z_list)):\n        assert_allclose(zero_list[i], z_list[i], atol=1e-05)",
                "def test_stop_gradient(self):\n    # This test checks the consistency of the stop_gradient backend API.\n    # It doesn't check the functionality (which is checked at the\n    # test_gradient test).\n    val = np.random.random((4, 2))\n    a = K.variable(val)\n    b = K.square(a)\n    c, d = K.stop_gradient([a, b])\n    e = K.stop_gradient(b)",
                "@pytest.mark.skipif(K.backend() == 'cntk',\n                    reason='cntk currently not support function in this '\n                           'way, so can\\'t test as this.')\ndef test_function(self):\n    test_backend = [KTH, KTF]\n    val = np.random.random((4, 2))\n    input_val = np.random.random((4, 2))\n\n    f_list = []\n    x_list = []\n    for k in test_backend:\n        x = k.variable(val)\n        x_list.append(x)\n        y = k.placeholder(ndim=2)\n        exp = k.square(x) + y\n        update = x * 2\n        f = k.function([y], [exp], updates=[(x, update)])\n        f_list.append(f)\n\n    function_outputs_list = [f([input_val])[0] for f in f_list]\n    assert_list_pairwise(function_outputs_list)\n\n    new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n    assert_list_pairwise(new_val_list)",
                "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n                    reason='Uses the `fetches` argument.')\ndef test_function_tf_fetches(self):\n    # Additional operations can be passed to tf.Session().run() via its\n    # `fetches` arguments. In contrast to `updates` argument of\n    # KTF.function() these do not have control dependency on `outputs`, so\n    # they can run in parallel. Also they should not contribute to output of\n    # KTF.function().\n\n    x = K.variable(0.)\n    y = K.variable(0.)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n\n    f = K.function(inputs=[x_placeholder, y_placeholder],\n                   outputs=[x_placeholder + y_placeholder],\n                   updates=[(x, x_placeholder + 1.)],\n                   fetches=[K.update(y, 5.)])\n    output = f([10., 20.])\n    assert output == [30.]\n    assert K.get_session().run(fetches=[x, y]) == [11., 5.]",
                "@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n                    reason='Uses the `feed_dict` argument.')\ndef test_function_tf_feed_dict(self):\n    # Additional substitutions can be passed to `tf.Session().run()` via its\n    # `feed_dict` arguments. Note that the feed_dict is passed once in the\n    # constructor but we can modify the values in the dictionary. Through\n    # this feed_dict we can provide additional substitutions besides Keras\n    # inputs.\n\n    x = K.variable(0.)\n    y = K.variable(0.)\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n\n    feed_dict = {y_placeholder: 3.}\n\n    f = K.function(inputs=[x_placeholder],\n                   outputs=[x_placeholder + 1.],\n                   updates=[(x, x_placeholder + 10.)],\n                   feed_dict=feed_dict,\n                   fetches=[K.update(y, y_placeholder * 10.)])\n    output = f([10.])\n    assert output == [11.]\n    assert K.get_session().run(fetches=[x, y]) == [20., 30.]\n\n    # updated value in feed_dict will be modified within the K.function()\n    feed_dict[y_placeholder] = 4.\n    output = f([20.])\n    assert output == [21.]\n    assert K.get_session().run(fetches=[x, y]) == [30., 40.]",
                "@pytest.mark.skipif(K.backend() != 'tensorflow',\n                    reason='Uses the `options` and `run_metadata` arguments.')\ndef test_function_tf_run_options_with_run_metadata(self):\n    from tensorflow.core.protobuf import config_pb2\n    x_placeholder = K.placeholder(shape=())\n    y_placeholder = K.placeholder(shape=())\n\n    run_options = config_pb2.RunOptions(output_partition_graphs=True)\n    run_metadata = config_pb2.RunMetadata()\n    # enable run_options.\n    f = K.function(inputs=[x_placeholder, y_placeholder],\n                   outputs=[x_placeholder + y_placeholder],\n                   options=run_options,\n                   run_metadata=run_metadata)\n    output = f([10., 20.])\n    assert output == [30.]\n    assert len(run_metadata.partition_graphs) > 0\n    # disable run_options.\n    f = K.function(inputs=[x_placeholder, y_placeholder],\n                   outputs=[x_placeholder + y_placeholder],\n                   run_metadata=run_metadata)\n    output = f([10., 20.])\n    assert output == [30.]\n    assert len(run_metadata.partition_graphs) == 0",
                "@pytest.mark.skipif(K.backend() != 'tensorflow',\n                    reason='Uses the `string` type for a tensor.')\ndef test_function_tf_string_input(self):\n    # Test functions with string inputs.\n\n    x_placeholder = K.placeholder(shape=(), dtype=\"string\")\n    x_identity = K.identity(x_placeholder)\n\n    f = K.function(inputs=[x_placeholder], outputs=[x_identity])\n    output = f([b'test'])\n    assert output == [b'test']",
                "def test_rnn(self):\n    # implement a simple RNN\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn(inputs, states):\n            assert len(states) == 1\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return y, [y]\n\n        return simple_rnn\n\n    kwargs_list = [\n        {'go_backwards': False, 'mask': None},\n        {'go_backwards': True, 'mask': None},\n        {'go_backwards': False, 'mask': mask},\n        {'go_backwards': True, 'mask': mask},\n    ]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k),\n                            step_function_np=get_step_function(KNP, wi, wh),\n                            inputs_np=x,\n                            initial_states_np=[h0],\n                            mask_np=kwargs.pop('mask', None),\n                            **kwargs)",
                "def test_rnn_additional_states(self):\n    # implement a simple RNN with an additional state\n    # whose shape is different from that of the output\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    h1 = np.concatenate([h0, h0], axis=-1)\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_with_extra_mock_state(inputs, states):\n            assert len(states) == 2\n            h = states[0]\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n            return y, [y, backend.concatenate([y, y], axis=-1)]\n\n        return simple_rnn_with_extra_mock_state\n\n    kwargs_list = [\n        {'go_backwards': False, 'mask': None},\n        {'go_backwards': True, 'mask': None},\n        {'go_backwards': False, 'mask': mask},\n        {'go_backwards': True, 'mask': mask},\n    ]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k),\n                            step_function_np=get_step_function(KNP, wi, wh),\n                            inputs_np=x,\n                            initial_states_np=[h0, h1],\n                            mask_np=kwargs.pop('mask', None),\n                            **kwargs)",
                "def test_rnn_no_states(self):\n    # implement a simple RNN without states\n    num_samples = 3\n    input_dim = 8\n    output_dim = 4\n    timesteps = 5\n\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n\n    wi_k = K.variable(wi)\n\n    def get_step_function(backend, w_i):\n\n        def simple_no_states(inputs, states):\n            assert len(states) == 0\n            y = backend.dot(inputs, w_i)\n            return y, []\n\n        return simple_no_states\n\n    kwargs_list = [\n        {'go_backwards': False},\n        {'go_backwards': True},\n    ]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k),\n                            step_function_np=get_step_function(KNP, wi),\n                            inputs_np=x,\n                            initial_states_np=[],\n                            mask_np=None,\n                            **kwargs)",
                "def test_rnn_constants(self):\n    # implement a simple RNN\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n\n    _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n    _, h0 = parse_shape_or_val((num_samples, output_dim))\n    _, c = parse_shape_or_val((num_samples, output_dim))\n    _, wi = parse_shape_or_val((input_dim, output_dim))\n    _, wh = parse_shape_or_val((output_dim, output_dim))\n    mask = np.random.randint(2, size=(num_samples, timesteps))\n\n    wi_k = K.variable(wi)\n    wh_k = K.variable(wh)\n\n    def get_step_function(backend, w_i, w_h):\n\n        def simple_rnn_add_constant(inputs, states_and_constants):\n            # constants are appended to states in K.rnn\n            [h, c] = states_and_constants\n            y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n            return y, [y]\n\n        return simple_rnn_add_constant\n\n    kwargs_list = [\n        {'go_backwards': False, 'mask': None},\n        {'go_backwards': True, 'mask': None},\n        {'go_backwards': False, 'mask': mask},\n        {'go_backwards': True, 'mask': mask},\n    ]\n    for kwargs in kwargs_list:\n        check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k),\n                            step_function_np=get_step_function(KNP, wi, wh),\n                            inputs_np=x,\n                            initial_states_np=[h0],\n                            mask_np=kwargs.pop('mask', None),\n                            constants_np=[c],\n                            **kwargs)",
                "def test_rnn_output_and_state_masking_independent(self):\n    num_samples = 2\n    num_timesteps = 4\n    state_and_io_size = 5\n    mask_last_num_timesteps = 2  # for second sample only\n\n    # a step function that just outputs inputs,\n    # but increments states +1 per timestep\n    def step_function(inputs, states):\n        return inputs, [s + 1 for s in states]\n\n    inputs_vals = np.random.random(\n        (num_samples, num_timesteps, state_and_io_size))\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\n    # masking of two last timesteps for second sample only\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[1, -mask_last_num_timesteps:] = 0\n\n    # outputs expected to be same as inputs for the first sample\n    expected_outputs = inputs_vals.copy()\n    # but for the second sample all outputs in masked region should be the same\n    # as last output before masked region\n    expected_outputs[1, -mask_last_num_timesteps:] = \\\n        expected_outputs[1, -(mask_last_num_timesteps + 1)]\n\n    expected_state = initial_state_vals.copy()\n    # first state should be incremented for every timestep (no masking)\n    expected_state[0] += num_timesteps\n    # second state should not be incremented for last two timesteps\n    expected_state[1] += (num_timesteps - mask_last_num_timesteps)\n\n    # verify same expected output for `unroll=true/false`\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(\n            step_function,\n            inputs,\n            initial_states,\n            mask=mask,\n            unroll=unroll,\n            input_length=num_timesteps if unroll else None)\n\n        assert_allclose(K.eval(outputs), expected_outputs)\n        assert_allclose(K.eval(last_states[0]), expected_state)",
                "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_output_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n    num_features = 5\n\n    def step_function(inputs, states):\n        outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n        return outputs, states\n\n    inputs_vals = np.random.random((num_samples, num_timesteps, num_features))\n    initial_state_vals = np.random.random((num_samples, 6))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[-1, -1] = 0  # final timestep masked for last sample\n\n    expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n    # for the last sample, the final timestep (in masked region) should be the\n    # same as the second to final output (before masked region)\n    expected_outputs[-1, -1] = expected_outputs[-1, -2]\n\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(\n            step_function,\n            inputs,\n            initial_states,\n            mask=mask,\n            unroll=unroll,\n            input_length=num_timesteps if unroll else None)\n\n        assert_allclose(K.eval(outputs), expected_outputs)",
                "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')\ndef test_rnn_state_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n\n    def step_function(inputs, states):\n        return inputs, [s + 1 for s in states]\n\n    inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n    initial_state_vals = np.random.random((num_samples, 6, 7))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[0, -2:] = 0  # final two timesteps masked for first sample\n\n    expected_last_state = initial_state_vals.copy()\n    expected_last_state[0] += (num_timesteps - 2)\n    expected_last_state[1:] += num_timesteps\n\n    inputs = K.variable(inputs_vals)\n    initial_states = [K.variable(initial_state_vals)]\n    mask = K.variable(mask_vals)\n    for unroll in [True, False]:\n        last_output, outputs, last_states = K.rnn(\n            step_function,\n            inputs,\n            initial_states,\n            mask=mask,\n            unroll=unroll,\n            input_length=num_timesteps if unroll else None)\n\n        # not updated last timestep:\n        assert_allclose(K.eval(last_states[0]), expected_last_state)",
                "@pytest.mark.parametrize('x_np,axis,keepdims', [\n    (np.array([1.1, 0.8, 0.9]), 0, False),\n    (np.array([[1.1, 0.8, 0.9]]), 0, False),\n    (np.array([[1.1, 0.8, 0.9]]), 1, False),\n    (np.array([[1.1, 0.8, 0.9]]), -1, False),\n    (np.array([[1.1, 0.8, 0.9]]), 1, True),\n    (np.array([[1.1], [1.2]]), 0, False),\n    (np.array([[1.1], [1.2]]), 1, False),\n    (np.array([[1.1], [1.2]]), -1, False),\n    (np.array([[1.1], [1.2]]), -1, True),\n    (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), None, False),\n    (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 0, False),\n    (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False),\n    (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False),\n])\ndef test_logsumexp(self, x_np, axis, keepdims):\n    '''\n    Check if K.logsumexp works properly for values close to one.\n    '''\n    x = K.variable(x_np)\n    assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)),\n                    np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)),\n                    rtol=1e-5)",
                "@pytest.mark.skipif(K.backend() != 'tensorflow',\n                    reason='The optimization is applied only with TensorFlow.')\ndef test_logsumexp_optim(self):\n    '''\n    Check if optimization works.\n    '''\n    x_np = np.array([1e+4, 1e-4])\n    result = K.eval(K.logsumexp(K.variable(x_np), axis=0))\n    assert_allclose(result, 1e4, rtol=1e-5)",
                "def test_switch(self):\n    # scalar\n    val = np.random.random()\n    z_list = []\n    for k in WITH_NP:\n        x = k.variable(val)\n        x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n        z_list.append(k.eval(x))\n    assert_list_pairwise(z_list)\n    # non scalar\n    shapes = []\n    shapes.append([(4, 3, 2), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4, 3,), (4, 3, 2), (4, 3, 2)])\n    shapes.append([(4,), (4, 3, 2), (4, 3, 2)])\n    for s in shapes:\n        z_list = []\n        arrays = list(map(np.random.random, s))\n        for k in WITH_NP:\n            x, then_expr, else_expr = map(k.variable, arrays)\n            cond = k.greater_equal(x, 0.5)\n            z_list.append(k.eval(k.switch(cond, then_expr, else_expr)))\n        assert_list_pairwise(z_list)",
                "def test_dropout(self):\n    val = np.random.random((100, 100))\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2))\n              for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    # dropout patterns are different, only check mean\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n\n    z_list = [k.eval(k.dropout(k.variable(val), level=0.2,\n                               noise_shape=list(val.shape)))\n              for k in WITH_NP]\n    assert_list_pairwise(z_list, allclose=False)\n    # dropout patterns are different, only check mean\n    for i in range(len(z_list) - 1):\n        assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n\n    # Test invalid use cases\n    with pytest.raises(ValueError):\n        z = K.dropout(K.variable(val), level=-0.5)",
                "@pytest.mark.parametrize('alpha,max_value,threshold', [\n    (0.0, None, 0.0),  # standard relu\n    (0.1, None, 0.0),  # set alpha only\n    (0.0, 5.0, 0.0),   # set max_value only\n    (0.0, None, 0.8),  # set threshold only\n    (0.1, 5.0, 0.0),   # set alpha and max_value\n    (0.1, None, 0.8),  # set alpha and threshold\n    (0.0, 5.0, 0.8),   # set max_value and threshold\n    (0.1, 5.0, 0.8),   # set all\n    (0.1, 0.0, 0.8),   # max_value is zero\n    (0.1, 5.0, -2.8),  # threshold is negative\n    (0.1, 9.0, 0.8),   # max_value > 6\n])\ndef test_relu(self, alpha, max_value, threshold):\n    check_single_tensor_operation('relu', (4, 2), WITH_NP, alpha=alpha,\n                                  max_value=max_value, threshold=threshold)",
                "def test_nn_operations(self):\n    check_single_tensor_operation('softsign', (4, 10), WITH_NP)\n    check_single_tensor_operation('softplus', (4, 10), WITH_NP)\n    check_single_tensor_operation('elu', (4, 10), WITH_NP, alpha=0.5)\n\n    check_single_tensor_operation('sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('hard_sigmoid', (4, 2), WITH_NP)\n    check_single_tensor_operation('tanh', (4, 2), WITH_NP)\n\n    check_single_tensor_operation('softmax', (4, 10), WITH_NP)\n    check_single_tensor_operation('softmax', (4, 5, 3), WITH_NP, axis=1)\n    check_single_tensor_operation('softmax', (4, 5, 3, 10), WITH_NP, axis=2)\n\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2),\n                               WITH_NP, from_logits=True)\n    # cross_entropy call require the label is a valid probability distribution,\n    # otherwise it is garbage in garbage out...\n    # due to the algo difference, we can't guarantee CNTK has the same result\n    # on the garbage input.\n    # so create a separate test case for valid label input\n    if K.backend() != 'cntk':\n        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2),\n                                   WITH_NP, from_logits=True)\n    xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841],\n                       [0.20225059, -0.38956559], [-0.13805378, 0.08506755]],\n                      dtype=np.float32)\n    yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016],\n                       [0.64916514, 0.35083486], [0.47028078, 0.52971922]],\n                      dtype=np.float32)\n    check_two_tensor_operation('categorical_crossentropy', yval, xval, WITH_NP,\n                               cntk_two_dynamicity=True, from_logits=True)\n    check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2),\n                               WITH_NP, from_logits=False)\n    check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2),\n                               WITH_NP, from_logits=False)\n\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n    check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)",
                "def test_in_top_k(self):\n    batch_size = 20\n    num_classes = 10\n\n    # Random prediction test case\n    predictions = np.random.random((batch_size, num_classes)).astype('float32')\n    targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n\n    # (k == 0 or k > num_classes) does not raise an error\n    # but just return an unmeaningful tensor.\n    for k in range(num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'),\n                                    b.variable(targets, dtype='int32'), k))\n                  for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)\n\n    # Identical prediction test case:\n    # randomly set half of the predictions to an identical value\n    num_identical = num_classes // 2\n    for i in range(batch_size):\n        idx_identical = np.random.choice(num_classes,\n                                         size=num_identical, replace=False)\n        predictions[i, idx_identical] = predictions[i, 0]\n    targets = np.zeros(batch_size, dtype='int32')\n\n    for k in range(1, num_classes + 1):\n        z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'),\n                                    b.variable(targets, dtype='int32'), k))\n                  for b in [KTH, KTF]]\n        assert_list_pairwise(z_list)",
                "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [\n    ('conv1d', (2, 8, 2), (3, 2, 3), 'same', 'channels_last'),\n    ('conv1d', (1, 8, 2), (3, 2, 3), 'valid', 'channels_last'),\n    ('conv1d', (1, 2, 8), (3, 2, 3), 'valid', 'channels_first'),\n    ('conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'),\n    ('conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'),\n    ('conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'),\n    ('conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'),\n    ('conv3d', (2, 3, 4, 5, 4), (3, 3, 3, 3, 4), 'same', 'channels_first'),\n    ('conv3d', (2, 3, 5, 4, 6), (3, 2, 4, 3, 4), 'valid', 'channels_first'),\n    ('conv3d', (1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'valid', 'channels_last'),\n    ('conv3d', (1, 3, 5, 4, 2), (3, 3, 3, 2, 3), 'same', 'channels_last'),\n])\ndef test_conv(self, op, input_shape, kernel_shape, padding, data_format):\n    check_two_tensor_operation(\n        op, input_shape, kernel_shape, WITH_NP,\n        padding=padding, data_format=data_format,\n        cntk_dynamicity=True)",
                "@pytest.mark.parametrize(\n    'op,input_shape,kernel_shape,output_shape,padding,data_format', [\n        ('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2),\n         'same', 'channels_last'),\n        ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9),\n         'same', 'channels_first'),\n    ])\ndef test_conv_transpose(self,\n                        op,\n                        input_shape,\n                        kernel_shape,\n                        output_shape,\n                        padding,\n                        data_format):\n    check_two_tensor_operation(\n        op, input_shape, kernel_shape, WITH_NP,\n        output_shape=output_shape, padding=padding, data_format=data_format,\n        cntk_dynamicity=True)",
                "@pytest.mark.skipif((K.backend() == 'cntk' and KC.dev.type() == 0),\n                    reason='cntk only supports dilated conv on GPU')\n@pytest.mark.parametrize(\n    'op,input_shape,kernel_shape,padding,data_format,dilation_rate', [\n        ('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2),\n        ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2),\n        ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2),\n         'same', 'channels_last', (2, 2)),\n        ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4),\n         'valid', 'channels_first', (2, 2)),\n        ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4),\n         'valid', 'channels_last', (2, 2, 2)),\n        ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4),\n         'same', 'channels_first', (2, 2, 2)),\n    ])\ndef test_dilated_conv(self,\n                      op,\n                      input_shape,\n                      kernel_shape,\n                      padding,\n                      data_format,\n                      dilation_rate):\n    check_two_tensor_operation(\n        op, input_shape, kernel_shape, WITH_NP,\n        padding=padding, data_format=data_format,\n        dilation_rate=dilation_rate, cntk_dynamicity=True)",
                "@pytest.mark.skipif((K.backend() == 'cntk' and KC.dev.type() == 0),\n                    reason='cntk only supports dilated conv transpose on GPU')\n@pytest.mark.parametrize(\n    'op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate',\n    [\n        ('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2),\n         'same', 'channels_last', (2, 2)),\n        ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9),\n         'same', 'channels_first', (2, 2)),\n    ])\ndef test_dilated_conv_transpose(self,\n                                op,\n                                input_shape,\n                                kernel_shape,\n                                output_shape,\n                                padding,\n                                data_format,\n                                dilation_rate):\n    check_two_tensor_operation(\n        op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape,\n        padding=padding, data_format=data_format, dilation_rate=dilation_rate,\n        cntk_dynamicity=True)",
                "@pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [\n    ('depthwise_conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'),\n    ('depthwise_conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'),\n    ('depthwise_conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'),\n    ('depthwise_conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'),\n])\ndef test_depthwise_conv(self,\n                        op,\n                        input_shape,\n                        kernel_shape,\n                        padding,\n                        data_format):\n    check_two_tensor_operation(\n        op, input_shape, kernel_shape, WITH_NP,\n        padding=padding, data_format=data_format,\n        cntk_dynamicity=True)",
                "@pytest.mark.parametrize(\n    'op,input_shape,pool_size,strides,padding,data_format,pool_mode', [\n        ('pool2d', (2, 3, 7, 7), (3, 3), (1, 1),\n         'same', 'channels_first', 'avg'),\n        ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1),\n         'valid', 'channels_first', 'max'),\n        ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1),\n         'valid', 'channels_last', 'avg'),\n        ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1),\n         'same', 'channels_last', 'max'),\n        ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1),\n         'same', 'channels_first', 'avg'),\n        ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1),\n         'valid', 'channels_first', 'max'),\n        ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1),\n         'valid', 'channels_last', 'avg'),\n        ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1),\n         'same', 'channels_last', 'max'),\n    ])\ndef test_pool(self,\n              op,\n              input_shape,\n              pool_size,\n              strides,\n              padding,\n              data_format,\n              pool_mode):\n    check_single_tensor_operation(\n        op, input_shape, WITH_NP,\n        pool_size=pool_size, strides=strides,\n        padding=padding, data_format=data_format, pool_mode=pool_mode,\n        cntk_dynamicity=True)",
                "@pytest.mark.parametrize(\n    'op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [\n        ('separable_conv1d', (2, 8, 2), (3,), 1, 'same', 'channels_last'),\n        ('separable_conv1d', (1, 8, 2), (3,), 2, 'valid', 'channels_last'),\n        ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'),\n        ('separable_conv2d', (2, 3, 5, 6), (4, 3), 2, 'valid', 'channels_first'),\n        ('separable_conv2d', (1, 6, 5, 3), (3, 4), 1, 'valid', 'channels_last'),\n        ('separable_conv2d', (1, 7, 6, 3), (3, 3), 2, 'same', 'channels_last'),\n    ])\ndef test_separable_conv(self,\n                        op,\n                        input_shape,\n                        kernel_shape,\n                        depth_multiplier,\n                        padding,\n                        data_format):\n    if data_format == 'channels_first':\n        input_depth = input_shape[1]\n    else:\n        input_depth = input_shape[-1]\n    _, x = parse_shape_or_val(input_shape)\n    _, depthwise = parse_shape_or_val(kernel_shape +\n                                      (input_depth, depth_multiplier))\n    _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) +\n                                      (input_depth * depth_multiplier, 7))\n    y1 = KNP.separable_conv(x, depthwise, pointwise,\n                            padding=padding, data_format=data_format)\n    if K.backend() == 'cntk':\n        _, cntk_func = cntk_func_tensors(\n            op, [input_shape, depthwise, pointwise],\n            padding=padding, data_format=data_format)\n        y2 = cntk_func([x])[0]\n    else:\n        y2 = K.eval(getattr(K, op)(\n            K.variable(x),\n            K.variable(depthwise), K.variable(pointwise),\n            padding=padding, data_format=data_format))\n    assert_allclose(y1, y2, atol=1e-05)",
                "def test_random_normal(self):\n    # test standard normal as well as a normal with a different set of parameters\n    for mean, std in [(0., 1.), (-10., 5.)]:\n        rand = K.eval(K.random_normal((300, 200),\n                                      mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n        assert np.abs(np.mean(rand) - mean) < std * 0.015\n        assert np.abs(np.std(rand) - std) < std * 0.015\n\n        # test that random_normal also generates different values when used\n        # within a function\n        r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - mean) < std * 0.015\n        assert np.abs(np.std(samples) - std) < std * 0.015",
                "def test_random_uniform(self):\n    min_val = -1.\n    max_val = 1.\n    rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand)) < 0.015\n    assert max_val - 0.015 < np.max(rand) <= max_val\n    assert min_val + 0.015 > np.min(rand) >= min_val\n\n    r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples)) < 0.015\n    assert max_val - 0.015 < np.max(samples) <= max_val\n    assert min_val + 0.015 > np.min(samples) >= min_val",
                "def test_random_binomial(self):\n    p = 0.5\n    rand = K.eval(K.random_binomial((200, 100), p))\n    assert rand.shape == (200, 100)\n    assert np.abs(np.mean(rand) - p) < 0.015\n    assert np.max(rand) == 1\n    assert np.min(rand) == 0\n\n    r = K.random_binomial((10, 10), p)\n    samples = np.array([K.eval(r) for _ in range(200)])\n    assert np.abs(np.mean(samples) - p) < 0.015\n    assert np.max(samples) == 1\n    assert np.min(samples) == 0",
                "def test_truncated_normal(self):\n    mean = 0.\n    std = 1.\n    min_val = -2.\n    max_val = 2.\n    rand = K.eval(K.truncated_normal((300, 200),\n                                     mean=mean, stddev=std, seed=1337))\n    assert rand.shape == (300, 200)\n    assert np.abs(np.mean(rand) - mean) < 0.015\n    assert np.max(rand) <= max_val\n    assert np.min(rand) >= min_val\n\n    # assumption in initializers.VarianceScaling\n    assert np.abs(np.std(rand) - std * 0.87962) < 0.015",
                "def test_conv_invalid_use(self):\n    dummy_x_1d = K.variable(np.ones((4, 8, 2)))\n    dummy_w_1d = K.variable(np.ones((3, 2, 3)))\n    dummy_x_2d = K.variable(np.ones((2, 3, 4, 5)))\n    dummy_w_2d = K.variable(np.ones((2, 2, 3, 4)))\n    dummy_x_3d = K.variable(np.ones((2, 3, 4, 5, 4)))\n    dummy_w_3d = K.variable(np.ones((2, 2, 2, 3, 4)))\n    dummy_w1x1_2d = K.variable(np.ones((1, 1, 12, 7)))\n\n    with pytest.raises(ValueError):\n        K.conv1d(dummy_x_1d, dummy_w_1d, data_format='channels_middle')\n\n    with pytest.raises(ValueError):\n        K.conv2d(dummy_x_2d, dummy_w_2d, data_format='channels_middle')\n\n    with pytest.raises(ValueError):\n        K.conv3d(dummy_x_3d, dummy_w_3d, data_format='channels_middle')\n\n    if K.backend() != 'theano':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d,\n                               data_format='channels_middle')\n\n    with pytest.raises(ValueError):\n        K.depthwise_conv2d(dummy_x_2d, dummy_w_2d,\n                           data_format='channels_middle')\n\n    if K.backend() == 'cntk':\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d,\n                               dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.separable_conv2d(dummy_x_2d, dummy_w_2d, dummy_w1x1_2d,\n                               strides=(2, 2), dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d,\n                               dilation_rate=(1, 2))\n        with pytest.raises(ValueError):\n            K.depthwise_conv2d(dummy_x_2d, dummy_w_2d,\n                               strides=(2, 2), dilation_rate=(1, 2))",
                "def test_pooling_invalid_use(self):\n    for (input_shape, pool_size) in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)],\n                                        [(2, 2), (2, 2, 2)]):\n        x = K.variable(np.random.random(input_shape))\n        if len(pool_size) == 2:\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool2d(x, pool_size=pool_size, pool_mode='median')\n        else:\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, padding='twice')\n            with pytest.raises(ValueError):\n                K.pool3d(x, pool_size=pool_size, pool_mode='median')",
                "def test_resize_images(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_images', x_shape,\n                                      WITH_NP, cntk_dynamicity=True,\n                                      height_factor=2,\n                                      width_factor=2,\n                                      data_format=data_format)\n\n    # Test invalid use cases\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_images(K.variable(xval), 2, 2,\n                        data_format='channels_middle')",
                "@staticmethod\ndef _helper_bilinear(data_format, height_factor, width_factor):\n    x_shape = (2, 3, 4, 5)\n    check_single_tensor_operation('resize_images', x_shape,\n                                  [KTF, KTH],\n                                  height_factor=height_factor,\n                                  width_factor=width_factor,\n                                  data_format=data_format,\n                                  interpolation='bilinear')",
                "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\ndef test_resize_images_bilinear(self, data_format):\n    self._helper_bilinear(data_format, 2, 2)\n    with pytest.raises(NotImplementedError):\n        self._helper_bilinear(data_format, 4, 4)",
                "def test_resize_volumes(self):\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (2, 3) + shape\n        elif data_format == 'channels_last':\n            x_shape = (2,) + shape + (3,)\n        check_single_tensor_operation('resize_volumes', x_shape,\n                                      WITH_NP, cntk_dynamicity=True,\n                                      depth_factor=2,\n                                      height_factor=2,\n                                      width_factor=2,\n                                      data_format=data_format)\n\n    # Test invalid use cases\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.resize_volumes(K.variable(xval), 2, 2, 2,\n                         data_format='channels_middle')",
                "def test_temporal_padding(self):\n    check_single_tensor_operation('temporal_padding', (4, 3, 3),\n                                  WITH_NP)\n    check_single_tensor_operation('temporal_padding', (2, 3, 4),\n                                  WITH_NP, padding=(1, 2))",
                "def test_spatial_2d_padding(self):\n    padding = ((1, 2), (2, 1))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP,\n                                      padding=padding, data_format=data_format)\n    # Check handling of dynamic shapes.\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, 1))\n        y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, 1)\n\n    # Test invalid use cases\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_2d_padding(K.variable(xval), padding=padding,\n                             data_format='channels_middle')",
                "def test_spatial_3d_padding(self):\n    padding = ((1, 2), (2, 1), (1, 2))\n    for data_format in ['channels_first', 'channels_last']:\n        shape = (5, 5, 5)\n        if data_format == 'channels_first':\n            x_shape = (1, 3) + shape\n        else:\n            x_shape = (1,) + shape + (3,)\n        check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP,\n                                      padding=padding, data_format=data_format)\n    # Check handling of dynamic shapes.\n    if K in [KTF, KTH]:\n        x = K.placeholder(shape=(1, None, None, None, 1))\n        y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n        assert K.int_shape(y) == (1, None, None, None, 1)\n\n    # Test invalid use cases\n    xval = np.random.random(x_shape)\n    with pytest.raises(ValueError):\n        K.spatial_3d_padding(K.variable(xval), padding=padding,\n                             data_format='channels_middle')",
                "def test_bias_add(self):\n    for data_format in ['channels_first', 'channels_last']:\n        for shape in [(), (3,), (2, 3), (5, 3, 2)]:\n            if data_format == 'channels_first':\n                x_shape = (1, 4) + shape\n            else:\n                x_shape = (1,) + shape + (4,)\n            bias_shape = (4,)\n            check_two_tensor_operation('bias_add', x_shape, bias_shape,\n                                       WITH_NP, cntk_dynamicity=True,\n                                       data_format=data_format)\n\n        if data_format == 'channels_first':\n            x_shape = (20, 6, 10)\n        else:\n            x_shape = (20, 10, 6)\n        check_two_tensor_operation('bias_add', x_shape, (10, 6),\n                                   WITH_NP, cntk_dynamicity=True,\n                                   data_format=data_format)\n\n    # Test invalid use cases\n    x = K.variable(np.random.random(x_shape))\n    b = K.variable(np.random.random(bias_shape))\n    with pytest.raises(ValueError):\n        K.bias_add(x, b, data_format='channels_middle')",
                "@pytest.mark.skipif(K.backend() != 'theano',\n                    reason='Specific to Theano.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_th(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(\n        x, None, None, reduction_axes='per-activation')\n    z = K.eval(z)\n    assert z.shape == x_shape",
                "@pytest.mark.skipif(K.backend() != 'tensorflow',\n                    reason='Specific to Tensorflow.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_tf(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.variable(x_val)\n    z, _, _ = K.normalize_batch_in_training(\n        x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.eval(z)\n    assert z.shape == x_shape",
                "@pytest.mark.skipif(K.backend() != 'cntk', reason='Specific to CNTK.')\n@pytest.mark.parametrize('x_shape', [(1, 4, 2, 3), (1, 2, 3, 4)])\ndef test_batchnorm_cntk(self, x_shape):\n    x_val = np.random.random(x_shape).astype(np.float32)\n    x = K.placeholder(x_shape)\n    z, _, _ = K.normalize_batch_in_training(\n        x, None, None, reduction_axes=[0, 1, 2, 3])\n    z = K.function([x], [z])([x_val])[0]\n    assert z.shape == x_shape",
                "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_ctc(self):\n    if K.backend() == 'theano':\n        ref = [1.73308, 3.81351]\n    else:\n        ref = [3.34211, 5.42262]\n    # simplified version of TensorFlow's test\n\n    label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n    input_lens = np.expand_dims(np.asarray([5, 5]), 1)  # number of timesteps\n\n    # dimensions are batch x time x categories\n    labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n    inputs = np.asarray(\n        [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n          [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n          [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n          [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n          [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]],\n         [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508],\n          [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549],\n          [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456],\n          [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345],\n          [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]],\n        dtype=np.float32)\n\n    k_labels = K.variable(labels, dtype=\"int32\")\n    k_inputs = K.variable(inputs, dtype=\"float32\")\n    k_input_lens = K.variable(input_lens, dtype=\"int32\")\n    k_label_lens = K.variable(label_lens, dtype=\"int32\")\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens,\n                                  k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)\n\n    # test when batch_size = 1, that is, one sample only\n    # get only first sample from above test case\n    if K.backend() == 'theano':\n        ref = [1.73308]\n    else:\n        ref = [3.34211]\n\n    input_lens = np.expand_dims(np.asarray([5]), 1)\n    label_lens = np.expand_dims(np.asarray([5]), 1)\n\n    labels = np.asarray([[0, 1, 2, 1, 0]])\n    inputs = np.asarray(\n        [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n          [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n          [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n          [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n          [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]],\n        dtype=np.float32)\n\n    k_labels = K.variable(labels, dtype=\"int32\")\n    k_inputs = K.variable(inputs, dtype=\"float32\")\n    k_input_lens = K.variable(input_lens, dtype=\"int32\")\n    k_label_lens = K.variable(label_lens, dtype=\"int32\")\n    res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens,\n                                  k_label_lens))\n    if K.backend() == 'theano':\n        assert_allclose(res[0, :], ref, atol=1e-05)\n    else:\n        assert_allclose(res[:, 0], ref, atol=1e-05)",
                "@pytest.mark.skipif(K.backend() != 'tensorflow',\n                    reason='Test adapted from tensorflow.')\ndef test_ctc_decode_greedy(self):\n    \"\"\"Test two batch entries - best path decoder.\"\"\"\n    max_time_steps = 6\n\n    seq_len_0 = 4\n    input_prob_matrix_0 = np.asarray(\n        [[1.0, 0.0, 0.0, 0.0],  # t=0\n         [0.0, 0.0, 0.4, 0.6],  # t=1\n         [0.0, 0.0, 0.4, 0.6],  # t=2\n         [0.0, 0.9, 0.1, 0.0],  # t=3\n         [0.0, 0.0, 0.0, 0.0],  # t=4 (ignored)\n         [0.0, 0.0, 0.0, 0.0]],  # t=5 (ignored)\n        dtype=np.float32)\n\n    seq_len_1 = 5\n    # dimensions are time x depth\n\n    input_prob_matrix_1 = np.asarray(\n        [[0.1, 0.9, 0.0, 0.0],  # t=0\n         [0.0, 0.9, 0.1, 0.0],  # t=1\n         [0.0, 0.0, 0.1, 0.9],  # t=2\n         [0.0, 0.9, 0.1, 0.1],  # t=3\n         [0.9, 0.1, 0.0, 0.0],  # t=4\n         [0.0, 0.0, 0.0, 0.0]],  # t=5 (ignored)\n        dtype=np.float32)\n\n    # len max_time_steps array of batch_size x depth matrices\n    inputs = [np.vstack([input_prob_matrix_0[t, :],\n                         input_prob_matrix_1[t, :]])\n              for t in range(max_time_steps)]\n\n    # change tensorflow order to keras backend order\n    inputs = np.asarray(inputs).transpose((1, 0, 2))\n\n    # batch_size length vector of sequence_lengths\n    input_length = np.array([seq_len_0, seq_len_1], dtype=np.int32)\n\n    decode_pred_np, log_prob_pred_np = KNP.ctc_decode(inputs,\n                                                      input_length, greedy=True)\n    inputs = K.variable(inputs)\n    input_length = K.variable(input_length)\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs,\n                                                    input_length, greedy=True)\n\n    assert len(decode_pred_tf) == 1\n\n    decode_pred = K.eval(decode_pred_tf[0])\n    log_prob_pred = K.eval(log_prob_pred_tf)\n\n    assert np.alltrue(decode_pred_np == decode_pred)\n    assert np.allclose(log_prob_pred_np, log_prob_pred)",
                "@pytest.mark.skipif(K.backend() != 'tensorflow',\n                    reason='tensorflow-way slice is '\n                    'only supported in tensorflow.')\n@pytest.mark.parametrize('x_size', [\n    [1, 1, 3],\n    [1, 2, 3],\n    [2, 1, 3]\n])\ndef test_slice(self, x_size):\n    npt = np.array([[[1, 1, 1], [2, 2, 2]],\n                   [[3, 3, 3], [4, 4, 4]],\n                   [[5, 5, 5], [6, 6, 6]]])\n    x_start = [1, 0, 0]\n    tft = K.constant(npt)\n    test_input = K.eval(K.slice(tft, x_start, x_size))\n    expected = KNP.slice(npt, x_start, x_size)\n    assert np.allclose(test_input, expected)",
                "@pytest.mark.skipif(K.backend() != 'tensorflow',\n                    reason='Beam search is only implemented with '\n                           'the TensorFlow backend.')\ndef test_ctc_decode_beam_search(self):\n    \"\"\"Test one batch, two beams - hibernating beam search.\"\"\"\n\n    depth = 6\n\n    seq_len_0 = 5\n    input_prob_matrix_0 = np.asarray(\n        [[0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908],\n         [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517],\n         [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763],\n         [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655],\n         [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878],\n         # Random entry added in at time=5\n         [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]],\n        dtype=np.float32)\n\n    # Add arbitrary offset - this is fine\n    input_prob_matrix_0 = input_prob_matrix_0 + 2.0\n\n    # len max_time_steps array of batch_size x depth matrices\n    inputs = ([input_prob_matrix_0[t, :][np.newaxis, :]\n               for t in range(seq_len_0)] +  # Pad to max_time_steps = 8\n              2 * [np.zeros((1, depth), dtype=np.float32)])\n\n    # Take exponential as we directly apply ctc_decode_beam_search\n    inputs = np.exp(inputs)\n\n    # change tensorflow order to keras backend order\n    inputs = K.variable(inputs.transpose((1, 0, 2)))\n\n    # batch_size length vector of sequence_lengths\n    input_length = K.variable(np.array([seq_len_0], dtype=np.int32))\n    # batch_size length vector of log probabilities\n    log_prob_truth = np.array(\n        [\n            -5.811451,  # output beam 0\n            -6.63339  # output beam 1\n        ],\n        np.float32)[np.newaxis, :]\n\n    decode_truth = [np.array([1, 0]), np.array([[1]])]\n\n    beam_width = 2\n    top_paths = 2\n\n    decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs,\n                                                    input_length,\n                                                    greedy=False,\n                                                    beam_width=beam_width,\n                                                    top_paths=top_paths)\n\n    assert len(decode_pred_tf) == top_paths\n\n    log_prob_pred = K.eval(log_prob_pred_tf)\n\n    for i in range(top_paths):\n        assert np.alltrue(decode_truth[i] == K.eval(decode_pred_tf[i]))\n\n    assert np.allclose(log_prob_truth, log_prob_pred)",
                "@pytest.mark.skipif(K.backend() != 'tensorflow',\n                    reason='Beam search is only implemented with '\n                           'the TensorFlow backend.')\ndef test_ctc_decode_beam_search_no_merge(self):\n    # A simple CTC probability map with some repeating characters,\n    # shape(batch, input_width, char_count)\n    # Without merging should be decoded as: \"AABB\", with merging as: \"AB\".\n    input_prob = np.array([\n        [  # blank, A ,B\n            [0, 0, 1],  # blank\n            [1, 0, 0],  # A\n            [0, 0, 1],  # blank\n            [1, 0, 0],  # A\n            [0, 1, 0],  # B\n            [0, 0, 1],  # blank\n            [0, 1, 0]  # B\n        ]\n    ])\n    input_len = np.array(input_prob.shape[0] * [input_prob.shape[1]])\n\n    def decode(merge_repeated):\n        input_prob_tensor = K.placeholder(shape=(None, None, None),\n                                          dtype='float32')\n        input_len_tensor = K.placeholder(shape=(None), dtype='int64')\n        paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor,\n                                        greedy=False, beam_width=1, top_paths=1,\n                                        merge_repeated=merge_repeated)\n        decode_func = K.function([input_prob_tensor, input_len_tensor],\n                                 paths_tensors)\n        paths = decode_func([input_prob, input_len])\n        return paths\n\n    # merged: A B\n    assert np.allclose(decode(merge_repeated=True), [np.array([[0, 1]])])\n    # not merged: A A B B\n    assert np.allclose(decode(merge_repeated=False), [np.array([[0, 0, 1, 1]])])",
                "def test_one_hot(self):\n    input_length = 10\n    num_classes = 20\n    batch_size = 30\n    indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n    oh = KNP.one_hot(np.int32(indices), num_classes)\n    koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n    assert np.all(koh == oh)",
                "@pytest.mark.skipif(not supports_sparse,\n                    reason='Sparse tensors are not supported in cntk '\n                           'and Theano has some dependency issues for sparse.')\ndef test_sparse_dot(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n\n    x_sparse = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n    x_dense = x_sparse.toarray()\n\n    W = np.random.random((5, 4))\n    t_W = K.variable(W)\n    k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n    k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n\n    assert k_s.shape == k_d.shape\n    assert_allclose(k_s, k_d, atol=1e-05)",
                "@pytest.mark.skipif(not supports_sparse,\n                    reason='Sparse tensors are not supported in cntk '\n                           'and Theano has some dependency issues for sparse.')\ndef test_sparse_concat(self):\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n\n    x_sparse_1 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n\n    x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n    x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n    x_c = np.array([4, 3, 2, 3], dtype=np.int64)\n\n    x_sparse_2 = sparse.csr_matrix((x_d, (x_r, x_c)), shape=(4, 5))\n\n    x_dense_1 = x_sparse_1.toarray()\n    x_dense_2 = x_sparse_2.toarray()\n\n    k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n    assert K.is_sparse(k_s)\n\n    k_s_d = K.eval(k_s)\n\n    k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n\n    assert k_s_d.shape == k_d.shape\n    assert_allclose(k_s_d, k_d, atol=1e-05)",
                "def test_stack(self):\n    tensor_list = [np.random.randn(5, 4, 6, 10) for _ in range(5)]\n    stack_axis = 3\n    results = []\n    if WITH_NP[0] == KC:\n        check_two_tensor_operation('stack', (5, 4, 6, 10),\n                                   (5, 4, 6, 10), WITH_NP,\n                                   axis=stack_axis, concat_args=True)\n    else:\n        for k in WITH_NP:\n            tensor_list_var = [k.variable(tensor) for tensor in tensor_list]\n            out = k.eval(k.stack(tensor_list_var, axis=stack_axis))\n            results.append(out)\n\n        assert_list_pairwise(results)",
                "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_map(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    vx = K.variable(x)\n    kx = K.eval(K.map_fn(K.sum, vx))\n    # make sure we can also walk the indexes in tensorflow which we\n    # can't without specifying dtype\n    kx2 = K.eval(K.map_fn(\n        lambda i: K.sum(vx[i]),\n        K.arange(10),\n        dtype=K.floatx()\n    ))\n\n    assert (10,) == kx.shape\n    assert (10,) == kx2.shape\n    assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n    assert_allclose(kx, kx2, atol=1e-05)",
                "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldl(self):\n    x = np.random.rand(10, 3).astype(np.float32)\n    kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n\n    assert (3,) == kx.shape\n    assert_allclose(x.sum(axis=0), kx, atol=1e-05)",
                "@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_foldr(self):\n    # This test aims to make sure that we walk the array from right to left\n    # and checks it in the following way: multiplying left to right 1e-40\n    # cannot be held into a float32 so it causes an underflow while from\n    # right to left we have no such problem and the result is larger\n    x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n    vx = K.variable(x)\n    p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n    p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n\n    assert p1 < p2\n    assert 9e-38 < p2 <= 1e-37",
                "@pytest.mark.skipif(K.backend() == 'cntk',\n                    reason='cntk has issues with negative number.')\ndef test_arange(self):\n    for test_value in (-20, 0, 1, 10):\n        a_list = []\n        dtype_list = []\n        for k in WITH_NP:\n            t = k.arange(test_value)\n            a = k.eval(t)\n            assert np.array_equal(a, np.arange(test_value))\n            dtype_list.append(k.dtype(t))\n            a_list.append(a)\n\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n\n    for start, stop, step in ((0, 5, 1), (-5, 5, 2), (0, 1, 2)):\n        a_list = []\n        for k in WITH_NP:\n            a = k.eval(k.arange(start, stop, step))\n            assert np.array_equal(a, np.arange(start, stop, step))\n            a_list.append(a)\n        for i in range(len(a_list) - 1):\n            assert np.array_equal(a_list[i], a_list[i + 1])\n\n    for dtype in ('int32', 'int64', 'float32', 'float64'):\n        for k in WITH_NP:\n            t = k.arange(10, dtype=dtype)\n            assert k.dtype(t) == dtype\n\n    start = K.constant(1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 1\n\n    start = K.constant(-1, dtype='int32')\n    t = K.arange(start)\n    assert len(K.eval(t)) == 0",
                "@pytest.mark.parametrize('training', [True, False])\ndef test_in_train_phase(self, training):\n    check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), WITH_NP,\n                               training=training)\n    check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), WITH_NP,\n                               training=training)",
                "@pytest.mark.parametrize('training', [True, False])\ndef test_in_test_phase(self, training):\n    check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), WITH_NP,\n                               training=training)\n    check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), WITH_NP,\n                               training=training)",
                "def test_setfloatx_incorrect_values(self):\n    # Keep track of the old value\n    old_floatx = floatx()\n    # Try some incorrect values\n    initial = floatx()\n    for value in ['', 'beerfloat', 123]:\n        with pytest.raises(ValueError):\n            set_floatx(value)\n    assert floatx() == initial\n    # Restore old value\n    set_floatx(old_floatx)",
                "def DISABLED_test_setfloatx_correct_values(self):\n    \"\"\"Disabled because it is not thread safe at this time.\"\"\"\n    # Keep track of the old value\n    old_floatx = floatx()\n    # Check correct values\n    for value in ['float16', 'float32', 'float64']:\n        set_floatx(value)\n        assert floatx() == value\n    # Restore old value\n    set_floatx(old_floatx)",
                "@pytest.mark.skipif((K.backend() == 'cntk'),\n                    reason='cntk does not support float16')\ndef DISABLED_test_set_floatx(self):\n    \"\"\"Disabled because it is not thread safe at this time.\n\n    Make sure that changes to the global floatx are effectively\n    taken into account by the backend.\n    \"\"\"\n    # Keep track of the old value\n    old_floatx = floatx()\n\n    set_floatx('float16')\n    var = variable([10])\n    check_dtype(var, 'float16')\n\n    set_floatx('float64')\n    var = variable([10])\n    check_dtype(var, 'float64')\n\n    # Restore old value\n    set_floatx(old_floatx)",
                "def test_dtype(self):\n    assert K.dtype(K.variable(1, dtype='float64')) == 'float64'\n    assert K.dtype(K.variable(1, dtype='float32')) == 'float32'\n    assert K.dtype(K.variable(1, dtype='float16')) == 'float16'",
                "def test_variable_support_bool_dtype(self):\n    # Github issue: 7819\n    if K.backend() == 'tensorflow':\n        assert K.dtype(K.variable(1, dtype='int16')) == 'int16'\n        assert K.dtype(K.variable(False, dtype='bool')) == 'bool'\n        with pytest.raises(TypeError):\n            K.variable('', dtype='unsupported')",
                "def test_clip_supports_tensor_arguments(self):\n    # GitHub issue: 11435\n    x = K.variable([-10., -5., 0., 5., 10.])\n    min_value = K.variable([-5., -4., 0., 3., 5.])\n    max_value = K.variable([5., 4., 1., 4., 9.])\n\n    assert np.allclose(K.eval(K.clip(x, min_value, max_value)),\n                       np.asarray([-5., -4., 0., 4., 9.],\n                                  dtype=np.float32))",
                "@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                    reason='This test is for tensorflow parallelism.')\ndef test_tensorflow_session_parallelism_settings(self, monkeypatch):\n    for threads in [1, 2]:\n        K.clear_session()\n        monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n        cfg = K.get_session()._config\n        assert cfg.intra_op_parallelism_threads == threads\n        assert cfg.inter_op_parallelism_threads == threads",
                "def batch_shape(shape):\n    return (batch_size, ) + shape[1:]",
                "def random(shape):\n    return np.random.random(batch_shape(shape))",
                "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn(inputs, states):\n        assert len(states) == 1\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return y, [y]\n\n    return simple_rnn",
                "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_with_extra_mock_state(inputs, states):\n        assert len(states) == 2\n        h = states[0]\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n        return y, [y, backend.concatenate([y, y], axis=-1)]\n\n    return simple_rnn_with_extra_mock_state",
                "def get_step_function(backend, w_i):\n\n    def simple_no_states(inputs, states):\n        assert len(states) == 0\n        y = backend.dot(inputs, w_i)\n        return y, []\n\n    return simple_no_states",
                "def get_step_function(backend, w_i, w_h):\n\n    def simple_rnn_add_constant(inputs, states_and_constants):\n        # constants are appended to states in K.rnn\n        [h, c] = states_and_constants\n        y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n        return y, [y]\n\n    return simple_rnn_add_constant",
                "def step_function(inputs, states):\n    return inputs, [s + 1 for s in states]",
                "def step_function(inputs, states):\n    outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])\n    return outputs, states",
                "def step_function(inputs, states):\n    return inputs, [s + 1 for s in states]",
                "def decode(merge_repeated):\n    input_prob_tensor = K.placeholder(shape=(None, None, None),\n                                      dtype='float32')\n    input_len_tensor = K.placeholder(shape=(None), dtype='int64')\n    paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor,\n                                    greedy=False, beam_width=1, top_paths=1,\n                                    merge_repeated=merge_repeated)\n    decode_func = K.function([input_prob_tensor, input_len_tensor],\n                             paths_tensors)\n    paths = decode_func([input_prob, input_len])\n    return paths",
                "def simple_rnn(inputs, states):\n    assert len(states) == 1\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return y, [y]",
                "def simple_rnn_with_extra_mock_state(inputs, states):\n    assert len(states) == 2\n    h = states[0]\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n    return y, [y, backend.concatenate([y, y], axis=-1)]",
                "def simple_no_states(inputs, states):\n    assert len(states) == 0\n    y = backend.dot(inputs, w_i)\n    return y, []",
                "def simple_rnn_add_constant(inputs, states_and_constants):\n    # constants are appended to states in K.rnn\n    [h, c] = states_and_constants\n    y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c\n    return y, [y]"
            ],
            "inscope_function_signatures": [
                "check_dtype(var, dtype)",
                "cntk_func_tensors(function_name, shapes_or_vals, **kwargs)",
                "parse_shape_or_val(shape_or_val)",
                "assert_list_pairwise(z_list, shape=True, allclose=True, itself=False, atol=1e-05)",
                "assert_list_keras_shape(t_list, z_list)",
                "check_single_tensor_operation(function_name, x_shape_or_val, backend_list, **kwargs)",
                "check_two_tensor_operation(function_name, x_shape_or_val, y_shape_or_val, backend_list, **kwargs)",
                "check_composed_tensor_operations(first_function_name, first_function_args, second_function_name, second_function_args, input_shape, backend_list)",
                "check_rnn_operation(step_function_k, step_function_np, inputs_np, initial_states_np, mask_np=None, constants_np=None, **kwargs)",
                "test_is_keras_tensor(self)",
                "test_set_learning_phase(self)",
                "test_eye(self)",
                "test_ones(self)",
                "test_zeros(self)",
                "test_ones_like(self)",
                "test_zeros_like(self)",
                "test_linear_operations(self)",
                "test_random_variables(self)",
                "test_batch_dot_shape(self)",
                "test_shape_operations(self)",
                "test_none_shape_operations(self)",
                "test_repeat_elements(self)",
                "test_tile(self)",
                "test_gather(self)",
                "test_value_manipulation(self, function_name)",
                "test_print_tensor(self)",
                "test_elementwise_operations(self)",
                "test_reset_uids(self)",
                "test_cumsum_cumprod(self)",
                "test_log(self)",
                "test_update_add(self)",
                "test_update_sub(self)",
                "test_gradient(self)",
                "test_stop_gradient(self)",
                "test_function(self)",
                "test_function_tf_fetches(self)",
                "test_function_tf_feed_dict(self)",
                "test_function_tf_run_options_with_run_metadata(self)",
                "test_function_tf_string_input(self)",
                "test_rnn(self)",
                "test_rnn_additional_states(self)",
                "test_rnn_no_states(self)",
                "test_rnn_constants(self)",
                "test_rnn_output_and_state_masking_independent(self)",
                "test_rnn_output_num_dim_larger_than_2_masking(self)",
                "test_rnn_state_num_dim_larger_than_2_masking(self)",
                "test_logsumexp(self, x_np, axis, keepdims)",
                "test_logsumexp_optim(self)",
                "test_switch(self)",
                "test_dropout(self)",
                "test_relu(self, alpha, max_value, threshold)",
                "test_nn_operations(self)",
                "test_in_top_k(self)",
                "test_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                "test_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format)",
                "test_dilated_conv(self, op, input_shape, kernel_shape, padding, data_format, dilation_rate)",
                "test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape, padding, data_format, dilation_rate)",
                "test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format)",
                "test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode)",
                "test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format)",
                "test_random_normal(self)",
                "test_random_uniform(self)",
                "test_random_binomial(self)",
                "test_truncated_normal(self)",
                "test_conv_invalid_use(self)",
                "test_pooling_invalid_use(self)",
                "test_resize_images(self)",
                "_helper_bilinear(data_format, height_factor, width_factor)",
                "test_resize_images_bilinear(self, data_format)",
                "test_resize_volumes(self)",
                "test_temporal_padding(self)",
                "test_spatial_2d_padding(self)",
                "test_spatial_3d_padding(self)",
                "test_bias_add(self)",
                "test_batchnorm_th(self, x_shape)",
                "test_batchnorm_tf(self, x_shape)",
                "test_batchnorm_cntk(self, x_shape)",
                "test_ctc(self)",
                "test_ctc_decode_greedy(self)",
                "test_slice(self, x_size)",
                "test_ctc_decode_beam_search(self)",
                "test_ctc_decode_beam_search_no_merge(self)",
                "test_one_hot(self)",
                "test_sparse_dot(self)",
                "test_sparse_concat(self)",
                "test_stack(self)",
                "test_map(self)",
                "test_foldl(self)",
                "test_foldr(self)",
                "test_arange(self)",
                "test_in_train_phase(self, training)",
                "test_in_test_phase(self, training)",
                "test_setfloatx_incorrect_values(self)",
                "DISABLED_test_setfloatx_correct_values(self)",
                "DISABLED_test_set_floatx(self)",
                "test_dtype(self)",
                "test_variable_support_bool_dtype(self)",
                "test_clip_supports_tensor_arguments(self)",
                "test_tensorflow_session_parallelism_settings(self, monkeypatch)",
                "batch_shape(shape)",
                "random(shape)",
                "get_step_function(backend, w_i, w_h)",
                "get_step_function(backend, w_i, w_h)",
                "get_step_function(backend, w_i)",
                "get_step_function(backend, w_i, w_h)",
                "step_function(inputs, states)",
                "step_function(inputs, states)",
                "step_function(inputs, states)",
                "decode(merge_repeated)",
                "simple_rnn(inputs, states)",
                "simple_rnn_with_extra_mock_state(inputs, states)",
                "simple_no_states(inputs, states)",
                "simple_rnn_add_constant(inputs, states_and_constants)"
            ],
            "variables_in_file": {
                "ImportError": [
                    27,
                    21,
                    15
                ],
                "KC": [
                    65,
                    34,
                    67,
                    68,
                    1253,
                    1953,
                    1226,
                    16,
                    147,
                    116,
                    150,
                    62
                ],
                "warnings.warn": [
                    17,
                    29,
                    23
                ],
                "warnings": [
                    17,
                    29,
                    23
                ],
                "KTF": [
                    641,
                    610,
                    1185,
                    36,
                    613,
                    1508,
                    684,
                    1580,
                    2125,
                    1170,
                    22,
                    662,
                    1558
                ],
                "KTH": [
                    32,
                    641,
                    610,
                    1185,
                    1508,
                    613,
                    40,
                    1580,
                    1170,
                    1558,
                    153,
                    28
                ],
                "K.backend": [
                    1792,
                    399,
                    1685,
                    662,
                    1047,
                    1692,
                    31,
                    2080,
                    33,
                    1443,
                    38,
                    40,
                    425,
                    42,
                    684,
                    1452,
                    302,
                    1965,
                    50,
                    1715,
                    566,
                    1720,
                    441,
                    957,
                    2109,
                    575,
                    1855,
                    1983,
                    582,
                    1991,
                    1353,
                    1226,
                    715,
                    2125,
                    464,
                    1617,
                    594,
                    2005,
                    1628,
                    606,
                    991,
                    354,
                    740,
                    1253,
                    1639,
                    1514,
                    1774,
                    1138,
                    1653,
                    1655,
                    637
                ],
                "K": [
                    1538,
                    1042,
                    1043,
                    1558,
                    1047,
                    1559,
                    1560,
                    1561,
                    1939,
                    1054,
                    31,
                    1566,
                    33,
                    2080,
                    38,
                    40,
                    42,
                    1580,
                    1581,
                    1582,
                    1583,
                    561,
                    50,
                    562,
                    563,
                    564,
                    566,
                    1588,
                    2103,
                    2104,
                    2105,
                    2109,
                    2110,
                    575,
                    2111,
                    2113,
                    2117,
                    582,
                    2118,
                    2119,
                    2121,
                    586,
                    1099,
                    1612,
                    1613,
                    590,
                    1615,
                    592,
                    1617,
                    594,
                    2129,
                    2131,
                    2125,
                    598,
                    1622,
                    1623,
                    1625,
                    602,
                    604,
                    1628,
                    606,
                    1633,
                    1634,
                    1636,
                    1639,
                    1643,
                    1644,
                    1646,
                    1138,
                    1653,
                    1655,
                    632,
                    633,
                    634,
                    635,
                    637,
                    1679,
                    1680,
                    1681,
                    1682,
                    1683,
                    1685,
                    662,
                    1692,
                    671,
                    672,
                    673,
                    674,
                    1965,
                    676,
                    679,
                    682,
                    684,
                    1709,
                    1710,
                    1711,
                    1712,
                    1713,
                    1715,
                    1969,
                    693,
                    694,
                    695,
                    696,
                    1720,
                    700,
                    704,
                    1972,
                    707,
                    197,
                    198,
                    1973,
                    200,
                    713,
                    1226,
                    715,
                    204,
                    1974,
                    719,
                    720,
                    1975,
                    725,
                    218,
                    733,
                    1761,
                    1762,
                    1763,
                    228,
                    229,
                    230,
                    740,
                    1253,
                    745,
                    746,
                    1768,
                    748,
                    1769,
                    1774,
                    244,
                    246,
                    247,
                    248,
                    249,
                    1787,
                    1788,
                    765,
                    254,
                    766,
                    1792,
                    785,
                    1823,
                    1826,
                    807,
                    808,
                    302,
                    1840,
                    1848,
                    827,
                    1851,
                    1855,
                    1353,
                    845,
                    1359,
                    1360,
                    1361,
                    1876,
                    341,
                    342,
                    343,
                    1368,
                    345,
                    1878,
                    1879,
                    1882,
                    349,
                    861,
                    1376,
                    1377,
                    354,
                    355,
                    356,
                    357,
                    359,
                    1384,
                    1898,
                    364,
                    1390,
                    1391,
                    369,
                    370,
                    371,
                    882,
                    373,
                    883,
                    1398,
                    377,
                    1913,
                    1914,
                    1404,
                    1405,
                    1915,
                    902,
                    1415,
                    399,
                    1426,
                    1427,
                    1428,
                    405,
                    1429,
                    407,
                    1430,
                    1431,
                    1432,
                    411,
                    1435,
                    1940,
                    1438,
                    1942,
                    1944,
                    1441,
                    1443,
                    1445,
                    425,
                    1449,
                    428,
                    429,
                    942,
                    431,
                    943,
                    944,
                    946,
                    1452,
                    1454,
                    1457,
                    1460,
                    1463,
                    1968,
                    441,
                    442,
                    954,
                    444,
                    955,
                    957,
                    447,
                    1469,
                    1472,
                    1474,
                    1986,
                    964,
                    1476,
                    1983,
                    1479,
                    1991,
                    1481,
                    1483,
                    1998,
                    1999,
                    464,
                    465,
                    466,
                    467,
                    977,
                    978,
                    979,
                    981,
                    2000,
                    2005,
                    989,
                    1501,
                    991,
                    1514,
                    1008,
                    1009,
                    1010,
                    2035,
                    1012,
                    2036,
                    2037,
                    2039,
                    2040,
                    2041,
                    1021
                ],
                "WITH_NP": [
                    512,
                    514,
                    515,
                    2052,
                    517,
                    518,
                    519,
                    1543,
                    521,
                    522,
                    523,
                    1545,
                    525,
                    526,
                    527,
                    1529,
                    529,
                    530,
                    1555,
                    532,
                    533,
                    535,
                    536,
                    537,
                    538,
                    540,
                    541,
                    542,
                    543,
                    32,
                    34,
                    546,
                    36,
                    547,
                    550,
                    551,
                    552,
                    553,
                    554,
                    555,
                    556,
                    557,
                    1061,
                    1577,
                    2045,
                    1074,
                    569,
                    570,
                    1083,
                    572,
                    573,
                    1600,
                    1091,
                    580,
                    1608,
                    1115,
                    1119,
                    1120,
                    1121,
                    2054,
                    1123,
                    1124,
                    1125,
                    1127,
                    1128,
                    1129,
                    1132,
                    1140,
                    1147,
                    1150,
                    1152,
                    1154,
                    1155,
                    1203,
                    1222,
                    1249,
                    1272,
                    257,
                    261,
                    265,
                    1289,
                    269,
                    273,
                    276,
                    277,
                    280,
                    282,
                    284,
                    286,
                    288,
                    290,
                    292,
                    294,
                    296,
                    1321,
                    298,
                    300,
                    301,
                    303,
                    306,
                    310,
                    381,
                    384,
                    385,
                    387,
                    388,
                    389,
                    391,
                    392,
                    393,
                    394,
                    397,
                    1953,
                    1955,
                    422,
                    1958,
                    436,
                    437,
                    455,
                    457,
                    1493,
                    2011,
                    476,
                    484,
                    485,
                    486,
                    487,
                    2023,
                    490,
                    491,
                    492,
                    494,
                    495,
                    496,
                    2031,
                    498,
                    499,
                    501,
                    502,
                    504,
                    505,
                    506,
                    508,
                    509,
                    510,
                    2047
                ],
                "KNP": [
                    32,
                    34,
                    36,
                    903,
                    1351,
                    1897,
                    208,
                    338,
                    786,
                    828,
                    1789,
                    862,
                    1759
                ],
                "supports_sparse": [
                    1920,
                    39,
                    41,
                    44,
                    1901,
                    46
                ],
                "KTH.th_sparse_module": [
                    40
                ],
                "var.dtype": [
                    51,
                    53
                ],
                "var": [
                    2092,
                    2093,
                    2096,
                    2097,
                    51,
                    53
                ],
                "dtype": [
                    2030,
                    2032,
                    2033,
                    51,
                    53
                ],
                "var.dtype.name": [
                    53
                ],
                "placeholders": [
                    57,
                    67,
                    68,
                    62
                ],
                "variables": [
                    65,
                    58,
                    67
                ],
                "shape_or_val": [
                    64,
                    73,
                    74,
                    105,
                    76,
                    109,
                    115,
                    59,
                    60,
                    61
                ],
                "shapes_or_vals": [
                    59
                ],
                "isinstance": [
                    73,
                    60
                ],
                "tuple": [
                    60,
                    430
                ],
                "shape": [
                    1550,
                    1552,
                    1554,
                    418,
                    419,
                    1572,
                    1574,
                    1576,
                    426,
                    427,
                    428,
                    430,
                    434,
                    435,
                    1593,
                    1595,
                    61,
                    62,
                    1597,
                    451,
                    452,
                    330,
                    333,
                    1487,
                    1489,
                    1491,
                    85,
                    1523,
                    1525,
                    1527
                ],
                "placeholders.append": [
                    62
                ],
                "KC.placeholder": [
                    62
                ],
                "value": [
                    64,
                    65,
                    2062,
                    2064,
                    2074,
                    2075,
                    2076
                ],
                "variables.append": [
                    65
                ],
                "KC.variable": [
                    65
                ],
                "output_cntk": [
                    67,
                    68,
                    69
                ],
                "getattr": [
                    162,
                    67,
                    1359,
                    184,
                    475,
                    183,
                    120,
                    154,
                    123,
                    158
                ],
                "function_name": [
                    162,
                    67,
                    478,
                    148,
                    117,
                    475,
                    151,
                    120,
                    153,
                    154,
                    123,
                    158
                ],
                "kwargs": [
                    901,
                    137,
                    138,
                    139,
                    906,
                    908,
                    784,
                    148,
                    789,
                    790,
                    151,
                    155,
                    159,
                    163,
                    826,
                    831,
                    832,
                    67,
                    214,
                    860,
                    226,
                    866,
                    105,
                    106,
                    107,
                    117,
                    120,
                    123
                ],
                "cntk_func": [
                    1354,
                    68,
                    69,
                    1357
                ],
                "KC.function": [
                    68
                ],
                "np.ndarray": [
                    73
                ],
                "np": [
                    1024,
                    1025,
                    1026,
                    1027,
                    1028,
                    1029,
                    1030,
                    1031,
                    1032,
                    1033,
                    1034,
                    1035,
                    1036,
                    1044,
                    1564,
                    1053,
                    1059,
                    1073,
                    1536,
                    1586,
                    1081,
                    1087,
                    1095,
                    73,
                    585,
                    587,
                    76,
                    1612,
                    1613,
                    2121,
                    2122,
                    2123,
                    597,
                    1621,
                    599,
                    1632,
                    609,
                    1642,
                    1141,
                    1143,
                    631,
                    1144,
                    1146,
                    1661,
                    1662,
                    1665,
                    642,
                    643,
                    1666,
                    1162,
                    1163,
                    1677,
                    1177,
                    1180,
                    1697,
                    1698,
                    1700,
                    1701,
                    1707,
                    178,
                    1727,
                    1734,
                    1739,
                    1746,
                    1749,
                    1754,
                    1757,
                    1771,
                    1772,
                    242,
                    1783,
                    763,
                    1790,
                    1801,
                    1809,
                    1815,
                    1817,
                    1820,
                    802,
                    1826,
                    1828,
                    805,
                    1833,
                    1835,
                    1851,
                    1853,
                    1862,
                    843,
                    333,
                    1873,
                    1371,
                    1372,
                    1888,
                    1377,
                    1378,
                    1379,
                    1890,
                    1896,
                    1897,
                    1386,
                    1387,
                    1388,
                    1899,
                    1391,
                    880,
                    1392,
                    1393,
                    1394,
                    1905,
                    1906,
                    1907,
                    1400,
                    1401,
                    1402,
                    1912,
                    1405,
                    1406,
                    1407,
                    1408,
                    1924,
                    1925,
                    1926,
                    1418,
                    1419,
                    1420,
                    1930,
                    1931,
                    1423,
                    1932,
                    1426,
                    1427,
                    1428,
                    1429,
                    1430,
                    1431,
                    1432,
                    921,
                    923,
                    925,
                    1950,
                    418,
                    419,
                    1967,
                    435,
                    1469,
                    1985,
                    452,
                    967,
                    968,
                    969,
                    972,
                    1997,
                    474,
                    1499,
                    2014,
                    2019,
                    999,
                    1000,
                    1001,
                    2025,
                    2028
                ],
                "shape_or_val.shape": [
                    74
                ],
                "astype": [
                    1632,
                    1985,
                    1162,
                    1642,
                    76,
                    1967,
                    1621
                ],
                "np.random.random": [
                    1536,
                    642,
                    643,
                    1162,
                    921,
                    923,
                    1564,
                    1059,
                    1073,
                    178,
                    1586,
                    1081,
                    1469,
                    967,
                    968,
                    76,
                    333,
                    1612,
                    1613,
                    1621,
                    474,
                    1499,
                    1632,
                    609,
                    999,
                    1000,
                    1642,
                    631,
                    1912
                ],
                "np.random": [
                    1536,
                    642,
                    643,
                    1162,
                    1163,
                    921,
                    1177,
                    923,
                    1564,
                    1950,
                    1059,
                    805,
                    1967,
                    1073,
                    178,
                    1586,
                    1081,
                    1469,
                    1985,
                    967,
                    968,
                    585,
                    587,
                    76,
                    333,
                    843,
                    1612,
                    1613,
                    597,
                    1621,
                    599,
                    474,
                    1499,
                    1632,
                    609,
                    999,
                    1000,
                    1896,
                    1642,
                    880,
                    631,
                    1912,
                    763
                ],
                "np.float32": [
                    1924,
                    1930,
                    1677,
                    1809,
                    1817,
                    1833,
                    1707,
                    1967,
                    1985,
                    1734,
                    2123,
                    76,
                    1997,
                    1746,
                    1621,
                    1632,
                    1642,
                    1905,
                    1143,
                    1146
                ],
                "z1": [
                    88,
                    90,
                    84,
                    86
                ],
                "z2": [
                    88,
                    90,
                    84,
                    86
                ],
                "zip": [
                    613,
                    235,
                    659,
                    84,
                    1467,
                    94
                ],
                "z_list": [
                    128,
                    129,
                    1168,
                    145,
                    1171,
                    1183,
                    1186,
                    1060,
                    166,
                    168,
                    169,
                    1064,
                    1065,
                    1072,
                    180,
                    1077,
                    1078,
                    185,
                    1082,
                    187,
                    1084,
                    1086,
                    1087,
                    1089,
                    1092,
                    1094,
                    1095,
                    456,
                    459,
                    460,
                    84,
                    94,
                    611,
                    619,
                    622,
                    624,
                    113,
                    625,
                    126
                ],
                "z1.shape": [
                    86
                ],
                "z2.shape": [
                    86
                ],
                "allclose": [
                    87
                ],
                "assert_allclose": [
                    1043,
                    1686,
                    1688,
                    1947,
                    1055,
                    1716,
                    1718,
                    954,
                    955,
                    1980,
                    1981,
                    1989,
                    592,
                    1363,
                    88,
                    604,
                    989,
                    351,
                    232,
                    233,
                    236,
                    366,
                    625,
                    378,
                    1021,
                    1918
                ],
                "atol": [
                    88
                ],
                "itself": [
                    89
                ],
                "t": [
                    148,
                    151,
                    1815,
                    1816,
                    154,
                    2040,
                    156,
                    2041,
                    158,
                    160,
                    162,
                    164,
                    165,
                    1749,
                    1750,
                    1751,
                    2012,
                    2013,
                    94,
                    95,
                    96,
                    2015,
                    98,
                    2032,
                    2033,
                    2036,
                    117,
                    2037,
                    120,
                    121,
                    123,
                    124,
                    125
                ],
                "z": [
                    149,
                    152,
                    156,
                    160,
                    164,
                    166,
                    184,
                    185,
                    1099,
                    343,
                    1623,
                    345,
                    1625,
                    1626,
                    349,
                    94,
                    98,
                    1634,
                    1636,
                    357,
                    1637,
                    359,
                    377,
                    364,
                    1644,
                    1646,
                    1647,
                    371,
                    373,
                    118,
                    121,
                    378,
                    124,
                    126
                ],
                "t_list": [
                    129,
                    165,
                    454,
                    169,
                    460,
                    112,
                    144,
                    125,
                    94
                ],
                "hasattr": [
                    408,
                    412,
                    95
                ],
                "len": [
                    771,
                    813,
                    1846,
                    1470,
                    1086,
                    1349,
                    1094,
                    850,
                    731,
                    95,
                    738,
                    355,
                    356,
                    2018,
                    1766,
                    361,
                    234,
                    2027,
                    624,
                    2037,
                    2041
                ],
                "t._keras_shape": [
                    96,
                    98,
                    95
                ],
                "i": [
                    96,
                    1851,
                    98,
                    2018,
                    2019,
                    1094,
                    1095,
                    2027,
                    2028,
                    624,
                    625,
                    1973,
                    1176,
                    1850,
                    1179,
                    1086,
                    1087
                ],
                "s": [
                    96,
                    97,
                    228,
                    997,
                    198,
                    1071,
                    1073,
                    919
                ],
                "enumerate": [
                    96
                ],
                "z.shape": [
                    98,
                    1637,
                    1626,
                    1647
                ],
                "kwargs.pop": [
                    137,
                    105,
                    106,
                    107,
                    138,
                    139,
                    906,
                    789,
                    831
                ],
                "assert_value_equality": [
                    128,
                    106
                ],
                "cntk_dynamicity": [
                    138,
                    107,
                    116,
                    147
                ],
                "x_shape": [
                    1536,
                    141,
                    1552,
                    1554,
                    1555,
                    148,
                    151,
                    1564,
                    1574,
                    1576,
                    1577,
                    1586,
                    1595,
                    1597,
                    1599,
                    1604,
                    1606,
                    1607,
                    1612,
                    335,
                    336,
                    1489,
                    1491,
                    1492,
                    341,
                    1621,
                    1626,
                    1499,
                    1632,
                    1506,
                    355,
                    1507,
                    1637,
                    1642,
                    1643,
                    110,
                    1647,
                    117,
                    1525,
                    1527,
                    1528
                ],
                "x_val": [
                    1632,
                    1633,
                    163,
                    1642,
                    141,
                    110,
                    1646,
                    149,
                    118,
                    1621,
                    120,
                    1622,
                    155,
                    152,
                    159
                ],
                "parse_shape_or_val": [
                    141,
                    142,
                    800,
                    801,
                    803,
                    804,
                    1346,
                    1347,
                    1349,
                    841,
                    842,
                    875,
                    876,
                    877,
                    110,
                    878,
                    879,
                    759,
                    760,
                    761,
                    762
                ],
                "x_shape_or_val": [
                    123,
                    141,
                    110
                ],
                "k": [
                    647,
                    648,
                    650,
                    651,
                    653,
                    1167,
                    1169,
                    146,
                    147,
                    659,
                    150,
                    153,
                    154,
                    155,
                    156,
                    158,
                    159,
                    160,
                    1182,
                    162,
                    163,
                    164,
                    1061,
                    1062,
                    1063,
                    1064,
                    1184,
                    1958,
                    1959,
                    1960,
                    1074,
                    1075,
                    1076,
                    181,
                    182,
                    183,
                    184,
                    1077,
                    1082,
                    1083,
                    1089,
                    1091,
                    454,
                    455,
                    456,
                    457,
                    475,
                    476,
                    2011,
                    2012,
                    2013,
                    2015,
                    610,
                    613,
                    614,
                    615,
                    616,
                    617,
                    618,
                    619,
                    620,
                    2023,
                    2024,
                    2031,
                    2032,
                    2033,
                    114,
                    116,
                    120,
                    121,
                    123,
                    124
                ],
                "backend_list": [
                    114,
                    181,
                    146
                ],
                "f": [
                    653,
                    654,
                    656,
                    148,
                    149,
                    151,
                    152,
                    676,
                    680,
                    700,
                    705,
                    711,
                    725,
                    729,
                    349,
                    733,
                    351,
                    736,
                    364,
                    748,
                    366,
                    749,
                    117,
                    118
                ],
                "cntk_func_tensors": [
                    1354,
                    148,
                    117,
                    151
                ],
                "k.variable": [
                    1089,
                    610,
                    163,
                    454,
                    1062,
                    456,
                    648,
                    1959,
                    1075,
                    182,
                    475,
                    120,
                    1082,
                    155,
                    159
                ],
                "k.eval": [
                    160,
                    1089,
                    164,
                    456,
                    1064,
                    1960,
                    619,
                    620,
                    2024,
                    156,
                    1077,
                    184,
                    121,
                    1082,
                    124,
                    2013
                ],
                "assert_list_pairwise": [
                    128,
                    481,
                    1186,
                    1092,
                    168,
                    1065,
                    459,
                    1963,
                    622,
                    623,
                    657,
                    1171,
                    660,
                    1078,
                    187,
                    1084,
                    479
                ],
                "assert_list_keras_shape": [
                    129,
                    169,
                    460
                ],
                "concat_args": [
                    137,
                    157
                ],
                "cntk_two_dynamicity": [
                    139,
                    150
                ],
                "y_shape": [
                    356,
                    142,
                    335,
                    337,
                    342,
                    151
                ],
                "y_val": [
                    163,
                    142,
                    148,
                    152,
                    155,
                    159
                ],
                "y_shape_or_val": [
                    142
                ],
                "convert_kernel": [
                    155
                ],
                "val": [
                    609,
                    610,
                    642,
                    1059,
                    1082,
                    1062,
                    1089,
                    648,
                    1090,
                    1099,
                    178,
                    182,
                    631,
                    632,
                    1081,
                    474,
                    475
                ],
                "input_shape": [
                    1249,
                    1345,
                    1346,
                    1222,
                    1289,
                    1321,
                    1355,
                    178,
                    1203,
                    1272,
                    1467,
                    1469,
                    1343
                ],
                "x": [
                    1042,
                    1043,
                    1559,
                    1560,
                    1062,
                    1063,
                    1064,
                    1581,
                    1582,
                    1075,
                    1076,
                    2117,
                    585,
                    586,
                    2121,
                    1612,
                    589,
                    1615,
                    592,
                    597,
                    598,
                    1622,
                    1624,
                    601,
                    604,
                    1633,
                    1635,
                    613,
                    614,
                    1643,
                    1645,
                    1646,
                    648,
                    649,
                    651,
                    652,
                    653,
                    659,
                    671,
                    678,
                    682,
                    693,
                    182,
                    183,
                    702,
                    707,
                    713,
                    759,
                    787,
                    800,
                    829,
                    1346,
                    1351,
                    841,
                    1357,
                    1360,
                    341,
                    343,
                    349,
                    863,
                    355,
                    357,
                    875,
                    364,
                    369,
                    371,
                    904,
                    405,
                    407,
                    411,
                    428,
                    429,
                    1967,
                    1968,
                    442,
                    444,
                    1469,
                    1980,
                    447,
                    1472,
                    1985,
                    1474,
                    1986,
                    1476,
                    1989,
                    1479,
                    1481,
                    1483,
                    1997,
                    1998,
                    465,
                    467
                ],
                "y": [
                    773,
                    774,
                    650,
                    651,
                    653,
                    407,
                    408,
                    409,
                    1560,
                    411,
                    412,
                    413,
                    1561,
                    672,
                    679,
                    682,
                    429,
                    430,
                    431,
                    815,
                    816,
                    1582,
                    1583,
                    694,
                    183,
                    184,
                    444,
                    445,
                    447,
                    448,
                    704,
                    707,
                    713,
                    467,
                    468,
                    851,
                    342,
                    343,
                    852,
                    349,
                    356,
                    357,
                    364,
                    370,
                    371,
                    890,
                    891
                ],
                "first_function_name": [
                    183
                ],
                "first_function_args": [
                    183
                ],
                "second_function_name": [
                    184
                ],
                "second_function_args": [
                    184
                ],
                "inputs_k": [
                    220,
                    197
                ],
                "K.variable": [
                    1538,
                    1042,
                    1054,
                    1566,
                    1588,
                    2103,
                    2104,
                    2105,
                    2110,
                    2111,
                    2113,
                    2117,
                    2118,
                    2119,
                    586,
                    1099,
                    1612,
                    1613,
                    598,
                    1622,
                    1633,
                    632,
                    1679,
                    1680,
                    1681,
                    1682,
                    671,
                    672,
                    1709,
                    1710,
                    1711,
                    1712,
                    693,
                    694,
                    197,
                    198,
                    200,
                    204,
                    1761,
                    1762,
                    246,
                    765,
                    766,
                    1823,
                    1826,
                    807,
                    808,
                    845,
                    1360,
                    1361,
                    1898,
                    369,
                    370,
                    882,
                    883,
                    1913,
                    1914,
                    1915,
                    1426,
                    1427,
                    1428,
                    1429,
                    1430,
                    1431,
                    1432,
                    1939,
                    1944,
                    942,
                    943,
                    944,
                    1968,
                    1469,
                    1986,
                    1998,
                    977,
                    978,
                    979,
                    1501,
                    1008,
                    1009,
                    1010
                ],
                "inputs_np": [
                    225,
                    210,
                    197
                ],
                "initial_states_k": [
                    221,
                    198
                ],
                "initial_states_np": [
                    211,
                    198
                ],
                "mask_np": [
                    200,
                    212,
                    199
                ],
                "mask_k": [
                    200,
                    202,
                    222
                ],
                "constants_np": [
                    203,
                    204,
                    213
                ],
                "constants_k": [
                    204,
                    206,
                    223
                ],
                "c": [
                    907,
                    204,
                    877,
                    889,
                    634,
                    890
                ],
                "last_output_np": [
                    208,
                    232
                ],
                "output_np": [
                    208,
                    233
                ],
                "last_states_np": [
                    208,
                    234,
                    235
                ],
                "KNP.rnn": [
                    208
                ],
                "step_function_np": [
                    209
                ],
                "unroll": [
                    224,
                    225,
                    1018,
                    945,
                    1011,
                    980,
                    951,
                    952,
                    217,
                    986,
                    987,
                    1017
                ],
                "last_output_k": [
                    232,
                    218,
                    229
                ],
                "output_k": [
                    233,
                    218,
                    230
                ],
                "last_states_k": [
                    218,
                    234,
                    228,
                    235
                ],
                "K.rnn": [
                    946,
                    218,
                    1012,
                    981
                ],
                "step_function_k": [
                    219
                ],
                "inputs_np.shape": [
                    225
                ],
                "K.eval": [
                    1415,
                    1043,
                    1683,
                    1942,
                    1944,
                    1405,
                    1054,
                    2041,
                    1713,
                    1969,
                    1972,
                    1848,
                    954,
                    955,
                    1851,
                    1986,
                    2121,
                    590,
                    1359,
                    592,
                    1999,
                    2000,
                    1368,
                    1625,
                    602,
                    604,
                    989,
                    1377,
                    228,
                    229,
                    230,
                    1636,
                    1384,
                    1768,
                    1769,
                    1898,
                    1391,
                    2037,
                    1398,
                    377,
                    1914,
                    1915,
                    1788,
                    1021
                ],
                "s_k": [
                    235,
                    236
                ],
                "s_np": [
                    235,
                    236
                ],
                "object": [
                    239
                ],
                "np_var": [
                    242,
                    244,
                    246
                ],
                "np.array": [
                    1024,
                    1025,
                    1026,
                    1027,
                    1028,
                    1029,
                    1030,
                    1031,
                    1032,
                    1033,
                    1034,
                    1035,
                    1036,
                    1924,
                    1925,
                    1926,
                    1930,
                    1931,
                    1932,
                    1053,
                    1826,
                    1828,
                    1835,
                    1862,
                    1997,
                    1873,
                    1757,
                    1888,
                    1377,
                    1890,
                    1391,
                    1905,
                    242,
                    1906,
                    1907,
                    1783,
                    1405
                ],
                "pytest.raises": [
                    1537,
                    2063,
                    1434,
                    1437,
                    1565,
                    1440,
                    1444,
                    1448,
                    1453,
                    1456,
                    1459,
                    1587,
                    1462,
                    1471,
                    2112,
                    1473,
                    1475,
                    1478,
                    1480,
                    1098,
                    1482,
                    1614,
                    1500,
                    1518,
                    243,
                    253
                ],
                "pytest": [
                    1537,
                    2050,
                    2063,
                    1047,
                    1565,
                    2080,
                    1587,
                    566,
                    575,
                    2112,
                    582,
                    1098,
                    1101,
                    1614,
                    2125,
                    1617,
                    594,
                    1619,
                    2137,
                    1628,
                    606,
                    1630,
                    1639,
                    1640,
                    1653,
                    637,
                    662,
                    1188,
                    684,
                    1207,
                    1720,
                    1226,
                    715,
                    1228,
                    740,
                    1253,
                    1255,
                    1774,
                    1777,
                    243,
                    1276,
                    253,
                    1792,
                    1293,
                    1326,
                    1855,
                    1901,
                    1920,
                    399,
                    1434,
                    1437,
                    1440,
                    1444,
                    1448,
                    1453,
                    1965,
                    1456,
                    1459,
                    1462,
                    957,
                    1471,
                    1983,
                    1473,
                    1475,
                    1478,
                    1991,
                    1480,
                    1482,
                    2005,
                    470,
                    1500,
                    991,
                    1514,
                    1515,
                    1518,
                    2043,
                    1023
                ],
                "ValueError": [
                    1537,
                    2063,
                    1434,
                    1437,
                    1565,
                    1440,
                    1444,
                    1448,
                    1453,
                    1456,
                    1459,
                    1587,
                    1462,
                    1471,
                    1473,
                    1475,
                    1478,
                    1480,
                    1098,
                    1482,
                    1614,
                    1500,
                    243,
                    253
                ],
                "K.is_keras_tensor": [
                    249,
                    244,
                    247
                ],
                "keras_var": [
                    246,
                    247
                ],
                "keras_placeholder": [
                    248,
                    249
                ],
                "K.placeholder": [
                    405,
                    1559,
                    673,
                    674,
                    428,
                    1581,
                    695,
                    696,
                    442,
                    719,
                    720,
                    465,
                    466,
                    1876,
                    341,
                    342,
                    1878,
                    355,
                    356,
                    745,
                    1643,
                    248
                ],
                "K.set_learning_phase": [
                    254
                ],
                "check_single_tensor_operation": [
                    512,
                    513,
                    515,
                    517,
                    518,
                    519,
                    1528,
                    521,
                    522,
                    523,
                    1542,
                    525,
                    526,
                    527,
                    1544,
                    529,
                    530,
                    1555,
                    532,
                    533,
                    535,
                    536,
                    537,
                    538,
                    540,
                    541,
                    542,
                    543,
                    546,
                    547,
                    1577,
                    569,
                    570,
                    572,
                    573,
                    580,
                    1115,
                    1119,
                    1120,
                    1121,
                    1123,
                    1124,
                    1125,
                    1127,
                    1128,
                    1129,
                    1154,
                    1155,
                    257,
                    260,
                    264,
                    268,
                    272,
                    1320,
                    300,
                    301,
                    303,
                    306,
                    310,
                    384,
                    385,
                    387,
                    388,
                    389,
                    391,
                    392,
                    393,
                    394,
                    422,
                    436,
                    437,
                    1492,
                    1507,
                    484,
                    485,
                    486,
                    487,
                    490,
                    491,
                    492,
                    494,
                    495,
                    496,
                    498,
                    499,
                    500,
                    502,
                    504,
                    505,
                    506,
                    508,
                    509,
                    510
                ],
                "check_two_tensor_operation": [
                    2052,
                    2054,
                    1288,
                    276,
                    277,
                    1149,
                    279,
                    281,
                    283,
                    285,
                    287,
                    289,
                    1954,
                    291,
                    293,
                    550,
                    295,
                    551,
                    297,
                    552,
                    553,
                    554,
                    555,
                    556,
                    557,
                    2045,
                    1202,
                    2047,
                    1599,
                    1221,
                    1607,
                    1248,
                    1131,
                    1139,
                    1271,
                    1147,
                    381,
                    1151
                ],
                "test_cases": [
                    320,
                    321,
                    322,
                    323,
                    324,
                    325,
                    335,
                    319
                ],
                "test_cases.append": [
                    320,
                    321,
                    322,
                    323,
                    324,
                    325
                ],
                "batch_size": [
                    1158,
                    327,
                    1895,
                    1896,
                    330,
                    1163,
                    1162,
                    1176,
                    1180
                ],
                "batch_shape": [
                    333
                ],
                "axes": [
                    357,
                    335,
                    338,
                    371,
                    343
                ],
                "x_np": [
                    366,
                    336,
                    369,
                    338,
                    1042,
                    1044,
                    1053,
                    1054,
                    351
                ],
                "random": [
                    336,
                    337
                ],
                "y_np": [
                    366,
                    337,
                    338,
                    370,
                    351
                ],
                "z_np": [
                    361,
                    366,
                    338,
                    375,
                    378,
                    347,
                    351
                ],
                "KNP.batch_dot": [
                    338
                ],
                "K.batch_dot": [
                    371,
                    357,
                    343
                ],
                "z_shape": [
                    359,
                    360,
                    361,
                    362,
                    373,
                    374,
                    375,
                    345,
                    346,
                    347
                ],
                "K.int_shape": [
                    359,
                    345,
                    431,
                    1583,
                    373,
                    1561
                ],
                "z_np.shape": [
                    347,
                    375
                ],
                "K.function": [
                    676,
                    733,
                    364,
                    748,
                    1646,
                    725,
                    1882,
                    700,
                    349
                ],
                "z_np.ndim": [
                    361
                ],
                "set": [
                    362
                ],
                "check_composed_tensor_operations": [
                    395
                ],
                "K.batch_flatten": [
                    407
                ],
                "y._keras_shape": [
                    448,
                    430,
                    431,
                    468,
                    409,
                    445,
                    413
                ],
                "K.flatten": [
                    411
                ],
                "pytest.mark.skipif": [
                    1792,
                    1920,
                    399,
                    662,
                    1047,
                    2080,
                    684,
                    1965,
                    566,
                    1720,
                    957,
                    575,
                    1855,
                    1983,
                    582,
                    1991,
                    1226,
                    715,
                    2125,
                    1617,
                    594,
                    2005,
                    1628,
                    606,
                    991,
                    740,
                    1253,
                    1639,
                    1514,
                    1901,
                    1774,
                    1653,
                    637
                ],
                "pytest.mark": [
                    1792,
                    1920,
                    2050,
                    1293,
                    399,
                    662,
                    1047,
                    2080,
                    1188,
                    684,
                    1965,
                    1326,
                    566,
                    1207,
                    1720,
                    957,
                    575,
                    1855,
                    1983,
                    582,
                    1991,
                    1226,
                    715,
                    1228,
                    1101,
                    2125,
                    1617,
                    594,
                    1619,
                    2005,
                    470,
                    1628,
                    606,
                    991,
                    1630,
                    740,
                    1253,
                    1255,
                    1639,
                    1640,
                    1514,
                    1515,
                    1901,
                    1774,
                    1777,
                    1653,
                    2043,
                    1276,
                    637,
                    1023
                ],
                "reps": [
                    416,
                    429,
                    423
                ],
                "ndims": [
                    417,
                    418,
                    421
                ],
                "np.arange": [
                    418,
                    419,
                    452,
                    2025,
                    435,
                    2014
                ],
                "arr": [
                    419,
                    436,
                    435,
                    422
                ],
                "reshape": [
                    419,
                    452,
                    435
                ],
                "np.prod": [
                    419,
                    452,
                    435
                ],
                "rep_axis": [
                    427,
                    429,
                    421,
                    423
                ],
                "range": [
                    1377,
                    2018,
                    421,
                    1094,
                    2027,
                    1167,
                    624,
                    1391,
                    1816,
                    1950,
                    1751,
                    1176,
                    1850,
                    1182,
                    1405,
                    1086
                ],
                "list": [
                    1073,
                    426,
                    1090
                ],
                "K.repeat_elements": [
                    429
                ],
                "n": [
                    443,
                    444,
                    446,
                    447
                ],
                "K.tile": [
                    964,
                    444,
                    447
                ],
                "ref": [
                    452,
                    454,
                    456,
                    1688,
                    1716,
                    1686,
                    1718,
                    1656,
                    1658,
                    1693,
                    1695
                ],
                "inds": [
                    456,
                    453,
                    454
                ],
                "k.gather": [
                    456,
                    454
                ],
                "indices": [
                    1896,
                    1897,
                    1898,
                    466,
                    467
                ],
                "K.gather": [
                    467
                ],
                "v_list": [
                    481,
                    475,
                    479
                ],
                "pytest.mark.parametrize": [
                    2050,
                    1188,
                    1255,
                    1640,
                    1515,
                    1228,
                    1101,
                    1293,
                    1326,
                    1777,
                    1619,
                    470,
                    1207,
                    2043,
                    1276,
                    1630,
                    1023
                ],
                "first": [
                    561,
                    564
                ],
                "K.get_uid": [
                    561,
                    562,
                    564
                ],
                "K.reset_uids": [
                    563
                ],
                "np.random.randn": [
                    585,
                    587,
                    597,
                    599,
                    1950
                ],
                "x_var": [
                    586,
                    590,
                    592,
                    598,
                    602,
                    604
                ],
                "increment": [
                    587,
                    589,
                    590
                ],
                "K.update_add": [
                    590
                ],
                "decrement": [
                    601,
                    602,
                    599
                ],
                "K.update_sub": [
                    602
                ],
                "x_list": [
                    610,
                    613,
                    646,
                    649,
                    659
                ],
                "zero_list": [
                    620,
                    625,
                    612,
                    623
                ],
                "exp": [
                    614,
                    615,
                    617,
                    618,
                    651,
                    653
                ],
                "k.exp": [
                    614
                ],
                "loss": [
                    616,
                    617,
                    618,
                    615
                ],
                "k.sum": [
                    615
                ],
                "zero_loss": [
                    616,
                    618
                ],
                "k.stop_gradient": [
                    616
                ],
                "grad": [
                    617,
                    619
                ],
                "k.gradients": [
                    617,
                    618
                ],
                "zero_grad": [
                    618,
                    620
                ],
                "z_list.append": [
                    1064,
                    619,
                    1077
                ],
                "zero_list.append": [
                    620
                ],
                "a": [
                    2016,
                    1986,
                    2024,
                    2025,
                    2026,
                    1999,
                    2000,
                    632,
                    633,
                    634,
                    2013,
                    2014
                ],
                "b": [
                    1184,
                    1185,
                    1986,
                    1613,
                    1615,
                    1168,
                    1169,
                    1170,
                    1999,
                    2000,
                    633,
                    634,
                    635,
                    1183
                ],
                "K.square": [
                    633
                ],
                "d": [
                    634
                ],
                "K.stop_gradient": [
                    634,
                    635
                ],
                "e": [
                    635
                ],
                "test_backend": [
                    641,
                    659,
                    647
                ],
                "input_val": [
                    656,
                    643
                ],
                "f_list": [
                    656,
                    645,
                    654
                ],
                "x_list.append": [
                    649
                ],
                "k.placeholder": [
                    650
                ],
                "k.square": [
                    651
                ],
                "update": [
                    652,
                    653
                ],
                "k.function": [
                    653
                ],
                "f_list.append": [
                    654
                ],
                "function_outputs_list": [
                    656,
                    657
                ],
                "new_val_list": [
                    659,
                    660
                ],
                "k.get_value": [
                    659
                ],
                "x_placeholder": [
                    673,
                    676,
                    677,
                    678,
                    733,
                    745,
                    746,
                    748,
                    734,
                    719,
                    725,
                    726,
                    695,
                    700,
                    701,
                    702
                ],
                "y_placeholder": [
                    704,
                    674,
                    676,
                    677,
                    710,
                    720,
                    725,
                    726,
                    696,
                    698,
                    733,
                    734
                ],
                "K.update": [
                    704,
                    679
                ],
                "output": [
                    736,
                    705,
                    706,
                    737,
                    711,
                    680,
                    681,
                    712,
                    749,
                    750,
                    729,
                    730
                ],
                "run": [
                    713,
                    682,
                    707
                ],
                "K.get_session": [
                    713,
                    682,
                    707,
                    2131
                ],
                "KTF._is_tf_1": [
                    684,
                    2125,
                    662
                ],
                "feed_dict": [
                    698,
                    710,
                    703
                ],
                "run_options": [
                    722,
                    727
                ],
                "config_pb2.RunOptions": [
                    722
                ],
                "config_pb2": [
                    722,
                    723
                ],
                "run_metadata": [
                    738,
                    723,
                    728,
                    731,
                    735
                ],
                "config_pb2.RunMetadata": [
                    723
                ],
                "run_metadata.partition_graphs": [
                    738,
                    731
                ],
                "x_identity": [
                    746,
                    748
                ],
                "K.identity": [
                    746
                ],
                "num_samples": [
                    911,
                    922,
                    923,
                    795,
                    925,
                    800,
                    801,
                    805,
                    959,
                    836,
                    967,
                    968,
                    841,
                    969,
                    843,
                    993,
                    870,
                    999,
                    1000,
                    1001,
                    875,
                    876,
                    877,
                    880,
                    754,
                    759,
                    760,
                    763
                ],
                "input_dim": [
                    800,
                    803,
                    837,
                    871,
                    841,
                    842,
                    875,
                    878,
                    755,
                    759,
                    761,
                    796
                ],
                "output_dim": [
                    801,
                    803,
                    804,
                    838,
                    872,
                    842,
                    876,
                    877,
                    878,
                    879,
                    756,
                    760,
                    761,
                    762,
                    797
                ],
                "timesteps": [
                    800,
                    805,
                    839,
                    841,
                    873,
                    843,
                    875,
                    880,
                    757,
                    759,
                    763,
                    798
                ],
                "_": [
                    1950,
                    800,
                    801,
                    803,
                    804,
                    1346,
                    1347,
                    1349,
                    841,
                    842,
                    1354,
                    1623,
                    1879,
                    1377,
                    1634,
                    875,
                    876,
                    877,
                    878,
                    879,
                    1391,
                    1644,
                    759,
                    760,
                    761,
                    762,
                    1405
                ],
                "h0": [
                    801,
                    802,
                    905,
                    876,
                    788,
                    760,
                    830
                ],
                "wi": [
                    803,
                    807,
                    903,
                    842,
                    845,
                    878,
                    786,
                    882,
                    761,
                    828,
                    765,
                    862
                ],
                "wh": [
                    804,
                    903,
                    808,
                    879,
                    786,
                    883,
                    762,
                    828,
                    766
                ],
                "mask": [
                    898,
                    899,
                    805,
                    843,
                    781,
                    782,
                    880,
                    944,
                    1010,
                    979,
                    950,
                    823,
                    824,
                    985,
                    763,
                    1016
                ],
                "np.random.randint": [
                    805,
                    1896,
                    843,
                    1163,
                    880,
                    763
                ],
                "wi_k": [
                    902,
                    807,
                    861,
                    845,
                    785,
                    882,
                    827,
                    765
                ],
                "wh_k": [
                    902,
                    808,
                    785,
                    883,
                    827,
                    766
                ],
                "states": [
                    771,
                    772,
                    965,
                    997,
                    813,
                    814,
                    850,
                    919
                ],
                "h": [
                    772,
                    773,
                    814,
                    815,
                    889,
                    890
                ],
                "backend.dot": [
                    890,
                    851,
                    773,
                    815
                ],
                "backend": [
                    773,
                    815,
                    816,
                    851,
                    890
                ],
                "inputs": [
                    1666,
                    773,
                    1680,
                    919,
                    1815,
                    1820,
                    1823,
                    1701,
                    942,
                    815,
                    1710,
                    1840,
                    948,
                    964,
                    977,
                    851,
                    1749,
                    983,
                    1754,
                    1759,
                    1761,
                    1763,
                    997,
                    1008,
                    1014,
                    890
                ],
                "w_i": [
                    890,
                    851,
                    773,
                    815
                ],
                "w_h": [
                    890,
                    773,
                    815
                ],
                "simple_rnn": [
                    776
                ],
                "kwargs_list": [
                    901,
                    778,
                    784,
                    820,
                    856,
                    826,
                    860,
                    895
                ],
                "check_rnn_operation": [
                    785,
                    827,
                    861,
                    902
                ],
                "get_step_function": [
                    902,
                    903,
                    785,
                    786,
                    827,
                    828,
                    861,
                    862
                ],
                "h1": [
                    802,
                    830
                ],
                "np.concatenate": [
                    802
                ],
                "backend.concatenate": [
                    816
                ],
                "simple_rnn_with_extra_mock_state": [
                    818
                ],
                "simple_no_states": [
                    854
                ],
                "states_and_constants": [
                    889
                ],
                "simple_rnn_add_constant": [
                    893
                ],
                "num_timesteps": [
                    960,
                    994,
                    1018,
                    967,
                    999,
                    937,
                    969,
                    939,
                    1001,
                    1005,
                    1006,
                    912,
                    952,
                    922,
                    987,
                    925
                ],
                "state_and_io_size": [
                    913,
                    922,
                    923
                ],
                "mask_last_num_timesteps": [
                    932,
                    933,
                    939,
                    914,
                    926
                ],
                "inputs_vals": [
                    929,
                    967,
                    999,
                    972,
                    942,
                    1008,
                    977,
                    921
                ],
                "initial_state_vals": [
                    935,
                    968,
                    1000,
                    1004,
                    943,
                    1009,
                    978,
                    923
                ],
                "mask_vals": [
                    969,
                    970,
                    1001,
                    1002,
                    944,
                    1010,
                    979,
                    925,
                    926
                ],
                "np.ones": [
                    969,
                    1001,
                    1426,
                    1427,
                    1428,
                    1429,
                    1430,
                    1431,
                    1432,
                    925
                ],
                "expected_outputs": [
                    929,
                    932,
                    933,
                    972,
                    975,
                    954,
                    989
                ],
                "inputs_vals.copy": [
                    929
                ],
                "expected_state": [
                    937,
                    939,
                    955,
                    935
                ],
                "initial_state_vals.copy": [
                    1004,
                    935
                ],
                "initial_states": [
                    943,
                    1009,
                    978,
                    949,
                    1015,
                    984
                ],
                "last_output": [
                    946,
                    1012,
                    981
                ],
                "outputs": [
                    964,
                    965,
                    946,
                    1012,
                    981,
                    954,
                    989
                ],
                "last_states": [
                    946,
                    1012,
                    981,
                    955,
                    1021
                ],
                "step_function": [
                    947,
                    1013,
                    982
                ],
                "num_features": [
                    961,
                    967
                ],
                "K.expand_dims": [
                    964
                ],
                "np.repeat": [
                    972
                ],
                "expected_last_state": [
                    1021,
                    1004,
                    1005,
                    1006
                ],
                "K.logsumexp": [
                    1043,
                    1054
                ],
                "axis": [
                    1043,
                    1044
                ],
                "keepdims": [
                    1043,
                    1044
                ],
                "np.log": [
                    1044
                ],
                "np.sum": [
                    1044
                ],
                "np.exp": [
                    1044,
                    1820
                ],
                "result": [
                    1054,
                    1055
                ],
                "k.switch": [
                    1077,
                    1063
                ],
                "k.greater_equal": [
                    1076,
                    1063
                ],
                "shapes": [
                    1067,
                    1068,
                    1069,
                    1070,
                    1071
                ],
                "shapes.append": [
                    1068,
                    1069,
                    1070
                ],
                "arrays": [
                    1073,
                    1075
                ],
                "map": [
                    1073,
                    1075
                ],
                "then_expr": [
                    1075,
                    1077
                ],
                "else_expr": [
                    1075,
                    1077
                ],
                "cond": [
                    1076,
                    1077
                ],
                "k.dropout": [
                    1089,
                    1082
                ],
                "np.abs": [
                    1378,
                    1379,
                    1095,
                    1386,
                    1418,
                    1423,
                    1392,
                    1400,
                    1371,
                    1372,
                    1406,
                    1087
                ],
                "mean": [
                    1376,
                    1378,
                    1411,
                    1095,
                    1416,
                    1418,
                    1367,
                    1369,
                    1371,
                    1087
                ],
                "val.shape": [
                    1090
                ],
                "K.dropout": [
                    1099
                ],
                "alpha": [
                    1115
                ],
                "max_value": [
                    2121,
                    1116,
                    2119
                ],
                "threshold": [
                    1116
                ],
                "xval": [
                    1536,
                    1538,
                    1586,
                    1588,
                    1141,
                    1499,
                    1147,
                    1564,
                    1501,
                    1566
                ],
                "np.asarray": [
                    1665,
                    1666,
                    1697,
                    1698,
                    1700,
                    1701,
                    1801,
                    2122,
                    1739,
                    1141,
                    1144,
                    1754,
                    1661,
                    1662,
                    1727
                ],
                "yval": [
                    1144,
                    1147
                ],
                "num_classes": [
                    1894,
                    1159,
                    1896,
                    1897,
                    1162,
                    1163,
                    1898,
                    1167,
                    1175,
                    1177,
                    1182
                ],
                "predictions": [
                    1168,
                    1162,
                    1179,
                    1183
                ],
                "targets": [
                    1184,
                    1169,
                    1163,
                    1180
                ],
                "b.eval": [
                    1168,
                    1183
                ],
                "b.in_top_k": [
                    1168,
                    1183
                ],
                "b.variable": [
                    1168,
                    1169,
                    1184,
                    1183
                ],
                "num_identical": [
                    1178,
                    1175
                ],
                "idx_identical": [
                    1177,
                    1179
                ],
                "np.random.choice": [
                    1177
                ],
                "np.zeros": [
                    1817,
                    1180
                ],
                "op": [
                    1249,
                    1222,
                    1289,
                    1321,
                    1355,
                    1359,
                    1203,
                    1272
                ],
                "kernel_shape": [
                    1249,
                    1347,
                    1349,
                    1222,
                    1289,
                    1203,
                    1272
                ],
                "padding": [
                    1250,
                    1570,
                    1223,
                    1352,
                    1290,
                    1323,
                    1356,
                    1548,
                    1578,
                    1582,
                    1362,
                    1204,
                    1556,
                    1588,
                    1560,
                    1273,
                    1566
                ],
                "data_format": [
                    1290,
                    1549,
                    1551,
                    1556,
                    1571,
                    1573,
                    1578,
                    1323,
                    1204,
                    1592,
                    1594,
                    1342,
                    1601,
                    1603,
                    1223,
                    1352,
                    1609,
                    1356,
                    1486,
                    1488,
                    1362,
                    1490,
                    1496,
                    1250,
                    1511,
                    1517,
                    1519,
                    1522,
                    1524,
                    1526,
                    1273,
                    1533
                ],
                "output_shape": [
                    1272,
                    1223
                ],
                "dilation_rate": [
                    1273,
                    1251
                ],
                "KC.dev.type": [
                    1226,
                    1253
                ],
                "KC.dev": [
                    1226,
                    1253
                ],
                "pool_size": [
                    1472,
                    1474,
                    1476,
                    1479,
                    1481,
                    1322,
                    1483,
                    1467,
                    1470
                ],
                "strides": [
                    1322
                ],
                "pool_mode": [
                    1323
                ],
                "input_depth": [
                    1345,
                    1348,
                    1350,
                    1343
                ],
                "depthwise": [
                    1361,
                    1355,
                    1347,
                    1351
                ],
                "depth_multiplier": [
                    1348,
                    1350
                ],
                "pointwise": [
                    1361,
                    1355,
                    1349,
                    1351
                ],
                "y1": [
                    1363,
                    1351
                ],
                "KNP.separable_conv": [
                    1351
                ],
                "y2": [
                    1363,
                    1357,
                    1359
                ],
                "std": [
                    1376,
                    1378,
                    1379,
                    1412,
                    1416,
                    1423,
                    1367,
                    1369,
                    1371,
                    1372
                ],
                "rand": [
                    1415,
                    1417,
                    1418,
                    1419,
                    1420,
                    1423,
                    1368,
                    1370,
                    1371,
                    1372,
                    1384,
                    1385,
                    1386,
                    1387,
                    1388,
                    1398,
                    1399,
                    1400,
                    1401,
                    1402
                ],
                "K.random_normal": [
                    1368,
                    1376
                ],
                "rand.shape": [
                    1385,
                    1370,
                    1417,
                    1399
                ],
                "np.mean": [
                    1378,
                    1386,
                    1418,
                    1392,
                    1400,
                    1371,
                    1406
                ],
                "np.std": [
                    1379,
                    1372,
                    1423
                ],
                "r": [
                    1376,
                    1377,
                    1390,
                    1391,
                    1404,
                    1405
                ],
                "samples": [
                    1408,
                    1377,
                    1378,
                    1379,
                    1391,
                    1392,
                    1393,
                    1394,
                    1405,
                    1406,
                    1407
                ],
                "min_val": [
                    1413,
                    1382,
                    1384,
                    1388,
                    1420,
                    1390,
                    1394
                ],
                "max_val": [
                    1414,
                    1383,
                    1384,
                    1387,
                    1419,
                    1390,
                    1393
                ],
                "K.random_uniform": [
                    1384,
                    1390
                ],
                "np.max": [
                    1387,
                    1419,
                    1393,
                    1401,
                    1407
                ],
                "np.min": [
                    1408,
                    1420,
                    1388,
                    1394,
                    1402
                ],
                "p": [
                    1397,
                    1398,
                    1400,
                    1404,
                    1406
                ],
                "K.random_binomial": [
                    1404,
                    1398
                ],
                "K.truncated_normal": [
                    1415
                ],
                "dummy_x_1d": [
                    1426,
                    1435
                ],
                "dummy_w_1d": [
                    1427,
                    1435
                ],
                "dummy_x_2d": [
                    1445,
                    1449,
                    1454,
                    1457,
                    1428,
                    1460,
                    1463,
                    1438
                ],
                "dummy_w_2d": [
                    1445,
                    1449,
                    1454,
                    1457,
                    1460,
                    1429,
                    1463,
                    1438
                ],
                "dummy_x_3d": [
                    1441,
                    1430
                ],
                "dummy_w_3d": [
                    1441,
                    1431
                ],
                "dummy_w1x1_2d": [
                    1432,
                    1457,
                    1445,
                    1454
                ],
                "K.conv1d": [
                    1435
                ],
                "K.conv2d": [
                    1438
                ],
                "K.conv3d": [
                    1441
                ],
                "K.separable_conv2d": [
                    1457,
                    1445,
                    1454
                ],
                "K.depthwise_conv2d": [
                    1449,
                    1460,
                    1463
                ],
                "K.pool2d": [
                    1472,
                    1474,
                    1476
                ],
                "K.pool3d": [
                    1481,
                    1483,
                    1479
                ],
                "K.resize_images": [
                    1501
                ],
                "height_factor": [
                    1509
                ],
                "width_factor": [
                    1510
                ],
                "staticmethod": [
                    1504
                ],
                "self._helper_bilinear": [
                    1517,
                    1519
                ],
                "self": [
                    1517,
                    1519
                ],
                "NotImplementedError": [
                    1518
                ],
                "K.resize_volumes": [
                    1538
                ],
                "K.spatial_2d_padding": [
                    1560,
                    1566
                ],
                "K.spatial_3d_padding": [
                    1588,
                    1582
                ],
                "bias_shape": [
                    1613,
                    1598,
                    1599
                ],
                "K.bias_add": [
                    1615
                ],
                "K.normalize_batch_in_training": [
                    1634,
                    1644,
                    1623
                ],
                "label_lens": [
                    1698,
                    1682,
                    1661,
                    1712
                ],
                "np.expand_dims": [
                    1697,
                    1698,
                    1661,
                    1662
                ],
                "input_lens": [
                    1681,
                    1697,
                    1662,
                    1711
                ],
                "labels": [
                    1665,
                    1700,
                    1709,
                    1679
                ],
                "k_labels": [
                    1713,
                    1683,
                    1709,
                    1679
                ],
                "k_inputs": [
                    1680,
                    1713,
                    1683,
                    1710
                ],
                "k_input_lens": [
                    1681,
                    1683,
                    1713,
                    1711
                ],
                "k_label_lens": [
                    1712,
                    1682,
                    1714,
                    1684
                ],
                "res": [
                    1713,
                    1683,
                    1716,
                    1686,
                    1718,
                    1688
                ],
                "K.ctc_batch_cost": [
                    1713,
                    1683
                ],
                "max_time_steps": [
                    1724,
                    1751
                ],
                "seq_len_0": [
                    1826,
                    1800,
                    1816,
                    1757,
                    1726
                ],
                "input_prob_matrix_0": [
                    1801,
                    1812,
                    1749,
                    1815,
                    1727
                ],
                "seq_len_1": [
                    1736,
                    1757
                ],
                "input_prob_matrix_1": [
                    1739,
                    1750
                ],
                "np.vstack": [
                    1749
                ],
                "transpose": [
                    1754
                ],
                "input_length": [
                    1760,
                    1762,
                    1826,
                    1764,
                    1893,
                    1896,
                    1841,
                    1757
                ],
                "np.int32": [
                    1897,
                    1826,
                    1757
                ],
                "decode_pred_np": [
                    1771,
                    1759
                ],
                "log_prob_pred_np": [
                    1772,
                    1759
                ],
                "KNP.ctc_decode": [
                    1759
                ],
                "decode_pred_tf": [
                    1763,
                    1766,
                    1768,
                    1840,
                    1846,
                    1851
                ],
                "log_prob_pred_tf": [
                    1840,
                    1769,
                    1848,
                    1763
                ],
                "K.ctc_decode": [
                    1840,
                    1763,
                    1879
                ],
                "decode_pred": [
                    1768,
                    1771
                ],
                "log_prob_pred": [
                    1848,
                    1769,
                    1772,
                    1853
                ],
                "np.alltrue": [
                    1851,
                    1771
                ],
                "np.allclose": [
                    1888,
                    1890,
                    2121,
                    1772,
                    1853,
                    1790
                ],
                "npt": [
                    1787,
                    1789,
                    1783
                ],
                "x_start": [
                    1786,
                    1788,
                    1789
                ],
                "tft": [
                    1787,
                    1788
                ],
                "K.constant": [
                    2039,
                    1787,
                    2035
                ],
                "test_input": [
                    1788,
                    1790
                ],
                "K.slice": [
                    1788
                ],
                "x_size": [
                    1788,
                    1789
                ],
                "expected": [
                    1789,
                    1790
                ],
                "KNP.slice": [
                    1789
                ],
                "depth": [
                    1817,
                    1798
                ],
                "np.newaxis": [
                    1833,
                    1815
                ],
                "inputs.transpose": [
                    1823
                ],
                "log_prob_truth": [
                    1828,
                    1853
                ],
                "decode_truth": [
                    1851,
                    1835
                ],
                "beam_width": [
                    1843,
                    1837
                ],
                "top_paths": [
                    1846,
                    1850,
                    1844,
                    1838
                ],
                "input_prob": [
                    1873,
                    1884,
                    1862
                ],
                "input_len": [
                    1873,
                    1884
                ],
                "input_prob.shape": [
                    1873
                ],
                "input_prob_tensor": [
                    1882,
                    1876,
                    1879
                ],
                "input_len_tensor": [
                    1882,
                    1878,
                    1879
                ],
                "paths_tensors": [
                    1883,
                    1879
                ],
                "merge_repeated": [
                    1881
                ],
                "decode_func": [
                    1882,
                    1884
                ],
                "paths": [
                    1884,
                    1885
                ],
                "decode": [
                    1888,
                    1890
                ],
                "oh": [
                    1897,
                    1899
                ],
                "KNP.one_hot": [
                    1897
                ],
                "koh": [
                    1898,
                    1899
                ],
                "K.one_hot": [
                    1898
                ],
                "np.all": [
                    1899
                ],
                "x_d": [
                    1924,
                    1928,
                    1930,
                    1934,
                    1905,
                    1909
                ],
                "x_r": [
                    1925,
                    1928,
                    1931,
                    1934,
                    1906,
                    1909
                ],
                "np.int64": [
                    1925,
                    1926,
                    1931,
                    1932,
                    1906,
                    1907
                ],
                "x_c": [
                    1926,
                    1928,
                    1932,
                    1934,
                    1907,
                    1909
                ],
                "x_sparse": [
                    1914,
                    1909,
                    1910
                ],
                "sparse.csr_matrix": [
                    1928,
                    1909,
                    1934
                ],
                "sparse": [
                    1928,
                    1909,
                    1934
                ],
                "x_dense": [
                    1915,
                    1910
                ],
                "x_sparse.toarray": [
                    1910
                ],
                "W": [
                    1912,
                    1913
                ],
                "t_W": [
                    1913,
                    1914,
                    1915
                ],
                "k_s": [
                    1939,
                    1940,
                    1942,
                    1914,
                    1917,
                    1918
                ],
                "K.dot": [
                    1914,
                    1915
                ],
                "k_d": [
                    1947,
                    1944,
                    1946,
                    1915,
                    1917,
                    1918
                ],
                "k_s.shape": [
                    1917
                ],
                "k_d.shape": [
                    1946,
                    1917
                ],
                "x_sparse_1": [
                    1928,
                    1939,
                    1936
                ],
                "x_sparse_2": [
                    1937,
                    1939,
                    1934
                ],
                "x_dense_1": [
                    1936,
                    1944
                ],
                "x_sparse_1.toarray": [
                    1936
                ],
                "x_dense_2": [
                    1944,
                    1937
                ],
                "x_sparse_2.toarray": [
                    1937
                ],
                "K.concatenate": [
                    1944,
                    1939
                ],
                "K.is_sparse": [
                    1940
                ],
                "k_s_d": [
                    1946,
                    1947,
                    1942
                ],
                "k_s_d.shape": [
                    1946
                ],
                "tensor_list": [
                    1950,
                    1959
                ],
                "stack_axis": [
                    1960,
                    1956,
                    1951
                ],
                "results": [
                    1952,
                    1961,
                    1963
                ],
                "tensor_list_var": [
                    1960,
                    1959
                ],
                "tensor": [
                    1959
                ],
                "out": [
                    1960,
                    1961
                ],
                "k.stack": [
                    1960
                ],
                "results.append": [
                    1961
                ],
                "np.random.rand": [
                    1985,
                    1967
                ],
                "vx": [
                    1998,
                    1999,
                    1968,
                    1969,
                    2000,
                    1973
                ],
                "kx": [
                    1986,
                    1988,
                    1989,
                    1969,
                    1978,
                    1980,
                    1981
                ],
                "K.map_fn": [
                    1969,
                    1972
                ],
                "K.sum": [
                    1969,
                    1973
                ],
                "kx2": [
                    1979,
                    1972,
                    1981
                ],
                "K.arange": [
                    2040,
                    2036,
                    1974
                ],
                "K.floatx": [
                    1975
                ],
                "kx.shape": [
                    1978,
                    1988
                ],
                "kx2.shape": [
                    1979
                ],
                "x.sum": [
                    1980,
                    1989
                ],
                "K.foldl": [
                    1986,
                    1999
                ],
                "p1": [
                    2002,
                    1999
                ],
                "p2": [
                    2000,
                    2002,
                    2003
                ],
                "K.foldr": [
                    2000
                ],
                "test_value": [
                    2008,
                    2012,
                    2014
                ],
                "a_list": [
                    2016,
                    2018,
                    2019,
                    2022,
                    2026,
                    2027,
                    2028,
                    2009
                ],
                "dtype_list": [
                    2010,
                    2015
                ],
                "k.arange": [
                    2024,
                    2032,
                    2012
                ],
                "np.array_equal": [
                    2025,
                    2019,
                    2028,
                    2014
                ],
                "dtype_list.append": [
                    2015
                ],
                "k.dtype": [
                    2033,
                    2015
                ],
                "a_list.append": [
                    2016,
                    2026
                ],
                "start": [
                    2021,
                    2024,
                    2025,
                    2035,
                    2036,
                    2039,
                    2040
                ],
                "stop": [
                    2024,
                    2025,
                    2021
                ],
                "step": [
                    2024,
                    2025,
                    2021
                ],
                "training": [
                    2048,
                    2053,
                    2046,
                    2055
                ],
                "old_floatx": [
                    2089,
                    2059,
                    2067,
                    2100,
                    2072,
                    2078
                ],
                "floatx": [
                    2089,
                    2059,
                    2061,
                    2065,
                    2072,
                    2076
                ],
                "initial": [
                    2065,
                    2061
                ],
                "set_floatx": [
                    2091,
                    2095,
                    2064,
                    2067,
                    2100,
                    2075,
                    2078
                ],
                "variable": [
                    2096,
                    2092
                ],
                "check_dtype": [
                    2097,
                    2093
                ],
                "K.dtype": [
                    2103,
                    2104,
                    2105,
                    2110,
                    2111
                ],
                "TypeError": [
                    2112
                ],
                "min_value": [
                    2121,
                    2118
                ],
                "K.clip": [
                    2121
                ],
                "threads": [
                    2128,
                    2130,
                    2132,
                    2133
                ],
                "K.clear_session": [
                    2129
                ],
                "monkeypatch.setenv": [
                    2130
                ],
                "monkeypatch": [
                    2130
                ],
                "str": [
                    2130
                ],
                "cfg": [
                    2131,
                    2132,
                    2133
                ],
                "_config": [
                    2131
                ],
                "cfg.intra_op_parallelism_threads": [
                    2132
                ],
                "cfg.inter_op_parallelism_threads": [
                    2133
                ],
                "__name__": [
                    2136
                ],
                "pytest.main": [
                    2137
                ],
                "__file__": [
                    2137
                ]
            },
            "filtered_variables_in_file": {
                "KC": [
                    65,
                    34,
                    67,
                    68,
                    1253,
                    1953,
                    1226,
                    16,
                    147,
                    116,
                    150,
                    62
                ],
                "warnings.warn": [
                    17,
                    29,
                    23
                ],
                "warnings": [
                    17,
                    29,
                    23
                ],
                "KTF": [
                    641,
                    610,
                    1185,
                    36,
                    613,
                    1508,
                    684,
                    1580,
                    2125,
                    1170,
                    22,
                    662,
                    1558
                ],
                "KTH": [
                    32,
                    641,
                    610,
                    1185,
                    1508,
                    613,
                    40,
                    1580,
                    1170,
                    1558,
                    153,
                    28
                ],
                "K.backend": [
                    1792,
                    399,
                    1685,
                    662,
                    1047,
                    1692,
                    31,
                    2080,
                    33,
                    1443,
                    38,
                    40,
                    425,
                    42,
                    684,
                    1452,
                    302,
                    1965,
                    50,
                    1715,
                    566,
                    1720,
                    441,
                    957,
                    2109,
                    575,
                    1855,
                    1983,
                    582,
                    1991,
                    1353,
                    1226,
                    715,
                    2125,
                    464,
                    1617,
                    594,
                    2005,
                    1628,
                    606,
                    991,
                    354,
                    740,
                    1253,
                    1639,
                    1514,
                    1774,
                    1138,
                    1653,
                    1655,
                    637
                ],
                "K": [
                    1538,
                    1042,
                    1043,
                    1558,
                    1047,
                    1559,
                    1560,
                    1561,
                    1939,
                    1054,
                    31,
                    1566,
                    33,
                    2080,
                    38,
                    40,
                    42,
                    1580,
                    1581,
                    1582,
                    1583,
                    561,
                    50,
                    562,
                    563,
                    564,
                    566,
                    1588,
                    2103,
                    2104,
                    2105,
                    2109,
                    2110,
                    575,
                    2111,
                    2113,
                    2117,
                    582,
                    2118,
                    2119,
                    2121,
                    586,
                    1099,
                    1612,
                    1613,
                    590,
                    1615,
                    592,
                    1617,
                    594,
                    2129,
                    2131,
                    2125,
                    598,
                    1622,
                    1623,
                    1625,
                    602,
                    604,
                    1628,
                    606,
                    1633,
                    1634,
                    1636,
                    1639,
                    1643,
                    1644,
                    1646,
                    1138,
                    1653,
                    1655,
                    632,
                    633,
                    634,
                    635,
                    637,
                    1679,
                    1680,
                    1681,
                    1682,
                    1683,
                    1685,
                    662,
                    1692,
                    671,
                    672,
                    673,
                    674,
                    1965,
                    676,
                    679,
                    682,
                    684,
                    1709,
                    1710,
                    1711,
                    1712,
                    1713,
                    1715,
                    1969,
                    693,
                    694,
                    695,
                    696,
                    1720,
                    700,
                    704,
                    1972,
                    707,
                    197,
                    198,
                    1973,
                    200,
                    713,
                    1226,
                    715,
                    204,
                    1974,
                    719,
                    720,
                    1975,
                    725,
                    218,
                    733,
                    1761,
                    1762,
                    1763,
                    228,
                    229,
                    230,
                    740,
                    1253,
                    745,
                    746,
                    1768,
                    748,
                    1769,
                    1774,
                    244,
                    246,
                    247,
                    248,
                    249,
                    1787,
                    1788,
                    765,
                    254,
                    766,
                    1792,
                    785,
                    1823,
                    1826,
                    807,
                    808,
                    302,
                    1840,
                    1848,
                    827,
                    1851,
                    1855,
                    1353,
                    845,
                    1359,
                    1360,
                    1361,
                    1876,
                    341,
                    342,
                    343,
                    1368,
                    345,
                    1878,
                    1879,
                    1882,
                    349,
                    861,
                    1376,
                    1377,
                    354,
                    355,
                    356,
                    357,
                    359,
                    1384,
                    1898,
                    364,
                    1390,
                    1391,
                    369,
                    370,
                    371,
                    882,
                    373,
                    883,
                    1398,
                    377,
                    1913,
                    1914,
                    1404,
                    1405,
                    1915,
                    902,
                    1415,
                    399,
                    1426,
                    1427,
                    1428,
                    405,
                    1429,
                    407,
                    1430,
                    1431,
                    1432,
                    411,
                    1435,
                    1940,
                    1438,
                    1942,
                    1944,
                    1441,
                    1443,
                    1445,
                    425,
                    1449,
                    428,
                    429,
                    942,
                    431,
                    943,
                    944,
                    946,
                    1452,
                    1454,
                    1457,
                    1460,
                    1463,
                    1968,
                    441,
                    442,
                    954,
                    444,
                    955,
                    957,
                    447,
                    1469,
                    1472,
                    1474,
                    1986,
                    964,
                    1476,
                    1983,
                    1479,
                    1991,
                    1481,
                    1483,
                    1998,
                    1999,
                    464,
                    465,
                    466,
                    467,
                    977,
                    978,
                    979,
                    981,
                    2000,
                    2005,
                    989,
                    1501,
                    991,
                    1514,
                    1008,
                    1009,
                    1010,
                    2035,
                    1012,
                    2036,
                    2037,
                    2039,
                    2040,
                    2041,
                    1021
                ],
                "WITH_NP": [
                    512,
                    514,
                    515,
                    2052,
                    517,
                    518,
                    519,
                    1543,
                    521,
                    522,
                    523,
                    1545,
                    525,
                    526,
                    527,
                    1529,
                    529,
                    530,
                    1555,
                    532,
                    533,
                    535,
                    536,
                    537,
                    538,
                    540,
                    541,
                    542,
                    543,
                    32,
                    34,
                    546,
                    36,
                    547,
                    550,
                    551,
                    552,
                    553,
                    554,
                    555,
                    556,
                    557,
                    1061,
                    1577,
                    2045,
                    1074,
                    569,
                    570,
                    1083,
                    572,
                    573,
                    1600,
                    1091,
                    580,
                    1608,
                    1115,
                    1119,
                    1120,
                    1121,
                    2054,
                    1123,
                    1124,
                    1125,
                    1127,
                    1128,
                    1129,
                    1132,
                    1140,
                    1147,
                    1150,
                    1152,
                    1154,
                    1155,
                    1203,
                    1222,
                    1249,
                    1272,
                    257,
                    261,
                    265,
                    1289,
                    269,
                    273,
                    276,
                    277,
                    280,
                    282,
                    284,
                    286,
                    288,
                    290,
                    292,
                    294,
                    296,
                    1321,
                    298,
                    300,
                    301,
                    303,
                    306,
                    310,
                    381,
                    384,
                    385,
                    387,
                    388,
                    389,
                    391,
                    392,
                    393,
                    394,
                    397,
                    1953,
                    1955,
                    422,
                    1958,
                    436,
                    437,
                    455,
                    457,
                    1493,
                    2011,
                    476,
                    484,
                    485,
                    486,
                    487,
                    2023,
                    490,
                    491,
                    492,
                    494,
                    495,
                    496,
                    2031,
                    498,
                    499,
                    501,
                    502,
                    504,
                    505,
                    506,
                    508,
                    509,
                    510,
                    2047
                ],
                "KNP": [
                    32,
                    34,
                    36,
                    903,
                    1351,
                    1897,
                    208,
                    338,
                    786,
                    828,
                    1789,
                    862,
                    1759
                ],
                "supports_sparse": [
                    1920,
                    39,
                    41,
                    44,
                    1901,
                    46
                ],
                "KTH.th_sparse_module": [
                    40
                ],
                "var.dtype": [
                    51,
                    53
                ],
                "var": [
                    2092,
                    2093,
                    2096,
                    2097,
                    51,
                    53
                ],
                "dtype": [
                    2030,
                    2032,
                    2033,
                    51,
                    53
                ],
                "var.dtype.name": [
                    53
                ],
                "placeholders": [
                    57,
                    67,
                    68,
                    62
                ],
                "variables": [
                    65,
                    58,
                    67
                ],
                "shape_or_val": [
                    64,
                    73,
                    74,
                    105,
                    76,
                    109,
                    115,
                    59,
                    60,
                    61
                ],
                "shapes_or_vals": [
                    59
                ],
                "shape": [
                    1550,
                    1552,
                    1554,
                    418,
                    419,
                    1572,
                    1574,
                    1576,
                    426,
                    427,
                    428,
                    430,
                    434,
                    435,
                    1593,
                    1595,
                    61,
                    62,
                    1597,
                    451,
                    452,
                    330,
                    333,
                    1487,
                    1489,
                    1491,
                    85,
                    1523,
                    1525,
                    1527
                ],
                "placeholders.append": [
                    62
                ],
                "KC.placeholder": [
                    62
                ],
                "value": [
                    64,
                    65,
                    2062,
                    2064,
                    2074,
                    2075,
                    2076
                ],
                "variables.append": [
                    65
                ],
                "KC.variable": [
                    65
                ],
                "output_cntk": [
                    67,
                    68,
                    69
                ],
                "function_name": [
                    162,
                    67,
                    478,
                    148,
                    117,
                    475,
                    151,
                    120,
                    153,
                    154,
                    123,
                    158
                ],
                "kwargs": [
                    901,
                    137,
                    138,
                    139,
                    906,
                    908,
                    784,
                    148,
                    789,
                    790,
                    151,
                    155,
                    159,
                    163,
                    826,
                    831,
                    832,
                    67,
                    214,
                    860,
                    226,
                    866,
                    105,
                    106,
                    107,
                    117,
                    120,
                    123
                ],
                "cntk_func": [
                    1354,
                    68,
                    69,
                    1357
                ],
                "KC.function": [
                    68
                ],
                "np.ndarray": [
                    73
                ],
                "np": [
                    1024,
                    1025,
                    1026,
                    1027,
                    1028,
                    1029,
                    1030,
                    1031,
                    1032,
                    1033,
                    1034,
                    1035,
                    1036,
                    1044,
                    1564,
                    1053,
                    1059,
                    1073,
                    1536,
                    1586,
                    1081,
                    1087,
                    1095,
                    73,
                    585,
                    587,
                    76,
                    1612,
                    1613,
                    2121,
                    2122,
                    2123,
                    597,
                    1621,
                    599,
                    1632,
                    609,
                    1642,
                    1141,
                    1143,
                    631,
                    1144,
                    1146,
                    1661,
                    1662,
                    1665,
                    642,
                    643,
                    1666,
                    1162,
                    1163,
                    1677,
                    1177,
                    1180,
                    1697,
                    1698,
                    1700,
                    1701,
                    1707,
                    178,
                    1727,
                    1734,
                    1739,
                    1746,
                    1749,
                    1754,
                    1757,
                    1771,
                    1772,
                    242,
                    1783,
                    763,
                    1790,
                    1801,
                    1809,
                    1815,
                    1817,
                    1820,
                    802,
                    1826,
                    1828,
                    805,
                    1833,
                    1835,
                    1851,
                    1853,
                    1862,
                    843,
                    333,
                    1873,
                    1371,
                    1372,
                    1888,
                    1377,
                    1378,
                    1379,
                    1890,
                    1896,
                    1897,
                    1386,
                    1387,
                    1388,
                    1899,
                    1391,
                    880,
                    1392,
                    1393,
                    1394,
                    1905,
                    1906,
                    1907,
                    1400,
                    1401,
                    1402,
                    1912,
                    1405,
                    1406,
                    1407,
                    1408,
                    1924,
                    1925,
                    1926,
                    1418,
                    1419,
                    1420,
                    1930,
                    1931,
                    1423,
                    1932,
                    1426,
                    1427,
                    1428,
                    1429,
                    1430,
                    1431,
                    1432,
                    921,
                    923,
                    925,
                    1950,
                    418,
                    419,
                    1967,
                    435,
                    1469,
                    1985,
                    452,
                    967,
                    968,
                    969,
                    972,
                    1997,
                    474,
                    1499,
                    2014,
                    2019,
                    999,
                    1000,
                    1001,
                    2025,
                    2028
                ],
                "shape_or_val.shape": [
                    74
                ],
                "astype": [
                    1632,
                    1985,
                    1162,
                    1642,
                    76,
                    1967,
                    1621
                ],
                "np.random.random": [
                    1536,
                    642,
                    643,
                    1162,
                    921,
                    923,
                    1564,
                    1059,
                    1073,
                    178,
                    1586,
                    1081,
                    1469,
                    967,
                    968,
                    76,
                    333,
                    1612,
                    1613,
                    1621,
                    474,
                    1499,
                    1632,
                    609,
                    999,
                    1000,
                    1642,
                    631,
                    1912
                ],
                "np.random": [
                    1536,
                    642,
                    643,
                    1162,
                    1163,
                    921,
                    1177,
                    923,
                    1564,
                    1950,
                    1059,
                    805,
                    1967,
                    1073,
                    178,
                    1586,
                    1081,
                    1469,
                    1985,
                    967,
                    968,
                    585,
                    587,
                    76,
                    333,
                    843,
                    1612,
                    1613,
                    597,
                    1621,
                    599,
                    474,
                    1499,
                    1632,
                    609,
                    999,
                    1000,
                    1896,
                    1642,
                    880,
                    631,
                    1912,
                    763
                ],
                "np.float32": [
                    1924,
                    1930,
                    1677,
                    1809,
                    1817,
                    1833,
                    1707,
                    1967,
                    1985,
                    1734,
                    2123,
                    76,
                    1997,
                    1746,
                    1621,
                    1632,
                    1642,
                    1905,
                    1143,
                    1146
                ],
                "z1": [
                    88,
                    90,
                    84,
                    86
                ],
                "z2": [
                    88,
                    90,
                    84,
                    86
                ],
                "z_list": [
                    128,
                    129,
                    1168,
                    145,
                    1171,
                    1183,
                    1186,
                    1060,
                    166,
                    168,
                    169,
                    1064,
                    1065,
                    1072,
                    180,
                    1077,
                    1078,
                    185,
                    1082,
                    187,
                    1084,
                    1086,
                    1087,
                    1089,
                    1092,
                    1094,
                    1095,
                    456,
                    459,
                    460,
                    84,
                    94,
                    611,
                    619,
                    622,
                    624,
                    113,
                    625,
                    126
                ],
                "z1.shape": [
                    86
                ],
                "z2.shape": [
                    86
                ],
                "allclose": [
                    87
                ],
                "assert_allclose": [
                    1043,
                    1686,
                    1688,
                    1947,
                    1055,
                    1716,
                    1718,
                    954,
                    955,
                    1980,
                    1981,
                    1989,
                    592,
                    1363,
                    88,
                    604,
                    989,
                    351,
                    232,
                    233,
                    236,
                    366,
                    625,
                    378,
                    1021,
                    1918
                ],
                "atol": [
                    88
                ],
                "itself": [
                    89
                ],
                "t": [
                    148,
                    151,
                    1815,
                    1816,
                    154,
                    2040,
                    156,
                    2041,
                    158,
                    160,
                    162,
                    164,
                    165,
                    1749,
                    1750,
                    1751,
                    2012,
                    2013,
                    94,
                    95,
                    96,
                    2015,
                    98,
                    2032,
                    2033,
                    2036,
                    117,
                    2037,
                    120,
                    121,
                    123,
                    124,
                    125
                ],
                "z": [
                    149,
                    152,
                    156,
                    160,
                    164,
                    166,
                    184,
                    185,
                    1099,
                    343,
                    1623,
                    345,
                    1625,
                    1626,
                    349,
                    94,
                    98,
                    1634,
                    1636,
                    357,
                    1637,
                    359,
                    377,
                    364,
                    1644,
                    1646,
                    1647,
                    371,
                    373,
                    118,
                    121,
                    378,
                    124,
                    126
                ],
                "t_list": [
                    129,
                    165,
                    454,
                    169,
                    460,
                    112,
                    144,
                    125,
                    94
                ],
                "t._keras_shape": [
                    96,
                    98,
                    95
                ],
                "i": [
                    96,
                    1851,
                    98,
                    2018,
                    2019,
                    1094,
                    1095,
                    2027,
                    2028,
                    624,
                    625,
                    1973,
                    1176,
                    1850,
                    1179,
                    1086,
                    1087
                ],
                "s": [
                    96,
                    97,
                    228,
                    997,
                    198,
                    1071,
                    1073,
                    919
                ],
                "z.shape": [
                    98,
                    1637,
                    1626,
                    1647
                ],
                "kwargs.pop": [
                    137,
                    105,
                    106,
                    107,
                    138,
                    139,
                    906,
                    789,
                    831
                ],
                "assert_value_equality": [
                    128,
                    106
                ],
                "cntk_dynamicity": [
                    138,
                    107,
                    116,
                    147
                ],
                "x_shape": [
                    1536,
                    141,
                    1552,
                    1554,
                    1555,
                    148,
                    151,
                    1564,
                    1574,
                    1576,
                    1577,
                    1586,
                    1595,
                    1597,
                    1599,
                    1604,
                    1606,
                    1607,
                    1612,
                    335,
                    336,
                    1489,
                    1491,
                    1492,
                    341,
                    1621,
                    1626,
                    1499,
                    1632,
                    1506,
                    355,
                    1507,
                    1637,
                    1642,
                    1643,
                    110,
                    1647,
                    117,
                    1525,
                    1527,
                    1528
                ],
                "x_val": [
                    1632,
                    1633,
                    163,
                    1642,
                    141,
                    110,
                    1646,
                    149,
                    118,
                    1621,
                    120,
                    1622,
                    155,
                    152,
                    159
                ],
                "parse_shape_or_val": [
                    141,
                    142,
                    800,
                    801,
                    803,
                    804,
                    1346,
                    1347,
                    1349,
                    841,
                    842,
                    875,
                    876,
                    877,
                    110,
                    878,
                    879,
                    759,
                    760,
                    761,
                    762
                ],
                "x_shape_or_val": [
                    123,
                    141,
                    110
                ],
                "k": [
                    647,
                    648,
                    650,
                    651,
                    653,
                    1167,
                    1169,
                    146,
                    147,
                    659,
                    150,
                    153,
                    154,
                    155,
                    156,
                    158,
                    159,
                    160,
                    1182,
                    162,
                    163,
                    164,
                    1061,
                    1062,
                    1063,
                    1064,
                    1184,
                    1958,
                    1959,
                    1960,
                    1074,
                    1075,
                    1076,
                    181,
                    182,
                    183,
                    184,
                    1077,
                    1082,
                    1083,
                    1089,
                    1091,
                    454,
                    455,
                    456,
                    457,
                    475,
                    476,
                    2011,
                    2012,
                    2013,
                    2015,
                    610,
                    613,
                    614,
                    615,
                    616,
                    617,
                    618,
                    619,
                    620,
                    2023,
                    2024,
                    2031,
                    2032,
                    2033,
                    114,
                    116,
                    120,
                    121,
                    123,
                    124
                ],
                "backend_list": [
                    114,
                    181,
                    146
                ],
                "f": [
                    653,
                    654,
                    656,
                    148,
                    149,
                    151,
                    152,
                    676,
                    680,
                    700,
                    705,
                    711,
                    725,
                    729,
                    349,
                    733,
                    351,
                    736,
                    364,
                    748,
                    366,
                    749,
                    117,
                    118
                ],
                "cntk_func_tensors": [
                    1354,
                    148,
                    117,
                    151
                ],
                "k.variable": [
                    1089,
                    610,
                    163,
                    454,
                    1062,
                    456,
                    648,
                    1959,
                    1075,
                    182,
                    475,
                    120,
                    1082,
                    155,
                    159
                ],
                "k.eval": [
                    160,
                    1089,
                    164,
                    456,
                    1064,
                    1960,
                    619,
                    620,
                    2024,
                    156,
                    1077,
                    184,
                    121,
                    1082,
                    124,
                    2013
                ],
                "assert_list_pairwise": [
                    128,
                    481,
                    1186,
                    1092,
                    168,
                    1065,
                    459,
                    1963,
                    622,
                    623,
                    657,
                    1171,
                    660,
                    1078,
                    187,
                    1084,
                    479
                ],
                "assert_list_keras_shape": [
                    129,
                    169,
                    460
                ],
                "concat_args": [
                    137,
                    157
                ],
                "cntk_two_dynamicity": [
                    139,
                    150
                ],
                "y_shape": [
                    356,
                    142,
                    335,
                    337,
                    342,
                    151
                ],
                "y_val": [
                    163,
                    142,
                    148,
                    152,
                    155,
                    159
                ],
                "y_shape_or_val": [
                    142
                ],
                "convert_kernel": [
                    155
                ],
                "val": [
                    609,
                    610,
                    642,
                    1059,
                    1082,
                    1062,
                    1089,
                    648,
                    1090,
                    1099,
                    178,
                    182,
                    631,
                    632,
                    1081,
                    474,
                    475
                ],
                "input_shape": [
                    1249,
                    1345,
                    1346,
                    1222,
                    1289,
                    1321,
                    1355,
                    178,
                    1203,
                    1272,
                    1467,
                    1469,
                    1343
                ],
                "x": [
                    1042,
                    1043,
                    1559,
                    1560,
                    1062,
                    1063,
                    1064,
                    1581,
                    1582,
                    1075,
                    1076,
                    2117,
                    585,
                    586,
                    2121,
                    1612,
                    589,
                    1615,
                    592,
                    597,
                    598,
                    1622,
                    1624,
                    601,
                    604,
                    1633,
                    1635,
                    613,
                    614,
                    1643,
                    1645,
                    1646,
                    648,
                    649,
                    651,
                    652,
                    653,
                    659,
                    671,
                    678,
                    682,
                    693,
                    182,
                    183,
                    702,
                    707,
                    713,
                    759,
                    787,
                    800,
                    829,
                    1346,
                    1351,
                    841,
                    1357,
                    1360,
                    341,
                    343,
                    349,
                    863,
                    355,
                    357,
                    875,
                    364,
                    369,
                    371,
                    904,
                    405,
                    407,
                    411,
                    428,
                    429,
                    1967,
                    1968,
                    442,
                    444,
                    1469,
                    1980,
                    447,
                    1472,
                    1985,
                    1474,
                    1986,
                    1476,
                    1989,
                    1479,
                    1481,
                    1483,
                    1997,
                    1998,
                    465,
                    467
                ],
                "y": [
                    773,
                    774,
                    650,
                    651,
                    653,
                    407,
                    408,
                    409,
                    1560,
                    411,
                    412,
                    413,
                    1561,
                    672,
                    679,
                    682,
                    429,
                    430,
                    431,
                    815,
                    816,
                    1582,
                    1583,
                    694,
                    183,
                    184,
                    444,
                    445,
                    447,
                    448,
                    704,
                    707,
                    713,
                    467,
                    468,
                    851,
                    342,
                    343,
                    852,
                    349,
                    356,
                    357,
                    364,
                    370,
                    371,
                    890,
                    891
                ],
                "first_function_name": [
                    183
                ],
                "first_function_args": [
                    183
                ],
                "second_function_name": [
                    184
                ],
                "second_function_args": [
                    184
                ],
                "inputs_k": [
                    220,
                    197
                ],
                "K.variable": [
                    1538,
                    1042,
                    1054,
                    1566,
                    1588,
                    2103,
                    2104,
                    2105,
                    2110,
                    2111,
                    2113,
                    2117,
                    2118,
                    2119,
                    586,
                    1099,
                    1612,
                    1613,
                    598,
                    1622,
                    1633,
                    632,
                    1679,
                    1680,
                    1681,
                    1682,
                    671,
                    672,
                    1709,
                    1710,
                    1711,
                    1712,
                    693,
                    694,
                    197,
                    198,
                    200,
                    204,
                    1761,
                    1762,
                    246,
                    765,
                    766,
                    1823,
                    1826,
                    807,
                    808,
                    845,
                    1360,
                    1361,
                    1898,
                    369,
                    370,
                    882,
                    883,
                    1913,
                    1914,
                    1915,
                    1426,
                    1427,
                    1428,
                    1429,
                    1430,
                    1431,
                    1432,
                    1939,
                    1944,
                    942,
                    943,
                    944,
                    1968,
                    1469,
                    1986,
                    1998,
                    977,
                    978,
                    979,
                    1501,
                    1008,
                    1009,
                    1010
                ],
                "inputs_np": [
                    225,
                    210,
                    197
                ],
                "initial_states_k": [
                    221,
                    198
                ],
                "initial_states_np": [
                    211,
                    198
                ],
                "mask_np": [
                    200,
                    212,
                    199
                ],
                "mask_k": [
                    200,
                    202,
                    222
                ],
                "constants_np": [
                    203,
                    204,
                    213
                ],
                "constants_k": [
                    204,
                    206,
                    223
                ],
                "c": [
                    907,
                    204,
                    877,
                    889,
                    634,
                    890
                ],
                "last_output_np": [
                    208,
                    232
                ],
                "output_np": [
                    208,
                    233
                ],
                "last_states_np": [
                    208,
                    234,
                    235
                ],
                "KNP.rnn": [
                    208
                ],
                "step_function_np": [
                    209
                ],
                "unroll": [
                    224,
                    225,
                    1018,
                    945,
                    1011,
                    980,
                    951,
                    952,
                    217,
                    986,
                    987,
                    1017
                ],
                "last_output_k": [
                    232,
                    218,
                    229
                ],
                "output_k": [
                    233,
                    218,
                    230
                ],
                "last_states_k": [
                    218,
                    234,
                    228,
                    235
                ],
                "K.rnn": [
                    946,
                    218,
                    1012,
                    981
                ],
                "step_function_k": [
                    219
                ],
                "inputs_np.shape": [
                    225
                ],
                "K.eval": [
                    1415,
                    1043,
                    1683,
                    1942,
                    1944,
                    1405,
                    1054,
                    2041,
                    1713,
                    1969,
                    1972,
                    1848,
                    954,
                    955,
                    1851,
                    1986,
                    2121,
                    590,
                    1359,
                    592,
                    1999,
                    2000,
                    1368,
                    1625,
                    602,
                    604,
                    989,
                    1377,
                    228,
                    229,
                    230,
                    1636,
                    1384,
                    1768,
                    1769,
                    1898,
                    1391,
                    2037,
                    1398,
                    377,
                    1914,
                    1915,
                    1788,
                    1021
                ],
                "s_k": [
                    235,
                    236
                ],
                "s_np": [
                    235,
                    236
                ],
                "np_var": [
                    242,
                    244,
                    246
                ],
                "np.array": [
                    1024,
                    1025,
                    1026,
                    1027,
                    1028,
                    1029,
                    1030,
                    1031,
                    1032,
                    1033,
                    1034,
                    1035,
                    1036,
                    1924,
                    1925,
                    1926,
                    1930,
                    1931,
                    1932,
                    1053,
                    1826,
                    1828,
                    1835,
                    1862,
                    1997,
                    1873,
                    1757,
                    1888,
                    1377,
                    1890,
                    1391,
                    1905,
                    242,
                    1906,
                    1907,
                    1783,
                    1405
                ],
                "pytest.raises": [
                    1537,
                    2063,
                    1434,
                    1437,
                    1565,
                    1440,
                    1444,
                    1448,
                    1453,
                    1456,
                    1459,
                    1587,
                    1462,
                    1471,
                    2112,
                    1473,
                    1475,
                    1478,
                    1480,
                    1098,
                    1482,
                    1614,
                    1500,
                    1518,
                    243,
                    253
                ],
                "pytest": [
                    1537,
                    2050,
                    2063,
                    1047,
                    1565,
                    2080,
                    1587,
                    566,
                    575,
                    2112,
                    582,
                    1098,
                    1101,
                    1614,
                    2125,
                    1617,
                    594,
                    1619,
                    2137,
                    1628,
                    606,
                    1630,
                    1639,
                    1640,
                    1653,
                    637,
                    662,
                    1188,
                    684,
                    1207,
                    1720,
                    1226,
                    715,
                    1228,
                    740,
                    1253,
                    1255,
                    1774,
                    1777,
                    243,
                    1276,
                    253,
                    1792,
                    1293,
                    1326,
                    1855,
                    1901,
                    1920,
                    399,
                    1434,
                    1437,
                    1440,
                    1444,
                    1448,
                    1453,
                    1965,
                    1456,
                    1459,
                    1462,
                    957,
                    1471,
                    1983,
                    1473,
                    1475,
                    1478,
                    1991,
                    1480,
                    1482,
                    2005,
                    470,
                    1500,
                    991,
                    1514,
                    1515,
                    1518,
                    2043,
                    1023
                ],
                "K.is_keras_tensor": [
                    249,
                    244,
                    247
                ],
                "keras_var": [
                    246,
                    247
                ],
                "keras_placeholder": [
                    248,
                    249
                ],
                "K.placeholder": [
                    405,
                    1559,
                    673,
                    674,
                    428,
                    1581,
                    695,
                    696,
                    442,
                    719,
                    720,
                    465,
                    466,
                    1876,
                    341,
                    342,
                    1878,
                    355,
                    356,
                    745,
                    1643,
                    248
                ],
                "K.set_learning_phase": [
                    254
                ],
                "check_single_tensor_operation": [
                    512,
                    513,
                    515,
                    517,
                    518,
                    519,
                    1528,
                    521,
                    522,
                    523,
                    1542,
                    525,
                    526,
                    527,
                    1544,
                    529,
                    530,
                    1555,
                    532,
                    533,
                    535,
                    536,
                    537,
                    538,
                    540,
                    541,
                    542,
                    543,
                    546,
                    547,
                    1577,
                    569,
                    570,
                    572,
                    573,
                    580,
                    1115,
                    1119,
                    1120,
                    1121,
                    1123,
                    1124,
                    1125,
                    1127,
                    1128,
                    1129,
                    1154,
                    1155,
                    257,
                    260,
                    264,
                    268,
                    272,
                    1320,
                    300,
                    301,
                    303,
                    306,
                    310,
                    384,
                    385,
                    387,
                    388,
                    389,
                    391,
                    392,
                    393,
                    394,
                    422,
                    436,
                    437,
                    1492,
                    1507,
                    484,
                    485,
                    486,
                    487,
                    490,
                    491,
                    492,
                    494,
                    495,
                    496,
                    498,
                    499,
                    500,
                    502,
                    504,
                    505,
                    506,
                    508,
                    509,
                    510
                ],
                "check_two_tensor_operation": [
                    2052,
                    2054,
                    1288,
                    276,
                    277,
                    1149,
                    279,
                    281,
                    283,
                    285,
                    287,
                    289,
                    1954,
                    291,
                    293,
                    550,
                    295,
                    551,
                    297,
                    552,
                    553,
                    554,
                    555,
                    556,
                    557,
                    2045,
                    1202,
                    2047,
                    1599,
                    1221,
                    1607,
                    1248,
                    1131,
                    1139,
                    1271,
                    1147,
                    381,
                    1151
                ],
                "test_cases": [
                    320,
                    321,
                    322,
                    323,
                    324,
                    325,
                    335,
                    319
                ],
                "test_cases.append": [
                    320,
                    321,
                    322,
                    323,
                    324,
                    325
                ],
                "batch_size": [
                    1158,
                    327,
                    1895,
                    1896,
                    330,
                    1163,
                    1162,
                    1176,
                    1180
                ],
                "batch_shape": [
                    333
                ],
                "axes": [
                    357,
                    335,
                    338,
                    371,
                    343
                ],
                "x_np": [
                    366,
                    336,
                    369,
                    338,
                    1042,
                    1044,
                    1053,
                    1054,
                    351
                ],
                "random": [
                    336,
                    337
                ],
                "y_np": [
                    366,
                    337,
                    338,
                    370,
                    351
                ],
                "z_np": [
                    361,
                    366,
                    338,
                    375,
                    378,
                    347,
                    351
                ],
                "KNP.batch_dot": [
                    338
                ],
                "K.batch_dot": [
                    371,
                    357,
                    343
                ],
                "z_shape": [
                    359,
                    360,
                    361,
                    362,
                    373,
                    374,
                    375,
                    345,
                    346,
                    347
                ],
                "K.int_shape": [
                    359,
                    345,
                    431,
                    1583,
                    373,
                    1561
                ],
                "z_np.shape": [
                    347,
                    375
                ],
                "K.function": [
                    676,
                    733,
                    364,
                    748,
                    1646,
                    725,
                    1882,
                    700,
                    349
                ],
                "z_np.ndim": [
                    361
                ],
                "check_composed_tensor_operations": [
                    395
                ],
                "K.batch_flatten": [
                    407
                ],
                "y._keras_shape": [
                    448,
                    430,
                    431,
                    468,
                    409,
                    445,
                    413
                ],
                "K.flatten": [
                    411
                ],
                "pytest.mark.skipif": [
                    1792,
                    1920,
                    399,
                    662,
                    1047,
                    2080,
                    684,
                    1965,
                    566,
                    1720,
                    957,
                    575,
                    1855,
                    1983,
                    582,
                    1991,
                    1226,
                    715,
                    2125,
                    1617,
                    594,
                    2005,
                    1628,
                    606,
                    991,
                    740,
                    1253,
                    1639,
                    1514,
                    1901,
                    1774,
                    1653,
                    637
                ],
                "pytest.mark": [
                    1792,
                    1920,
                    2050,
                    1293,
                    399,
                    662,
                    1047,
                    2080,
                    1188,
                    684,
                    1965,
                    1326,
                    566,
                    1207,
                    1720,
                    957,
                    575,
                    1855,
                    1983,
                    582,
                    1991,
                    1226,
                    715,
                    1228,
                    1101,
                    2125,
                    1617,
                    594,
                    1619,
                    2005,
                    470,
                    1628,
                    606,
                    991,
                    1630,
                    740,
                    1253,
                    1255,
                    1639,
                    1640,
                    1514,
                    1515,
                    1901,
                    1774,
                    1777,
                    1653,
                    2043,
                    1276,
                    637,
                    1023
                ],
                "reps": [
                    416,
                    429,
                    423
                ],
                "ndims": [
                    417,
                    418,
                    421
                ],
                "np.arange": [
                    418,
                    419,
                    452,
                    2025,
                    435,
                    2014
                ],
                "arr": [
                    419,
                    436,
                    435,
                    422
                ],
                "reshape": [
                    419,
                    452,
                    435
                ],
                "np.prod": [
                    419,
                    452,
                    435
                ],
                "rep_axis": [
                    427,
                    429,
                    421,
                    423
                ],
                "K.repeat_elements": [
                    429
                ],
                "n": [
                    443,
                    444,
                    446,
                    447
                ],
                "K.tile": [
                    964,
                    444,
                    447
                ],
                "ref": [
                    452,
                    454,
                    456,
                    1688,
                    1716,
                    1686,
                    1718,
                    1656,
                    1658,
                    1693,
                    1695
                ],
                "inds": [
                    456,
                    453,
                    454
                ],
                "k.gather": [
                    456,
                    454
                ],
                "indices": [
                    1896,
                    1897,
                    1898,
                    466,
                    467
                ],
                "K.gather": [
                    467
                ],
                "v_list": [
                    481,
                    475,
                    479
                ],
                "pytest.mark.parametrize": [
                    2050,
                    1188,
                    1255,
                    1640,
                    1515,
                    1228,
                    1101,
                    1293,
                    1326,
                    1777,
                    1619,
                    470,
                    1207,
                    2043,
                    1276,
                    1630,
                    1023
                ],
                "first": [
                    561,
                    564
                ],
                "K.get_uid": [
                    561,
                    562,
                    564
                ],
                "K.reset_uids": [
                    563
                ],
                "np.random.randn": [
                    585,
                    587,
                    597,
                    599,
                    1950
                ],
                "x_var": [
                    586,
                    590,
                    592,
                    598,
                    602,
                    604
                ],
                "increment": [
                    587,
                    589,
                    590
                ],
                "K.update_add": [
                    590
                ],
                "decrement": [
                    601,
                    602,
                    599
                ],
                "K.update_sub": [
                    602
                ],
                "x_list": [
                    610,
                    613,
                    646,
                    649,
                    659
                ],
                "zero_list": [
                    620,
                    625,
                    612,
                    623
                ],
                "exp": [
                    614,
                    615,
                    617,
                    618,
                    651,
                    653
                ],
                "k.exp": [
                    614
                ],
                "loss": [
                    616,
                    617,
                    618,
                    615
                ],
                "k.sum": [
                    615
                ],
                "zero_loss": [
                    616,
                    618
                ],
                "k.stop_gradient": [
                    616
                ],
                "grad": [
                    617,
                    619
                ],
                "k.gradients": [
                    617,
                    618
                ],
                "zero_grad": [
                    618,
                    620
                ],
                "z_list.append": [
                    1064,
                    619,
                    1077
                ],
                "zero_list.append": [
                    620
                ],
                "a": [
                    2016,
                    1986,
                    2024,
                    2025,
                    2026,
                    1999,
                    2000,
                    632,
                    633,
                    634,
                    2013,
                    2014
                ],
                "b": [
                    1184,
                    1185,
                    1986,
                    1613,
                    1615,
                    1168,
                    1169,
                    1170,
                    1999,
                    2000,
                    633,
                    634,
                    635,
                    1183
                ],
                "K.square": [
                    633
                ],
                "d": [
                    634
                ],
                "K.stop_gradient": [
                    634,
                    635
                ],
                "e": [
                    635
                ],
                "test_backend": [
                    641,
                    659,
                    647
                ],
                "input_val": [
                    656,
                    643
                ],
                "f_list": [
                    656,
                    645,
                    654
                ],
                "x_list.append": [
                    649
                ],
                "k.placeholder": [
                    650
                ],
                "k.square": [
                    651
                ],
                "update": [
                    652,
                    653
                ],
                "k.function": [
                    653
                ],
                "f_list.append": [
                    654
                ],
                "function_outputs_list": [
                    656,
                    657
                ],
                "new_val_list": [
                    659,
                    660
                ],
                "k.get_value": [
                    659
                ],
                "x_placeholder": [
                    673,
                    676,
                    677,
                    678,
                    733,
                    745,
                    746,
                    748,
                    734,
                    719,
                    725,
                    726,
                    695,
                    700,
                    701,
                    702
                ],
                "y_placeholder": [
                    704,
                    674,
                    676,
                    677,
                    710,
                    720,
                    725,
                    726,
                    696,
                    698,
                    733,
                    734
                ],
                "K.update": [
                    704,
                    679
                ],
                "output": [
                    736,
                    705,
                    706,
                    737,
                    711,
                    680,
                    681,
                    712,
                    749,
                    750,
                    729,
                    730
                ],
                "run": [
                    713,
                    682,
                    707
                ],
                "K.get_session": [
                    713,
                    682,
                    707,
                    2131
                ],
                "KTF._is_tf_1": [
                    684,
                    2125,
                    662
                ],
                "feed_dict": [
                    698,
                    710,
                    703
                ],
                "run_options": [
                    722,
                    727
                ],
                "config_pb2.RunOptions": [
                    722
                ],
                "config_pb2": [
                    722,
                    723
                ],
                "run_metadata": [
                    738,
                    723,
                    728,
                    731,
                    735
                ],
                "config_pb2.RunMetadata": [
                    723
                ],
                "run_metadata.partition_graphs": [
                    738,
                    731
                ],
                "x_identity": [
                    746,
                    748
                ],
                "K.identity": [
                    746
                ],
                "num_samples": [
                    911,
                    922,
                    923,
                    795,
                    925,
                    800,
                    801,
                    805,
                    959,
                    836,
                    967,
                    968,
                    841,
                    969,
                    843,
                    993,
                    870,
                    999,
                    1000,
                    1001,
                    875,
                    876,
                    877,
                    880,
                    754,
                    759,
                    760,
                    763
                ],
                "input_dim": [
                    800,
                    803,
                    837,
                    871,
                    841,
                    842,
                    875,
                    878,
                    755,
                    759,
                    761,
                    796
                ],
                "output_dim": [
                    801,
                    803,
                    804,
                    838,
                    872,
                    842,
                    876,
                    877,
                    878,
                    879,
                    756,
                    760,
                    761,
                    762,
                    797
                ],
                "timesteps": [
                    800,
                    805,
                    839,
                    841,
                    873,
                    843,
                    875,
                    880,
                    757,
                    759,
                    763,
                    798
                ],
                "_": [
                    1950,
                    800,
                    801,
                    803,
                    804,
                    1346,
                    1347,
                    1349,
                    841,
                    842,
                    1354,
                    1623,
                    1879,
                    1377,
                    1634,
                    875,
                    876,
                    877,
                    878,
                    879,
                    1391,
                    1644,
                    759,
                    760,
                    761,
                    762,
                    1405
                ],
                "h0": [
                    801,
                    802,
                    905,
                    876,
                    788,
                    760,
                    830
                ],
                "wi": [
                    803,
                    807,
                    903,
                    842,
                    845,
                    878,
                    786,
                    882,
                    761,
                    828,
                    765,
                    862
                ],
                "wh": [
                    804,
                    903,
                    808,
                    879,
                    786,
                    883,
                    762,
                    828,
                    766
                ],
                "mask": [
                    898,
                    899,
                    805,
                    843,
                    781,
                    782,
                    880,
                    944,
                    1010,
                    979,
                    950,
                    823,
                    824,
                    985,
                    763,
                    1016
                ],
                "np.random.randint": [
                    805,
                    1896,
                    843,
                    1163,
                    880,
                    763
                ],
                "wi_k": [
                    902,
                    807,
                    861,
                    845,
                    785,
                    882,
                    827,
                    765
                ],
                "wh_k": [
                    902,
                    808,
                    785,
                    883,
                    827,
                    766
                ],
                "states": [
                    771,
                    772,
                    965,
                    997,
                    813,
                    814,
                    850,
                    919
                ],
                "h": [
                    772,
                    773,
                    814,
                    815,
                    889,
                    890
                ],
                "backend.dot": [
                    890,
                    851,
                    773,
                    815
                ],
                "backend": [
                    773,
                    815,
                    816,
                    851,
                    890
                ],
                "inputs": [
                    1666,
                    773,
                    1680,
                    919,
                    1815,
                    1820,
                    1823,
                    1701,
                    942,
                    815,
                    1710,
                    1840,
                    948,
                    964,
                    977,
                    851,
                    1749,
                    983,
                    1754,
                    1759,
                    1761,
                    1763,
                    997,
                    1008,
                    1014,
                    890
                ],
                "w_i": [
                    890,
                    851,
                    773,
                    815
                ],
                "w_h": [
                    890,
                    773,
                    815
                ],
                "simple_rnn": [
                    776
                ],
                "kwargs_list": [
                    901,
                    778,
                    784,
                    820,
                    856,
                    826,
                    860,
                    895
                ],
                "check_rnn_operation": [
                    785,
                    827,
                    861,
                    902
                ],
                "get_step_function": [
                    902,
                    903,
                    785,
                    786,
                    827,
                    828,
                    861,
                    862
                ],
                "h1": [
                    802,
                    830
                ],
                "np.concatenate": [
                    802
                ],
                "backend.concatenate": [
                    816
                ],
                "simple_rnn_with_extra_mock_state": [
                    818
                ],
                "simple_no_states": [
                    854
                ],
                "states_and_constants": [
                    889
                ],
                "simple_rnn_add_constant": [
                    893
                ],
                "num_timesteps": [
                    960,
                    994,
                    1018,
                    967,
                    999,
                    937,
                    969,
                    939,
                    1001,
                    1005,
                    1006,
                    912,
                    952,
                    922,
                    987,
                    925
                ],
                "state_and_io_size": [
                    913,
                    922,
                    923
                ],
                "mask_last_num_timesteps": [
                    932,
                    933,
                    939,
                    914,
                    926
                ],
                "inputs_vals": [
                    929,
                    967,
                    999,
                    972,
                    942,
                    1008,
                    977,
                    921
                ],
                "initial_state_vals": [
                    935,
                    968,
                    1000,
                    1004,
                    943,
                    1009,
                    978,
                    923
                ],
                "mask_vals": [
                    969,
                    970,
                    1001,
                    1002,
                    944,
                    1010,
                    979,
                    925,
                    926
                ],
                "np.ones": [
                    969,
                    1001,
                    1426,
                    1427,
                    1428,
                    1429,
                    1430,
                    1431,
                    1432,
                    925
                ],
                "expected_outputs": [
                    929,
                    932,
                    933,
                    972,
                    975,
                    954,
                    989
                ],
                "inputs_vals.copy": [
                    929
                ],
                "expected_state": [
                    937,
                    939,
                    955,
                    935
                ],
                "initial_state_vals.copy": [
                    1004,
                    935
                ],
                "initial_states": [
                    943,
                    1009,
                    978,
                    949,
                    1015,
                    984
                ],
                "last_output": [
                    946,
                    1012,
                    981
                ],
                "outputs": [
                    964,
                    965,
                    946,
                    1012,
                    981,
                    954,
                    989
                ],
                "last_states": [
                    946,
                    1012,
                    981,
                    955,
                    1021
                ],
                "step_function": [
                    947,
                    1013,
                    982
                ],
                "num_features": [
                    961,
                    967
                ],
                "K.expand_dims": [
                    964
                ],
                "np.repeat": [
                    972
                ],
                "expected_last_state": [
                    1021,
                    1004,
                    1005,
                    1006
                ],
                "K.logsumexp": [
                    1043,
                    1054
                ],
                "axis": [
                    1043,
                    1044
                ],
                "keepdims": [
                    1043,
                    1044
                ],
                "np.log": [
                    1044
                ],
                "np.sum": [
                    1044
                ],
                "np.exp": [
                    1044,
                    1820
                ],
                "result": [
                    1054,
                    1055
                ],
                "k.switch": [
                    1077,
                    1063
                ],
                "k.greater_equal": [
                    1076,
                    1063
                ],
                "shapes": [
                    1067,
                    1068,
                    1069,
                    1070,
                    1071
                ],
                "shapes.append": [
                    1068,
                    1069,
                    1070
                ],
                "arrays": [
                    1073,
                    1075
                ],
                "then_expr": [
                    1075,
                    1077
                ],
                "else_expr": [
                    1075,
                    1077
                ],
                "cond": [
                    1076,
                    1077
                ],
                "k.dropout": [
                    1089,
                    1082
                ],
                "np.abs": [
                    1378,
                    1379,
                    1095,
                    1386,
                    1418,
                    1423,
                    1392,
                    1400,
                    1371,
                    1372,
                    1406,
                    1087
                ],
                "mean": [
                    1376,
                    1378,
                    1411,
                    1095,
                    1416,
                    1418,
                    1367,
                    1369,
                    1371,
                    1087
                ],
                "val.shape": [
                    1090
                ],
                "K.dropout": [
                    1099
                ],
                "alpha": [
                    1115
                ],
                "max_value": [
                    2121,
                    1116,
                    2119
                ],
                "threshold": [
                    1116
                ],
                "xval": [
                    1536,
                    1538,
                    1586,
                    1588,
                    1141,
                    1499,
                    1147,
                    1564,
                    1501,
                    1566
                ],
                "np.asarray": [
                    1665,
                    1666,
                    1697,
                    1698,
                    1700,
                    1701,
                    1801,
                    2122,
                    1739,
                    1141,
                    1144,
                    1754,
                    1661,
                    1662,
                    1727
                ],
                "yval": [
                    1144,
                    1147
                ],
                "num_classes": [
                    1894,
                    1159,
                    1896,
                    1897,
                    1162,
                    1163,
                    1898,
                    1167,
                    1175,
                    1177,
                    1182
                ],
                "predictions": [
                    1168,
                    1162,
                    1179,
                    1183
                ],
                "targets": [
                    1184,
                    1169,
                    1163,
                    1180
                ],
                "b.eval": [
                    1168,
                    1183
                ],
                "b.in_top_k": [
                    1168,
                    1183
                ],
                "b.variable": [
                    1168,
                    1169,
                    1184,
                    1183
                ],
                "num_identical": [
                    1178,
                    1175
                ],
                "idx_identical": [
                    1177,
                    1179
                ],
                "np.random.choice": [
                    1177
                ],
                "np.zeros": [
                    1817,
                    1180
                ],
                "op": [
                    1249,
                    1222,
                    1289,
                    1321,
                    1355,
                    1359,
                    1203,
                    1272
                ],
                "kernel_shape": [
                    1249,
                    1347,
                    1349,
                    1222,
                    1289,
                    1203,
                    1272
                ],
                "padding": [
                    1250,
                    1570,
                    1223,
                    1352,
                    1290,
                    1323,
                    1356,
                    1548,
                    1578,
                    1582,
                    1362,
                    1204,
                    1556,
                    1588,
                    1560,
                    1273,
                    1566
                ],
                "data_format": [
                    1290,
                    1549,
                    1551,
                    1556,
                    1571,
                    1573,
                    1578,
                    1323,
                    1204,
                    1592,
                    1594,
                    1342,
                    1601,
                    1603,
                    1223,
                    1352,
                    1609,
                    1356,
                    1486,
                    1488,
                    1362,
                    1490,
                    1496,
                    1250,
                    1511,
                    1517,
                    1519,
                    1522,
                    1524,
                    1526,
                    1273,
                    1533
                ],
                "output_shape": [
                    1272,
                    1223
                ],
                "dilation_rate": [
                    1273,
                    1251
                ],
                "KC.dev.type": [
                    1226,
                    1253
                ],
                "KC.dev": [
                    1226,
                    1253
                ],
                "pool_size": [
                    1472,
                    1474,
                    1476,
                    1479,
                    1481,
                    1322,
                    1483,
                    1467,
                    1470
                ],
                "strides": [
                    1322
                ],
                "pool_mode": [
                    1323
                ],
                "input_depth": [
                    1345,
                    1348,
                    1350,
                    1343
                ],
                "depthwise": [
                    1361,
                    1355,
                    1347,
                    1351
                ],
                "depth_multiplier": [
                    1348,
                    1350
                ],
                "pointwise": [
                    1361,
                    1355,
                    1349,
                    1351
                ],
                "y1": [
                    1363,
                    1351
                ],
                "KNP.separable_conv": [
                    1351
                ],
                "y2": [
                    1363,
                    1357,
                    1359
                ],
                "std": [
                    1376,
                    1378,
                    1379,
                    1412,
                    1416,
                    1423,
                    1367,
                    1369,
                    1371,
                    1372
                ],
                "rand": [
                    1415,
                    1417,
                    1418,
                    1419,
                    1420,
                    1423,
                    1368,
                    1370,
                    1371,
                    1372,
                    1384,
                    1385,
                    1386,
                    1387,
                    1388,
                    1398,
                    1399,
                    1400,
                    1401,
                    1402
                ],
                "K.random_normal": [
                    1368,
                    1376
                ],
                "rand.shape": [
                    1385,
                    1370,
                    1417,
                    1399
                ],
                "np.mean": [
                    1378,
                    1386,
                    1418,
                    1392,
                    1400,
                    1371,
                    1406
                ],
                "np.std": [
                    1379,
                    1372,
                    1423
                ],
                "r": [
                    1376,
                    1377,
                    1390,
                    1391,
                    1404,
                    1405
                ],
                "samples": [
                    1408,
                    1377,
                    1378,
                    1379,
                    1391,
                    1392,
                    1393,
                    1394,
                    1405,
                    1406,
                    1407
                ],
                "min_val": [
                    1413,
                    1382,
                    1384,
                    1388,
                    1420,
                    1390,
                    1394
                ],
                "max_val": [
                    1414,
                    1383,
                    1384,
                    1387,
                    1419,
                    1390,
                    1393
                ],
                "K.random_uniform": [
                    1384,
                    1390
                ],
                "np.max": [
                    1387,
                    1419,
                    1393,
                    1401,
                    1407
                ],
                "np.min": [
                    1408,
                    1420,
                    1388,
                    1394,
                    1402
                ],
                "p": [
                    1397,
                    1398,
                    1400,
                    1404,
                    1406
                ],
                "K.random_binomial": [
                    1404,
                    1398
                ],
                "K.truncated_normal": [
                    1415
                ],
                "dummy_x_1d": [
                    1426,
                    1435
                ],
                "dummy_w_1d": [
                    1427,
                    1435
                ],
                "dummy_x_2d": [
                    1445,
                    1449,
                    1454,
                    1457,
                    1428,
                    1460,
                    1463,
                    1438
                ],
                "dummy_w_2d": [
                    1445,
                    1449,
                    1454,
                    1457,
                    1460,
                    1429,
                    1463,
                    1438
                ],
                "dummy_x_3d": [
                    1441,
                    1430
                ],
                "dummy_w_3d": [
                    1441,
                    1431
                ],
                "dummy_w1x1_2d": [
                    1432,
                    1457,
                    1445,
                    1454
                ],
                "K.conv1d": [
                    1435
                ],
                "K.conv2d": [
                    1438
                ],
                "K.conv3d": [
                    1441
                ],
                "K.separable_conv2d": [
                    1457,
                    1445,
                    1454
                ],
                "K.depthwise_conv2d": [
                    1449,
                    1460,
                    1463
                ],
                "K.pool2d": [
                    1472,
                    1474,
                    1476
                ],
                "K.pool3d": [
                    1481,
                    1483,
                    1479
                ],
                "K.resize_images": [
                    1501
                ],
                "height_factor": [
                    1509
                ],
                "width_factor": [
                    1510
                ],
                "self._helper_bilinear": [
                    1517,
                    1519
                ],
                "self": [
                    1517,
                    1519
                ],
                "K.resize_volumes": [
                    1538
                ],
                "K.spatial_2d_padding": [
                    1560,
                    1566
                ],
                "K.spatial_3d_padding": [
                    1588,
                    1582
                ],
                "bias_shape": [
                    1613,
                    1598,
                    1599
                ],
                "K.bias_add": [
                    1615
                ],
                "K.normalize_batch_in_training": [
                    1634,
                    1644,
                    1623
                ],
                "label_lens": [
                    1698,
                    1682,
                    1661,
                    1712
                ],
                "np.expand_dims": [
                    1697,
                    1698,
                    1661,
                    1662
                ],
                "input_lens": [
                    1681,
                    1697,
                    1662,
                    1711
                ],
                "labels": [
                    1665,
                    1700,
                    1709,
                    1679
                ],
                "k_labels": [
                    1713,
                    1683,
                    1709,
                    1679
                ],
                "k_inputs": [
                    1680,
                    1713,
                    1683,
                    1710
                ],
                "k_input_lens": [
                    1681,
                    1683,
                    1713,
                    1711
                ],
                "k_label_lens": [
                    1712,
                    1682,
                    1714,
                    1684
                ],
                "res": [
                    1713,
                    1683,
                    1716,
                    1686,
                    1718,
                    1688
                ],
                "K.ctc_batch_cost": [
                    1713,
                    1683
                ],
                "max_time_steps": [
                    1724,
                    1751
                ],
                "seq_len_0": [
                    1826,
                    1800,
                    1816,
                    1757,
                    1726
                ],
                "input_prob_matrix_0": [
                    1801,
                    1812,
                    1749,
                    1815,
                    1727
                ],
                "seq_len_1": [
                    1736,
                    1757
                ],
                "input_prob_matrix_1": [
                    1739,
                    1750
                ],
                "np.vstack": [
                    1749
                ],
                "transpose": [
                    1754
                ],
                "input_length": [
                    1760,
                    1762,
                    1826,
                    1764,
                    1893,
                    1896,
                    1841,
                    1757
                ],
                "np.int32": [
                    1897,
                    1826,
                    1757
                ],
                "decode_pred_np": [
                    1771,
                    1759
                ],
                "log_prob_pred_np": [
                    1772,
                    1759
                ],
                "KNP.ctc_decode": [
                    1759
                ],
                "decode_pred_tf": [
                    1763,
                    1766,
                    1768,
                    1840,
                    1846,
                    1851
                ],
                "log_prob_pred_tf": [
                    1840,
                    1769,
                    1848,
                    1763
                ],
                "K.ctc_decode": [
                    1840,
                    1763,
                    1879
                ],
                "decode_pred": [
                    1768,
                    1771
                ],
                "log_prob_pred": [
                    1848,
                    1769,
                    1772,
                    1853
                ],
                "np.alltrue": [
                    1851,
                    1771
                ],
                "np.allclose": [
                    1888,
                    1890,
                    2121,
                    1772,
                    1853,
                    1790
                ],
                "npt": [
                    1787,
                    1789,
                    1783
                ],
                "x_start": [
                    1786,
                    1788,
                    1789
                ],
                "tft": [
                    1787,
                    1788
                ],
                "K.constant": [
                    2039,
                    1787,
                    2035
                ],
                "test_input": [
                    1788,
                    1790
                ],
                "K.slice": [
                    1788
                ],
                "x_size": [
                    1788,
                    1789
                ],
                "expected": [
                    1789,
                    1790
                ],
                "KNP.slice": [
                    1789
                ],
                "depth": [
                    1817,
                    1798
                ],
                "np.newaxis": [
                    1833,
                    1815
                ],
                "inputs.transpose": [
                    1823
                ],
                "log_prob_truth": [
                    1828,
                    1853
                ],
                "decode_truth": [
                    1851,
                    1835
                ],
                "beam_width": [
                    1843,
                    1837
                ],
                "top_paths": [
                    1846,
                    1850,
                    1844,
                    1838
                ],
                "input_prob": [
                    1873,
                    1884,
                    1862
                ],
                "input_len": [
                    1873,
                    1884
                ],
                "input_prob.shape": [
                    1873
                ],
                "input_prob_tensor": [
                    1882,
                    1876,
                    1879
                ],
                "input_len_tensor": [
                    1882,
                    1878,
                    1879
                ],
                "paths_tensors": [
                    1883,
                    1879
                ],
                "merge_repeated": [
                    1881
                ],
                "decode_func": [
                    1882,
                    1884
                ],
                "paths": [
                    1884,
                    1885
                ],
                "decode": [
                    1888,
                    1890
                ],
                "oh": [
                    1897,
                    1899
                ],
                "KNP.one_hot": [
                    1897
                ],
                "koh": [
                    1898,
                    1899
                ],
                "K.one_hot": [
                    1898
                ],
                "np.all": [
                    1899
                ],
                "x_d": [
                    1924,
                    1928,
                    1930,
                    1934,
                    1905,
                    1909
                ],
                "x_r": [
                    1925,
                    1928,
                    1931,
                    1934,
                    1906,
                    1909
                ],
                "np.int64": [
                    1925,
                    1926,
                    1931,
                    1932,
                    1906,
                    1907
                ],
                "x_c": [
                    1926,
                    1928,
                    1932,
                    1934,
                    1907,
                    1909
                ],
                "x_sparse": [
                    1914,
                    1909,
                    1910
                ],
                "sparse.csr_matrix": [
                    1928,
                    1909,
                    1934
                ],
                "sparse": [
                    1928,
                    1909,
                    1934
                ],
                "x_dense": [
                    1915,
                    1910
                ],
                "x_sparse.toarray": [
                    1910
                ],
                "W": [
                    1912,
                    1913
                ],
                "t_W": [
                    1913,
                    1914,
                    1915
                ],
                "k_s": [
                    1939,
                    1940,
                    1942,
                    1914,
                    1917,
                    1918
                ],
                "K.dot": [
                    1914,
                    1915
                ],
                "k_d": [
                    1947,
                    1944,
                    1946,
                    1915,
                    1917,
                    1918
                ],
                "k_s.shape": [
                    1917
                ],
                "k_d.shape": [
                    1946,
                    1917
                ],
                "x_sparse_1": [
                    1928,
                    1939,
                    1936
                ],
                "x_sparse_2": [
                    1937,
                    1939,
                    1934
                ],
                "x_dense_1": [
                    1936,
                    1944
                ],
                "x_sparse_1.toarray": [
                    1936
                ],
                "x_dense_2": [
                    1944,
                    1937
                ],
                "x_sparse_2.toarray": [
                    1937
                ],
                "K.concatenate": [
                    1944,
                    1939
                ],
                "K.is_sparse": [
                    1940
                ],
                "k_s_d": [
                    1946,
                    1947,
                    1942
                ],
                "k_s_d.shape": [
                    1946
                ],
                "tensor_list": [
                    1950,
                    1959
                ],
                "stack_axis": [
                    1960,
                    1956,
                    1951
                ],
                "results": [
                    1952,
                    1961,
                    1963
                ],
                "tensor_list_var": [
                    1960,
                    1959
                ],
                "tensor": [
                    1959
                ],
                "out": [
                    1960,
                    1961
                ],
                "k.stack": [
                    1960
                ],
                "results.append": [
                    1961
                ],
                "np.random.rand": [
                    1985,
                    1967
                ],
                "vx": [
                    1998,
                    1999,
                    1968,
                    1969,
                    2000,
                    1973
                ],
                "kx": [
                    1986,
                    1988,
                    1989,
                    1969,
                    1978,
                    1980,
                    1981
                ],
                "K.map_fn": [
                    1969,
                    1972
                ],
                "K.sum": [
                    1969,
                    1973
                ],
                "kx2": [
                    1979,
                    1972,
                    1981
                ],
                "K.arange": [
                    2040,
                    2036,
                    1974
                ],
                "K.floatx": [
                    1975
                ],
                "kx.shape": [
                    1978,
                    1988
                ],
                "kx2.shape": [
                    1979
                ],
                "x.sum": [
                    1980,
                    1989
                ],
                "K.foldl": [
                    1986,
                    1999
                ],
                "p1": [
                    2002,
                    1999
                ],
                "p2": [
                    2000,
                    2002,
                    2003
                ],
                "K.foldr": [
                    2000
                ],
                "test_value": [
                    2008,
                    2012,
                    2014
                ],
                "a_list": [
                    2016,
                    2018,
                    2019,
                    2022,
                    2026,
                    2027,
                    2028,
                    2009
                ],
                "dtype_list": [
                    2010,
                    2015
                ],
                "k.arange": [
                    2024,
                    2032,
                    2012
                ],
                "np.array_equal": [
                    2025,
                    2019,
                    2028,
                    2014
                ],
                "dtype_list.append": [
                    2015
                ],
                "k.dtype": [
                    2033,
                    2015
                ],
                "a_list.append": [
                    2016,
                    2026
                ],
                "start": [
                    2021,
                    2024,
                    2025,
                    2035,
                    2036,
                    2039,
                    2040
                ],
                "stop": [
                    2024,
                    2025,
                    2021
                ],
                "step": [
                    2024,
                    2025,
                    2021
                ],
                "training": [
                    2048,
                    2053,
                    2046,
                    2055
                ],
                "old_floatx": [
                    2089,
                    2059,
                    2067,
                    2100,
                    2072,
                    2078
                ],
                "floatx": [
                    2089,
                    2059,
                    2061,
                    2065,
                    2072,
                    2076
                ],
                "initial": [
                    2065,
                    2061
                ],
                "set_floatx": [
                    2091,
                    2095,
                    2064,
                    2067,
                    2100,
                    2075,
                    2078
                ],
                "variable": [
                    2096,
                    2092
                ],
                "check_dtype": [
                    2097,
                    2093
                ],
                "K.dtype": [
                    2103,
                    2104,
                    2105,
                    2110,
                    2111
                ],
                "min_value": [
                    2121,
                    2118
                ],
                "K.clip": [
                    2121
                ],
                "threads": [
                    2128,
                    2130,
                    2132,
                    2133
                ],
                "K.clear_session": [
                    2129
                ],
                "monkeypatch.setenv": [
                    2130
                ],
                "monkeypatch": [
                    2130
                ],
                "cfg": [
                    2131,
                    2132,
                    2133
                ],
                "_config": [
                    2131
                ],
                "cfg.intra_op_parallelism_threads": [
                    2132
                ],
                "cfg.inter_op_parallelism_threads": [
                    2133
                ],
                "pytest.main": [
                    2137
                ],
                "__file__": [
                    2137
                ]
            }
        },
        "test_data": [
            {
                "test_path": "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_1/tests/keras/initializers_test.py",
                "test_function": "test_statefulness",
                "test_function_code": "@pytest.mark.parametrize('initializer',\n                         [initializers.orthogonal,\n                          initializers.uniform,\n                          initializers.normal,\n                          initializers.truncated_normal,\n                          initializers.VarianceScaling],\n                         ids=['orthogonal',\n                              'uniform',\n                              'normal',\n                              'truncated_normal',\n                              'variance_scaling'])\ndef test_statefulness(initializer):\n    # Test that calling a same seeded random initializer\n    # in succession results in different values.\n    init = initializer(seed=1337)\n    samples = [init((2, 2)) for _ in range(2)]\n    samples = [K.get_value(K.variable(x)) for x in samples]\n    assert np.mean(np.abs(samples[0] - samples[1])) > 0.",
                "test_error": "AssertionError: assert 0.0 > 0.0  +  where 0.0 = <function mean at 0x102031d40>(array([[0., 0.],\\n       [0., 0.]], dtype=float32))  +    where <function mean at 0x102031d40> = np.mean  +    and   array([[0., 0.],\\n       [0., 0.]], dtype=float32) = <ufunc 'absolute'>((array([[-0.02842846,  0.0409245 ],\\n       [-0.02760386, -0.03162803]], dtype=float32) - array([[-0.02842846,  0.0409245 ],\\n       [-0.02760386, -0.03162803]], dtype=float32)))  +      where <ufunc 'absolute'> = np.abs",
                "full_test_error": "initializer = <class 'keras.initializers.RandomUniform'>\n\n    @pytest.mark.parametrize('initializer',\n                             [initializers.orthogonal,\n                              initializers.uniform,\n                              initializers.normal,\n                              initializers.truncated_normal,\n                              initializers.VarianceScaling],\n                             ids=['orthogonal',\n                                  'uniform',\n                                  'normal',\n                                  'truncated_normal',\n                                  'variance_scaling'])\n    def test_statefulness(initializer):\n        # Test that calling a same seeded random initializer\n        # in succession results in different values.\n        init = initializer(seed=1337)\n        samples = [init((2, 2)) for _ in range(2)]\n        samples = [K.get_value(K.variable(x)) for x in samples]\n>       assert np.mean(np.abs(samples[0] - samples[1])) > 0.\nE       AssertionError: assert 0.0 > 0.0\nE        +  where 0.0 = <function mean at 0x102031d40>(array([[0., 0.],\\n       [0., 0.]], dtype=float32))\nE        +    where <function mean at 0x102031d40> = np.mean\nE        +    and   array([[0., 0.],\\n       [0., 0.]], dtype=float32) = <ufunc 'absolute'>((array([[-0.02842846,  0.0409245 ],\\n       [-0.02760386, -0.03162803]], dtype=float32) - array([[-0.02842846,  0.0409245 ],\\n       [-0.02760386, -0.03162803]], dtype=float32)))\nE        +      where <ufunc 'absolute'> = np.abs\n\ntests/keras/initializers_test.py:162: AssertionError",
                "traceback": null,
                "test_error_location": null,
                "test_function_decorators": [
                    "pytest.mark.parametrize('initializer', [initializers.orthogonal, initializers.uniform, initializers.normal, initializers.truncated_normal, initializers.VarianceScaling], ids=['orthogonal', 'uniform', 'normal', 'truncated_normal', 'variance_scaling'])"
                ]
            },
            {
                "test_path": "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_1/tests/keras/initializers_test.py",
                "test_function": "test_statefulness",
                "test_function_code": "@pytest.mark.parametrize('initializer',\n                         [initializers.orthogonal,\n                          initializers.uniform,\n                          initializers.normal,\n                          initializers.truncated_normal,\n                          initializers.VarianceScaling],\n                         ids=['orthogonal',\n                              'uniform',\n                              'normal',\n                              'truncated_normal',\n                              'variance_scaling'])\ndef test_statefulness(initializer):\n    # Test that calling a same seeded random initializer\n    # in succession results in different values.\n    init = initializer(seed=1337)\n    samples = [init((2, 2)) for _ in range(2)]\n    samples = [K.get_value(K.variable(x)) for x in samples]\n    assert np.mean(np.abs(samples[0] - samples[1])) > 0.",
                "test_error": "AssertionError: assert 0.0 > 0.0  +  where 0.0 = <function mean at 0x10fcd1d40>(array([[0., 0.],\\n       [0., 0.]], dtype=float32))  +    where <function mean at 0x10fcd1d40> = np.mean  +    and   array([[0., 0.],\\n       [0., 0.]], dtype=float32) = <ufunc 'absolute'>((array([[ 0.35502383,  0.93485725],\\n       [ 0.93485725, -0.35502383]], dtype=float32) - array([[ 0.35502383,  0.93485725],\\n       [ 0.93485725, -0.35502383]], dtype=float32)))  +      where <ufunc 'absolute'> = np.abs",
                "full_test_error": "initializer = <class 'keras.initializers.Orthogonal'>\n\n    @pytest.mark.parametrize('initializer',\n                             [initializers.orthogonal,\n                              initializers.uniform,\n                              initializers.normal,\n                              initializers.truncated_normal,\n                              initializers.VarianceScaling],\n                             ids=['orthogonal',\n                                  'uniform',\n                                  'normal',\n                                  'truncated_normal',\n                                  'variance_scaling'])\n    def test_statefulness(initializer):\n        # Test that calling a same seeded random initializer\n        # in succession results in different values.\n        init = initializer(seed=1337)\n        samples = [init((2, 2)) for _ in range(2)]\n        samples = [K.get_value(K.variable(x)) for x in samples]\n>       assert np.mean(np.abs(samples[0] - samples[1])) > 0.\nE       AssertionError: assert 0.0 > 0.0\nE        +  where 0.0 = <function mean at 0x10fcd1d40>(array([[0., 0.],\\n       [0., 0.]], dtype=float32))\nE        +    where <function mean at 0x10fcd1d40> = np.mean\nE        +    and   array([[0., 0.],\\n       [0., 0.]], dtype=float32) = <ufunc 'absolute'>((array([[ 0.35502383,  0.93485725],\\n       [ 0.93485725, -0.35502383]], dtype=float32) - array([[ 0.35502383,  0.93485725],\\n       [ 0.93485725, -0.35502383]], dtype=float32)))\nE        +      where <ufunc 'absolute'> = np.abs\n\ntests/keras/initializers_test.py:162: AssertionError",
                "traceback": null,
                "test_error_location": null,
                "test_function_decorators": [
                    "pytest.mark.parametrize('initializer', [initializers.orthogonal, initializers.uniform, initializers.normal, initializers.truncated_normal, initializers.VarianceScaling], ids=['orthogonal', 'uniform', 'normal', 'truncated_normal', 'variance_scaling'])"
                ]
            },
            {
                "test_path": "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_1/tests/keras/initializers_test.py",
                "test_function": "test_statefulness",
                "test_function_code": "@pytest.mark.parametrize('initializer',\n                         [initializers.orthogonal,\n                          initializers.uniform,\n                          initializers.normal,\n                          initializers.truncated_normal,\n                          initializers.VarianceScaling],\n                         ids=['orthogonal',\n                              'uniform',\n                              'normal',\n                              'truncated_normal',\n                              'variance_scaling'])\ndef test_statefulness(initializer):\n    # Test that calling a same seeded random initializer\n    # in succession results in different values.\n    init = initializer(seed=1337)\n    samples = [init((2, 2)) for _ in range(2)]\n    samples = [K.get_value(K.variable(x)) for x in samples]\n    assert np.mean(np.abs(samples[0] - samples[1])) > 0.",
                "test_error": "AssertionError: assert 0.0 > 0.0  +  where 0.0 = <function mean at 0x102031d40>(array([[0., 0.],\\n       [0., 0.]], dtype=float32))  +    where <function mean at 0x102031d40> = np.mean  +    and   array([[0., 0.],\\n       [0., 0.]], dtype=float32) = <ufunc 'absolute'>((array([[-0.04727401,  0.07371666],\\n       [ 0.07910243,  0.03498879]], dtype=float32) - array([[-0.04727401,  0.07371666],\\n       [ 0.07910243,  0.03498879]], dtype=float32)))  +      where <ufunc 'absolute'> = np.abs",
                "full_test_error": "initializer = <class 'keras.initializers.TruncatedNormal'>\n\n    @pytest.mark.parametrize('initializer',\n                             [initializers.orthogonal,\n                              initializers.uniform,\n                              initializers.normal,\n                              initializers.truncated_normal,\n                              initializers.VarianceScaling],\n                             ids=['orthogonal',\n                                  'uniform',\n                                  'normal',\n                                  'truncated_normal',\n                                  'variance_scaling'])\n    def test_statefulness(initializer):\n        # Test that calling a same seeded random initializer\n        # in succession results in different values.\n        init = initializer(seed=1337)\n        samples = [init((2, 2)) for _ in range(2)]\n        samples = [K.get_value(K.variable(x)) for x in samples]\n>       assert np.mean(np.abs(samples[0] - samples[1])) > 0.\nE       AssertionError: assert 0.0 > 0.0\nE        +  where 0.0 = <function mean at 0x102031d40>(array([[0., 0.],\\n       [0., 0.]], dtype=float32))\nE        +    where <function mean at 0x102031d40> = np.mean\nE        +    and   array([[0., 0.],\\n       [0., 0.]], dtype=float32) = <ufunc 'absolute'>((array([[-0.04727401,  0.07371666],\\n       [ 0.07910243,  0.03498879]], dtype=float32) - array([[-0.04727401,  0.07371666],\\n       [ 0.07910243,  0.03498879]], dtype=float32)))\nE        +      where <ufunc 'absolute'> = np.abs\n\ntests/keras/initializers_test.py:162: AssertionError",
                "traceback": null,
                "test_error_location": null,
                "test_function_decorators": [
                    "pytest.mark.parametrize('initializer', [initializers.orthogonal, initializers.uniform, initializers.normal, initializers.truncated_normal, initializers.VarianceScaling], ids=['orthogonal', 'uniform', 'normal', 'truncated_normal', 'variance_scaling'])"
                ]
            },
            {
                "test_path": "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_1/tests/keras/initializers_test.py",
                "test_function": "test_statefulness",
                "test_function_code": "@pytest.mark.parametrize('initializer',\n                         [initializers.orthogonal,\n                          initializers.uniform,\n                          initializers.normal,\n                          initializers.truncated_normal,\n                          initializers.VarianceScaling],\n                         ids=['orthogonal',\n                              'uniform',\n                              'normal',\n                              'truncated_normal',\n                              'variance_scaling'])\ndef test_statefulness(initializer):\n    # Test that calling a same seeded random initializer\n    # in succession results in different values.\n    init = initializer(seed=1337)\n    samples = [init((2, 2)) for _ in range(2)]\n    samples = [K.get_value(K.variable(x)) for x in samples]\n    assert np.mean(np.abs(samples[0] - samples[1])) > 0.",
                "test_error": "AssertionError: assert 0.0 > 0.0  +  where 0.0 = <function mean at 0x10fcd1d40>(array([[0., 0.],\\n       [0., 0.]], dtype=float32))  +    where <function mean at 0x10fcd1d40> = np.mean  +    and   array([[0., 0.],\\n       [0., 0.]], dtype=float32) = <ufunc 'absolute'>((array([[-0.04727401,  0.07371666],\\n       [ 0.07910243,  0.03498879]], dtype=float32) - array([[-0.04727401,  0.07371666],\\n       [ 0.07910243,  0.03498879]], dtype=float32)))  +      where <ufunc 'absolute'> = np.abs",
                "full_test_error": "initializer = <class 'keras.initializers.RandomNormal'>\n\n    @pytest.mark.parametrize('initializer',\n                             [initializers.orthogonal,\n                              initializers.uniform,\n                              initializers.normal,\n                              initializers.truncated_normal,\n                              initializers.VarianceScaling],\n                             ids=['orthogonal',\n                                  'uniform',\n                                  'normal',\n                                  'truncated_normal',\n                                  'variance_scaling'])\n    def test_statefulness(initializer):\n        # Test that calling a same seeded random initializer\n        # in succession results in different values.\n        init = initializer(seed=1337)\n        samples = [init((2, 2)) for _ in range(2)]\n        samples = [K.get_value(K.variable(x)) for x in samples]\n>       assert np.mean(np.abs(samples[0] - samples[1])) > 0.\nE       AssertionError: assert 0.0 > 0.0\nE        +  where 0.0 = <function mean at 0x10fcd1d40>(array([[0., 0.],\\n       [0., 0.]], dtype=float32))\nE        +    where <function mean at 0x10fcd1d40> = np.mean\nE        +    and   array([[0., 0.],\\n       [0., 0.]], dtype=float32) = <ufunc 'absolute'>((array([[-0.04727401,  0.07371666],\\n       [ 0.07910243,  0.03498879]], dtype=float32) - array([[-0.04727401,  0.07371666],\\n       [ 0.07910243,  0.03498879]], dtype=float32)))\nE        +      where <ufunc 'absolute'> = np.abs\n\ntests/keras/initializers_test.py:162: AssertionError",
                "traceback": null,
                "test_error_location": null,
                "test_function_decorators": [
                    "pytest.mark.parametrize('initializer', [initializers.orthogonal, initializers.uniform, initializers.normal, initializers.truncated_normal, initializers.VarianceScaling], ids=['orthogonal', 'uniform', 'normal', 'truncated_normal', 'variance_scaling'])"
                ]
            },
            {
                "test_path": "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_1/tests/keras/initializers_test.py",
                "test_function": "test_statefulness",
                "test_function_code": "@pytest.mark.parametrize('initializer',\n                         [initializers.orthogonal,\n                          initializers.uniform,\n                          initializers.normal,\n                          initializers.truncated_normal,\n                          initializers.VarianceScaling],\n                         ids=['orthogonal',\n                              'uniform',\n                              'normal',\n                              'truncated_normal',\n                              'variance_scaling'])\ndef test_statefulness(initializer):\n    # Test that calling a same seeded random initializer\n    # in succession results in different values.\n    init = initializer(seed=1337)\n    samples = [init((2, 2)) for _ in range(2)]\n    samples = [K.get_value(K.variable(x)) for x in samples]\n    assert np.mean(np.abs(samples[0] - samples[1])) > 0.",
                "test_error": "AssertionError: assert 0.0 > 0.0  +  where 0.0 = <function mean at 0x102031d40>(array([[0., 0.],\\n       [0., 0.]], dtype=float32))  +    where <function mean at 0x102031d40> = np.mean  +    and   array([[0., 0.],\\n       [0., 0.]], dtype=float32) = <ufunc 'absolute'>((array([[-0.7600454,  1.1851757],\\n       [ 1.2717651,  0.5625304]], dtype=float32) - array([[-0.7600454,  1.1851757],\\n       [ 1.2717651,  0.5625304]], dtype=float32)))  +      where <ufunc 'absolute'> = np.abs",
                "full_test_error": "initializer = <class 'keras.initializers.VarianceScaling'>\n\n    @pytest.mark.parametrize('initializer',\n                             [initializers.orthogonal,\n                              initializers.uniform,\n                              initializers.normal,\n                              initializers.truncated_normal,\n                              initializers.VarianceScaling],\n                             ids=['orthogonal',\n                                  'uniform',\n                                  'normal',\n                                  'truncated_normal',\n                                  'variance_scaling'])\n    def test_statefulness(initializer):\n        # Test that calling a same seeded random initializer\n        # in succession results in different values.\n        init = initializer(seed=1337)\n        samples = [init((2, 2)) for _ in range(2)]\n        samples = [K.get_value(K.variable(x)) for x in samples]\n>       assert np.mean(np.abs(samples[0] - samples[1])) > 0.\nE       AssertionError: assert 0.0 > 0.0\nE        +  where 0.0 = <function mean at 0x102031d40>(array([[0., 0.],\\n       [0., 0.]], dtype=float32))\nE        +    where <function mean at 0x102031d40> = np.mean\nE        +    and   array([[0., 0.],\\n       [0., 0.]], dtype=float32) = <ufunc 'absolute'>((array([[-0.7600454,  1.1851757],\\n       [ 1.2717651,  0.5625304]], dtype=float32) - array([[-0.7600454,  1.1851757],\\n       [ 1.2717651,  0.5625304]], dtype=float32)))\nE        +      where <ufunc 'absolute'> = np.abs\n\ntests/keras/initializers_test.py:162: AssertionError",
                "traceback": null,
                "test_error_location": null,
                "test_function_decorators": [
                    "pytest.mark.parametrize('initializer', [initializers.orthogonal, initializers.uniform, initializers.normal, initializers.truncated_normal, initializers.VarianceScaling], ids=['orthogonal', 'uniform', 'normal', 'truncated_normal', 'variance_scaling'])"
                ]
            }
        ]
    }
}