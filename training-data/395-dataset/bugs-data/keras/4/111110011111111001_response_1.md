The error message indicates that the `compute_gradients` method is being called with three positional arguments when it only takes two. This is likely the cause of the bug in the `get_updates` method of the `TFOptimizer` class.

The reason for this bug is that the `compute_gradients` method in the `MyTfOptimizer` class only expects `loss` as an argument, but it is being called with both `loss` and `params` in the `get_updates` method of the `TFOptimizer` class. This leads to a TypeError due to the mismatch in the number of arguments.

To fix this bug, the `compute_gradients` method should be modified to accept `params` as well. This would ensure that the `get_updates` method can call `compute_gradients` with both `loss` and `params` without causing a TypeError.

In addition, the `apply_gradients` method in the `MyTfOptimizer` class should be modified to accept `global_step` as an additional keyword argument, in order to support the call from the `get_updates` method.

Here's the corrected code for the `TFOptimizer` class:

```python
class TFOptimizer(Optimizer):
    """
    Wrapper class for native TensorFlow optimizers.
        
    """

    # ... omitted code ...

    @interfaces.legacy_get_updates_support
    def get_updates(self, loss, params):
        grads = self.optimizer.compute_gradients(loss, params)
        self.updates = [K.update_add(self.iterations, 1)]
        opt_update = self.optimizer.apply_gradients(
            grads, global_step=self.iterations)
        self.updates.append(opt_update)
        return self.updates
```

And the corrected code for the `MyTfOptimizer` class:

```python
class MyTfOptimizer(train.Optimizer):
    wrapping_optimizer = train.AdamOptimizer()

    def compute_gradients(self, loss, params):
        return super(MyTfOptimizer, self).compute_gradients(loss, params)

    def apply_gradients(self, grads_and_vars, global_step=None):
        return self.wrapping_optimizer.apply_gradients(grads_and_vars,
                                                       global_step=global_step)
```