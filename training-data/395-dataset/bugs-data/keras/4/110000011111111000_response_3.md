```python
# This is the fixed function
@interfaces.legacy_get_updates_support
def get_updates(self, loss, params):
    grads_and_vars = self.optimizer.compute_gradients(loss, var_list=params)
    if isinstance(self.optimizer, tf.train.Optimizer):
        self.updates = [K.update_add(self.iterations, 1).op]
        for var, grad in grads_and_vars:
            if grad is None:
                continue
            update_op = self.optimizer.apply_gradients([(grad, var)])
            self.updates.append(update_op)
    else:
        # check out how to make updates for AdamOptimizer
        raise ValueError('Do not know how to make update for Keras engine. '
                         'Try other Optimizer in tensorflow.keras.optimizers')
    return self.updates
```