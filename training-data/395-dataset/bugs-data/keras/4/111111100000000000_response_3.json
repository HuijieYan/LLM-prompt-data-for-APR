{
    "keras": [
        {
            "bugID": 4,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 1,
                "3": 1,
                "4": 0,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 1,
                "3": 1,
                "4": 0,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "start_line": 704,
            "file_name": "keras/optimizers.py",
            "replace_code": "def get_updates(self, loss, params):\n\n    grads_and_vars = self.optimizer.compute_gradients(loss, params)\n    self.updates = [K.update_add(self.iterations, 1)]\n    if isinstance(self.optimizer, tf.train.Optimizer):\n        apply_updates = self.optimizer.apply_gradients(grads_and_vars, global_step=self.iterations)\n    else:\n        apply_updates = self.optimizer.apply_gradients(grads_and_vars)\n    self.updates.append(apply_updates)\n    return self.updates"
        }
    ]
}