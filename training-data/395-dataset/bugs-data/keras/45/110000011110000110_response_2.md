```python
def call(self, inputs, states, training=None):
    # dropout matrices for input units
    dp_mask = self._dropout_mask
    # dropout matrices for recurrent units
    rec_dp_mask = self._recurrent_dropout_mask
    
    h_tm1 = states[0]  # previous memory state
    c_tm1 = states[1]  # previous carry state
    
    if self.implementation == 1:

        if 0 < self.dropout < 1.:
            inputs_i = inputs * dp_mask[0]
            inputs_f = inputs * dp_mask[1]
            inputs_c = inputs * dp_mask[2]
            inputs_o = inputs * dp_mask[3]
        else:
            inputs_i = inputs
            inputs_f = inputs
            inputs_c = inputs
            inputs_o = inputs

        if 0 < self.recurrent_dropout < 1.:
            h_tm1_i = h_tm1 * rec_dp_mask[0]
            h_tm1_f = h_tm1 * rec_dp_mask[1]
            h_tm1_c = h_tm1 * rec_dp_mask[2]
            h_tm1_o = h_tm1 * rec_dp_mask[3]
        else:
            h_tm1_i = h_tm1
            h_tm1_f = h_tm1
            h_tm1_c = h_tm1
            h_tm1_o = h_tm1
        
        i = self.recurrent_activation(
            K.dot(inputs_i, self.kernel_i) + K.dot(h_tm1_i, self.recurrent_kernel_i) + self.bias_i
        )
        f = self.recurrent_activation(
            K.dot(inputs_f, self.kernel_f) + K.dot(h_tm1_f, self.recurrent_kernel_f) + self.bias_f
        )
        c = f * c_tm1 + i * self.activation(
            K.dot(inputs_c, self.kernel_c) + K.dot(h_tm1_c, self.recurrent_kernel_c) + self.bias_c
        )
        o = self.recurrent_activation(
            K.dot(inputs_o, self.kernel_o) + K.dot(h_tm1_o, self.recurrent_kernel_o) + self.bias_o
        )

    else:
        if 0. < self.dropout < 1.:
            inputs *= dp_mask[0]
        z = K.dot(inputs, self.kernel)
        if 0. < self.recurrent_dropout < 1.:
            h_tm1 *= rec_dp_mask[0]
        z += K.dot(h_tm1, self.recurrent_kernel)
        if self.use_bias:
            z = K.bias_add(z, self.bias)

        z = K.reshape(z, (-1, 4, z.shape[1] // 4))

        z = K.amath.sigmoid(z)
        i = z[:, 0]
        f = z[:, 1]
        c = z[:, 2]
        o = z[:, 3]

        i = self.recurrent_activation(i)
        f = self.recurrent_activation(f)
        c = f * c_tm1 + i * self.activation(c)
        o = self.recurrent_activation(o)

    h = o * self.activation(c)
    if 0 < self.dropout + self.recurrent_dropout:
        if training is None:
            h._uses_learning_phase = True
    return h, [h, c]
```