{
    "keras": [
        {
            "bugID": 45,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 0,
                "6": 1,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "start_line": 1786,
            "file_name": "keras/layers/recurrent.py",
            "replace_code": "def call(self, inputs, states, training=None):\n    # dropout matrices for input units\n\n    dp_mask = self._dropout_mask\n    # dropout matrices for recurrent units\n    rec_dp_mask = self._recurrent_dropout_mask\n    \n    h_tm1 = states[0]  # previous memory state\n    c_tm1 = states[1]  # previous carry state\n    \n    if self.implementation == 1:\n    \n        if 0 < self.dropout < 1.:\n            inputs_i = inputs * dp_mask[0]\n            inputs_f = inputs * dp_mask[1]\n            inputs_c = inputs * dp_mask[2]\n            inputs_o = inputs * dp_mask[3]\n        else:\n            inputs_i = inputs\n            inputs_f = inputs\n            inputs_c = inputs\n            inputs_o = inputs\n    \n        if 0 < self.recurrent_dropout < 1.:\n            h_tm1_i = h_tm1 * rec_dp_mask[0]\n            h_tm1_f = h_tm1 * rec_dp_mask[1]\n            h_tm1_c = h_tm1 * rec_dp_mask[2]\n            h_tm1_o = h_tm1 * rec_dp_mask[3]\n        else:\n            h_tm1_i = h_tm1\n            h_tm1_f = h_tm1\n            h_tm1_c = h_tm1\n            h_tm1_o = h_tm1\n        \n        i = self.recurrent_activation(\n            K.dot(inputs_i, self.kernel_i) + K.dot(h_tm1_i, self.recurrent_kernel_i) + self.bias_i\n        )\n        f = self.recurrent_activation(\n            K.dot(inputs_f, self.kernel_f) + K.dot(h_tm1_f, self.recurrent_kernel_f) + self.bias_f\n        )\n        c = f * c_tm1 + i * self.activation(\n            K.dot(inputs_c, self.kernel_c) + K.dot(h_tm1_c, self.recurrent_kernel_c) + self.bias_c\n        )\n        o = self.recurrent_activation(\n            K.dot(inputs_o, self.kernel_o) + K.dot(h_tm1_o, self.recurrent_kernel_o) + self.bias_o\n        )\n    \n    else:\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n        z = K.dot(inputs, self.kernel)\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1 *= rec_dp_mask[0]\n        z += K.dot(h_tm1, self.recurrent_kernel)\n        if self.use_bias:\n            z = K.bias_add(z, self.bias)\n    \n        z = K.reshape(z, (-1, 4, z.shape[1] // 4))\n    \n        z = K.amath.sigmoid(z)\n        i = z[:, 0]\n        f = z[:, 1]\n        c = z[:, 2]\n        o = z[:, 3]\n    \n        i = self.recurrent_activation(i)\n        f = self.recurrent_activation(f)\n        c = f * c_tm1 + i * self.activation(c)\n        o = self.recurrent_activation(o)\n    \n    h = o * self.activation(c)\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            h._uses_learning_phase = True\n    return h, [h, c]"
        }
    ]
}