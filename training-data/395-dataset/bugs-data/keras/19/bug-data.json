{
    "keras:19": {
        "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_19/keras/backend/cntk_backend.py": {
            "buggy_functions": [
                {
                    "function_name": "rnn",
                    "function_code": "def rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('CNTK Backend: the input of rnn has only rank %d '\n                         'Need at least rank 3 to run RNN.' % dims)\n\n    if _get_dynamic_axis_num(inputs) == 0 or unroll:\n        return _static_rnn(\n            step_function,\n            inputs,\n            initial_states,\n            go_backwards,\n            mask,\n            constants,\n            unroll,\n            input_length)\n\n    if constants is None:\n        constants = []\n\n    num_time_step = shape[1]\n    if num_time_step is None and not has_seq_axis(inputs):\n        num_time_step = inputs.shape[0]\n\n    initial = []\n    for s in initial_states:\n        if _get_dynamic_axis_num(s) == 0:\n            if hasattr(C, 'to_batch'):\n                initial.append(C.to_batch(s))\n            else:\n                initial.append(C.user_function(ConvertToBatch(s)))\n        else:\n            initial.append(s)\n\n    need_convert = not has_seq_axis(inputs)\n    if go_backwards and need_convert is False:\n        raise NotImplementedError('CNTK Backend: `go_backwards` is not supported with '\n                                  'variable-length sequences. Please specify a '\n                                  'static length for your sequences.')\n\n    rnn_inputs = inputs\n    if need_convert:\n        if go_backwards:\n            rnn_inputs = reverse(rnn_inputs, 1)\n\n        rnn_inputs = C.to_sequence(rnn_inputs)\n\n        rnn_constants = []\n        for constant in constants:\n            if isinstance(constant, list):\n                new_c = []\n                for c in constant:\n                    if _get_dynamic_axis_num(c) == 1:\n                        new_c.append(C.sequence.broadcast_as(c, rnn_inputs))\n                    else:\n                        new_c.append(c)\n                rnn_constants.append(new_c)\n            else:\n                if _get_dynamic_axis_num(constant) == 1:\n                    rnn_constants.append(C.sequence.broadcast_as(constant, rnn_inputs))\n                else:\n                    rnn_constants.append(constant)\n    else:\n        rnn_constants = constants\n\n    if mask is not None and not has_seq_axis(mask):\n        if go_backwards:\n            mask = reverse(mask, 1)\n        if len(int_shape(mask)) == 2:\n            mask = expand_dims(mask)\n        mask = C.to_sequence_like(mask, rnn_inputs)\n\n    states = tuple(initial)\n\n    with C.default_options(axis_offset=1):\n        def _recurrence(x, states, m):\n            # create place holder\n            place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]\n            past_values = []\n            for s, p in zip(states, place_holders):\n                past_values.append(C.sequence.past_value(p, s))\n            new_output, new_states = step_function(\n                x, tuple(past_values) + tuple(rnn_constants))\n\n            if getattr(new_output, '_uses_learning_phase', False):\n                global uses_learning_phase\n                uses_learning_phase = True\n\n            if m is not None:\n                new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]\n            n_s = []\n            for o, p in zip(new_states, place_holders):\n                n_s.append(o.replace_placeholders({p: o.output}))\n            if len(n_s) > 0:\n                new_output = n_s[0]\n            return new_output, n_s\n\n        final_output, final_states = _recurrence(rnn_inputs, states, mask)\n        last_output = C.sequence.last(final_output)\n        last_states = [C.sequence.last(s) for s in final_states]\n\n    if need_convert:\n        final_output = C.sequence.unpack(final_output, 0, no_mask_output=True)\n        if num_time_step is not None and num_time_step is not C.FreeDimension:\n            final_output = _reshape_sequence(final_output, num_time_step)\n\n    f_stats = []\n    for l_s, i_s in zip(last_states, initial_states):\n        if _get_dynamic_axis_num(i_s) == 0 and _get_dynamic_axis_num(l_s) == 1:\n            if hasattr(C, 'unpack_batch'):\n                f_stats.append(C.unpack_batch(l_s))\n            else:\n                f_stats.append(C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))\n        else:\n            f_stats.append(l_s)\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, f_stats\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 1340,
                    "end_line": 1465,
                    "variables": {
                        "shape": [
                            1344,
                            1345,
                            1368
                        ],
                        "int_shape": [
                            1344,
                            1416
                        ],
                        "inputs": [
                            1344,
                            1382,
                            1354,
                            1388,
                            1357,
                            1369,
                            1370
                        ],
                        "dims": [
                            1352,
                            1345,
                            1350
                        ],
                        "len": [
                            1416,
                            1345,
                            1441
                        ],
                        "uses_learning_phase": [
                            1464,
                            1434,
                            1348
                        ],
                        "ValueError": [
                            1351
                        ],
                        "_get_dynamic_axis_num": [
                            1374,
                            1354,
                            1456,
                            1400,
                            1406
                        ],
                        "unroll": [
                            1354,
                            1362
                        ],
                        "_static_rnn": [
                            1355
                        ],
                        "step_function": [
                            1356,
                            1429
                        ],
                        "initial_states": [
                            1373,
                            1358,
                            1455
                        ],
                        "go_backwards": [
                            1414,
                            1383,
                            1390,
                            1359
                        ],
                        "mask": [
                            1413,
                            1445,
                            1415,
                            1416,
                            1417,
                            1418,
                            1360
                        ],
                        "constants": [
                            1411,
                            1361,
                            1396,
                            1365,
                            1366
                        ],
                        "input_length": [
                            1363
                        ],
                        "num_time_step": [
                            1451,
                            1452,
                            1368,
                            1369,
                            1370
                        ],
                        "has_seq_axis": [
                            1369,
                            1413,
                            1382
                        ],
                        "inputs.shape": [
                            1370
                        ],
                        "initial": [
                            1376,
                            1378,
                            1380,
                            1420,
                            1372
                        ],
                        "s": [
                            1376,
                            1378,
                            1380,
                            1447,
                            1427,
                            1428,
                            1437,
                            1373,
                            1374
                        ],
                        "hasattr": [
                            1457,
                            1375
                        ],
                        "C": [
                            1376,
                            1378,
                            1446,
                            1407,
                            1447,
                            1418,
                            1450,
                            1451,
                            1422,
                            1393,
                            1425,
                            1457,
                            1428,
                            1458,
                            1460,
                            1401,
                            1437,
                            1375
                        ],
                        "initial.append": [
                            1376,
                            1378,
                            1380
                        ],
                        "C.to_batch": [
                            1376
                        ],
                        "C.user_function": [
                            1378,
                            1460
                        ],
                        "ConvertToBatch": [
                            1378
                        ],
                        "need_convert": [
                            1449,
                            1389,
                            1382,
                            1383
                        ],
                        "NotImplementedError": [
                            1384
                        ],
                        "rnn_inputs": [
                            1445,
                            1418,
                            1388,
                            1391,
                            1393,
                            1401,
                            1407
                        ],
                        "reverse": [
                            1415,
                            1391
                        ],
                        "C.to_sequence": [
                            1393
                        ],
                        "rnn_constants": [
                            1409,
                            1411,
                            1395,
                            1430,
                            1404,
                            1407
                        ],
                        "constant": [
                            1409,
                            1396,
                            1397,
                            1399,
                            1406,
                            1407
                        ],
                        "isinstance": [
                            1397
                        ],
                        "list": [
                            1397
                        ],
                        "new_c": [
                            1401,
                            1403,
                            1404,
                            1398
                        ],
                        "c": [
                            1400,
                            1401,
                            1403,
                            1399
                        ],
                        "new_c.append": [
                            1401,
                            1403
                        ],
                        "C.sequence.broadcast_as": [
                            1401,
                            1407
                        ],
                        "C.sequence": [
                            1446,
                            1447,
                            1450,
                            1428,
                            1401,
                            1407
                        ],
                        "rnn_constants.append": [
                            1409,
                            1404,
                            1407
                        ],
                        "expand_dims": [
                            1417
                        ],
                        "C.to_sequence_like": [
                            1418
                        ],
                        "states": [
                            1425,
                            1427,
                            1420,
                            1445
                        ],
                        "tuple": [
                            1420,
                            1430
                        ],
                        "C.default_options": [
                            1422
                        ],
                        "place_holders": [
                            1425,
                            1427,
                            1439
                        ],
                        "C.placeholder": [
                            1425
                        ],
                        "x.dynamic_axes": [
                            1425
                        ],
                        "x": [
                            1425,
                            1430
                        ],
                        "_": [
                            1425
                        ],
                        "past_values": [
                            1426,
                            1428,
                            1437,
                            1430
                        ],
                        "p": [
                            1440,
                            1427,
                            1428,
                            1439
                        ],
                        "zip": [
                            1455,
                            1427,
                            1437,
                            1439
                        ],
                        "past_values.append": [
                            1428
                        ],
                        "C.sequence.past_value": [
                            1428
                        ],
                        "new_output": [
                            1432,
                            1442,
                            1443,
                            1429
                        ],
                        "new_states": [
                            1429,
                            1437,
                            1439
                        ],
                        "getattr": [
                            1432
                        ],
                        "m": [
                            1436,
                            1437
                        ],
                        "C.element_select": [
                            1437
                        ],
                        "n": [
                            1437
                        ],
                        "n_s": [
                            1440,
                            1441,
                            1442,
                            1443,
                            1438
                        ],
                        "o": [
                            1440,
                            1439
                        ],
                        "n_s.append": [
                            1440
                        ],
                        "o.replace_placeholders": [
                            1440
                        ],
                        "o.output": [
                            1440
                        ],
                        "final_output": [
                            1445,
                            1446,
                            1450,
                            1452,
                            1465
                        ],
                        "final_states": [
                            1445,
                            1447
                        ],
                        "_recurrence": [
                            1445
                        ],
                        "last_output": [
                            1464,
                            1465,
                            1446
                        ],
                        "C.sequence.last": [
                            1446,
                            1447
                        ],
                        "last_states": [
                            1455,
                            1447
                        ],
                        "C.sequence.unpack": [
                            1450
                        ],
                        "C.FreeDimension": [
                            1451
                        ],
                        "_reshape_sequence": [
                            1452
                        ],
                        "f_stats": [
                            1454,
                            1458,
                            1460,
                            1462,
                            1465
                        ],
                        "l_s": [
                            1455,
                            1456,
                            1458,
                            1460,
                            1462
                        ],
                        "i_s": [
                            1456,
                            1460,
                            1455
                        ],
                        "f_stats.append": [
                            1458,
                            1460,
                            1462
                        ],
                        "C.unpack_batch": [
                            1458
                        ],
                        "ConvertToStatic": [
                            1460
                        ],
                        "i_s.shape": [
                            1460
                        ],
                        "last_output._uses_learning_phase": [
                            1464
                        ]
                    },
                    "filtered_variables": {
                        "shape": [
                            1344,
                            1345,
                            1368
                        ],
                        "int_shape": [
                            1344,
                            1416
                        ],
                        "inputs": [
                            1344,
                            1382,
                            1354,
                            1388,
                            1357,
                            1369,
                            1370
                        ],
                        "dims": [
                            1352,
                            1345,
                            1350
                        ],
                        "uses_learning_phase": [
                            1464,
                            1434,
                            1348
                        ],
                        "_get_dynamic_axis_num": [
                            1374,
                            1354,
                            1456,
                            1400,
                            1406
                        ],
                        "unroll": [
                            1354,
                            1362
                        ],
                        "_static_rnn": [
                            1355
                        ],
                        "step_function": [
                            1356,
                            1429
                        ],
                        "initial_states": [
                            1373,
                            1358,
                            1455
                        ],
                        "go_backwards": [
                            1414,
                            1383,
                            1390,
                            1359
                        ],
                        "mask": [
                            1413,
                            1445,
                            1415,
                            1416,
                            1417,
                            1418,
                            1360
                        ],
                        "constants": [
                            1411,
                            1361,
                            1396,
                            1365,
                            1366
                        ],
                        "input_length": [
                            1363
                        ],
                        "num_time_step": [
                            1451,
                            1452,
                            1368,
                            1369,
                            1370
                        ],
                        "has_seq_axis": [
                            1369,
                            1413,
                            1382
                        ],
                        "inputs.shape": [
                            1370
                        ],
                        "initial": [
                            1376,
                            1378,
                            1380,
                            1420,
                            1372
                        ],
                        "s": [
                            1376,
                            1378,
                            1380,
                            1447,
                            1427,
                            1428,
                            1437,
                            1373,
                            1374
                        ],
                        "C": [
                            1376,
                            1378,
                            1446,
                            1407,
                            1447,
                            1418,
                            1450,
                            1451,
                            1422,
                            1393,
                            1425,
                            1457,
                            1428,
                            1458,
                            1460,
                            1401,
                            1437,
                            1375
                        ],
                        "initial.append": [
                            1376,
                            1378,
                            1380
                        ],
                        "C.to_batch": [
                            1376
                        ],
                        "C.user_function": [
                            1378,
                            1460
                        ],
                        "ConvertToBatch": [
                            1378
                        ],
                        "need_convert": [
                            1449,
                            1389,
                            1382,
                            1383
                        ],
                        "rnn_inputs": [
                            1445,
                            1418,
                            1388,
                            1391,
                            1393,
                            1401,
                            1407
                        ],
                        "reverse": [
                            1415,
                            1391
                        ],
                        "C.to_sequence": [
                            1393
                        ],
                        "rnn_constants": [
                            1409,
                            1411,
                            1395,
                            1430,
                            1404,
                            1407
                        ],
                        "constant": [
                            1409,
                            1396,
                            1397,
                            1399,
                            1406,
                            1407
                        ],
                        "new_c": [
                            1401,
                            1403,
                            1404,
                            1398
                        ],
                        "c": [
                            1400,
                            1401,
                            1403,
                            1399
                        ],
                        "new_c.append": [
                            1401,
                            1403
                        ],
                        "C.sequence.broadcast_as": [
                            1401,
                            1407
                        ],
                        "C.sequence": [
                            1446,
                            1447,
                            1450,
                            1428,
                            1401,
                            1407
                        ],
                        "rnn_constants.append": [
                            1409,
                            1404,
                            1407
                        ],
                        "expand_dims": [
                            1417
                        ],
                        "C.to_sequence_like": [
                            1418
                        ],
                        "states": [
                            1425,
                            1427,
                            1420,
                            1445
                        ],
                        "C.default_options": [
                            1422
                        ],
                        "place_holders": [
                            1425,
                            1427,
                            1439
                        ],
                        "C.placeholder": [
                            1425
                        ],
                        "x.dynamic_axes": [
                            1425
                        ],
                        "x": [
                            1425,
                            1430
                        ],
                        "_": [
                            1425
                        ],
                        "past_values": [
                            1426,
                            1428,
                            1437,
                            1430
                        ],
                        "p": [
                            1440,
                            1427,
                            1428,
                            1439
                        ],
                        "past_values.append": [
                            1428
                        ],
                        "C.sequence.past_value": [
                            1428
                        ],
                        "new_output": [
                            1432,
                            1442,
                            1443,
                            1429
                        ],
                        "new_states": [
                            1429,
                            1437,
                            1439
                        ],
                        "m": [
                            1436,
                            1437
                        ],
                        "C.element_select": [
                            1437
                        ],
                        "n": [
                            1437
                        ],
                        "n_s": [
                            1440,
                            1441,
                            1442,
                            1443,
                            1438
                        ],
                        "o": [
                            1440,
                            1439
                        ],
                        "n_s.append": [
                            1440
                        ],
                        "o.replace_placeholders": [
                            1440
                        ],
                        "o.output": [
                            1440
                        ],
                        "final_output": [
                            1445,
                            1446,
                            1450,
                            1452,
                            1465
                        ],
                        "final_states": [
                            1445,
                            1447
                        ],
                        "_recurrence": [
                            1445
                        ],
                        "last_output": [
                            1464,
                            1465,
                            1446
                        ],
                        "C.sequence.last": [
                            1446,
                            1447
                        ],
                        "last_states": [
                            1455,
                            1447
                        ],
                        "C.sequence.unpack": [
                            1450
                        ],
                        "C.FreeDimension": [
                            1451
                        ],
                        "_reshape_sequence": [
                            1452
                        ],
                        "f_stats": [
                            1454,
                            1458,
                            1460,
                            1462,
                            1465
                        ],
                        "l_s": [
                            1455,
                            1456,
                            1458,
                            1460,
                            1462
                        ],
                        "i_s": [
                            1456,
                            1460,
                            1455
                        ],
                        "f_stats.append": [
                            1458,
                            1460,
                            1462
                        ],
                        "C.unpack_batch": [
                            1458
                        ],
                        "ConvertToStatic": [
                            1460
                        ],
                        "i_s.shape": [
                            1460
                        ],
                        "last_output._uses_learning_phase": [
                            1464
                        ]
                    },
                    "diff_line_number": 1442,
                    "class_data": null,
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "inscope_functions": [
                "@contextmanager\ndef name_scope(name):\n    global NAME_SCOPE_STACK\n    NAME_SCOPE_STACK.append(name)\n    yield\n    NAME_SCOPE_STACK.pop()",
                "def get_uid(prefix=''):\n    _UID_PREFIXES[prefix] += 1\n    return _UID_PREFIXES[prefix]",
                "def learning_phase():\n    # If _LEARNING_PHASE is not 0 or 1, return dynamic learning phase tensor\n    return _LEARNING_PHASE if _LEARNING_PHASE in {0, 1} else _LEARNING_PHASE_PLACEHOLDER",
                "def set_learning_phase(value):\n    global _LEARNING_PHASE\n    if value not in {0, 1}:\n        raise ValueError('CNTK Backend: Set learning phase '\n                         'with value %s is not supported, '\n                         'expected 0 or 1.' % value)\n    _LEARNING_PHASE = value",
                "def clear_session():\n    \"\"\"Reset learning phase flag for cntk backend.\n    \"\"\"\n    global _LEARNING_PHASE\n    global _LEARNING_PHASE_PLACEHOLDER\n    _LEARNING_PHASE = -1\n    _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(1.0)",
                "def in_train_phase(x, alt, training=None):\n    global _LEARNING_PHASE\n    if training is None:\n        training = learning_phase()\n        uses_learning_phase = True\n    else:\n        uses_learning_phase = False\n\n    # CNTK currently don't support cond op, so here we use\n    # element_select approach as workaround. It may have\n    # perf issue, will resolve it later with cntk cond op.\n    if callable(x) and isinstance(x, C.cntk_py.Function) is False:\n        x = x()\n    if callable(alt) and isinstance(alt, C.cntk_py.Function) is False:\n        alt = alt()\n\n    if training is True:\n        x._uses_learning_phase = uses_learning_phase\n        return x\n    else:\n        # if _LEARNING_PHASE is static\n        if isinstance(training, int) or isinstance(training, bool):\n            result = x if training == 1 or training is True else alt\n        else:\n            result = C.element_select(training, x, alt)\n        result._uses_learning_phase = uses_learning_phase\n        return result",
                "def in_test_phase(x, alt, training=None):\n    return in_train_phase(alt, x, training=training)",
                "def _convert_string_dtype(dtype):\n    if dtype == 'float32':\n        return np.float32\n    elif dtype == 'float64':\n        return np.float64\n    elif dtype == 'float16':\n        return np.float16\n    else:\n        # cntk only running with float,\n        # try to cast to float to run the model\n        return np.float32",
                "def _convert_dtype_string(dtype):\n    if dtype == np.float32:\n        return 'float32'\n    elif dtype == np.float64:\n        return 'float64'\n    elif dtype == np.float16:\n        return 'float16'\n    else:\n        raise ValueError('CNTK Backend: Unsupported dtype: %s. '\n                         'CNTK only supports float32, float64, and '\n                         'float16.' % dtype)",
                "def variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n\n    if name is None:\n        name = ''\n\n    if isinstance(\n            value,\n            C.variables.Constant) or isinstance(\n            value,\n            C.variables.Parameter):\n        value = value.value\n\n    # we don't support init parameter with symbolic op, so eval it first as\n    # workaround\n    if isinstance(value, C.cntk_py.Function):\n        value = eval(value)\n\n    shape = value.shape if hasattr(value, 'shape') else ()\n    if hasattr(value, 'dtype') and value.dtype != dtype and len(shape) > 0:\n        value = value.astype(dtype)\n\n    # TODO: remove the conversion when cntk supports int32, int64\n    # https://docs.microsoft.com/en-us/python/api/cntk.variables.parameter\n    dtype = 'float32' if 'int' in str(dtype) else dtype\n\n    v = C.parameter(shape=shape,\n                    init=value,\n                    dtype=dtype,\n                    name=_prepare_name(name, 'variable'))\n    v._keras_shape = v.shape\n    v._uses_learning_phase = False\n    v.constraint = constraint\n    return v",
                "def bias_add(x, bias, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    dims = len(x.shape)\n    if dims > 0 and x.shape[0] == C.InferredDimension:\n        dims -= 1\n\n    bias_dims = len(bias.shape)\n    if bias_dims != 1 and bias_dims != dims:\n        raise ValueError('Unexpected bias dimensions %d, '\n                         'expected 1 or %d dimensions' % (bias_dims, dims))\n\n    if dims == 4:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1, 1)\n            else:\n                shape = (bias.shape[3],) + bias.shape[:3]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 3:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1)\n            else:\n                shape = (bias.shape[2],) + bias.shape[:2]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 2:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1)\n            else:\n                shape = (bias.shape[1],) + bias.shape[:1]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, bias.shape[0])\n            else:\n                shape = bias.shape\n    else:\n        shape = bias.shape\n    return x + reshape(bias, shape)",
                "def eval(x):\n    if isinstance(x, C.cntk_py.Function):\n        return x.eval()\n    elif isinstance(x, C.variables.Constant) or isinstance(x, C.variables.Parameter):\n        return x.value\n    else:\n        raise ValueError('CNTK Backend: `eval` method on '\n                         '`%s` type is not supported. '\n                         'CNTK only supports `eval` with '\n                         '`Function`, `Constant` or '\n                         '`Parameter`.' % type(x))",
                "def placeholder(\n        shape=None,\n        ndim=None,\n        dtype=None,\n        sparse=False,\n        name=None,\n        dynamic_axis_num=1):\n    if dtype is None:\n        dtype = floatx()\n    if not shape:\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n\n    dynamic_dimension = C.FreeDimension if _get_cntk_version() >= 2.2 else C.InferredDimension\n    cntk_shape = [dynamic_dimension if s is None else s for s in shape]\n    cntk_shape = tuple(cntk_shape)\n\n    if dynamic_axis_num > len(cntk_shape):\n        raise ValueError('CNTK backend: creating placeholder with '\n                         '%d dimension is not supported, at least '\n                         '%d dimensions are needed.'\n                         % (len(cntk_shape), dynamic_axis_num))\n\n    if name is None:\n        name = ''\n\n    cntk_shape = cntk_shape[dynamic_axis_num:]\n\n    x = C.input(\n        shape=cntk_shape,\n        dtype=_convert_string_dtype(dtype),\n        is_sparse=sparse,\n        name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    x._cntk_placeholder = True\n    return x",
                "def is_placeholder(x):\n    \"\"\"Returns whether `x` is a placeholder.\n\n    # Arguments\n        x: A candidate placeholder.\n\n    # Returns\n        Boolean.\n    \"\"\"\n    return hasattr(x, '_cntk_placeholder') and x._cntk_placeholder",
                "def is_keras_tensor(x):\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')",
                "def is_tensor(x):\n    return isinstance(x, (C.variables.Constant,\n                          C.variables.Variable,\n                          C.variables.Parameter,\n                          C.ops.functions.Function))",
                "def shape(x):\n    shape = list(int_shape(x))\n    num_dynamic = _get_dynamic_axis_num(x)\n    non_dyn_shape = []\n    for i in range(len(x.shape)):\n        if shape[i + num_dynamic] is None:\n            non_dyn_shape.append(x.shape[i])\n        else:\n            non_dyn_shape.append(shape[i + num_dynamic])\n    return shape[:num_dynamic] + non_dyn_shape",
                "def is_sparse(tensor):\n    return tensor.is_sparse",
                "def int_shape(x):\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n\n    shape = x.shape\n    if hasattr(x, 'dynamic_axes'):\n        dynamic_shape = [None for a in x.dynamic_axes]\n        shape = tuple(dynamic_shape) + shape\n    return shape",
                "def ndim(x):\n    shape = int_shape(x)\n    return len(shape)",
                "def _prepare_name(name, default):\n    prefix = '_'.join(NAME_SCOPE_STACK)\n    if name is None or name == '':\n        return prefix + '/' + default\n    return prefix + '/' + name",
                "def constant(value, dtype=None, shape=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    if shape is None:\n        shape = ()\n    np_value = value * np.ones(shape)\n    const = C.constant(np_value,\n                       dtype=dtype,\n                       name=_prepare_name(name, 'constant'))\n    const._keras_shape = const.shape\n    const._uses_learning_phase = False\n    return const",
                "def random_binomial(shape, p=0.0, dtype=None, seed=None):\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    return C.random.bernoulli(shape=shape, dtype=dtype, mean=p, seed=seed)",
                "def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n    return C.random.uniform(shape=shape, dtype=dtype, low=minval, high=maxval, seed=seed)",
                "def random_uniform_variable(shape, low, high,\n                            dtype=None, name=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    scale = (high - low) / 2\n    p = C.parameter(\n        shape,\n        init=C.initializer.uniform(\n            scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + low + scale)",
                "def random_normal_variable(\n        shape,\n        mean,\n        scale,\n        dtype=None,\n        name=None,\n        seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    p = C.parameter(\n        shape=shape,\n        init=C.initializer.normal(\n            scale=scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + mean)",
                "def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n    return C.random.normal(shape=shape, mean=mean, scale=stddev, seed=seed, dtype=dtype)",
                "def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    return C.parameter(\n        shape, init=C.initializer.truncated_normal(\n            stddev, seed=seed), dtype=dtype)",
                "def dtype(x):\n    return _convert_dtype_string(x.dtype)",
                "def zeros(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.zeros(shape, ctype), dtype=dtype, name=name)",
                "def ones(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.ones(shape, ctype), dtype=dtype, name=name)",
                "def eye(size, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    return variable(np.eye(size), dtype, name)",
                "def zeros_like(x, dtype=None, name=None):\n    return x * 0",
                "def ones_like(x, dtype=None, name=None):\n    return zeros_like(x) + 1",
                "def count_params(x):\n    for _ in x.shape:\n        if _ == C.InferredDimension or _ == C.FreeDimension:\n            raise ValueError('CNTK backend: `count_params` with dynamic '\n                             'shape is not supported. Please provide '\n                             'fixed dimension instead of `None`.')\n\n    return np.prod(int_shape(x))",
                "def cast(x, dtype):\n    # cntk calculate everything in float, so don't need case from bool / int\n    return x",
                "def dot(x, y):\n    if len(x.shape) > 2 or len(y.shape) > 2:\n        y_shape = int_shape(y)\n        if len(y_shape) > 2:\n            permutation = [len(y_shape) - 2]\n            permutation += list(range(len(y_shape) - 2))\n            permutation += [len(y_shape) - 1]\n            y = C.transpose(y, perm=permutation)\n        return C.times(x, y, len(y_shape) - 1)\n    else:\n        return C.times(x, y)",
                "def batch_dot(x, y, axes=None):\n    x_shape = int_shape(x)\n    y_shape = int_shape(y)\n\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        axes = [len(x_shape) - 1, len(y_shape) - 2]\n    if b_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n\n    if len(x_shape) == 2 and len(y_shape) == 2:\n        if axes[0] == axes[1]:\n            result = sum(x * y, axis=axes[0], keepdims=True)\n            return result if axes[0] == 1 else transpose(result)\n        else:\n            return sum(x * transpose(y), axis=axes[0], keepdims=True)\n    else:\n        if len(y_shape) == 2:\n            y = expand_dims(y)\n\n        normalized_axis = []\n        normalized_axis.append(_normalize_axis(axes[0], x)[0])\n        normalized_axis.append(_normalize_axis(axes[1], y)[0])\n        # transpose\n        i = normalized_axis[0]\n        while i < len(x.shape) - 1:\n            x = C.swapaxes(x, i, i + 1)\n            i += 1\n        i = normalized_axis[1]\n        while i > 0:\n            y = C.swapaxes(y, i, i - 1)\n            i -= 1\n        result = C.times(x, y, output_rank=(len(y.shape) - 1)\n                         if len(y.shape) > 1 else 1)\n        if len(y_shape) == 2:\n            result = squeeze(result, -1)\n        return result",
                "def transpose(x):\n    return C.swapaxes(x, 0, 1)",
                "def gather(reference, indices):\n    # There is a bug in cntk gather op which may cause crash.\n    # We have made a fix but not catched in CNTK 2.1 release.\n    # Will update with gather op in next release\n    if _get_cntk_version() >= 2.2:\n        return C.ops.gather(reference, indices)\n    else:\n        num_classes = reference.shape[0]\n        one_hot_matrix = C.ops.one_hot(indices, num_classes)\n        return C.times(one_hot_matrix, reference, output_rank=len(reference.shape) - 1)",
                "def _remove_dims(x, axis, keepdims=False):\n    if keepdims is False and isinstance(axis, list):\n        # sequence axis is removed by default, so don't need reshape on it\n        reduce_axes = []\n        for a in axis:\n            if isinstance(a, C.Axis) is False:\n                reduce_axes.append(a)\n        return _reshape_dummy_dim(x, reduce_axes)\n    else:\n        if isinstance(axis, list):\n            has_seq = False\n            for a in axis:\n                if isinstance(a, C.Axis):\n                    has_seq = True\n                    break\n            if has_seq:\n                nones = _get_dynamic_axis_num(x)\n                x = expand_dims(x, nones)\n        return x",
                "def max(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_max')\n\n    return _remove_dims(output, axis, keepdims)",
                "def min(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_min')\n\n    return _remove_dims(output, axis, keepdims)",
                "def sum(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_sum')\n\n    return _remove_dims(output, axis, keepdims)",
                "def prod(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_prod')\n\n    return _remove_dims(output, axis, keepdims)",
                "def logsumexp(x, axis=None, keepdims=False):\n    return log(sum(exp(x), axis=axis, keepdims=keepdims))",
                "def var(x, axis=None, keepdims=False):\n    m = mean(x, axis, keepdims=True)\n    devs_squared = C.square(x - m)\n    return mean(devs_squared, axis=axis, keepdims=keepdims)",
                "def std(x, axis=None, keepdims=False):\n    return C.sqrt(var(x, axis=axis, keepdims=keepdims))",
                "def expand_dims(x, axis=-1):\n    shape = list(int_shape(x))\n    nones = _get_dynamic_axis_num(x)\n    index = axis if axis >= 0 else len(shape) + 1\n    shape.insert(index, 1)\n    new_shape = shape[nones:]\n    new_shape = tuple(\n        [C.InferredDimension if _ is None else _ for _ in new_shape])\n    result = C.reshape(x, new_shape)\n    if index < nones:\n        result._keras_shape = shape\n    return result",
                "def squeeze(x, axis):\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    if not isinstance(axis, list):\n        axis = [axis]\n\n    shape = list(int_shape(x))\n\n    _axis = []\n    for _ in axis:\n        if isinstance(_, int):\n            _axis.append(_ if _ >= 0 else _ + len(shape))\n\n    if len(_axis) == 0:\n        return x\n\n    nones = _get_dynamic_axis_num(x)\n    for _ in sorted(_axis, reverse=True):\n        del shape[_]\n\n    new_shape = shape[nones:]\n    new_shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in new_shape])\n    return C.reshape(x, new_shape)",
                "def tile(x, n):\n    if isinstance(n, int):\n        n = (n,)\n    elif isinstance(n, list):\n        n = tuple(n)\n\n    shape = int_shape(x)\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    # Padding the axis\n    if len(n) < len(shape):\n        n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n\n    if len(n) != len(shape):\n        raise NotImplementedError\n\n    i = num_dynamic_axis\n    for i, rep in enumerate(n):\n        if i >= num_dynamic_axis and shape[i] is not None:\n            tmp = [x] * rep\n            x = C.splice(*tmp, axis=i - num_dynamic_axis)\n        i += 1\n\n    return x",
                "def _normalize_axis(axis, x):\n    shape = int_shape(x)\n    ndim = len(shape)\n\n    nones = _get_dynamic_axis_num(x)\n\n    if nones > ndim:\n        raise ValueError('CNTK Backend: tensor with keras shape: `%s` has '\n                         '%d cntk dynamic axis, this is not expected, please '\n                         'double check the keras shape history.' % (str(shape), nones))\n\n    # Current cntk does not support shape like (1, batch). so using the workaround\n    # here to mapping the correct axis. Will remove this tricky after we add support\n    # in native cntk op\n    cntk_axis = []\n    dynamic_axis_index = 0\n    for i in range(ndim):\n        if shape[i] is None and dynamic_axis_index < nones:\n            cntk_axis.append(x.dynamic_axes[dynamic_axis_index])\n            dynamic_axis_index += 1\n        else:\n            cntk_axis.append(i - dynamic_axis_index)\n\n    if dynamic_axis_index < nones:\n        i = 0\n        while dynamic_axis_index < nones:\n            cntk_axis[i] = x.dynamic_axes[dynamic_axis_index]\n            i += 1\n            dynamic_axis_index += 1\n\n        while i < len(cntk_axis):\n            cntk_axis[i] -= nones\n            i += 1\n\n    if isinstance(axis, tuple):\n        _axis = list(axis)\n    elif isinstance(axis, int):\n        _axis = [axis]\n    elif isinstance(axis, list):\n        _axis = list(axis)\n    else:\n        _axis = axis\n\n    if isinstance(_axis, list):\n        for i, a in enumerate(_axis):\n            if a is not None and a < 0:\n                _axis[i] = (a % ndim)\n            if _axis[i] is not None:\n                _axis[i] = cntk_axis[_axis[i]]\n    else:\n        if _axis is None:\n            _axis = C.Axis.all_axes()\n\n    return _axis",
                "def _reshape_dummy_dim(x, axis):\n    shape = list(x.shape)\n\n    _axis = [_ + len(shape) if _ < 0 else _ for _ in axis]\n\n    if shape.count(C.InferredDimension) > 1 or shape.count(C.FreeDimension) > 1:\n        result = x\n        for index in sorted(_axis, reverse=True):\n            result = C.reshape(result,\n                               shape=(),\n                               begin_axis=index,\n                               end_axis=index + 1)\n        return result\n    else:\n        for index in sorted(_axis, reverse=True):\n            del shape[index]\n\n        shape = [C.InferredDimension if _ == C.FreeDimension else _ for _ in shape]\n        return C.reshape(x, shape)",
                "def mean(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_mean')\n\n    return _remove_dims(output, axis, keepdims)",
                "def any(x, axis=None, keepdims=False):\n    reduce_result = sum(x, axis, keepdims=keepdims)\n    any_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(any_matrix)\n    else:\n        return any_matrix",
                "def all(x, axis=None, keepdims=False):\n    reduce_result = prod(x, axis, keepdims=keepdims)\n    all_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(all_matrix)\n    else:\n        return all_matrix",
                "def classification_error(target, output, axis=-1):\n    return C.ops.reduce_mean(\n        C.equal(\n            argmax(\n                output,\n                axis=-1),\n            argmax(\n                target,\n                axis=-1)),\n        axis=C.Axis.all_axes())",
                "def argmax(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmax(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)",
                "def argmin(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmin(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)",
                "def square(x):\n    return C.square(x)",
                "def abs(x):\n    return C.abs(x)",
                "def sqrt(x):\n    return C.sqrt(x)",
                "def exp(x):\n    return C.exp(x)",
                "def log(x):\n    return C.log(x)",
                "def round(x):\n    return C.round(x)",
                "def sigmoid(x):\n    return C.sigmoid(x)",
                "def sign(x):\n    return x / C.abs(x)",
                "def pow(x, a):\n    return C.pow(x, a)",
                "def clip(x, min_value, max_value):\n    if max_value is not None and max_value < min_value:\n        max_value = min_value\n    if max_value is None:\n        max_value = np.inf\n    if min_value is None:\n        min_value = -np.inf\n    return C.clip(x, min_value, max_value)",
                "def binary_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        output = C.sigmoid(output)\n    output = C.clip(output, epsilon(), 1.0 - epsilon())\n    output = -target * C.log(output) - (1.0 - target) * C.log(1.0 - output)\n    return output",
                "def get_variable_shape(x):\n    return int_shape(x)",
                "def update(x, new_x):\n    return C.assign(x, new_x)",
                "def moving_average_update(variable, value, momentum):\n    return C.assign(variable, variable * momentum + value * (1. - momentum))",
                "def update_add(x, increment):\n    result = x + increment\n    return C.assign(x, result)",
                "def gradients(loss, variables):\n    # cntk does not support gradients as symbolic op,\n    # to hook up with keras model\n    # we will return a constant as place holder, the cntk learner will apply\n    # the gradient during training.\n    global grad_parameter_dict\n    if isinstance(variables, list) is False:\n        variables = [variables]\n    grads = []\n    for v in variables:\n        g = C.constant(0, shape=v.shape, name='keras_grad_placeholder')\n        grads.append(g)\n        grad_parameter_dict[g] = v\n    return grads",
                "def equal(x, y):\n    return C.equal(x, y)",
                "def not_equal(x, y):\n    return C.not_equal(x, y)",
                "def greater(x, y):\n    return C.greater(x, y)",
                "def greater_equal(x, y):\n    return C.greater_equal(x, y)",
                "def less(x, y):\n    return C.less(x, y)",
                "def less_equal(x, y):\n    return C.less_equal(x, y)",
                "def maximum(x, y):\n    return C.element_max(x, y)",
                "def minimum(x, y):\n    return C.element_min(x, y)",
                "def sin(x):\n    return C.sin(x)",
                "def cos(x):\n    return C.cos(x)",
                "def normalize_batch_in_training(x, gamma, beta,\n                                reduction_axes, epsilon=1e-3):\n    if gamma is None:\n        if beta is None:\n            gamma = ones_like(x)\n        else:\n            gamma = ones_like(beta)\n    if beta is None:\n        if gamma is None:\n            beta = zeros_like(x)\n        else:\n            beta = zeros_like(gamma)\n\n    mean, variant = _moments(x, _normalize_axis(reduction_axes, x))\n\n    if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:\n        normalized = batch_normalization(\n            x, mean, variant, beta, gamma, epsilon)\n    else:\n        # need broadcasting\n        target_shape = []\n        x_shape = int_shape(x)\n        # skip the batch axis\n        for axis in range(1, ndim(x)):\n            if axis in reduction_axes:\n                target_shape.append(1)\n                if ndim(gamma) > axis:\n                    gamma = C.reduce_mean(gamma, axis - 1)\n                    beta = C.reduce_mean(beta, axis - 1)\n            else:\n                target_shape.append(x_shape[axis])\n\n        broadcast_mean = C.reshape(mean, target_shape)\n        broadcast_var = C.reshape(variant, target_shape)\n        broadcast_gamma = C.reshape(gamma, target_shape)\n        broadcast_beta = C.reshape(beta, target_shape)\n        normalized = batch_normalization(\n            x,\n            broadcast_mean,\n            broadcast_var,\n            broadcast_beta,\n            broadcast_gamma,\n            epsilon)\n\n    return normalized, mean, variant",
                "def _moments(x, axes=None, shift=None, keep_dims=False):\n    _axes = tuple(axes)\n    if shift is None:\n        shift = x\n        # Compute true mean while keeping the dims for proper broadcasting.\n        for axis in _axes:\n            shift = C.reduce_mean(shift, axis=axis)\n\n    shift = C.stop_gradient(shift)\n    shifted_mean = C.minus(x, shift)\n    for axis in _axes:\n        shifted_mean = C.reduce_mean(shifted_mean, axis=axis)\n\n    variance_mean = C.square(C.minus(x, shift))\n    for axis in _axes:\n        variance_mean = C.reduce_mean(variance_mean, axis=axis)\n\n    variance = C.minus(variance_mean, C.square(shifted_mean))\n    mean = C.plus(shifted_mean, shift)\n\n    if not keep_dims:\n        mean = squeeze(mean, _axes)\n        variance = squeeze(variance, _axes)\n\n    return mean, variance",
                "def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n    # The mean / var / beta / gamma may be processed by broadcast\n    # so it may have an extra batch axis with 1, it is not needed\n    # in cntk, need to remove those dummy axis.\n    if ndim(mean) == ndim(x) and shape(mean)[0] == 1:\n        mean = _reshape_dummy_dim(mean, [0])\n    if ndim(var) == ndim(x) and shape(var)[0] == 1:\n        var = _reshape_dummy_dim(var, [0])\n\n    if gamma is None:\n        gamma = ones_like(var)\n    elif ndim(gamma) == ndim(x) and shape(gamma)[0] == 1:\n        gamma = _reshape_dummy_dim(gamma, [0])\n\n    if beta is None:\n        beta = zeros_like(mean)\n    elif ndim(beta) == ndim(x) and shape(beta)[0] == 1:\n        beta = _reshape_dummy_dim(beta, [0])\n\n    return (x - mean) / C.sqrt(var + epsilon) * gamma + beta",
                "def concatenate(tensors, axis=-1):\n    if len(tensors) == 0:\n        return None\n\n    axis = [axis]\n    axis = _normalize_axis(axis, tensors[0])\n    return C.splice(*tensors, axis=axis[0])",
                "def flatten(x):\n    return reshape(x, (-1,))",
                "def reshape(x, shape):\n    shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in shape])\n    if isinstance(x, C.variables.Parameter):\n        return C.reshape(x, shape)\n    else:\n        num_dynamic_axis = _get_dynamic_axis_num(x)\n\n        if num_dynamic_axis == 1 and len(shape) > 0 and shape[0] == -1:\n            # collapse axis with batch axis\n            if b_any(_ == C.InferredDimension for _ in x.shape) or b_any(\n                    _ == C.FreeDimension for _ in x.shape):\n                warnings.warn(\n                    'Warning: CNTK backend does not support '\n                    'collapse of batch axis with inferred dimension. '\n                    'The reshape did not take place.')\n                return x\n            return _reshape_batch(x, shape)\n        else:\n            # no collapse, then first need to padding the shape\n            if num_dynamic_axis >= len(shape):\n                i = 0\n                while i < len(shape):\n                    if shape[i] is None or shape[i] == -1:\n                        i += 1\n                    else:\n                        break\n                shape = tuple([-1 for _ in range(num_dynamic_axis - i)]) + shape\n\n            new_shape = list(shape)\n            new_shape = new_shape[num_dynamic_axis:]\n            new_shape = [C.InferredDimension if _ is None else _ for _ in new_shape]\n            return C.reshape(x, new_shape)",
                "def permute_dimensions(x, pattern):\n    dims = len(int_shape(x))\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    if isinstance(pattern, list):\n        current_layout = [i for i in range(dims)]\n    else:\n        current_layout = tuple([i for i in range(dims)])\n\n    if num_dynamic_axis > 0 and pattern[:num_dynamic_axis] != current_layout[:num_dynamic_axis]:\n        raise ValueError('CNTK backend: the permute pattern %s '\n                         'requested permute on dynamic axis, '\n                         'which is not supported. Please do permute '\n                         'on static axis.' % pattern)\n\n    axis = list(pattern)\n    axis = axis[num_dynamic_axis:]\n    axis = _normalize_axis(axis, x)\n    return C.transpose(x, axis)",
                "def resize_images(x, height_factor, width_factor, data_format, interpolation='nearest'):\n    if interpolation == 'nearest':\n        if data_format == 'channels_first':\n            output = repeat_elements(x, height_factor, axis=2)\n            output = repeat_elements(output, width_factor, axis=3)\n            return output\n        elif data_format == 'channels_last':\n            output = repeat_elements(x, height_factor, axis=1)\n            output = repeat_elements(output, width_factor, axis=2)\n            return output\n        else:\n            raise ValueError('CNTK Backend: Invalid data_format: %s' % data_format)\n    else:\n        raise NotImplementedError('CNTK only supports `nearest` interpolation.')",
                "def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        output = repeat_elements(x, depth_factor, axis=2)\n        output = repeat_elements(output, height_factor, axis=3)\n        output = repeat_elements(output, width_factor, axis=4)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, depth_factor, axis=1)\n        output = repeat_elements(output, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    else:\n        raise ValueError('CNTK Backend: Invalid data_format: %s' % data_format)",
                "def repeat_elements(x, rep, axis):\n    axis = _normalize_axis(axis, x)\n    axis = axis[0]\n    slices = []\n    shape = x.shape\n    i = 0\n    while i < shape[axis]:\n        tmp = C.ops.slice(x, axis, i, i + 1)\n        for _ in range(rep):\n            slices.append(tmp)\n        i += 1\n    return C.splice(*slices, axis=axis)",
                "def repeat(x, n):\n    # this is a workaround for recurrent layer\n    # if n is inferred dimension,\n    # we can't figure out how to repeat it in cntk now\n    # return the same x to take cntk broadcast feature\n    # to make the recurrent layer work.\n    # need to be fixed in GA.\n    if n is C.InferredDimension or n is C.FreeDimension:\n        return x\n    index = 1 - _get_dynamic_axis_num(x)\n    if index < 0 or index > 1:\n        raise NotImplementedError\n\n    new_shape = list(x.shape)\n    new_shape.insert(index, 1)\n    new_shape = tuple(new_shape)\n    x = C.reshape(x, new_shape)\n    temp = [x] * n\n    return C.splice(*temp, axis=index)",
                "def tanh(x):\n    return C.tanh(x)",
                "def _static_rnn(step_function, inputs, initial_states,\n                go_backwards=False, mask=None, constants=None,\n                unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('Input should be at least 3D.')\n\n    # if the second axis is static axis, CNTK will do unroll by default\n    if shape[1] is None:\n        raise ValueError('CNTK Backend: the input of static rnn '\n                         'has shape `%s`, the second axis '\n                         'is not static. If you want to run '\n                         'rnn with non-static axis, please try '\n                         'dynamic rnn with sequence axis.' % shape)\n    if constants is None:\n        constants = []\n\n    if mask is not None:\n        mask_shape = int_shape(mask)\n        if len(mask_shape) == dims - 1:\n            mask = expand_dims(mask)\n\n    nones = _get_dynamic_axis_num(inputs)\n\n    states = tuple(initial_states)\n\n    outputs = []\n\n    time_axis = 1 - nones if nones > 0 else 1\n\n    if go_backwards:\n        i = shape[1] - 1\n        while i >= 0:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, time_axis)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, time_axis)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states\n            i -= 1\n    else:\n        i = 0\n        while i < shape[1]:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, 1)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, 1)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states[:len(states)]\n            i += 1\n\n    i = 1\n    # add the time_step axis back\n    final_output = expand_dims(outputs[0], 1)\n    last_output = outputs[0]\n    while i < len(outputs):\n        # add the time_step axis back\n        output_slice = expand_dims(outputs[i], 1)\n        final_output = C.splice(final_output, output_slice, axis=time_axis)\n        last_output = outputs[i]\n        i += 1\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, states",
                "def rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('CNTK Backend: the input of rnn has only rank %d '\n                         'Need at least rank 3 to run RNN.' % dims)\n\n    if _get_dynamic_axis_num(inputs) == 0 or unroll:\n        return _static_rnn(\n            step_function,\n            inputs,\n            initial_states,\n            go_backwards,\n            mask,\n            constants,\n            unroll,\n            input_length)\n\n    if constants is None:\n        constants = []\n\n    num_time_step = shape[1]\n    if num_time_step is None and not has_seq_axis(inputs):\n        num_time_step = inputs.shape[0]\n\n    initial = []\n    for s in initial_states:\n        if _get_dynamic_axis_num(s) == 0:\n            if hasattr(C, 'to_batch'):\n                initial.append(C.to_batch(s))\n            else:\n                initial.append(C.user_function(ConvertToBatch(s)))\n        else:\n            initial.append(s)\n\n    need_convert = not has_seq_axis(inputs)\n    if go_backwards and need_convert is False:\n        raise NotImplementedError('CNTK Backend: `go_backwards` is not supported with '\n                                  'variable-length sequences. Please specify a '\n                                  'static length for your sequences.')\n\n    rnn_inputs = inputs\n    if need_convert:\n        if go_backwards:\n            rnn_inputs = reverse(rnn_inputs, 1)\n\n        rnn_inputs = C.to_sequence(rnn_inputs)\n\n        rnn_constants = []\n        for constant in constants:\n            if isinstance(constant, list):\n                new_c = []\n                for c in constant:\n                    if _get_dynamic_axis_num(c) == 1:\n                        new_c.append(C.sequence.broadcast_as(c, rnn_inputs))\n                    else:\n                        new_c.append(c)\n                rnn_constants.append(new_c)\n            else:\n                if _get_dynamic_axis_num(constant) == 1:\n                    rnn_constants.append(C.sequence.broadcast_as(constant, rnn_inputs))\n                else:\n                    rnn_constants.append(constant)\n    else:\n        rnn_constants = constants\n\n    if mask is not None and not has_seq_axis(mask):\n        if go_backwards:\n            mask = reverse(mask, 1)\n        if len(int_shape(mask)) == 2:\n            mask = expand_dims(mask)\n        mask = C.to_sequence_like(mask, rnn_inputs)\n\n    states = tuple(initial)\n\n    with C.default_options(axis_offset=1):\n        def _recurrence(x, states, m):\n            # create place holder\n            place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]\n            past_values = []\n            for s, p in zip(states, place_holders):\n                past_values.append(C.sequence.past_value(p, s))\n            new_output, new_states = step_function(\n                x, tuple(past_values) + tuple(rnn_constants))\n\n            if getattr(new_output, '_uses_learning_phase', False):\n                global uses_learning_phase\n                uses_learning_phase = True\n\n            if m is not None:\n                new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]\n            n_s = []\n            for o, p in zip(new_states, place_holders):\n                n_s.append(o.replace_placeholders({p: o.output}))\n            if len(n_s) > 0:\n                new_output = n_s[0]\n            return new_output, n_s\n\n        final_output, final_states = _recurrence(rnn_inputs, states, mask)\n        last_output = C.sequence.last(final_output)\n        last_states = [C.sequence.last(s) for s in final_states]\n\n    if need_convert:\n        final_output = C.sequence.unpack(final_output, 0, no_mask_output=True)\n        if num_time_step is not None and num_time_step is not C.FreeDimension:\n            final_output = _reshape_sequence(final_output, num_time_step)\n\n    f_stats = []\n    for l_s, i_s in zip(last_states, initial_states):\n        if _get_dynamic_axis_num(i_s) == 0 and _get_dynamic_axis_num(l_s) == 1:\n            if hasattr(C, 'unpack_batch'):\n                f_stats.append(C.unpack_batch(l_s))\n            else:\n                f_stats.append(C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))\n        else:\n            f_stats.append(l_s)\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, f_stats",
                "def has_seq_axis(x):\n    return hasattr(x, 'dynamic_axes') and len(x.dynamic_axes) > 1",
                "def l2_normalize(x, axis=None):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    norm = C.sqrt(C.reduce_sum(C.square(x), axis=axis[0]))\n    return x / norm",
                "def hard_sigmoid(x):\n    x = (0.2 * x) + 0.5\n    x = C.clip(x, 0.0, 1.0)\n    return x",
                "def conv1d(x, kernel, strides=1, padding='valid',\n           data_format=None, dilation_rate=1):\n    data_format = normalize_data_format(data_format)\n\n    if padding == 'causal':\n        # causal (dilated) convolution:\n        left_pad = dilation_rate * (kernel.shape[0] - 1)\n        x = temporal_padding(x, (left_pad, 0))\n        padding = 'valid'\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n        kernel = C.swapaxes(kernel, 0, 2)\n\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != 1:\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to 1. You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides=strides,\n        auto_padding=[False, padding],\n        dilation=dilation_rate)\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n    return x",
                "def conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != (1, 1):\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to (1, 1). '\n                         'You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(kernel,\n                      x,\n                      strides,\n                      auto_padding=[False, padding, padding],\n                      dilation=dilation_rate)\n\n    return _postprocess_conv2d_output(x, data_format)",
                "def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                     padding='valid', data_format=None, dilation_rate=1):\n    data_format = normalize_data_format(data_format)\n    if isinstance(strides, int):\n        strides = (strides,)\n    if isinstance(dilation_rate, int):\n        dilation_rate = (dilation_rate,)\n\n    if data_format == 'channels_last':\n        spatial_start_dim = 2\n    else:\n        spatial_start_dim = 3\n    x = expand_dims(x, spatial_start_dim)\n    depthwise_kernel = expand_dims(depthwise_kernel, 1)\n    pointwise_kernel = expand_dims(pointwise_kernel, 1)\n    strides = (1,) + strides + (1,)\n    dilation_rate = (1,) + dilation_rate\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    x = _postprocess_conv2d_output(x, data_format)\n    return squeeze(x, spatial_start_dim)",
                "def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    return _postprocess_conv2d_output(x, data_format)",
                "def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                     data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    padding = _preprocess_border_mode(padding)\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    return _postprocess_conv2d_output(x, data_format)",
                "def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != (1, 1, 1):\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to (1, 1, 1). '\n                         'You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides,\n        auto_padding=[False, padding, padding, padding],\n        dilation=dilation_rate)\n\n    return _postprocess_conv3d_output(x, data_format)",
                "def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                     padding='valid', data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        output_shape = transpose_shape(output_shape, 'channels_first',\n                                       spatial_axes=(0, 1, 2))\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding,\n            padding],\n        output_shape=output_shape)\n    return _postprocess_conv3d_output(x, data_format)",
                "def pool2d(x, pool_size, strides=(1, 1),\n           padding='valid', data_format=None,\n           pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    padding = _preprocess_border_mode(padding)\n    strides = strides\n    pool_size = pool_size\n    x = _preprocess_conv2d_input(x, data_format)\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n    return _postprocess_conv2d_output(x, data_format)",
                "def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    padding = _preprocess_border_mode(padding)\n\n    x = _preprocess_conv3d_input(x, data_format)\n\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n\n    return _postprocess_conv3d_output(x, data_format)",
                "def relu(x, alpha=0., max_value=None):\n    if alpha != 0.:\n        negative_part = C.relu(-x)\n    x = C.relu(x)\n    if max_value is not None:\n        x = C.clip(x, 0.0, max_value)\n    if alpha != 0.:\n        x -= alpha * negative_part\n    return x",
                "def dropout(x, level, noise_shape=None, seed=None):\n    if level < 0. or level >= 1:\n        raise ValueError('CNTK Backend: Invalid dropout level %s, '\n                         'must be in interval [0, 1].' % level)\n    return C.dropout(x, level)",
                "def batch_flatten(x):\n    # cntk's batch axis is not in shape,\n    # so just flatten all the dim in x.shape\n    dim = np.prod(x.shape)\n    x = C.reshape(x, (-1,))\n    x._keras_shape = (None, dim)\n    return x",
                "def softmax(x, axis=-1):\n    return C.softmax(x, axis=axis)",
                "def softplus(x):\n    return C.softplus(x)",
                "def softsign(x):\n    return x / (1 + C.abs(x))",
                "def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    # Here, unlike other backends, the tensors lack a batch dimension:\n    axis_without_batch = -1 if axis == -1 else axis - 1\n    output_dimensions = list(range(len(output.shape)))\n    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis_without_batch),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.shape))))\n    # If the channels are not in the last axis, move them to be there:\n    if axis_without_batch != -1 and axis_without_batch != output_dimensions[-1]:\n        permutation = output_dimensions[:axis_without_batch]\n        permutation += output_dimensions[axis_without_batch + 1:]\n        permutation += [axis_without_batch]\n        output = C.transpose(output, permutation)\n        target = C.transpose(target, permutation)\n    if from_logits:\n        result = C.cross_entropy_with_softmax(output, target)\n        # cntk's result shape is (batch, 1), while keras expect (batch, )\n        return C.reshape(result, ())\n    else:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= C.reduce_sum(output, axis=-1)\n        # avoid numerical instability with epsilon clipping\n        output = C.clip(output, epsilon(), 1.0 - epsilon())\n        return -sum(target * C.log(output), axis=-1)",
                "def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    # Here, unlike other backends, the tensors lack a batch dimension:\n    axis_without_batch = -1 if axis == -1 else axis - 1\n    output_dimensions = list(range(len(output.shape)))\n    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis_without_batch),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.shape))))\n    target = C.one_hot(target, output.shape[axis_without_batch],\n                       axis=axis_without_batch)\n    target = C.reshape(target, output.shape)\n    return categorical_crossentropy(target, output, from_logits, axis=axis)",
                "def function(inputs, outputs, updates=[], **kwargs):\n    return Function(inputs, outputs, updates=updates, **kwargs)",
                "def temporal_padding(x, padding=(1, 1)):\n    assert len(padding) == 2\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 3 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, [padding], 'channels_last', num_dynamic_axis)",
                "def _padding(x, pattern, axis):  # pragma: no cover\n    base_shape = x.shape\n    if b_any([dim < 0 for dim in base_shape]):\n        raise ValueError('CNTK Backend: padding input tensor with '\n                         'shape `%s` contains non-specified dimension, '\n                         'which is not supported. Please give fixed '\n                         'dimension to enable padding.' % base_shape)\n    if pattern[0] > 0:\n        prefix_shape = list(base_shape)\n        prefix_shape[axis] = pattern[0]\n        prefix_shape = tuple(prefix_shape)\n        x = C.splice(C.constant(value=0, shape=prefix_shape), x, axis=axis)\n        base_shape = x.shape\n    if pattern[1] > 0:\n        postfix_shape = list(base_shape)\n        postfix_shape[axis] = pattern[1]\n        postfix_shape = tuple(postfix_shape)\n        x = C.splice(x, C.constant(value=0, shape=postfix_shape), axis=axis)\n    return x",
                "def pad(x, pad_info, data_format, num_dynamic_axis):\n    if hasattr(C, 'pad'):\n        pattern = [list(p) for p in pad_info]\n        if data_format == 'channels_first':\n            pattern = [[0, 0]] + pattern\n        else:\n            pattern = pattern + [[0, 0]]\n        if num_dynamic_axis == 0:\n            pattern = [[0, 0]] + pattern\n        return C.pad(x, pattern=pattern)\n    else:  # pragma: no cover\n        for (a, p) in enumerate(pad_info):\n            x = _padding(x, p,\n                         a + (1 if num_dynamic_axis == 0 else 0) +\n                         (1 if data_format == 'channels_first' else 0))\n        return x",
                "def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 2\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    data_format = normalize_data_format(data_format)\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 4 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, padding, data_format, num_dynamic_axis)",
                "def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 3\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    assert len(padding[2]) == 2\n    data_format = normalize_data_format(data_format)\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 5 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, padding, data_format, num_dynamic_axis)",
                "def one_hot(indices, num_classes):\n    return C.one_hot(indices, num_classes)",
                "def get_value(x):\n    if isinstance(\n            x,\n            C.variables.Parameter) or isinstance(\n            x,\n            C.variables.Constant):\n        return x.value\n    else:\n        return eval(x)",
                "def batch_get_value(xs):\n    result = []\n    for x in xs:\n        if (isinstance(x, C.variables.Parameter) or\n           isinstance(x, C.variables.Constant)):\n            result.append(x.value)\n        else:\n            result.append(eval(x))\n    return result",
                "def set_value(x, value):\n    if (isinstance(x, C.variables.Parameter) or\n       isinstance(x, C.variables.Constant)):\n        if isinstance(value, (float, int)):\n            value = np.full(x.shape, value, dtype=floatx())\n        x.value = value\n    else:\n        raise NotImplementedError",
                "def print_tensor(x, message=''):\n    return C.user_function(\n        LambdaFunc(x,\n                   when=lambda x: True,\n                   execute=lambda x: print(message)))",
                "def batch_set_value(tuples):\n    for t in tuples:\n        x = t[0]\n        value = t[1]\n        if isinstance(value, np.ndarray) is False:\n            value = np.asarray(value)\n        if isinstance(x, C.variables.Parameter):\n            x.value = value\n        else:\n            raise NotImplementedError",
                "def stop_gradient(variables):\n    if isinstance(variables, (list, tuple)):\n        return map(C.stop_gradient, variables)\n    else:\n        return C.stop_gradient(variables)",
                "def switch(condition, then_expression, else_expression):\n    ndim_cond = ndim(condition)\n    ndim_expr = ndim(then_expression)\n    if ndim_cond > ndim_expr:\n        raise ValueError('Rank of condition should be less'\n                         ' than or equal to rank of then and'\n                         ' else expressions. ndim(condition)=' +\n                         str(ndim_cond) + ', ndim(then_expression)'\n                         '=' + str(ndim_expr))\n    elif ndim_cond < ndim_expr:\n        shape_expr = int_shape(then_expression)\n        ndim_diff = ndim_expr - ndim_cond\n        for i in range(ndim_diff):\n            condition = expand_dims(condition)\n            condition = tile(condition, shape_expr[ndim_cond + i])\n    return C.element_select(condition,\n                            then_expression,\n                            else_expression)",
                "def elu(x, alpha=1.):\n    res = C.elu(x)\n    if alpha == 1:\n        return res\n    else:\n        return C.element_select(C.greater(x, 0), res, alpha * res)",
                "def in_top_k(predictions, targets, k):\n    _targets = C.one_hot(targets, predictions.shape[-1])\n    result = C.classification_error(predictions, _targets, topN=k)\n    return 1 - C.reshape(result, shape=())",
                "def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        output_shape = transpose_shape(output_shape, 'channels_first',\n                                       spatial_axes=(0, 1))\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding],\n        output_shape=output_shape,\n        dilation=dilation_rate)\n    return _postprocess_conv2d_output(x, data_format)",
                "def identity(x, name=None):\n    if name is None:\n        name = '%s_alias' % x.name\n    return C.alias(x, name=name)",
                "def _preprocess_conv2d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols)\n        # TF input shape: (samples, rows, cols, input_depth)\n        x = C.transpose(x, (2, 0, 1))\n    return x",
                "def _preprocess_conv2d_kernel(kernel, data_format):\n    # As of Keras 2.0.0, all kernels are normalized\n    # on the format `(rows, cols, input_depth, depth)`,\n    # independently of `data_format`.\n    # CNTK expects `(depth, input_depth, rows, cols)`.\n    kernel = C.transpose(kernel, (3, 2, 0, 1))\n    return kernel",
                "def _preprocess_border_mode(padding):\n    if padding == 'same':\n        padding = True\n    elif padding == 'valid':\n        padding = False\n    else:\n        raise ValueError('Invalid border mode: ' + str(padding))\n    return padding",
                "def _postprocess_conv2d_output(x, data_format):\n    if data_format == 'channels_last':\n        x = C.transpose(x, (1, 2, 0))\n    return x",
                "def _preprocess_conv3d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n        # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3,\n        # input_depth)\n        x = C.transpose(x, (3, 0, 1, 2))\n    return x",
                "def _preprocess_conv3d_kernel(kernel, dim_ordering):\n    kernel = C.transpose(kernel, (4, 3, 0, 1, 2))\n    return kernel",
                "def _postprocess_conv3d_output(x, dim_ordering):\n    if dim_ordering == 'channels_last':\n        x = C.transpose(x, (1, 2, 3, 0))\n    return x",
                "def _get_dynamic_axis_num(x):\n    if hasattr(x, 'dynamic_axes'):\n        return len(x.dynamic_axes)\n    else:\n        return 0",
                "def _contain_seqence_axis(x):\n    if _get_dynamic_axis_num(x) > 1:\n        return x.dynamic_axes[1] == C.Axis.default_dynamic_axis()\n    else:\n        return False",
                "def get_num_dynamic_axis(x):\n    return _get_dynamic_axis_num(x)",
                "def _reduce_on_axis(x, axis, reduce_fun_name):\n    if isinstance(axis, list):\n        for a in axis:\n            if isinstance(a, C.Axis) \\\n                    and a != C.Axis.default_batch_axis() \\\n                    and hasattr(C.sequence, reduce_fun_name):\n                x = getattr(C.sequence, reduce_fun_name)(x, a)\n            else:\n                x = getattr(C, reduce_fun_name)(x, a)\n    else:\n        x = getattr(C, reduce_fun_name)(x, axis)\n    return x",
                "def _reshape_sequence(x, time_step):\n    tmp_shape = list(int_shape(x))\n    tmp_shape[1] = time_step\n    return reshape(x, tmp_shape)",
                "def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride = strides[0]\n    kernel_shape = int_shape(kernel)\n    output_length, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_length):\n        slice_length = py_slice(i * stride,\n                                i * stride + kernel_size[0])\n        xs.append(reshape(inputs[:, slice_length, :],\n                          (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to output_filters first, to apply broadcast\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # Shape: (batch, filters, output_length, input_length * kernel_size)\n    output = x_aggregate * weight\n    # Shape: (batch, filters, output_length)\n    output = sum(output, axis=3)\n    # Shape: (batch, output_length, filters)\n    return permute_dimensions(output, (0, 2, 1))",
                "def local_conv2d(inputs,\n                 kernel,\n                 kernel_size,\n                 strides,\n                 output_shape,\n                 data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride_row, stride_col = strides\n    output_row, output_col = output_shape\n    kernel_shape = int_shape(kernel)\n    _, feature_dim, filters = kernel_shape\n    xs = []\n\n    for i in range(output_row):\n        for j in range(output_col):\n            slice_row = py_slice(i * stride_row,\n                                 i * stride_row + kernel_size[0])\n            slice_col = py_slice(j * stride_col,\n                                 j * stride_col + kernel_size[1])\n            if data_format == 'channels_first':\n                xs.append(reshape(inputs[:, :, slice_row, slice_col],\n                                  (-1, 1, feature_dim)))\n            else:\n                xs.append(reshape(inputs[:, slice_row, slice_col, :],\n                                  (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to put filters first\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # shape: batch, filters, output_length, input_length * kernel_size\n    output = x_aggregate * weight\n    # shape: batch, filters, output_length\n    output = sum(output, axis=3)\n    # shape: batch, filters, row, col\n    output = reshape(output,\n                     (-1, filters, output_row, output_col))\n\n    if data_format == 'channels_last':\n        # shape: batch, row, col, filters\n        output = permute_dimensions(output, (0, 2, 3, 1))\n\n    return output",
                "def reverse(x, axes):\n    if isinstance(axes, int):\n        axes = [axes]\n    cntk_axes = _normalize_axis(axes, x)\n    begin_index = [0 for _ in cntk_axes]\n    end_index = [0 for _ in cntk_axes]\n    strides = [-1 for _ in cntk_axes]\n    return C.slice(x, cntk_axes, begin_index, end_index, strides)",
                "def slice(x, start, size):\n    raise NotImplementedError",
                "def _reshape_batch(x, shape):\n    # there is a bug in cntk 2.1's unpack_batch implementation\n    if hasattr(C, 'unpack_batch') and _get_cntk_version() >= 2.2:\n        const_a = C.unpack_batch(x)\n        const_a = C.reshape(const_a, shape)\n        return C.to_batch(const_a)\n    else:\n        return C.user_function(ReshapeBatch(x, shape[1:]))",
                "def _get_cntk_version():\n    version = C.__version__\n    if version.endswith('+'):\n        version = version[:-1]\n    # for hot fix, ignore all the . except the first one.\n    if len(version) > 2 and version[1] == '.':\n        version = version[:2] + version[2:].replace('.', '')\n    try:\n        return float(version)\n    except:\n        warnings.warn(\n            'CNTK backend warning: CNTK version not detected. '\n            'Will using CNTK 2.0 GA as default.')\n        return float(2.0)",
                "def __init__(self, inputs, outputs, updates=[], **kwargs):\n    self.placeholders = inputs\n    self.trainer = None\n    self.unrelated_updates = None\n    self.updates = updates\n    if len(updates) > 0:\n        assert len(outputs) > 0\n        self.loss = outputs[0]\n        # need group update by gradient place holder\n        u_ops = []\n        unrelated_updates = []\n        for update in updates:\n            if isinstance(update, tuple):\n                if len(update) != 2:\n                    raise NotImplementedError\n                else:\n                    u = C.assign(update[0], update[1])\n            else:\n                u = update\n\n            if len(u.arguments) == 0:\n                u_ops.append(u)\n            else:\n                unrelated_updates.append(u)\n\n        update_func = C.combine([u.output for u in u_ops])\n\n        grads = update_func.find_all_with_name('keras_grad_placeholder')\n\n        u_list = []\n        p_list = []\n        for g in grads:\n            if g in grad_parameter_dict:\n                p_list.append(grad_parameter_dict[g])\n                u_list.append(g)\n            else:\n                raise ValueError(\n                    'CNTK backend: when constructing trainer, '\n                    'found gradient node `%s` which is not '\n                    'related to any parameters in the model. '\n                    'Please double check how the gradient node '\n                    'is constructed.' % g)\n\n        if len(u_list) > 0:\n            learner = C.cntk_py.universal_learner(p_list, u_list, update_func)\n\n            criterion = (\n                outputs[0],\n                outputs[1]) if len(outputs) > 1 else (\n                outputs[0],\n            )\n            self.trainer = C.trainer.Trainer(\n                outputs[0], criterion, [learner])\n            self.trainer_output = tuple([f.output for f in criterion])\n        elif len(u_ops) > 0:\n            unrelated_updates.extend(u_ops)\n\n        if len(unrelated_updates) > 0:\n            self.unrelated_updates = C.combine([_.output for _ in unrelated_updates])\n\n    if self.trainer is None:\n        self.metrics_outputs = [f.output for f in outputs]\n        self.metrics_func = C.combine(self.metrics_outputs)\n    # cntk only could handle loss and 1 metric in trainer, for metrics more\n    # than 2, need manual eval\n    elif len(outputs) > 2:\n        self.metrics_outputs = [f.output for f in outputs[2:]]\n        self.metrics_func = C.combine(self.metrics_outputs)\n    else:\n        self.metrics_func = None",
                "@staticmethod\ndef _is_input_shape_compatible(input, placeholder):\n    if hasattr(input, 'shape') and hasattr(placeholder, 'shape'):\n        num_dynamic = get_num_dynamic_axis(placeholder)\n        input_shape = input.shape[num_dynamic:]\n        placeholder_shape = placeholder.shape\n        for i, p in zip(input_shape, placeholder_shape):\n            if i != p and p != C.InferredDimension and p != C.FreeDimension:\n                return False\n    return True",
                "def __call__(self, inputs):\n    global _LEARNING_PHASE_PLACEHOLDER\n    global _LEARNING_PHASE\n    assert isinstance(inputs, (list, tuple))\n    feed_dict = {}\n    for tensor, value in zip(self.placeholders, inputs):\n        # cntk only support calculate on float, do auto cast here\n        if (hasattr(value, 'dtype') and\n           value.dtype != np.float32 and\n           value.dtype != np.float64):\n            value = value.astype(np.float32)\n\n        if tensor == _LEARNING_PHASE_PLACEHOLDER:\n            _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(value)\n        else:\n            # in current version cntk can't support input with variable\n            # length. Will support it in next release.\n            if not self._is_input_shape_compatible(value, tensor):\n                raise ValueError('CNTK backend: The placeholder has been resolved '\n                                 'to shape `%s`, but input shape is `%s`. Currently '\n                                 'CNTK can not take variable length inputs. Please '\n                                 'pass inputs that have a static shape.'\n                                 % (str(tensor.shape), str(value.shape)))\n        feed_dict[tensor] = value\n\n    updated = []\n    if self.trainer is not None:\n        input_dict = {}\n        for argument in self.loss.arguments:\n            if argument in feed_dict:\n                input_dict[argument] = feed_dict[argument]\n            else:\n                raise ValueError(\n                    'CNTK backend: argument %s is not found in inputs. '\n                    'Please double check the model and inputs in '\n                    '`train_function`.' % argument.name)\n\n        result = self.trainer.train_minibatch(\n            input_dict, self.trainer_output)\n\n        assert(len(result) == 2)\n        outputs = result[1]\n        for o in self.trainer_output:\n            updated.append(outputs[o])\n\n    if self.metrics_func is not None:\n        input_dict = {}\n        for argument in self.metrics_func.arguments:\n            if argument in feed_dict:\n                input_dict[argument] = feed_dict[argument]\n            else:\n                raise ValueError('CNTK backend: metrics argument %s '\n                                 'is not found in inputs. Please double '\n                                 'check the model and inputs.' % argument.name)\n        # Some ops (like dropout) won't be applied during \"eval\" in cntk.\n        # They only evaluated in training phase. To make it work, call\n        # \"forward\" method to let cntk know we want to evaluate them.from\n        # But the assign ops won't be executed under this mode, that's why\n        # we need this check.\n        if (self.unrelated_updates is None and\n                (_LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1)):\n            _, output_values = self.metrics_func.forward(\n                input_dict,\n                self.metrics_func.outputs,\n                (self.metrics_func.outputs[0],),\n                as_numpy=False)\n        else:\n            output_values = self.metrics_func.eval(input_dict, as_numpy=False)\n        if isinstance(output_values, dict):\n            for o in self.metrics_outputs:\n                value = output_values[o]\n                v = value.asarray()\n                updated.append(v)\n        else:\n            v = output_values.asarray()\n            for o in self.metrics_outputs:\n                updated.append(v)\n\n    if self.unrelated_updates is not None:\n        input_dict = {}\n        for argument in self.unrelated_updates.arguments:\n            if argument in feed_dict:\n                input_dict[argument] = feed_dict[argument]\n            else:\n                raise ValueError(\n                    'CNTK backend: assign ops argument %s '\n                    'is not found in inputs. Please double '\n                    'check the model and inputs.' % argument.name)\n        self.unrelated_updates.eval(input_dict, as_numpy=False)\n    return updated",
                "def __init__(self, input, shape, name='reshape_with_batch'):\n    super(ReshapeBatch, self).__init__([input], as_numpy=False, name=name)\n    self.from_shape = input.shape\n    self.target_shape = shape",
                "def infer_outputs(self):\n    batch_axis = C.Axis.default_batch_axis()\n    return [\n        C.output_variable(\n            self.target_shape,\n            self.inputs[0].dtype,\n            [batch_axis])]",
                "def forward(self, arguments, device=None, outputs_to_retain=None):\n    num_element = arguments.shape()[0] * np.prod(np.asarray(self.from_shape))\n    num_static_element = np.prod(np.asarray(self.target_shape))\n    num_batch = int(num_element / num_static_element)\n    result = arguments.data().as_shape((num_batch,) + self.target_shape)\n    return None, C.cntk_py.Value(result)",
                "def backward(self, state, root_gradients):\n    grad_array_view = root_gradients.data()\n    num_element = root_gradients.shape()[0] * np.prod(np.asarray(self.target_shape))\n    num_static_element = np.prod(np.asarray(self.from_shape))\n    num_old_batch = int(num_element / num_static_element)\n    return C.cntk_py.Value(\n        grad_array_view.as_shape(\n            (num_old_batch,) + self.from_shape))",
                "def __init__(self, input, name='convert_to_batch'):\n    super(ConvertToBatch, self).__init__([input], as_numpy=False, name=name)",
                "def infer_outputs(self):\n    batch_axis = C.Axis.default_batch_axis()\n    return [\n        C.output_variable(\n            self.inputs[0].shape[1:],\n            self.inputs[0].dtype,\n            [batch_axis])]",
                "def forward(self, arguments, device=None, outputs_to_retain=None):\n    return None, C.cntk_py.Value(arguments.data())",
                "def backward(self, state, root_gradients):\n    return C.cntk_py.Value(root_gradients.data())",
                "def __init__(self, input, batch_size, name='convert_to_static'):\n    super(ConvertToStatic, self).__init__([input], as_numpy=False, name=name)\n    self.target_shape = (batch_size,) + input.shape",
                "def infer_outputs(self):\n    return [\n        C.output_variable(\n            self.target_shape,\n            self.inputs[0].dtype,\n            [])]",
                "def forward(self, arguments, device=None, outputs_to_retain=None):\n    return None, C.cntk_py.Value(arguments.data())",
                "def backward(self, state, root_gradients):\n    return C.cntk_py.Value(root_gradients.data())",
                "def __init__(self,\n             arg,\n             when=lambda arg: True,\n             execute=lambda arg: print(arg),\n             name=''):\n    self.when = when\n    self.execute = execute\n\n    super(LambdaFunc, self).__init__([arg], name=name)",
                "def infer_outputs(self):\n    return [\n        C.output_variable(\n            self.inputs[0].shape,\n            self.inputs[0].dtype,\n            self.inputs[0].dynamic_axes)]",
                "def forward(self, argument, device=None, outputs_to_retain=None):\n    if self.when(argument):\n        self.execute(argument)\n\n    return None, argument",
                "def backward(self, state, root_gradients):\n    return root_gradients",
                "def _recurrence(x, states, m):\n    # create place holder\n    place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]\n    past_values = []\n    for s, p in zip(states, place_holders):\n        past_values.append(C.sequence.past_value(p, s))\n    new_output, new_states = step_function(\n        x, tuple(past_values) + tuple(rnn_constants))\n\n    if getattr(new_output, '_uses_learning_phase', False):\n        global uses_learning_phase\n        uses_learning_phase = True\n\n    if m is not None:\n        new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]\n    n_s = []\n    for o, p in zip(new_states, place_holders):\n        n_s.append(o.replace_placeholders({p: o.output}))\n    if len(n_s) > 0:\n        new_output = n_s[0]\n    return new_output, n_s"
            ],
            "inscope_function_signatures": [
                "name_scope(name)",
                "get_uid(prefix='')",
                "learning_phase()",
                "set_learning_phase(value)",
                "clear_session()",
                "in_train_phase(x, alt, training=None)",
                "in_test_phase(x, alt, training=None)",
                "_convert_string_dtype(dtype)",
                "_convert_dtype_string(dtype)",
                "variable(value, dtype=None, name=None, constraint=None)",
                "bias_add(x, bias, data_format=None)",
                "eval(x)",
                "placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None, dynamic_axis_num=1)",
                "is_placeholder(x)",
                "is_keras_tensor(x)",
                "is_tensor(x)",
                "shape(x)",
                "is_sparse(tensor)",
                "int_shape(x)",
                "ndim(x)",
                "_prepare_name(name, default)",
                "constant(value, dtype=None, shape=None, name=None)",
                "random_binomial(shape, p=0.0, dtype=None, seed=None)",
                "random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None)",
                "random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None)",
                "random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None)",
                "random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)",
                "truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)",
                "dtype(x)",
                "zeros(shape, dtype=None, name=None)",
                "ones(shape, dtype=None, name=None)",
                "eye(size, dtype=None, name=None)",
                "zeros_like(x, dtype=None, name=None)",
                "ones_like(x, dtype=None, name=None)",
                "count_params(x)",
                "cast(x, dtype)",
                "dot(x, y)",
                "batch_dot(x, y, axes=None)",
                "transpose(x)",
                "gather(reference, indices)",
                "_remove_dims(x, axis, keepdims=False)",
                "max(x, axis=None, keepdims=False)",
                "min(x, axis=None, keepdims=False)",
                "sum(x, axis=None, keepdims=False)",
                "prod(x, axis=None, keepdims=False)",
                "logsumexp(x, axis=None, keepdims=False)",
                "var(x, axis=None, keepdims=False)",
                "std(x, axis=None, keepdims=False)",
                "expand_dims(x, axis=-1)",
                "squeeze(x, axis)",
                "tile(x, n)",
                "_normalize_axis(axis, x)",
                "_reshape_dummy_dim(x, axis)",
                "mean(x, axis=None, keepdims=False)",
                "any(x, axis=None, keepdims=False)",
                "all(x, axis=None, keepdims=False)",
                "classification_error(target, output, axis=-1)",
                "argmax(x, axis=-1)",
                "argmin(x, axis=-1)",
                "square(x)",
                "abs(x)",
                "sqrt(x)",
                "exp(x)",
                "log(x)",
                "round(x)",
                "sigmoid(x)",
                "sign(x)",
                "pow(x, a)",
                "clip(x, min_value, max_value)",
                "binary_crossentropy(target, output, from_logits=False)",
                "get_variable_shape(x)",
                "update(x, new_x)",
                "moving_average_update(variable, value, momentum)",
                "update_add(x, increment)",
                "gradients(loss, variables)",
                "equal(x, y)",
                "not_equal(x, y)",
                "greater(x, y)",
                "greater_equal(x, y)",
                "less(x, y)",
                "less_equal(x, y)",
                "maximum(x, y)",
                "minimum(x, y)",
                "sin(x)",
                "cos(x)",
                "normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
                "_moments(x, axes=None, shift=None, keep_dims=False)",
                "batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=0.001)",
                "concatenate(tensors, axis=-1)",
                "flatten(x)",
                "reshape(x, shape)",
                "permute_dimensions(x, pattern)",
                "resize_images(x, height_factor, width_factor, data_format, interpolation='nearest')",
                "resize_volumes(x, depth_factor, height_factor, width_factor, data_format)",
                "repeat_elements(x, rep, axis)",
                "repeat(x, n)",
                "tanh(x)",
                "_static_rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)",
                "rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)",
                "has_seq_axis(x)",
                "l2_normalize(x, axis=None)",
                "hard_sigmoid(x)",
                "conv1d(x, kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)",
                "conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)",
                "separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "conv3d(x, kernel, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1))",
                "conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1), padding='valid', data_format=None)",
                "pool2d(x, pool_size, strides=(1, 1), padding='valid', data_format=None, pool_mode='max')",
                "pool3d(x, pool_size, strides=(1, 1, 1), padding='valid', data_format=None, pool_mode='max')",
                "relu(x, alpha=0.0, max_value=None)",
                "dropout(x, level, noise_shape=None, seed=None)",
                "batch_flatten(x)",
                "softmax(x, axis=-1)",
                "softplus(x)",
                "softsign(x)",
                "categorical_crossentropy(target, output, from_logits=False, axis=-1)",
                "sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1)",
                "function(inputs, outputs, updates=[], **kwargs)",
                "temporal_padding(x, padding=(1, 1))",
                "_padding(x, pattern, axis)",
                "pad(x, pad_info, data_format, num_dynamic_axis)",
                "spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None)",
                "spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None)",
                "one_hot(indices, num_classes)",
                "get_value(x)",
                "batch_get_value(xs)",
                "set_value(x, value)",
                "print_tensor(x, message='')",
                "batch_set_value(tuples)",
                "stop_gradient(variables)",
                "switch(condition, then_expression, else_expression)",
                "elu(x, alpha=1.0)",
                "in_top_k(predictions, targets, k)",
                "conv2d_transpose(x, kernel, output_shape, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
                "identity(x, name=None)",
                "_preprocess_conv2d_input(x, data_format)",
                "_preprocess_conv2d_kernel(kernel, data_format)",
                "_preprocess_border_mode(padding)",
                "_postprocess_conv2d_output(x, data_format)",
                "_preprocess_conv3d_input(x, data_format)",
                "_preprocess_conv3d_kernel(kernel, dim_ordering)",
                "_postprocess_conv3d_output(x, dim_ordering)",
                "_get_dynamic_axis_num(x)",
                "_contain_seqence_axis(x)",
                "get_num_dynamic_axis(x)",
                "_reduce_on_axis(x, axis, reduce_fun_name)",
                "_reshape_sequence(x, time_step)",
                "local_conv1d(inputs, kernel, kernel_size, strides, data_format=None)",
                "local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None)",
                "reverse(x, axes)",
                "slice(x, start, size)",
                "_reshape_batch(x, shape)",
                "_get_cntk_version()",
                "__init__(self, inputs, outputs, updates=[], **kwargs)",
                "_is_input_shape_compatible(input, placeholder)",
                "__call__(self, inputs)",
                "__init__(self, input, shape, name='reshape_with_batch')",
                "infer_outputs(self)",
                "forward(self, arguments, device=None, outputs_to_retain=None)",
                "backward(self, state, root_gradients)",
                "__init__(self, input, name='convert_to_batch')",
                "infer_outputs(self)",
                "forward(self, arguments, device=None, outputs_to_retain=None)",
                "backward(self, state, root_gradients)",
                "__init__(self, input, batch_size, name='convert_to_static')",
                "infer_outputs(self)",
                "forward(self, arguments, device=None, outputs_to_retain=None)",
                "backward(self, state, root_gradients)",
                "__init__(self, arg, when=lambda arg: True, execute=lambda arg: print(arg), name='')",
                "infer_outputs(self)",
                "forward(self, argument, device=None, outputs_to_retain=None)",
                "backward(self, state, root_gradients)",
                "_recurrence(x, states, m)"
            ],
            "variables_in_file": {
                "C.set_global_option": [
                    17
                ],
                "C": [
                    1024,
                    1028,
                    1029,
                    1030,
                    1031,
                    2052,
                    523,
                    2060,
                    17,
                    23,
                    1559,
                    1049,
                    1051,
                    1052,
                    1565,
                    1054,
                    543,
                    544,
                    33,
                    546,
                    1056,
                    1058,
                    1060,
                    1061,
                    1569,
                    1578,
                    2093,
                    1582,
                    2099,
                    2101,
                    1595,
                    2047,
                    2110,
                    2111,
                    1089,
                    1602,
                    579,
                    1606,
                    583,
                    2119,
                    585,
                    1098,
                    2120,
                    1615,
                    593,
                    1106,
                    1107,
                    1108,
                    1618,
                    2129,
                    601,
                    1114,
                    1115,
                    604,
                    93,
                    605,
                    95,
                    1630,
                    2141,
                    1635,
                    613,
                    2149,
                    2151,
                    106,
                    620,
                    1645,
                    1135,
                    1136,
                    2169,
                    2175,
                    1665,
                    2179,
                    1156,
                    2183,
                    2184,
                    2185,
                    663,
                    1690,
                    2203,
                    668,
                    162,
                    164,
                    678,
                    679,
                    169,
                    2219,
                    1197,
                    1201,
                    1713,
                    1715,
                    180,
                    2228,
                    1720,
                    1722,
                    1211,
                    2237,
                    194,
                    706,
                    707,
                    1220,
                    1222,
                    1226,
                    1740,
                    2253,
                    1742,
                    1747,
                    1749,
                    2264,
                    729,
                    2269,
                    1761,
                    1762,
                    2275,
                    1764,
                    1774,
                    2288,
                    241,
                    243,
                    1267,
                    1781,
                    1787,
                    2300,
                    1277,
                    2301,
                    1791,
                    2302,
                    2303,
                    2305,
                    1283,
                    1795,
                    2307,
                    1288,
                    266,
                    1297,
                    786,
                    1813,
                    1814,
                    1816,
                    281,
                    1818,
                    1307,
                    796,
                    1821,
                    799,
                    1823,
                    1313,
                    1824,
                    1318,
                    808,
                    809,
                    1837,
                    1839,
                    1332,
                    821,
                    313,
                    314,
                    315,
                    316,
                    826,
                    833,
                    1861,
                    838,
                    844,
                    845,
                    1870,
                    852,
                    2392,
                    858,
                    1375,
                    1376,
                    865,
                    1378,
                    1889,
                    2401,
                    2402,
                    870,
                    2403,
                    1896,
                    2404,
                    874,
                    2406,
                    364,
                    2410,
                    878,
                    1903,
                    1393,
                    882,
                    1907,
                    886,
                    1912,
                    1401,
                    890,
                    2425,
                    894,
                    1407,
                    2432,
                    898,
                    387,
                    1923,
                    2434,
                    902,
                    1418,
                    2444,
                    1422,
                    912,
                    401,
                    1425,
                    2451,
                    1428,
                    917,
                    918,
                    919,
                    2456,
                    1437,
                    928,
                    932,
                    421,
                    1446,
                    423,
                    1447,
                    937,
                    1450,
                    1451,
                    2471,
                    2473,
                    2479,
                    1457,
                    1458,
                    2482,
                    1460,
                    2485,
                    950,
                    957,
                    961,
                    451,
                    1475,
                    453,
                    965,
                    2503,
                    969,
                    1481,
                    973,
                    2509,
                    2512,
                    977,
                    2515,
                    981,
                    1496,
                    473,
                    985,
                    1497,
                    989,
                    2528,
                    993,
                    1505,
                    484,
                    485,
                    1513,
                    2041,
                    1530,
                    1023
                ],
                "b_any": [
                    2032,
                    1114,
                    19,
                    558
                ],
                "any": [
                    19
                ],
                "py_slice": [
                    20,
                    2357,
                    2326,
                    2359
                ],
                "slice": [
                    20
                ],
                "dev": [
                    1525,
                    23,
                    24,
                    1660,
                    1501
                ],
                "C.device.use_default_device": [
                    23
                ],
                "C.device": [
                    23
                ],
                "dev.type": [
                    24,
                    1525,
                    1660,
                    1501
                ],
                "warnings.warn": [
                    25,
                    2419,
                    1116
                ],
                "warnings": [
                    25,
                    2419,
                    1116
                ],
                "_LEARNING_PHASE_PLACEHOLDER": [
                    33,
                    1987,
                    79,
                    1939,
                    1940,
                    61
                ],
                "C.constant": [
                    33,
                    364,
                    950,
                    2041,
                    2047
                ],
                "np.float32": [
                    480,
                    33,
                    129,
                    1935,
                    1937,
                    117,
                    377,
                    444,
                    125,
                    413
                ],
                "np": [
                    129,
                    131,
                    133,
                    2440,
                    2441,
                    909,
                    911,
                    400,
                    528,
                    1935,
                    1936,
                    1937,
                    1940,
                    2448,
                    2449,
                    410,
                    413,
                    33,
                    442,
                    444,
                    2122,
                    79,
                    472,
                    2139,
                    2140,
                    478,
                    375,
                    480,
                    121,
                    363,
                    497,
                    1780,
                    117,
                    119,
                    504,
                    377,
                    125,
                    510
                ],
                "_LEARNING_PHASE": [
                    1987,
                    35,
                    70,
                    78,
                    61
                ],
                "_UID_PREFIXES": [
                    56,
                    36,
                    55
                ],
                "defaultdict": [
                    36
                ],
                "int": [
                    771,
                    36,
                    1542,
                    711,
                    103,
                    553,
                    1544,
                    2121,
                    2442,
                    2386,
                    2450,
                    695
                ],
                "grad_parameter_dict": [
                    952,
                    41,
                    1877,
                    1878
                ],
                "NAME_SCOPE_STACK": [
                    352,
                    49,
                    43,
                    51
                ],
                "NAME_SCOPE_STACK.append": [
                    49
                ],
                "name": [
                    276,
                    277,
                    285,
                    157,
                    158,
                    417,
                    418,
                    2468,
                    2217,
                    2218,
                    427,
                    2219,
                    49,
                    183,
                    448,
                    449,
                    2498,
                    457,
                    2524,
                    353,
                    355,
                    366,
                    497,
                    504,
                    2427,
                    510
                ],
                "NAME_SCOPE_STACK.pop": [
                    51
                ],
                "contextmanager": [
                    46
                ],
                "prefix": [
                    352,
                    354,
                    355,
                    55,
                    56
                ],
                "value": [
                    1932,
                    1934,
                    1935,
                    1936,
                    1937,
                    1940,
                    1944,
                    1949,
                    1950,
                    161,
                    163,
                    932,
                    165,
                    169,
                    170,
                    172,
                    173,
                    174,
                    181,
                    66,
                    69,
                    70,
                    2121,
                    2122,
                    2123,
                    1997,
                    1998,
                    2138,
                    2139,
                    2140,
                    2142,
                    363
                ],
                "ValueError": [
                    136,
                    393,
                    1803,
                    524,
                    271,
                    1170,
                    1945,
                    1187,
                    1574,
                    1959,
                    1832,
                    1577,
                    559,
                    306,
                    1978,
                    1727,
                    67,
                    199,
                    1351,
                    2247,
                    1611,
                    1614,
                    466,
                    1239,
                    1881,
                    1754,
                    1243,
                    2011,
                    1502,
                    742,
                    1641,
                    1644,
                    1772,
                    2158,
                    2033,
                    246,
                    1526,
                    1148,
                    1661,
                    383
                ],
                "_LEARNING_PHASE_PLACEHOLDER.value": [
                    1987,
                    1940,
                    79
                ],
                "np.asarray": [
                    2440,
                    2441,
                    79,
                    2448,
                    2449,
                    1940,
                    2140
                ],
                "training": [
                    98,
                    103,
                    104,
                    106,
                    112,
                    84,
                    85
                ],
                "learning_phase": [
                    85
                ],
                "uses_learning_phase": [
                    1304,
                    1336,
                    99,
                    1348,
                    1464,
                    107,
                    1434,
                    1236,
                    86,
                    88,
                    1274
                ],
                "callable": [
                    93,
                    95
                ],
                "x": [
                    1536,
                    2048,
                    514,
                    518,
                    1033,
                    522,
                    2060,
                    1551,
                    528,
                    2063,
                    2066,
                    533,
                    1046,
                    1531,
                    1557,
                    537,
                    2075,
                    1052,
                    1565,
                    2041,
                    2076,
                    544,
                    1056,
                    546,
                    1568,
                    1569,
                    2042,
                    550,
                    2087,
                    2088,
                    2089,
                    1578,
                    1581,
                    1582,
                    1585,
                    1074,
                    1586,
                    1076,
                    565,
                    2098,
                    2100,
                    568,
                    1081,
                    1593,
                    2102,
                    2104,
                    2109,
                    574,
                    1086,
                    2110,
                    1089,
                    578,
                    579,
                    1602,
                    1605,
                    1606,
                    2111,
                    2112,
                    585,
                    2114,
                    2119,
                    2120,
                    2122,
                    1102,
                    1615,
                    2123,
                    593,
                    1618,
                    1107,
                    1108,
                    1621,
                    1110,
                    2130,
                    2137,
                    1114,
                    1115,
                    1628,
                    93,
                    94,
                    2141,
                    1120,
                    1121,
                    2142,
                    99,
                    100,
                    1635,
                    1638,
                    615,
                    104,
                    106,
                    1645,
                    112,
                    624,
                    625,
                    626,
                    1136,
                    1140,
                    630,
                    631,
                    1141,
                    1648,
                    1649,
                    1656,
                    637,
                    638,
                    2175,
                    1665,
                    1155,
                    644,
                    645,
                    1156,
                    1667,
                    1672,
                    2179,
                    1162,
                    651,
                    652,
                    1166,
                    1679,
                    2192,
                    658,
                    662,
                    663,
                    1177,
                    1690,
                    2203,
                    668,
                    1692,
                    1182,
                    2205,
                    672,
                    673,
                    1700,
                    2213,
                    679,
                    1191,
                    1194,
                    2218,
                    2219,
                    1197,
                    1711,
                    1713,
                    1714,
                    691,
                    2228,
                    2229,
                    1720,
                    1721,
                    699,
                    1212,
                    701,
                    1213,
                    1728,
                    193,
                    194,
                    707,
                    1217,
                    1220,
                    1221,
                    1737,
                    1226,
                    716,
                    717,
                    1740,
                    1741,
                    2253,
                    2254,
                    2077,
                    1747,
                    1748,
                    728,
                    729,
                    2264,
                    2265,
                    732,
                    1756,
                    736,
                    1761,
                    1762,
                    739,
                    1764,
                    2275,
                    1766,
                    1767,
                    2276,
                    2280,
                    2281,
                    237,
                    1774,
                    2287,
                    2288,
                    241,
                    242,
                    243,
                    244,
                    753,
                    1780,
                    1781,
                    1782,
                    761,
                    250,
                    1783,
                    1787,
                    2294,
                    1791,
                    2303,
                    2305,
                    1795,
                    2307,
                    2308,
                    2312,
                    2314,
                    792,
                    281,
                    797,
                    286,
                    287,
                    288,
                    289,
                    809,
                    301,
                    813,
                    814,
                    305,
                    307,
                    820,
                    309,
                    313,
                    825,
                    320,
                    321,
                    832,
                    323,
                    325,
                    837,
                    336,
                    337,
                    339,
                    340,
                    341,
                    2388,
                    2392,
                    857,
                    858,
                    347,
                    864,
                    865,
                    2402,
                    870,
                    2406,
                    874,
                    878,
                    882,
                    886,
                    890,
                    894,
                    898,
                    902,
                    912,
                    1425,
                    1430,
                    924,
                    928,
                    936,
                    937,
                    957,
                    1469,
                    961,
                    1474,
                    1475,
                    1476,
                    965,
                    1480,
                    969,
                    1481,
                    1482,
                    973,
                    977,
                    1492,
                    981,
                    1496,
                    985,
                    989,
                    993,
                    1505,
                    1507,
                    1000,
                    1513,
                    490,
                    1514,
                    2025,
                    1005,
                    2026,
                    2027,
                    2031,
                    1009,
                    1521,
                    1011,
                    1013,
                    1017,
                    1530,
                    1019,
                    2047
                ],
                "isinstance": [
                    769,
                    771,
                    773,
                    1542,
                    1544,
                    778,
                    1930,
                    160,
                    162,
                    169,
                    553,
                    558,
                    686,
                    688,
                    2097,
                    946,
                    2099,
                    695,
                    313,
                    2110,
                    2111,
                    1857,
                    711,
                    2119,
                    713,
                    2120,
                    1995,
                    2121,
                    2386,
                    1107,
                    2139,
                    93,
                    2141,
                    95,
                    609,
                    2148,
                    613,
                    103,
                    617,
                    620,
                    241,
                    243,
                    1397,
                    1142,
                    2298,
                    2300
                ],
                "C.cntk_py.Function": [
                    169,
                    241,
                    93,
                    95
                ],
                "C.cntk_py": [
                    1889,
                    169,
                    2444,
                    2509,
                    2479,
                    2512,
                    241,
                    2482,
                    2451,
                    93,
                    95
                ],
                "alt": [
                    96,
                    104,
                    106,
                    112,
                    95
                ],
                "x._uses_learning_phase": [
                    99,
                    287
                ],
                "bool": [
                    103
                ],
                "result": [
                    2184,
                    2185,
                    2443,
                    2444,
                    1816,
                    1818,
                    797,
                    799,
                    803,
                    679,
                    936,
                    681,
                    682,
                    937,
                    1964,
                    1967,
                    1968,
                    565,
                    566,
                    2108,
                    2112,
                    2114,
                    2115,
                    585,
                    588,
                    589,
                    104,
                    106,
                    107,
                    108
                ],
                "C.element_select": [
                    833,
                    2179,
                    106,
                    821,
                    2169,
                    1437
                ],
                "result._uses_learning_phase": [
                    107
                ],
                "in_train_phase": [
                    112
                ],
                "dtype": [
                    129,
                    131,
                    260,
                    133,
                    261,
                    387,
                    2436,
                    138,
                    401,
                    406,
                    407,
                    154,
                    283,
                    155,
                    412,
                    413,
                    415,
                    426,
                    2475,
                    173,
                    174,
                    178,
                    182,
                    438,
                    439,
                    443,
                    444,
                    446,
                    456,
                    2505,
                    462,
                    463,
                    473,
                    479,
                    480,
                    482,
                    503,
                    376,
                    2530,
                    486,
                    359,
                    360,
                    504,
                    365,
                    494,
                    495,
                    496,
                    497,
                    116,
                    501,
                    118,
                    502,
                    120,
                    377,
                    379,
                    508,
                    509,
                    510
                ],
                "np.float64": [
                    1936,
                    131,
                    119
                ],
                "np.float16": [
                    121,
                    133
                ],
                "floatx": [
                    407,
                    261,
                    360,
                    2122,
                    463,
                    495,
                    502,
                    439,
                    155,
                    509
                ],
                "C.variables.Constant": [
                    162,
                    2120,
                    243,
                    2101,
                    313,
                    2111
                ],
                "C.variables": [
                    162,
                    164,
                    2119,
                    2120,
                    243,
                    1107,
                    2099,
                    2101,
                    313,
                    314,
                    315,
                    2141,
                    2110,
                    2111
                ],
                "C.variables.Parameter": [
                    164,
                    2119,
                    2099,
                    1107,
                    243,
                    315,
                    2141,
                    2110
                ],
                "value.value": [
                    165
                ],
                "eval": [
                    2104,
                    170,
                    2114
                ],
                "shape": [
                    1074,
                    1076,
                    1081,
                    1086,
                    1106,
                    1108,
                    1112,
                    1121,
                    1124,
                    1126,
                    1127,
                    1131,
                    1133,
                    672,
                    674,
                    675,
                    676,
                    681,
                    1194,
                    172,
                    173,
                    1196,
                    691,
                    180,
                    696,
                    703,
                    705,
                    716,
                    205,
                    207,
                    719,
                    720,
                    722,
                    210,
                    212,
                    1233,
                    1234,
                    727,
                    216,
                    218,
                    1242,
                    221,
                    223,
                    736,
                    737,
                    1247,
                    227,
                    229,
                    232,
                    744,
                    234,
                    236,
                    237,
                    752,
                    1265,
                    262,
                    264,
                    267,
                    1296,
                    792,
                    794,
                    796,
                    286,
                    806,
                    808,
                    809,
                    320,
                    1344,
                    1345,
                    324,
                    327,
                    328,
                    339,
                    342,
                    343,
                    1368,
                    347,
                    348,
                    2403,
                    2406,
                    361,
                    362,
                    363,
                    381,
                    2429,
                    387,
                    391,
                    401,
                    422,
                    2474,
                    452,
                    464,
                    473,
                    2529,
                    485,
                    497,
                    504
                ],
                "hasattr": [
                    2401,
                    2052,
                    2280,
                    172,
                    301,
                    173,
                    1934,
                    336,
                    1457,
                    340,
                    309,
                    2302,
                    1469,
                    1918,
                    1375
                ],
                "value.shape": [
                    172,
                    1949
                ],
                "value.dtype": [
                    1936,
                    173,
                    1935
                ],
                "len": [
                    1416,
                    1801,
                    270,
                    1807,
                    274,
                    2070,
                    2071,
                    2072,
                    537,
                    794,
                    539,
                    540,
                    541,
                    542,
                    1309,
                    544,
                    1441,
                    674,
                    2076,
                    2081,
                    2082,
                    1830,
                    2083,
                    2084,
                    2088,
                    1322,
                    1836,
                    173,
                    557,
                    1967,
                    1329,
                    563,
                    696,
                    825,
                    570,
                    698,
                    1850,
                    1469,
                    1851,
                    193,
                    578,
                    323,
                    1345,
                    197,
                    837,
                    1093,
                    1858,
                    585,
                    586,
                    587,
                    1865,
                    719,
                    720,
                    722,
                    1234,
                    1112,
                    348,
                    605,
                    1888,
                    737,
                    1124,
                    1253,
                    1126,
                    1893,
                    2024,
                    2281,
                    2026,
                    1899,
                    1902,
                    2414,
                    1140,
                    1910,
                    765,
                    1279
                ],
                "value.astype": [
                    1937,
                    174
                ],
                "str": [
                    2247,
                    744,
                    561,
                    178,
                    307,
                    2161,
                    2162,
                    1754,
                    1949,
                    1727
                ],
                "v": [
                    1998,
                    1999,
                    2001,
                    2003,
                    180,
                    949,
                    950,
                    952,
                    184,
                    185,
                    186,
                    187
                ],
                "C.parameter": [
                    484,
                    451,
                    180,
                    421
                ],
                "_prepare_name": [
                    366,
                    183
                ],
                "v._keras_shape": [
                    184
                ],
                "v.shape": [
                    184,
                    950
                ],
                "v._uses_learning_phase": [
                    185
                ],
                "v.constraint": [
                    186
                ],
                "constraint": [
                    186
                ],
                "data_format": [
                    1536,
                    1541,
                    2054,
                    1672,
                    1161,
                    1547,
                    1165,
                    1677,
                    1679,
                    1680,
                    2065,
                    1170,
                    2190,
                    2192,
                    1557,
                    1558,
                    1686,
                    1176,
                    1561,
                    2073,
                    2193,
                    2199,
                    1181,
                    2077,
                    1187,
                    1700,
                    2085,
                    2213,
                    2089,
                    1706,
                    2347,
                    1711,
                    2223,
                    1585,
                    1591,
                    1593,
                    1594,
                    2361,
                    1597,
                    191,
                    1728,
                    1733,
                    1737,
                    2378,
                    203,
                    2252,
                    2318,
                    1487,
                    208,
                    2258,
                    1621,
                    214,
                    1495,
                    1626,
                    219,
                    1628,
                    1629,
                    1756,
                    225,
                    230,
                    1512,
                    1519,
                    1521,
                    1522,
                    1649,
                    1654,
                    1656,
                    1657
                ],
                "normalize_data_format": [
                    1541,
                    1733,
                    2085,
                    1706,
                    2347,
                    1677,
                    2190,
                    1487,
                    1519,
                    2318,
                    1654,
                    1591,
                    2073,
                    1626,
                    191
                ],
                "dims": [
                    224,
                    193,
                    194,
                    195,
                    1345,
                    1253,
                    198,
                    1350,
                    200,
                    1352,
                    202,
                    1234,
                    1140,
                    213,
                    1238,
                    1143,
                    1145
                ],
                "x.shape": [
                    522,
                    792,
                    537,
                    2076,
                    1568,
                    2088,
                    1194,
                    1581,
                    193,
                    194,
                    323,
                    578,
                    325,
                    1217,
                    1605,
                    2122,
                    339,
                    1114,
                    1115,
                    1638,
                    2026,
                    2031,
                    1648,
                    1780,
                    2042
                ],
                "C.InferredDimension": [
                    706,
                    194,
                    1923,
                    678,
                    808,
                    266,
                    523,
                    1135,
                    1106,
                    1114,
                    1211,
                    796
                ],
                "bias_dims": [
                    226,
                    197,
                    198,
                    231,
                    200,
                    204,
                    209,
                    215,
                    220
                ],
                "bias.shape": [
                    227,
                    197,
                    229,
                    232,
                    234,
                    236,
                    205,
                    207,
                    210,
                    212,
                    216,
                    218,
                    221,
                    223
                ],
                "bias": [
                    227,
                    197,
                    229,
                    232,
                    234,
                    236,
                    205,
                    237,
                    207,
                    210,
                    212,
                    216,
                    218,
                    221,
                    223
                ],
                "reshape": [
                    2375,
                    2314,
                    237,
                    1102,
                    2328,
                    2362,
                    2365
                ],
                "x.eval": [
                    242
                ],
                "x.value": [
                    2112,
                    2123,
                    244,
                    2102,
                    2142
                ],
                "type": [
                    250,
                    307
                ],
                "ndim": [
                    737,
                    741,
                    263,
                    264,
                    2155,
                    2156,
                    781,
                    751,
                    1074,
                    1011,
                    1076,
                    1086,
                    1081,
                    1019,
                    1022
                ],
                "tuple": [
                    769,
                    264,
                    1930,
                    268,
                    1420,
                    1044,
                    1302,
                    1430,
                    2040,
                    677,
                    558,
                    686,
                    1857,
                    706,
                    1219,
                    714,
                    720,
                    1106,
                    342,
                    2148,
                    1258,
                    1131,
                    1898,
                    1272,
                    1145,
                    2046
                ],
                "_": [
                    391,
                    264,
                    392,
                    522,
                    523,
                    1425,
                    794,
                    678,
                    808,
                    1198,
                    2352,
                    694,
                    695,
                    696,
                    702,
                    703,
                    706,
                    1988,
                    464,
                    465,
                    720,
                    1106,
                    2389,
                    2390,
                    2391,
                    1114,
                    1115,
                    1131,
                    1135,
                    1903,
                    381,
                    382
                ],
                "range": [
                    323,
                    1830,
                    264,
                    1801,
                    1131,
                    1198,
                    751,
                    720,
                    1011,
                    2355,
                    2325,
                    2166,
                    1143,
                    2356,
                    1145,
                    1019,
                    541
                ],
                "dynamic_dimension": [
                    266,
                    267
                ],
                "_get_cntk_version": [
                    600,
                    2401,
                    266
                ],
                "C.FreeDimension": [
                    706,
                    1923,
                    808,
                    266,
                    523,
                    1451,
                    1211,
                    1106,
                    1115,
                    796
                ],
                "cntk_shape": [
                    267,
                    268,
                    270,
                    274,
                    279,
                    282
                ],
                "s": [
                    1376,
                    1378,
                    1316,
                    1380,
                    1286,
                    1319,
                    1447,
                    1289,
                    267,
                    1427,
                    1428,
                    1437,
                    1373,
                    1374
                ],
                "dynamic_axis_num": [
                    274,
                    270,
                    279
                ],
                "C.input": [
                    281
                ],
                "_convert_string_dtype": [
                    482,
                    283,
                    496,
                    503,
                    379,
                    446,
                    415
                ],
                "sparse": [
                    284
                ],
                "x._keras_shape": [
                    1782,
                    337,
                    286
                ],
                "x._cntk_placeholder": [
                    288,
                    301
                ],
                "is_tensor": [
                    305
                ],
                "C.variables.Variable": [
                    314
                ],
                "C.ops.functions.Function": [
                    316
                ],
                "C.ops.functions": [
                    2515,
                    2485,
                    2456,
                    2425,
                    316
                ],
                "C.ops": [
                    1283,
                    1288,
                    1297,
                    2456,
                    1307,
                    1313,
                    1318,
                    1197,
                    2485,
                    316,
                    844,
                    2515,
                    601,
                    858,
                    604,
                    865,
                    1267,
                    2425,
                    1277
                ],
                "list": [
                    1153,
                    770,
                    773,
                    774,
                    2053,
                    2312,
                    1801,
                    778,
                    1930,
                    792,
                    541,
                    672,
                    1830,
                    558,
                    687,
                    688,
                    946,
                    691,
                    320,
                    1217,
                    713,
                    609,
                    2148,
                    617,
                    1133,
                    1011,
                    1397,
                    1142,
                    2038,
                    2298,
                    2044
                ],
                "int_shape": [
                    1416,
                    2312,
                    528,
                    2321,
                    538,
                    924,
                    672,
                    550,
                    551,
                    2351,
                    691,
                    320,
                    1344,
                    716,
                    1233,
                    347,
                    736,
                    1252,
                    1140,
                    2164,
                    1017
                ],
                "num_dynamic": [
                    1920,
                    321,
                    324,
                    327,
                    328,
                    1919
                ],
                "_get_dynamic_axis_num": [
                    2075,
                    673,
                    2087,
                    1456,
                    825,
                    1213,
                    701,
                    321,
                    837,
                    1354,
                    717,
                    1110,
                    1374,
                    739,
                    1256,
                    2025,
                    2287,
                    624,
                    1141,
                    2294,
                    1400,
                    1406
                ],
                "non_dyn_shape": [
                    328,
                    322,
                    325,
                    327
                ],
                "i": [
                    1145,
                    1922,
                    1923,
                    779,
                    781,
                    782,
                    783,
                    1293,
                    1295,
                    1296,
                    1297,
                    2325,
                    2326,
                    2327,
                    1277,
                    1307,
                    1195,
                    1196,
                    1197,
                    1323,
                    1325,
                    1200,
                    1329,
                    1331,
                    2355,
                    1333,
                    1334,
                    2357,
                    2358,
                    577,
                    578,
                    323,
                    324,
                    325,
                    579,
                    327,
                    580,
                    581,
                    582,
                    583,
                    584,
                    725,
                    726,
                    727,
                    1143,
                    729,
                    730,
                    1125,
                    1126,
                    1127,
                    1128,
                    1131,
                    751,
                    752,
                    1265,
                    1266,
                    1267,
                    756,
                    2166,
                    759,
                    2168,
                    761,
                    762,
                    765,
                    766,
                    767
                ],
                "non_dyn_shape.append": [
                    325,
                    327
                ],
                "tensor.is_sparse": [
                    332
                ],
                "tensor": [
                    1932,
                    332,
                    1939,
                    1944,
                    1949,
                    1950
                ],
                "dynamic_shape": [
                    341,
                    342
                ],
                "a": [
                    2305,
                    612,
                    613,
                    614,
                    902,
                    619,
                    620,
                    779,
                    558,
                    780,
                    781,
                    2062,
                    2064,
                    341,
                    2299,
                    2300,
                    2301,
                    2303
                ],
                "x.dynamic_axes": [
                    2281,
                    2288,
                    753,
                    1425,
                    341,
                    761,
                    1469
                ],
                "join": [
                    352
                ],
                "default": [
                    354
                ],
                "np_value": [
                    363,
                    364
                ],
                "np.ones": [
                    504,
                    363
                ],
                "const": [
                    368,
                    369,
                    364,
                    367
                ],
                "const._keras_shape": [
                    367
                ],
                "const.shape": [
                    367
                ],
                "const._uses_learning_phase": [
                    368
                ],
                "seed": [
                    387,
                    440,
                    455,
                    472,
                    425,
                    486,
                    398,
                    400,
                    401,
                    442,
                    373,
                    470,
                    375,
                    408,
                    473,
                    410,
                    477,
                    478
                ],
                "np.random.randint": [
                    410,
                    400,
                    375,
                    472,
                    442,
                    478
                ],
                "np.random": [
                    410,
                    400,
                    375,
                    472,
                    442,
                    478
                ],
                "C.random.bernoulli": [
                    387
                ],
                "C.random": [
                    401,
                    387,
                    473
                ],
                "p": [
                    1440,
                    1922,
                    387,
                    451,
                    421,
                    1923,
                    2053,
                    458,
                    428,
                    2062,
                    2063,
                    1427,
                    1428,
                    1439
                ],
                "C.random.uniform": [
                    401
                ],
                "minval": [
                    401
                ],
                "maxval": [
                    401
                ],
                "scale": [
                    424,
                    428,
                    420,
                    454
                ],
                "high": [
                    420
                ],
                "low": [
                    428,
                    420
                ],
                "C.initializer.uniform": [
                    423
                ],
                "C.initializer": [
                    485,
                    453,
                    423
                ],
                "variable": [
                    932,
                    458,
                    428,
                    497,
                    504,
                    510
                ],
                "p.value": [
                    458,
                    428
                ],
                "C.initializer.normal": [
                    453
                ],
                "mean": [
                    1089,
                    1028,
                    1061,
                    1064,
                    458,
                    1067,
                    1040,
                    1009,
                    1074,
                    1075,
                    1013,
                    662,
                    664,
                    473,
                    1085
                ],
                "C.random.normal": [
                    473
                ],
                "stddev": [
                    473,
                    486
                ],
                "C.initializer.truncated_normal": [
                    485
                ],
                "_convert_dtype_string": [
                    490
                ],
                "x.dtype": [
                    490
                ],
                "ctype": [
                    496,
                    497,
                    504,
                    503
                ],
                "np.zeros": [
                    497
                ],
                "np.eye": [
                    510
                ],
                "size": [
                    510
                ],
                "zeros_like": [
                    1280,
                    836,
                    518,
                    1005,
                    1007,
                    824,
                    1085,
                    1310
                ],
                "np.prod": [
                    2440,
                    2441,
                    528,
                    2448,
                    2449,
                    1780
                ],
                "y.shape": [
                    585,
                    537,
                    586
                ],
                "y": [
                    537,
                    538,
                    543,
                    544,
                    546,
                    551,
                    565,
                    568,
                    571,
                    957,
                    575,
                    961,
                    965,
                    583,
                    585,
                    586,
                    969,
                    973,
                    977,
                    981,
                    985
                ],
                "y_shape": [
                    544,
                    551,
                    587,
                    557,
                    563,
                    570,
                    538,
                    539,
                    540,
                    541,
                    542
                ],
                "permutation": [
                    1810,
                    1811,
                    1812,
                    1813,
                    1814,
                    540,
                    541,
                    542,
                    543
                ],
                "C.transpose": [
                    2275,
                    1156,
                    2269,
                    2253,
                    2228,
                    1813,
                    1814,
                    1559,
                    2264,
                    1595,
                    2237,
                    1630,
                    543
                ],
                "C.times": [
                    544,
                    585,
                    546,
                    605
                ],
                "x_shape": [
                    1026,
                    550,
                    557,
                    563,
                    1017
                ],
                "axes": [
                    553,
                    554,
                    555,
                    557,
                    558,
                    561,
                    2386,
                    2387,
                    564,
                    565,
                    566,
                    1044,
                    568,
                    2388,
                    574,
                    575
                ],
                "sum": [
                    1824,
                    2336,
                    2373,
                    658,
                    820,
                    565,
                    568
                ],
                "transpose": [
                    568,
                    566
                ],
                "expand_dims": [
                    1254,
                    1417,
                    1327,
                    1551,
                    625,
                    1552,
                    1331,
                    1553,
                    2167,
                    571
                ],
                "normalized_axis": [
                    577,
                    581,
                    573,
                    574,
                    575
                ],
                "normalized_axis.append": [
                    574,
                    575
                ],
                "_normalize_axis": [
                    864,
                    1474,
                    1155,
                    644,
                    1191,
                    1097,
                    651,
                    813,
                    1009,
                    2388,
                    630,
                    857,
                    637,
                    574,
                    575
                ],
                "C.swapaxes": [
                    579,
                    583,
                    1513,
                    593,
                    1496,
                    1497
                ],
                "squeeze": [
                    1064,
                    1065,
                    588,
                    1586,
                    1299,
                    1269,
                    1308,
                    1278
                ],
                "C.ops.gather": [
                    601
                ],
                "reference": [
                    601,
                    603,
                    605
                ],
                "indices": [
                    601,
                    604,
                    2093
                ],
                "num_classes": [
                    603,
                    604,
                    2093
                ],
                "reference.shape": [
                    603,
                    605
                ],
                "one_hot_matrix": [
                    604,
                    605
                ],
                "C.ops.one_hot": [
                    604
                ],
                "keepdims": [
                    640,
                    609,
                    832,
                    647,
                    654,
                    816,
                    658,
                    820,
                    664,
                    633,
                    668
                ],
                "axis": [
                    1024,
                    1026,
                    1048,
                    1049,
                    1053,
                    1054,
                    1057,
                    1058,
                    2047,
                    1096,
                    1097,
                    1098,
                    609,
                    612,
                    617,
                    619,
                    630,
                    631,
                    633,
                    637,
                    638,
                    640,
                    1153,
                    1154,
                    1155,
                    644,
                    645,
                    1156,
                    647,
                    651,
                    652,
                    654,
                    658,
                    662,
                    664,
                    668,
                    674,
                    1191,
                    1192,
                    1196,
                    1197,
                    686,
                    687,
                    688,
                    689,
                    1201,
                    694,
                    2298,
                    1787,
                    2299,
                    769,
                    770,
                    771,
                    772,
                    773,
                    774,
                    2307,
                    776,
                    1800,
                    794,
                    1829,
                    813,
                    814,
                    816,
                    1840,
                    820,
                    832,
                    856,
                    857,
                    858,
                    859,
                    863,
                    864,
                    865,
                    866,
                    1473,
                    1474,
                    1475,
                    2039,
                    2041,
                    1019,
                    1020,
                    2045,
                    1022,
                    1023
                ],
                "reduce_axes": [
                    611,
                    614,
                    615
                ],
                "C.Axis": [
                    2432,
                    613,
                    2471,
                    620,
                    2288,
                    786,
                    852,
                    2300,
                    2301
                ],
                "reduce_axes.append": [
                    614
                ],
                "_reshape_dummy_dim": [
                    866,
                    615,
                    1075,
                    1077,
                    1082,
                    859,
                    1087
                ],
                "has_seq": [
                    618,
                    621,
                    623
                ],
                "nones": [
                    673,
                    705,
                    739,
                    676,
                    741,
                    680,
                    744,
                    1256,
                    1262,
                    624,
                    625,
                    752,
                    758,
                    760,
                    701,
                    766
                ],
                "output": [
                    640,
                    1280,
                    1283,
                    645,
                    1273,
                    647,
                    1801,
                    1162,
                    1163,
                    652,
                    1164,
                    654,
                    1166,
                    1167,
                    1168,
                    1291,
                    1807,
                    917,
                    918,
                    919,
                    920,
                    1177,
                    1178,
                    1179,
                    1180,
                    1301,
                    1182,
                    1183,
                    1184,
                    1185,
                    1303,
                    1310,
                    1313,
                    1813,
                    1821,
                    1823,
                    1824,
                    1321,
                    1830,
                    2338,
                    1836,
                    1837,
                    814,
                    1839,
                    816,
                    1816,
                    1840,
                    2371,
                    2373,
                    2375,
                    2380,
                    2382,
                    847,
                    858,
                    859,
                    2334,
                    865,
                    866,
                    2336,
                    631,
                    633,
                    1271,
                    638
                ],
                "_reduce_on_axis": [
                    645,
                    652,
                    814,
                    631,
                    638
                ],
                "_remove_dims": [
                    640,
                    647,
                    654,
                    816,
                    633
                ],
                "log": [
                    658
                ],
                "exp": [
                    658
                ],
                "m": [
                    1436,
                    1437,
                    662,
                    663
                ],
                "devs_squared": [
                    664,
                    663
                ],
                "C.square": [
                    1056,
                    1475,
                    1060,
                    870,
                    663
                ],
                "C.sqrt": [
                    1089,
                    1475,
                    668,
                    878
                ],
                "var": [
                    1089,
                    1076,
                    1077,
                    1080,
                    668
                ],
                "index": [
                    801,
                    674,
                    675,
                    802,
                    805,
                    806,
                    1218,
                    680,
                    1222,
                    1214,
                    1213,
                    798
                ],
                "shape.insert": [
                    675
                ],
                "new_shape": [
                    705,
                    706,
                    707,
                    676,
                    677,
                    678,
                    679,
                    1217,
                    1218,
                    1219,
                    1220,
                    1133,
                    1134,
                    1135,
                    1136
                ],
                "C.reshape": [
                    1028,
                    1029,
                    1030,
                    1031,
                    2185,
                    1559,
                    1818,
                    799,
                    679,
                    809,
                    1839,
                    1595,
                    707,
                    1220,
                    1108,
                    1630,
                    2403,
                    1136,
                    1781
                ],
                "result._keras_shape": [
                    681
                ],
                "_axis": [
                    770,
                    772,
                    774,
                    776,
                    778,
                    779,
                    781,
                    782,
                    783,
                    785,
                    786,
                    788,
                    794,
                    798,
                    805,
                    693,
                    696,
                    698,
                    702
                ],
                "_axis.append": [
                    696
                ],
                "sorted": [
                    1011,
                    805,
                    702,
                    798
                ],
                "n": [
                    1221,
                    711,
                    712,
                    713,
                    714,
                    719,
                    720,
                    722,
                    726,
                    1211,
                    1437
                ],
                "num_dynamic_axis": [
                    1154,
                    2058,
                    2064,
                    2075,
                    2076,
                    2077,
                    2087,
                    2088,
                    2089,
                    717,
                    725,
                    1110,
                    727,
                    1112,
                    729,
                    1124,
                    2025,
                    2026,
                    1131,
                    2027,
                    1134,
                    1141,
                    1147
                ],
                "NotImplementedError": [
                    2144,
                    1859,
                    1384,
                    2125,
                    723,
                    1172,
                    2396,
                    1215
                ],
                "rep": [
                    728,
                    1198,
                    726
                ],
                "enumerate": [
                    779,
                    726,
                    2062
                ],
                "tmp": [
                    728,
                    729,
                    1197,
                    1199
                ],
                "C.splice": [
                    1222,
                    1098,
                    1201,
                    1332,
                    729,
                    2041,
                    2047
                ],
                "cntk_axis": [
                    749,
                    783,
                    753,
                    756,
                    761,
                    765,
                    766
                ],
                "dynamic_axis_index": [
                    750,
                    752,
                    753,
                    754,
                    756,
                    758,
                    760,
                    761,
                    763
                ],
                "cntk_axis.append": [
                    753,
                    756
                ],
                "C.Axis.all_axes": [
                    786,
                    852
                ],
                "shape.count": [
                    796
                ],
                "reduce_result": [
                    832,
                    834,
                    835,
                    836,
                    837,
                    820,
                    822,
                    823,
                    824,
                    825
                ],
                "any_matrix": [
                    826,
                    828,
                    821
                ],
                "ones_like": [
                    835,
                    1000,
                    1002,
                    823,
                    1080
                ],
                "reduce_result.shape": [
                    825,
                    837
                ],
                "C.reduce_sum": [
                    826,
                    1475,
                    1821,
                    838
                ],
                "prod": [
                    832
                ],
                "all_matrix": [
                    840,
                    833,
                    838
                ],
                "C.ops.reduce_mean": [
                    844
                ],
                "C.equal": [
                    845,
                    957
                ],
                "argmax": [
                    849,
                    846
                ],
                "target": [
                    1824,
                    1837,
                    1839,
                    1840,
                    850,
                    1814,
                    919,
                    1816
                ],
                "C.ops.argmax": [
                    858
                ],
                "C.ops.argmin": [
                    865
                ],
                "C.abs": [
                    874,
                    1795,
                    898
                ],
                "C.exp": [
                    882
                ],
                "C.log": [
                    1824,
                    886,
                    919
                ],
                "C.round": [
                    890
                ],
                "C.sigmoid": [
                    917,
                    894
                ],
                "C.pow": [
                    902
                ],
                "max_value": [
                    1763,
                    1764,
                    906,
                    907,
                    908,
                    909,
                    912
                ],
                "min_value": [
                    906,
                    907,
                    910,
                    911,
                    912
                ],
                "np.inf": [
                    909,
                    911
                ],
                "C.clip": [
                    1764,
                    1481,
                    912,
                    918,
                    1823
                ],
                "from_logits": [
                    1840,
                    916,
                    1815
                ],
                "epsilon": [
                    1089,
                    1038,
                    1013,
                    918,
                    1823
                ],
                "C.assign": [
                    928,
                    937,
                    932,
                    1861
                ],
                "new_x": [
                    928
                ],
                "momentum": [
                    932
                ],
                "increment": [
                    936
                ],
                "variables": [
                    2148,
                    2149,
                    2151,
                    946,
                    947,
                    949
                ],
                "grads": [
                    1872,
                    948,
                    1876,
                    951,
                    953
                ],
                "g": [
                    1879,
                    1876,
                    1877,
                    950,
                    951,
                    952,
                    1878,
                    1886
                ],
                "grads.append": [
                    951
                ],
                "C.not_equal": [
                    961
                ],
                "C.greater": [
                    2179,
                    965
                ],
                "C.greater_equal": [
                    969
                ],
                "C.less": [
                    973
                ],
                "C.less_equal": [
                    977
                ],
                "C.element_max": [
                    981
                ],
                "C.element_min": [
                    985
                ],
                "C.sin": [
                    989
                ],
                "C.cos": [
                    993
                ],
                "gamma": [
                    1089,
                    998,
                    1030,
                    1000,
                    1002,
                    1004,
                    1007,
                    1013,
                    1079,
                    1080,
                    1081,
                    1082,
                    1022,
                    1023
                ],
                "beta": [
                    1024,
                    1089,
                    999,
                    1031,
                    1002,
                    1003,
                    1005,
                    1007,
                    1013,
                    1084,
                    1085,
                    1086,
                    1087
                ],
                "variant": [
                    1040,
                    1009,
                    1029,
                    1013
                ],
                "_moments": [
                    1009
                ],
                "reduction_axes": [
                    1009,
                    1011,
                    1020
                ],
                "normalized": [
                    1032,
                    1040,
                    1012
                ],
                "batch_normalization": [
                    1032,
                    1012
                ],
                "target_shape": [
                    1026,
                    1028,
                    1029,
                    1030,
                    1031,
                    1016,
                    1021
                ],
                "target_shape.append": [
                    1026,
                    1021
                ],
                "C.reduce_mean": [
                    1024,
                    1058,
                    1049,
                    1054,
                    1023
                ],
                "broadcast_mean": [
                    1034,
                    1028
                ],
                "broadcast_var": [
                    1035,
                    1029
                ],
                "broadcast_gamma": [
                    1037,
                    1030
                ],
                "broadcast_beta": [
                    1036,
                    1031
                ],
                "_axes": [
                    1057,
                    1064,
                    1065,
                    1044,
                    1048,
                    1053
                ],
                "shift": [
                    1056,
                    1061,
                    1045,
                    1046,
                    1049,
                    1051,
                    1052
                ],
                "C.stop_gradient": [
                    1051,
                    2149,
                    2151
                ],
                "shifted_mean": [
                    1060,
                    1052,
                    1061,
                    1054
                ],
                "C.minus": [
                    1056,
                    1052,
                    1060
                ],
                "variance_mean": [
                    1056,
                    1058,
                    1060
                ],
                "variance": [
                    1065,
                    1067,
                    1060
                ],
                "C.plus": [
                    1061
                ],
                "keep_dims": [
                    1063
                ],
                "tensors": [
                    1097,
                    1098,
                    1093
                ],
                "_reshape_batch": [
                    1121
                ],
                "pattern": [
                    1153,
                    2053,
                    2043,
                    2055,
                    2057,
                    2059,
                    2060,
                    2037,
                    1142,
                    2039,
                    1147,
                    2045,
                    1151
                ],
                "current_layout": [
                    1145,
                    1147,
                    1143
                ],
                "interpolation": [
                    1160
                ],
                "repeat_elements": [
                    1184,
                    1162,
                    1163,
                    1166,
                    1167,
                    1177,
                    1178,
                    1179,
                    1182,
                    1183
                ],
                "height_factor": [
                    1183,
                    1162,
                    1166,
                    1178
                ],
                "width_factor": [
                    1184,
                    1163,
                    1179,
                    1167
                ],
                "depth_factor": [
                    1177,
                    1182
                ],
                "slices": [
                    1193,
                    1201,
                    1199
                ],
                "C.ops.slice": [
                    1197,
                    1297,
                    1267,
                    1307,
                    1277
                ],
                "slices.append": [
                    1199
                ],
                "new_shape.insert": [
                    1218
                ],
                "temp": [
                    1221,
                    1222
                ],
                "C.tanh": [
                    1226
                ],
                "inputs": [
                    1344,
                    2020,
                    1382,
                    1256,
                    1354,
                    1930,
                    1388,
                    1357,
                    1932,
                    2362,
                    1233,
                    1297,
                    1267,
                    1846,
                    2328,
                    1369,
                    1370,
                    2365
                ],
                "constants": [
                    1248,
                    1249,
                    1411,
                    1361,
                    1396,
                    1365,
                    1302,
                    1366,
                    1272
                ],
                "mask": [
                    1251,
                    1252,
                    1413,
                    1254,
                    1415,
                    1416,
                    1417,
                    1418,
                    1445,
                    1360,
                    1306,
                    1307,
                    1276,
                    1277
                ],
                "mask_shape": [
                    1252,
                    1253
                ],
                "states": [
                    1316,
                    1445,
                    1286,
                    1258,
                    1322,
                    1292,
                    1420,
                    1425,
                    1427,
                    1302,
                    1272,
                    1337
                ],
                "initial_states": [
                    1258,
                    1373,
                    1358,
                    1455
                ],
                "outputs": [
                    1282,
                    1291,
                    1309,
                    1312,
                    1321,
                    1327,
                    1328,
                    1329,
                    1968,
                    1331,
                    1970,
                    1333,
                    1851,
                    1852,
                    1892,
                    1893,
                    1894,
                    2020,
                    1897,
                    1260,
                    1906,
                    1910,
                    1911,
                    1279
                ],
                "time_axis": [
                    1262,
                    1297,
                    1267,
                    1332,
                    1269,
                    1307,
                    1277,
                    1278
                ],
                "go_backwards": [
                    1414,
                    1383,
                    1390,
                    1359,
                    1264
                ],
                "current": [
                    1297,
                    1299,
                    1267,
                    1269,
                    1302,
                    1272
                ],
                "new_states": [
                    1316,
                    1286,
                    1320,
                    1290,
                    1322,
                    1292,
                    1301,
                    1429,
                    1271,
                    1437,
                    1439
                ],
                "step_function": [
                    1356,
                    1301,
                    1429,
                    1271
                ],
                "getattr": [
                    2305,
                    2307,
                    1303,
                    1432,
                    1273,
                    2303
                ],
                "mask_slice": [
                    1313,
                    1283,
                    1319,
                    1289,
                    1307,
                    1308,
                    1277,
                    1278
                ],
                "prev_output": [
                    1280,
                    1312,
                    1282,
                    1283,
                    1313,
                    1310
                ],
                "C.ops.element_select": [
                    1288,
                    1313,
                    1283,
                    1318
                ],
                "return_states": [
                    1315,
                    1317,
                    1285,
                    1287,
                    1320,
                    1290
                ],
                "n_s": [
                    1440,
                    1441,
                    1442,
                    1443,
                    1316,
                    1286,
                    1319,
                    1289,
                    1438
                ],
                "zip": [
                    1922,
                    1316,
                    1286,
                    1932,
                    1455,
                    1427,
                    1437,
                    1439
                ],
                "return_states.append": [
                    1317,
                    1287
                ],
                "outputs.append": [
                    1321,
                    1291
                ],
                "final_output": [
                    1445,
                    1446,
                    1450,
                    1465,
                    1452,
                    1327,
                    1332,
                    1337
                ],
                "last_output": [
                    1446,
                    1464,
                    1465,
                    1328,
                    1333,
                    1336,
                    1337
                ],
                "output_slice": [
                    1331,
                    1332
                ],
                "last_output._uses_learning_phase": [
                    1336,
                    1464
                ],
                "unroll": [
                    1354,
                    1362
                ],
                "_static_rnn": [
                    1355
                ],
                "input_length": [
                    1363
                ],
                "num_time_step": [
                    1451,
                    1452,
                    1368,
                    1369,
                    1370
                ],
                "has_seq_axis": [
                    1369,
                    1413,
                    1382
                ],
                "inputs.shape": [
                    1370
                ],
                "initial": [
                    1376,
                    1378,
                    1380,
                    1420,
                    1372
                ],
                "initial.append": [
                    1376,
                    1378,
                    1380
                ],
                "C.to_batch": [
                    1376,
                    2404
                ],
                "C.user_function": [
                    2129,
                    1378,
                    1460,
                    2406
                ],
                "ConvertToBatch": [
                    1378,
                    2468
                ],
                "need_convert": [
                    1449,
                    1389,
                    1382,
                    1383
                ],
                "rnn_inputs": [
                    1445,
                    1418,
                    1388,
                    1391,
                    1393,
                    1401,
                    1407
                ],
                "reverse": [
                    1415,
                    1391
                ],
                "C.to_sequence": [
                    1393
                ],
                "rnn_constants": [
                    1409,
                    1411,
                    1395,
                    1430,
                    1404,
                    1407
                ],
                "constant": [
                    1409,
                    1396,
                    1397,
                    1399,
                    1406,
                    1407
                ],
                "new_c": [
                    1401,
                    1403,
                    1404,
                    1398
                ],
                "c": [
                    1400,
                    1401,
                    1403,
                    1399
                ],
                "new_c.append": [
                    1401,
                    1403
                ],
                "C.sequence.broadcast_as": [
                    1401,
                    1407
                ],
                "C.sequence": [
                    2303,
                    1446,
                    1447,
                    1450,
                    1428,
                    1401,
                    2302,
                    1407
                ],
                "rnn_constants.append": [
                    1409,
                    1404,
                    1407
                ],
                "C.to_sequence_like": [
                    1418
                ],
                "C.default_options": [
                    1422
                ],
                "place_holders": [
                    1425,
                    1427,
                    1439
                ],
                "C.placeholder": [
                    1425
                ],
                "past_values": [
                    1426,
                    1428,
                    1437,
                    1430
                ],
                "past_values.append": [
                    1428
                ],
                "C.sequence.past_value": [
                    1428
                ],
                "new_output": [
                    1432,
                    1442,
                    1443,
                    1429
                ],
                "o": [
                    1440,
                    1996,
                    1997,
                    1969,
                    1970,
                    2002,
                    1439
                ],
                "n_s.append": [
                    1440
                ],
                "o.replace_placeholders": [
                    1440
                ],
                "o.output": [
                    1440
                ],
                "final_states": [
                    1445,
                    1447
                ],
                "_recurrence": [
                    1445
                ],
                "C.sequence.last": [
                    1446,
                    1447
                ],
                "last_states": [
                    1455,
                    1447
                ],
                "C.sequence.unpack": [
                    1450
                ],
                "_reshape_sequence": [
                    1452
                ],
                "f_stats": [
                    1454,
                    1458,
                    1460,
                    1462,
                    1465
                ],
                "l_s": [
                    1455,
                    1456,
                    1458,
                    1460,
                    1462
                ],
                "i_s": [
                    1456,
                    1460,
                    1455
                ],
                "f_stats.append": [
                    1458,
                    1460,
                    1462
                ],
                "C.unpack_batch": [
                    1458,
                    2402
                ],
                "ConvertToStatic": [
                    2498,
                    1460
                ],
                "i_s.shape": [
                    1460
                ],
                "norm": [
                    1475,
                    1476
                ],
                "padding": [
                    1669,
                    1681,
                    2194,
                    2070,
                    2071,
                    2072,
                    1562,
                    2077,
                    1567,
                    1696,
                    1697,
                    1698,
                    2081,
                    2082,
                    2083,
                    2084,
                    2209,
                    2210,
                    2089,
                    1580,
                    1708,
                    1718,
                    1725,
                    1598,
                    2242,
                    2243,
                    1604,
                    2244,
                    2245,
                    1735,
                    2247,
                    2248,
                    1489,
                    1617,
                    1745,
                    1493,
                    1752,
                    1499,
                    1632,
                    1509,
                    1637,
                    2024,
                    2027,
                    1647,
                    1523,
                    1658,
                    1533
                ],
                "left_pad": [
                    1491,
                    1492
                ],
                "dilation_rate": [
                    1670,
                    1544,
                    1545,
                    1555,
                    1564,
                    2212,
                    1573,
                    1600,
                    1610,
                    1616,
                    1491,
                    1501,
                    1503,
                    1633,
                    1510,
                    1640,
                    1646,
                    1525,
                    1528,
                    1660,
                    1534,
                    1663
                ],
                "kernel.shape": [
                    1491
                ],
                "kernel": [
                    1666,
                    1680,
                    2193,
                    2321,
                    1691,
                    2332,
                    2204,
                    2351,
                    2237,
                    2238,
                    2369,
                    1491,
                    1497,
                    2269,
                    2270,
                    1506,
                    1522,
                    1657,
                    1530
                ],
                "temporal_padding": [
                    1492
                ],
                "_preprocess_border_mode": [
                    1632,
                    1735,
                    1708,
                    1681,
                    2194,
                    1523,
                    1658,
                    1562,
                    1499,
                    1598
                ],
                "C.convolution": [
                    1505,
                    1569,
                    1602,
                    1635,
                    1665,
                    1606,
                    1578,
                    1645,
                    1582,
                    1615,
                    1618,
                    1530,
                    1565
                ],
                "strides": [
                    1668,
                    1542,
                    1543,
                    2320,
                    1554,
                    1682,
                    2195,
                    1693,
                    1566,
                    2206,
                    1576,
                    1579,
                    1709,
                    2349,
                    1717,
                    1724,
                    1601,
                    1603,
                    1613,
                    1744,
                    1751,
                    2391,
                    2392,
                    1634,
                    1508,
                    1636,
                    1643,
                    1532
                ],
                "_preprocess_conv2d_input": [
                    1711,
                    2192,
                    1521,
                    1557,
                    1593,
                    1628
                ],
                "_preprocess_conv2d_kernel": [
                    1629,
                    2193,
                    1522,
                    1558,
                    1561,
                    1594,
                    1597
                ],
                "_postprocess_conv2d_output": [
                    1536,
                    1728,
                    2213,
                    1585,
                    1649,
                    1621
                ],
                "spatial_start_dim": [
                    1586,
                    1548,
                    1550,
                    1551
                ],
                "depthwise_kernel": [
                    1602,
                    1635,
                    1629,
                    1578,
                    1645,
                    1615,
                    1552,
                    1558,
                    1559,
                    1560,
                    1594,
                    1595,
                    1596,
                    1565,
                    1630,
                    1631
                ],
                "pointwise_kernel": [
                    1569,
                    1606,
                    1582,
                    1553,
                    1618,
                    1561,
                    1597
                ],
                "depthwise_kernel.shape": [
                    1560,
                    1596,
                    1631
                ],
                "_preprocess_conv3d_input": [
                    1656,
                    1737,
                    1679
                ],
                "_preprocess_conv3d_kernel": [
                    1680,
                    1657
                ],
                "_postprocess_conv3d_output": [
                    1672,
                    1756,
                    1700
                ],
                "output_shape": [
                    1699,
                    2211,
                    2350,
                    1684,
                    2197,
                    1687,
                    2200
                ],
                "transpose_shape": [
                    2200,
                    1687
                ],
                "C.convolution_transpose": [
                    1690,
                    2203
                ],
                "pool_size": [
                    1710,
                    1743,
                    1716,
                    1750,
                    1723
                ],
                "pool_mode": [
                    1739,
                    1712,
                    1746,
                    1719,
                    1754,
                    1727
                ],
                "C.pooling": [
                    1720,
                    1713,
                    1747,
                    1740
                ],
                "C.MAX_POOLING": [
                    1715,
                    1742
                ],
                "C.AVG_POOLING": [
                    1722,
                    1749
                ],
                "alpha": [
                    1760,
                    2176,
                    2179,
                    1765,
                    1766
                ],
                "negative_part": [
                    1761,
                    1766
                ],
                "C.relu": [
                    1761,
                    1762
                ],
                "level": [
                    1771,
                    1773,
                    1774
                ],
                "C.dropout": [
                    1774
                ],
                "dim": [
                    2032,
                    1780,
                    1782
                ],
                "C.softmax": [
                    1787
                ],
                "C.softplus": [
                    1791
                ],
                "axis_without_batch": [
                    1829,
                    1831,
                    1800,
                    1802,
                    1834,
                    1805,
                    1837,
                    1838,
                    1809,
                    1810,
                    1811,
                    1812
                ],
                "output_dimensions": [
                    1830,
                    1831,
                    1801,
                    1802,
                    1809,
                    1810,
                    1811
                ],
                "output.shape": [
                    1830,
                    1801,
                    1836,
                    1837,
                    1807,
                    1839
                ],
                "format": [
                    1833,
                    1834,
                    1804,
                    1805,
                    1836,
                    1807
                ],
                "C.cross_entropy_with_softmax": [
                    1816
                ],
                "C.one_hot": [
                    2093,
                    1837,
                    2183
                ],
                "categorical_crossentropy": [
                    1840
                ],
                "object": [
                    1843
                ],
                "self.placeholders": [
                    1932,
                    1846
                ],
                "self": [
                    2435,
                    2436,
                    2440,
                    2441,
                    2443,
                    1932,
                    2448,
                    2449,
                    2453,
                    1944,
                    1953,
                    1955,
                    2468,
                    2474,
                    2475,
                    1964,
                    1965,
                    1969,
                    1972,
                    1846,
                    1847,
                    1848,
                    1849,
                    1974,
                    1852,
                    1986,
                    2498,
                    1988,
                    2499,
                    1990,
                    1991,
                    2504,
                    2505,
                    1994,
                    1996,
                    2002,
                    2005,
                    2007,
                    2521,
                    2522,
                    2524,
                    2015,
                    2529,
                    2530,
                    2531,
                    2534,
                    2535,
                    1896,
                    1898,
                    1903,
                    1905,
                    1906,
                    1907,
                    1911,
                    1912,
                    1914,
                    2427,
                    2428,
                    2429
                ],
                "self.trainer": [
                    1953,
                    1896,
                    1964,
                    1905,
                    1847
                ],
                "self.unrelated_updates": [
                    1986,
                    1903,
                    2005,
                    2007,
                    1848,
                    2015
                ],
                "self.updates": [
                    1849
                ],
                "updates": [
                    1856,
                    1849,
                    1850,
                    2020
                ],
                "self.loss": [
                    1955,
                    1852
                ],
                "u_ops": [
                    1866,
                    1899,
                    1900,
                    1870,
                    1854
                ],
                "unrelated_updates": [
                    1900,
                    1868,
                    1902,
                    1903,
                    1855
                ],
                "update": [
                    1856,
                    1857,
                    1858,
                    1861,
                    1863
                ],
                "u": [
                    1861,
                    1863,
                    1865,
                    1866,
                    1868,
                    1870
                ],
                "u.arguments": [
                    1865
                ],
                "u_ops.append": [
                    1866
                ],
                "unrelated_updates.append": [
                    1868
                ],
                "update_func": [
                    1872,
                    1889,
                    1870
                ],
                "C.combine": [
                    1912,
                    1907,
                    1870,
                    1903
                ],
                "u.output": [
                    1870
                ],
                "update_func.find_all_with_name": [
                    1872
                ],
                "u_list": [
                    1888,
                    1889,
                    1874,
                    1879
                ],
                "p_list": [
                    1889,
                    1875,
                    1878
                ],
                "p_list.append": [
                    1878
                ],
                "u_list.append": [
                    1879
                ],
                "learner": [
                    1889,
                    1897
                ],
                "C.cntk_py.universal_learner": [
                    1889
                ],
                "criterion": [
                    1897,
                    1898,
                    1891
                ],
                "C.trainer.Trainer": [
                    1896
                ],
                "C.trainer": [
                    1896
                ],
                "self.trainer_output": [
                    1969,
                    1898,
                    1965
                ],
                "f.output": [
                    1898,
                    1906,
                    1911
                ],
                "f": [
                    1898,
                    1906,
                    1911
                ],
                "unrelated_updates.extend": [
                    1900
                ],
                "_.output": [
                    1903
                ],
                "self.metrics_outputs": [
                    1996,
                    1906,
                    1907,
                    2002,
                    1911,
                    1912
                ],
                "self.metrics_func": [
                    1988,
                    1990,
                    1991,
                    1994,
                    1907,
                    1972,
                    1974,
                    1912,
                    1914
                ],
                "input": [
                    1920,
                    2498,
                    2499,
                    2468,
                    2427,
                    2428,
                    1918
                ],
                "placeholder": [
                    1921,
                    1918,
                    1919
                ],
                "get_num_dynamic_axis": [
                    1919
                ],
                "input_shape": [
                    1920,
                    1922
                ],
                "input.shape": [
                    1920,
                    2499,
                    2428
                ],
                "placeholder_shape": [
                    1921,
                    1922
                ],
                "placeholder.shape": [
                    1921
                ],
                "staticmethod": [
                    1916
                ],
                "feed_dict": [
                    1956,
                    1957,
                    1931,
                    2008,
                    1975,
                    1976,
                    2009,
                    1950
                ],
                "self._is_input_shape_compatible": [
                    1944
                ],
                "tensor.shape": [
                    1949
                ],
                "updated": [
                    1952,
                    2016,
                    1999,
                    1970,
                    2003
                ],
                "input_dict": [
                    1954,
                    1957,
                    1989,
                    1994,
                    1965,
                    1973,
                    2006,
                    1976,
                    2009,
                    2015
                ],
                "argument": [
                    1955,
                    1956,
                    1957,
                    2534,
                    2535,
                    2537,
                    1962,
                    2007,
                    2008,
                    1974,
                    1975,
                    1976,
                    2009,
                    1980,
                    2014
                ],
                "self.loss.arguments": [
                    1955
                ],
                "argument.name": [
                    1962,
                    1980,
                    2014
                ],
                "self.trainer.train_minibatch": [
                    1964
                ],
                "updated.append": [
                    1970,
                    2003,
                    1999
                ],
                "self.metrics_func.arguments": [
                    1974
                ],
                "output_values": [
                    1988,
                    1994,
                    1995,
                    1997,
                    2001
                ],
                "self.metrics_func.forward": [
                    1988
                ],
                "self.metrics_func.outputs": [
                    1990,
                    1991
                ],
                "self.metrics_func.eval": [
                    1994
                ],
                "dict": [
                    1995
                ],
                "value.asarray": [
                    1998
                ],
                "output_values.asarray": [
                    2001
                ],
                "self.unrelated_updates.arguments": [
                    2007
                ],
                "self.unrelated_updates.eval": [
                    2015
                ],
                "Function": [
                    2020
                ],
                "kwargs": [
                    2020
                ],
                "pad": [
                    2089,
                    2027,
                    2077
                ],
                "base_shape": [
                    2031,
                    2032,
                    2036,
                    2038,
                    2042,
                    2044
                ],
                "prefix_shape": [
                    2040,
                    2041,
                    2038,
                    2039
                ],
                "postfix_shape": [
                    2044,
                    2045,
                    2046,
                    2047
                ],
                "pad_info": [
                    2053,
                    2062
                ],
                "C.pad": [
                    2060
                ],
                "_padding": [
                    2063
                ],
                "xs": [
                    2362,
                    2353,
                    2324,
                    2328,
                    2330,
                    2365,
                    2109,
                    2367
                ],
                "result.append": [
                    2112,
                    2114
                ],
                "float": [
                    2417,
                    2121,
                    2422
                ],
                "np.full": [
                    2122
                ],
                "LambdaFunc": [
                    2130,
                    2524
                ],
                "print": [
                    2132,
                    2519
                ],
                "message": [
                    2132
                ],
                "t": [
                    2136,
                    2137,
                    2138
                ],
                "tuples": [
                    2136
                ],
                "np.ndarray": [
                    2139
                ],
                "map": [
                    2149
                ],
                "ndim_cond": [
                    2155,
                    2157,
                    2161,
                    2163,
                    2165,
                    2168
                ],
                "condition": [
                    2168,
                    2169,
                    2155,
                    2167
                ],
                "ndim_expr": [
                    2156,
                    2157,
                    2162,
                    2163,
                    2165
                ],
                "then_expression": [
                    2164,
                    2170,
                    2156
                ],
                "shape_expr": [
                    2168,
                    2164
                ],
                "ndim_diff": [
                    2165,
                    2166
                ],
                "tile": [
                    2168
                ],
                "else_expression": [
                    2171
                ],
                "res": [
                    2177,
                    2179,
                    2175
                ],
                "C.elu": [
                    2175
                ],
                "_targets": [
                    2184,
                    2183
                ],
                "targets": [
                    2183
                ],
                "predictions.shape": [
                    2183
                ],
                "predictions": [
                    2184,
                    2183
                ],
                "C.classification_error": [
                    2184
                ],
                "k": [
                    2184
                ],
                "x.name": [
                    2218
                ],
                "C.alias": [
                    2219
                ],
                "dim_ordering": [
                    2274
                ],
                "C.Axis.default_dynamic_axis": [
                    2288
                ],
                "C.Axis.default_batch_axis": [
                    2432,
                    2301,
                    2471
                ],
                "reduce_fun_name": [
                    2305,
                    2307,
                    2302,
                    2303
                ],
                "tmp_shape": [
                    2312,
                    2313,
                    2314
                ],
                "time_step": [
                    2313
                ],
                "stride": [
                    2320,
                    2326,
                    2327
                ],
                "kernel_shape": [
                    2352,
                    2321,
                    2322,
                    2351
                ],
                "output_length": [
                    2322,
                    2325
                ],
                "feature_dim": [
                    2352,
                    2322,
                    2329,
                    2363,
                    2366
                ],
                "filters": [
                    2352,
                    2322,
                    2376
                ],
                "slice_length": [
                    2328,
                    2326
                ],
                "kernel_size": [
                    2360,
                    2358,
                    2327
                ],
                "xs.append": [
                    2328,
                    2362,
                    2365
                ],
                "x_aggregate": [
                    2330,
                    2371,
                    2334,
                    2367
                ],
                "concatenate": [
                    2330,
                    2367
                ],
                "weight": [
                    2369,
                    2371,
                    2332,
                    2334
                ],
                "permute_dimensions": [
                    2369,
                    2338,
                    2332,
                    2380
                ],
                "stride_row": [
                    2357,
                    2349,
                    2358
                ],
                "stride_col": [
                    2360,
                    2349,
                    2359
                ],
                "output_row": [
                    2376,
                    2355,
                    2350
                ],
                "output_col": [
                    2376,
                    2356,
                    2350
                ],
                "j": [
                    2360,
                    2356,
                    2359
                ],
                "slice_row": [
                    2362,
                    2365,
                    2357
                ],
                "slice_col": [
                    2362,
                    2365,
                    2359
                ],
                "cntk_axes": [
                    2388,
                    2389,
                    2390,
                    2391,
                    2392
                ],
                "begin_index": [
                    2392,
                    2389
                ],
                "end_index": [
                    2392,
                    2390
                ],
                "C.slice": [
                    2392
                ],
                "const_a": [
                    2402,
                    2403,
                    2404
                ],
                "ReshapeBatch": [
                    2427,
                    2406
                ],
                "version": [
                    2410,
                    2411,
                    2412,
                    2414,
                    2415,
                    2417
                ],
                "C.__version__": [
                    2410
                ],
                "version.endswith": [
                    2411
                ],
                "replace": [
                    2415
                ],
                "C.ops.functions.UserFunction": [
                    2456,
                    2425,
                    2515,
                    2485
                ],
                "__init__": [
                    2524,
                    2498,
                    2427,
                    2468
                ],
                "super": [
                    2524,
                    2498,
                    2427,
                    2468
                ],
                "self.from_shape": [
                    2440,
                    2449,
                    2428,
                    2453
                ],
                "self.target_shape": [
                    2435,
                    2499,
                    2504,
                    2441,
                    2443,
                    2448,
                    2429
                ],
                "batch_axis": [
                    2432,
                    2476,
                    2437,
                    2471
                ],
                "C.output_variable": [
                    2528,
                    2473,
                    2434,
                    2503
                ],
                "self.inputs": [
                    2529,
                    2530,
                    2531,
                    2436,
                    2505,
                    2474,
                    2475
                ],
                "num_element": [
                    2440,
                    2442,
                    2448,
                    2450
                ],
                "arguments.shape": [
                    2440
                ],
                "arguments": [
                    2440,
                    2443,
                    2509,
                    2479
                ],
                "num_static_element": [
                    2441,
                    2442,
                    2449,
                    2450
                ],
                "num_batch": [
                    2442,
                    2443
                ],
                "as_shape": [
                    2443
                ],
                "arguments.data": [
                    2443,
                    2509,
                    2479
                ],
                "C.cntk_py.Value": [
                    2444,
                    2509,
                    2479,
                    2512,
                    2482,
                    2451
                ],
                "grad_array_view": [
                    2452,
                    2447
                ],
                "root_gradients.data": [
                    2512,
                    2482,
                    2447
                ],
                "root_gradients": [
                    2540,
                    2447,
                    2448,
                    2512,
                    2482
                ],
                "root_gradients.shape": [
                    2448
                ],
                "num_old_batch": [
                    2450,
                    2453
                ],
                "grad_array_view.as_shape": [
                    2452
                ],
                "batch_size": [
                    2499
                ],
                "arg": [
                    2524,
                    2519
                ],
                "self.when": [
                    2521,
                    2534
                ],
                "when": [
                    2521
                ],
                "self.execute": [
                    2522,
                    2535
                ],
                "execute": [
                    2522
                ],
                "dynamic_axes": [
                    2531
                ]
            },
            "filtered_variables_in_file": {
                "C.set_global_option": [
                    17
                ],
                "C": [
                    1024,
                    1028,
                    1029,
                    1030,
                    1031,
                    2052,
                    523,
                    2060,
                    17,
                    23,
                    1559,
                    1049,
                    1051,
                    1052,
                    1565,
                    1054,
                    543,
                    544,
                    33,
                    546,
                    1056,
                    1058,
                    1060,
                    1061,
                    1569,
                    1578,
                    2093,
                    1582,
                    2099,
                    2101,
                    1595,
                    2047,
                    2110,
                    2111,
                    1089,
                    1602,
                    579,
                    1606,
                    583,
                    2119,
                    585,
                    1098,
                    2120,
                    1615,
                    593,
                    1106,
                    1107,
                    1108,
                    1618,
                    2129,
                    601,
                    1114,
                    1115,
                    604,
                    93,
                    605,
                    95,
                    1630,
                    2141,
                    1635,
                    613,
                    2149,
                    2151,
                    106,
                    620,
                    1645,
                    1135,
                    1136,
                    2169,
                    2175,
                    1665,
                    2179,
                    1156,
                    2183,
                    2184,
                    2185,
                    663,
                    1690,
                    2203,
                    668,
                    162,
                    164,
                    678,
                    679,
                    169,
                    2219,
                    1197,
                    1201,
                    1713,
                    1715,
                    180,
                    2228,
                    1720,
                    1722,
                    1211,
                    2237,
                    194,
                    706,
                    707,
                    1220,
                    1222,
                    1226,
                    1740,
                    2253,
                    1742,
                    1747,
                    1749,
                    2264,
                    729,
                    2269,
                    1761,
                    1762,
                    2275,
                    1764,
                    1774,
                    2288,
                    241,
                    243,
                    1267,
                    1781,
                    1787,
                    2300,
                    1277,
                    2301,
                    1791,
                    2302,
                    2303,
                    2305,
                    1283,
                    1795,
                    2307,
                    1288,
                    266,
                    1297,
                    786,
                    1813,
                    1814,
                    1816,
                    281,
                    1818,
                    1307,
                    796,
                    1821,
                    799,
                    1823,
                    1313,
                    1824,
                    1318,
                    808,
                    809,
                    1837,
                    1839,
                    1332,
                    821,
                    313,
                    314,
                    315,
                    316,
                    826,
                    833,
                    1861,
                    838,
                    844,
                    845,
                    1870,
                    852,
                    2392,
                    858,
                    1375,
                    1376,
                    865,
                    1378,
                    1889,
                    2401,
                    2402,
                    870,
                    2403,
                    1896,
                    2404,
                    874,
                    2406,
                    364,
                    2410,
                    878,
                    1903,
                    1393,
                    882,
                    1907,
                    886,
                    1912,
                    1401,
                    890,
                    2425,
                    894,
                    1407,
                    2432,
                    898,
                    387,
                    1923,
                    2434,
                    902,
                    1418,
                    2444,
                    1422,
                    912,
                    401,
                    1425,
                    2451,
                    1428,
                    917,
                    918,
                    919,
                    2456,
                    1437,
                    928,
                    932,
                    421,
                    1446,
                    423,
                    1447,
                    937,
                    1450,
                    1451,
                    2471,
                    2473,
                    2479,
                    1457,
                    1458,
                    2482,
                    1460,
                    2485,
                    950,
                    957,
                    961,
                    451,
                    1475,
                    453,
                    965,
                    2503,
                    969,
                    1481,
                    973,
                    2509,
                    2512,
                    977,
                    2515,
                    981,
                    1496,
                    473,
                    985,
                    1497,
                    989,
                    2528,
                    993,
                    1505,
                    484,
                    485,
                    1513,
                    2041,
                    1530,
                    1023
                ],
                "b_any": [
                    2032,
                    1114,
                    19,
                    558
                ],
                "py_slice": [
                    20,
                    2357,
                    2326,
                    2359
                ],
                "dev": [
                    1525,
                    23,
                    24,
                    1660,
                    1501
                ],
                "C.device.use_default_device": [
                    23
                ],
                "C.device": [
                    23
                ],
                "dev.type": [
                    24,
                    1525,
                    1660,
                    1501
                ],
                "warnings.warn": [
                    25,
                    2419,
                    1116
                ],
                "warnings": [
                    25,
                    2419,
                    1116
                ],
                "_LEARNING_PHASE_PLACEHOLDER": [
                    33,
                    1987,
                    79,
                    1939,
                    1940,
                    61
                ],
                "C.constant": [
                    33,
                    364,
                    950,
                    2041,
                    2047
                ],
                "np.float32": [
                    480,
                    33,
                    129,
                    1935,
                    1937,
                    117,
                    377,
                    444,
                    125,
                    413
                ],
                "np": [
                    129,
                    131,
                    133,
                    2440,
                    2441,
                    909,
                    911,
                    400,
                    528,
                    1935,
                    1936,
                    1937,
                    1940,
                    2448,
                    2449,
                    410,
                    413,
                    33,
                    442,
                    444,
                    2122,
                    79,
                    472,
                    2139,
                    2140,
                    478,
                    375,
                    480,
                    121,
                    363,
                    497,
                    1780,
                    117,
                    119,
                    504,
                    377,
                    125,
                    510
                ],
                "_LEARNING_PHASE": [
                    1987,
                    35,
                    70,
                    78,
                    61
                ],
                "_UID_PREFIXES": [
                    56,
                    36,
                    55
                ],
                "defaultdict": [
                    36
                ],
                "grad_parameter_dict": [
                    952,
                    41,
                    1877,
                    1878
                ],
                "NAME_SCOPE_STACK": [
                    352,
                    49,
                    43,
                    51
                ],
                "NAME_SCOPE_STACK.append": [
                    49
                ],
                "name": [
                    276,
                    277,
                    285,
                    157,
                    158,
                    417,
                    418,
                    2468,
                    2217,
                    2218,
                    427,
                    2219,
                    49,
                    183,
                    448,
                    449,
                    2498,
                    457,
                    2524,
                    353,
                    355,
                    366,
                    497,
                    504,
                    2427,
                    510
                ],
                "NAME_SCOPE_STACK.pop": [
                    51
                ],
                "contextmanager": [
                    46
                ],
                "prefix": [
                    352,
                    354,
                    355,
                    55,
                    56
                ],
                "value": [
                    1932,
                    1934,
                    1935,
                    1936,
                    1937,
                    1940,
                    1944,
                    1949,
                    1950,
                    161,
                    163,
                    932,
                    165,
                    169,
                    170,
                    172,
                    173,
                    174,
                    181,
                    66,
                    69,
                    70,
                    2121,
                    2122,
                    2123,
                    1997,
                    1998,
                    2138,
                    2139,
                    2140,
                    2142,
                    363
                ],
                "_LEARNING_PHASE_PLACEHOLDER.value": [
                    1987,
                    1940,
                    79
                ],
                "np.asarray": [
                    2440,
                    2441,
                    79,
                    2448,
                    2449,
                    1940,
                    2140
                ],
                "training": [
                    98,
                    103,
                    104,
                    106,
                    112,
                    84,
                    85
                ],
                "learning_phase": [
                    85
                ],
                "uses_learning_phase": [
                    1304,
                    1336,
                    99,
                    1348,
                    1464,
                    107,
                    1434,
                    1236,
                    86,
                    88,
                    1274
                ],
                "x": [
                    1536,
                    2048,
                    514,
                    518,
                    1033,
                    522,
                    2060,
                    1551,
                    528,
                    2063,
                    2066,
                    533,
                    1046,
                    1531,
                    1557,
                    537,
                    2075,
                    1052,
                    1565,
                    2041,
                    2076,
                    544,
                    1056,
                    546,
                    1568,
                    1569,
                    2042,
                    550,
                    2087,
                    2088,
                    2089,
                    1578,
                    1581,
                    1582,
                    1585,
                    1074,
                    1586,
                    1076,
                    565,
                    2098,
                    2100,
                    568,
                    1081,
                    1593,
                    2102,
                    2104,
                    2109,
                    574,
                    1086,
                    2110,
                    1089,
                    578,
                    579,
                    1602,
                    1605,
                    1606,
                    2111,
                    2112,
                    585,
                    2114,
                    2119,
                    2120,
                    2122,
                    1102,
                    1615,
                    2123,
                    593,
                    1618,
                    1107,
                    1108,
                    1621,
                    1110,
                    2130,
                    2137,
                    1114,
                    1115,
                    1628,
                    93,
                    94,
                    2141,
                    1120,
                    1121,
                    2142,
                    99,
                    100,
                    1635,
                    1638,
                    615,
                    104,
                    106,
                    1645,
                    112,
                    624,
                    625,
                    626,
                    1136,
                    1140,
                    630,
                    631,
                    1141,
                    1648,
                    1649,
                    1656,
                    637,
                    638,
                    2175,
                    1665,
                    1155,
                    644,
                    645,
                    1156,
                    1667,
                    1672,
                    2179,
                    1162,
                    651,
                    652,
                    1166,
                    1679,
                    2192,
                    658,
                    662,
                    663,
                    1177,
                    1690,
                    2203,
                    668,
                    1692,
                    1182,
                    2205,
                    672,
                    673,
                    1700,
                    2213,
                    679,
                    1191,
                    1194,
                    2218,
                    2219,
                    1197,
                    1711,
                    1713,
                    1714,
                    691,
                    2228,
                    2229,
                    1720,
                    1721,
                    699,
                    1212,
                    701,
                    1213,
                    1728,
                    193,
                    194,
                    707,
                    1217,
                    1220,
                    1221,
                    1737,
                    1226,
                    716,
                    717,
                    1740,
                    1741,
                    2253,
                    2254,
                    2077,
                    1747,
                    1748,
                    728,
                    729,
                    2264,
                    2265,
                    732,
                    1756,
                    736,
                    1761,
                    1762,
                    739,
                    1764,
                    2275,
                    1766,
                    1767,
                    2276,
                    2280,
                    2281,
                    237,
                    1774,
                    2287,
                    2288,
                    241,
                    242,
                    243,
                    244,
                    753,
                    1780,
                    1781,
                    1782,
                    761,
                    250,
                    1783,
                    1787,
                    2294,
                    1791,
                    2303,
                    2305,
                    1795,
                    2307,
                    2308,
                    2312,
                    2314,
                    792,
                    281,
                    797,
                    286,
                    287,
                    288,
                    289,
                    809,
                    301,
                    813,
                    814,
                    305,
                    307,
                    820,
                    309,
                    313,
                    825,
                    320,
                    321,
                    832,
                    323,
                    325,
                    837,
                    336,
                    337,
                    339,
                    340,
                    341,
                    2388,
                    2392,
                    857,
                    858,
                    347,
                    864,
                    865,
                    2402,
                    870,
                    2406,
                    874,
                    878,
                    882,
                    886,
                    890,
                    894,
                    898,
                    902,
                    912,
                    1425,
                    1430,
                    924,
                    928,
                    936,
                    937,
                    957,
                    1469,
                    961,
                    1474,
                    1475,
                    1476,
                    965,
                    1480,
                    969,
                    1481,
                    1482,
                    973,
                    977,
                    1492,
                    981,
                    1496,
                    985,
                    989,
                    993,
                    1505,
                    1507,
                    1000,
                    1513,
                    490,
                    1514,
                    2025,
                    1005,
                    2026,
                    2027,
                    2031,
                    1009,
                    1521,
                    1011,
                    1013,
                    1017,
                    1530,
                    1019,
                    2047
                ],
                "C.cntk_py.Function": [
                    169,
                    241,
                    93,
                    95
                ],
                "C.cntk_py": [
                    1889,
                    169,
                    2444,
                    2509,
                    2479,
                    2512,
                    241,
                    2482,
                    2451,
                    93,
                    95
                ],
                "alt": [
                    96,
                    104,
                    106,
                    112,
                    95
                ],
                "x._uses_learning_phase": [
                    99,
                    287
                ],
                "result": [
                    2184,
                    2185,
                    2443,
                    2444,
                    1816,
                    1818,
                    797,
                    799,
                    803,
                    679,
                    936,
                    681,
                    682,
                    937,
                    1964,
                    1967,
                    1968,
                    565,
                    566,
                    2108,
                    2112,
                    2114,
                    2115,
                    585,
                    588,
                    589,
                    104,
                    106,
                    107,
                    108
                ],
                "C.element_select": [
                    833,
                    2179,
                    106,
                    821,
                    2169,
                    1437
                ],
                "result._uses_learning_phase": [
                    107
                ],
                "in_train_phase": [
                    112
                ],
                "dtype": [
                    129,
                    131,
                    260,
                    133,
                    261,
                    387,
                    2436,
                    138,
                    401,
                    406,
                    407,
                    154,
                    283,
                    155,
                    412,
                    413,
                    415,
                    426,
                    2475,
                    173,
                    174,
                    178,
                    182,
                    438,
                    439,
                    443,
                    444,
                    446,
                    456,
                    2505,
                    462,
                    463,
                    473,
                    479,
                    480,
                    482,
                    503,
                    376,
                    2530,
                    486,
                    359,
                    360,
                    504,
                    365,
                    494,
                    495,
                    496,
                    497,
                    116,
                    501,
                    118,
                    502,
                    120,
                    377,
                    379,
                    508,
                    509,
                    510
                ],
                "np.float64": [
                    1936,
                    131,
                    119
                ],
                "np.float16": [
                    121,
                    133
                ],
                "floatx": [
                    407,
                    261,
                    360,
                    2122,
                    463,
                    495,
                    502,
                    439,
                    155,
                    509
                ],
                "C.variables.Constant": [
                    162,
                    2120,
                    243,
                    2101,
                    313,
                    2111
                ],
                "C.variables": [
                    162,
                    164,
                    2119,
                    2120,
                    243,
                    1107,
                    2099,
                    2101,
                    313,
                    314,
                    315,
                    2141,
                    2110,
                    2111
                ],
                "C.variables.Parameter": [
                    164,
                    2119,
                    2099,
                    1107,
                    243,
                    315,
                    2141,
                    2110
                ],
                "value.value": [
                    165
                ],
                "shape": [
                    1074,
                    1076,
                    1081,
                    1086,
                    1106,
                    1108,
                    1112,
                    1121,
                    1124,
                    1126,
                    1127,
                    1131,
                    1133,
                    672,
                    674,
                    675,
                    676,
                    681,
                    1194,
                    172,
                    173,
                    1196,
                    691,
                    180,
                    696,
                    703,
                    705,
                    716,
                    205,
                    207,
                    719,
                    720,
                    722,
                    210,
                    212,
                    1233,
                    1234,
                    727,
                    216,
                    218,
                    1242,
                    221,
                    223,
                    736,
                    737,
                    1247,
                    227,
                    229,
                    232,
                    744,
                    234,
                    236,
                    237,
                    752,
                    1265,
                    262,
                    264,
                    267,
                    1296,
                    792,
                    794,
                    796,
                    286,
                    806,
                    808,
                    809,
                    320,
                    1344,
                    1345,
                    324,
                    327,
                    328,
                    339,
                    342,
                    343,
                    1368,
                    347,
                    348,
                    2403,
                    2406,
                    361,
                    362,
                    363,
                    381,
                    2429,
                    387,
                    391,
                    401,
                    422,
                    2474,
                    452,
                    464,
                    473,
                    2529,
                    485,
                    497,
                    504
                ],
                "value.shape": [
                    172,
                    1949
                ],
                "value.dtype": [
                    1936,
                    173,
                    1935
                ],
                "value.astype": [
                    1937,
                    174
                ],
                "v": [
                    1998,
                    1999,
                    2001,
                    2003,
                    180,
                    949,
                    950,
                    952,
                    184,
                    185,
                    186,
                    187
                ],
                "C.parameter": [
                    484,
                    451,
                    180,
                    421
                ],
                "_prepare_name": [
                    366,
                    183
                ],
                "v._keras_shape": [
                    184
                ],
                "v.shape": [
                    184,
                    950
                ],
                "v._uses_learning_phase": [
                    185
                ],
                "v.constraint": [
                    186
                ],
                "constraint": [
                    186
                ],
                "data_format": [
                    1536,
                    1541,
                    2054,
                    1672,
                    1161,
                    1547,
                    1165,
                    1677,
                    1679,
                    1680,
                    2065,
                    1170,
                    2190,
                    2192,
                    1557,
                    1558,
                    1686,
                    1176,
                    1561,
                    2073,
                    2193,
                    2199,
                    1181,
                    2077,
                    1187,
                    1700,
                    2085,
                    2213,
                    2089,
                    1706,
                    2347,
                    1711,
                    2223,
                    1585,
                    1591,
                    1593,
                    1594,
                    2361,
                    1597,
                    191,
                    1728,
                    1733,
                    1737,
                    2378,
                    203,
                    2252,
                    2318,
                    1487,
                    208,
                    2258,
                    1621,
                    214,
                    1495,
                    1626,
                    219,
                    1628,
                    1629,
                    1756,
                    225,
                    230,
                    1512,
                    1519,
                    1521,
                    1522,
                    1649,
                    1654,
                    1656,
                    1657
                ],
                "normalize_data_format": [
                    1541,
                    1733,
                    2085,
                    1706,
                    2347,
                    1677,
                    2190,
                    1487,
                    1519,
                    2318,
                    1654,
                    1591,
                    2073,
                    1626,
                    191
                ],
                "dims": [
                    224,
                    193,
                    194,
                    195,
                    1345,
                    1253,
                    198,
                    1350,
                    200,
                    1352,
                    202,
                    1234,
                    1140,
                    213,
                    1238,
                    1143,
                    1145
                ],
                "x.shape": [
                    522,
                    792,
                    537,
                    2076,
                    1568,
                    2088,
                    1194,
                    1581,
                    193,
                    194,
                    323,
                    578,
                    325,
                    1217,
                    1605,
                    2122,
                    339,
                    1114,
                    1115,
                    1638,
                    2026,
                    2031,
                    1648,
                    1780,
                    2042
                ],
                "C.InferredDimension": [
                    706,
                    194,
                    1923,
                    678,
                    808,
                    266,
                    523,
                    1135,
                    1106,
                    1114,
                    1211,
                    796
                ],
                "bias_dims": [
                    226,
                    197,
                    198,
                    231,
                    200,
                    204,
                    209,
                    215,
                    220
                ],
                "bias.shape": [
                    227,
                    197,
                    229,
                    232,
                    234,
                    236,
                    205,
                    207,
                    210,
                    212,
                    216,
                    218,
                    221,
                    223
                ],
                "bias": [
                    227,
                    197,
                    229,
                    232,
                    234,
                    236,
                    205,
                    237,
                    207,
                    210,
                    212,
                    216,
                    218,
                    221,
                    223
                ],
                "reshape": [
                    2375,
                    2314,
                    237,
                    1102,
                    2328,
                    2362,
                    2365
                ],
                "x.eval": [
                    242
                ],
                "x.value": [
                    2112,
                    2123,
                    244,
                    2102,
                    2142
                ],
                "ndim": [
                    737,
                    741,
                    263,
                    264,
                    2155,
                    2156,
                    781,
                    751,
                    1074,
                    1011,
                    1076,
                    1086,
                    1081,
                    1019,
                    1022
                ],
                "_": [
                    391,
                    264,
                    392,
                    522,
                    523,
                    1425,
                    794,
                    678,
                    808,
                    1198,
                    2352,
                    694,
                    695,
                    696,
                    702,
                    703,
                    706,
                    1988,
                    464,
                    465,
                    720,
                    1106,
                    2389,
                    2390,
                    2391,
                    1114,
                    1115,
                    1131,
                    1135,
                    1903,
                    381,
                    382
                ],
                "dynamic_dimension": [
                    266,
                    267
                ],
                "_get_cntk_version": [
                    600,
                    2401,
                    266
                ],
                "C.FreeDimension": [
                    706,
                    1923,
                    808,
                    266,
                    523,
                    1451,
                    1211,
                    1106,
                    1115,
                    796
                ],
                "cntk_shape": [
                    267,
                    268,
                    270,
                    274,
                    279,
                    282
                ],
                "s": [
                    1376,
                    1378,
                    1316,
                    1380,
                    1286,
                    1319,
                    1447,
                    1289,
                    267,
                    1427,
                    1428,
                    1437,
                    1373,
                    1374
                ],
                "dynamic_axis_num": [
                    274,
                    270,
                    279
                ],
                "C.input": [
                    281
                ],
                "_convert_string_dtype": [
                    482,
                    283,
                    496,
                    503,
                    379,
                    446,
                    415
                ],
                "sparse": [
                    284
                ],
                "x._keras_shape": [
                    1782,
                    337,
                    286
                ],
                "x._cntk_placeholder": [
                    288,
                    301
                ],
                "is_tensor": [
                    305
                ],
                "C.variables.Variable": [
                    314
                ],
                "C.ops.functions.Function": [
                    316
                ],
                "C.ops.functions": [
                    2515,
                    2485,
                    2456,
                    2425,
                    316
                ],
                "C.ops": [
                    1283,
                    1288,
                    1297,
                    2456,
                    1307,
                    1313,
                    1318,
                    1197,
                    2485,
                    316,
                    844,
                    2515,
                    601,
                    858,
                    604,
                    865,
                    1267,
                    2425,
                    1277
                ],
                "int_shape": [
                    1416,
                    2312,
                    528,
                    2321,
                    538,
                    924,
                    672,
                    550,
                    551,
                    2351,
                    691,
                    320,
                    1344,
                    716,
                    1233,
                    347,
                    736,
                    1252,
                    1140,
                    2164,
                    1017
                ],
                "num_dynamic": [
                    1920,
                    321,
                    324,
                    327,
                    328,
                    1919
                ],
                "_get_dynamic_axis_num": [
                    2075,
                    673,
                    2087,
                    1456,
                    825,
                    1213,
                    701,
                    321,
                    837,
                    1354,
                    717,
                    1110,
                    1374,
                    739,
                    1256,
                    2025,
                    2287,
                    624,
                    1141,
                    2294,
                    1400,
                    1406
                ],
                "non_dyn_shape": [
                    328,
                    322,
                    325,
                    327
                ],
                "i": [
                    1145,
                    1922,
                    1923,
                    779,
                    781,
                    782,
                    783,
                    1293,
                    1295,
                    1296,
                    1297,
                    2325,
                    2326,
                    2327,
                    1277,
                    1307,
                    1195,
                    1196,
                    1197,
                    1323,
                    1325,
                    1200,
                    1329,
                    1331,
                    2355,
                    1333,
                    1334,
                    2357,
                    2358,
                    577,
                    578,
                    323,
                    324,
                    325,
                    579,
                    327,
                    580,
                    581,
                    582,
                    583,
                    584,
                    725,
                    726,
                    727,
                    1143,
                    729,
                    730,
                    1125,
                    1126,
                    1127,
                    1128,
                    1131,
                    751,
                    752,
                    1265,
                    1266,
                    1267,
                    756,
                    2166,
                    759,
                    2168,
                    761,
                    762,
                    765,
                    766,
                    767
                ],
                "non_dyn_shape.append": [
                    325,
                    327
                ],
                "tensor.is_sparse": [
                    332
                ],
                "tensor": [
                    1932,
                    332,
                    1939,
                    1944,
                    1949,
                    1950
                ],
                "dynamic_shape": [
                    341,
                    342
                ],
                "a": [
                    2305,
                    612,
                    613,
                    614,
                    902,
                    619,
                    620,
                    779,
                    558,
                    780,
                    781,
                    2062,
                    2064,
                    341,
                    2299,
                    2300,
                    2301,
                    2303
                ],
                "x.dynamic_axes": [
                    2281,
                    2288,
                    753,
                    1425,
                    341,
                    761,
                    1469
                ],
                "join": [
                    352
                ],
                "default": [
                    354
                ],
                "np_value": [
                    363,
                    364
                ],
                "np.ones": [
                    504,
                    363
                ],
                "const": [
                    368,
                    369,
                    364,
                    367
                ],
                "const._keras_shape": [
                    367
                ],
                "const.shape": [
                    367
                ],
                "const._uses_learning_phase": [
                    368
                ],
                "seed": [
                    387,
                    440,
                    455,
                    472,
                    425,
                    486,
                    398,
                    400,
                    401,
                    442,
                    373,
                    470,
                    375,
                    408,
                    473,
                    410,
                    477,
                    478
                ],
                "np.random.randint": [
                    410,
                    400,
                    375,
                    472,
                    442,
                    478
                ],
                "np.random": [
                    410,
                    400,
                    375,
                    472,
                    442,
                    478
                ],
                "C.random.bernoulli": [
                    387
                ],
                "C.random": [
                    401,
                    387,
                    473
                ],
                "p": [
                    1440,
                    1922,
                    387,
                    451,
                    421,
                    1923,
                    2053,
                    458,
                    428,
                    2062,
                    2063,
                    1427,
                    1428,
                    1439
                ],
                "C.random.uniform": [
                    401
                ],
                "minval": [
                    401
                ],
                "maxval": [
                    401
                ],
                "scale": [
                    424,
                    428,
                    420,
                    454
                ],
                "high": [
                    420
                ],
                "low": [
                    428,
                    420
                ],
                "C.initializer.uniform": [
                    423
                ],
                "C.initializer": [
                    485,
                    453,
                    423
                ],
                "variable": [
                    932,
                    458,
                    428,
                    497,
                    504,
                    510
                ],
                "p.value": [
                    458,
                    428
                ],
                "C.initializer.normal": [
                    453
                ],
                "mean": [
                    1089,
                    1028,
                    1061,
                    1064,
                    458,
                    1067,
                    1040,
                    1009,
                    1074,
                    1075,
                    1013,
                    662,
                    664,
                    473,
                    1085
                ],
                "C.random.normal": [
                    473
                ],
                "stddev": [
                    473,
                    486
                ],
                "C.initializer.truncated_normal": [
                    485
                ],
                "_convert_dtype_string": [
                    490
                ],
                "x.dtype": [
                    490
                ],
                "ctype": [
                    496,
                    497,
                    504,
                    503
                ],
                "np.zeros": [
                    497
                ],
                "np.eye": [
                    510
                ],
                "size": [
                    510
                ],
                "zeros_like": [
                    1280,
                    836,
                    518,
                    1005,
                    1007,
                    824,
                    1085,
                    1310
                ],
                "np.prod": [
                    2440,
                    2441,
                    528,
                    2448,
                    2449,
                    1780
                ],
                "y.shape": [
                    585,
                    537,
                    586
                ],
                "y": [
                    537,
                    538,
                    543,
                    544,
                    546,
                    551,
                    565,
                    568,
                    571,
                    957,
                    575,
                    961,
                    965,
                    583,
                    585,
                    586,
                    969,
                    973,
                    977,
                    981,
                    985
                ],
                "y_shape": [
                    544,
                    551,
                    587,
                    557,
                    563,
                    570,
                    538,
                    539,
                    540,
                    541,
                    542
                ],
                "permutation": [
                    1810,
                    1811,
                    1812,
                    1813,
                    1814,
                    540,
                    541,
                    542,
                    543
                ],
                "C.transpose": [
                    2275,
                    1156,
                    2269,
                    2253,
                    2228,
                    1813,
                    1814,
                    1559,
                    2264,
                    1595,
                    2237,
                    1630,
                    543
                ],
                "C.times": [
                    544,
                    585,
                    546,
                    605
                ],
                "x_shape": [
                    1026,
                    550,
                    557,
                    563,
                    1017
                ],
                "axes": [
                    553,
                    554,
                    555,
                    557,
                    558,
                    561,
                    2386,
                    2387,
                    564,
                    565,
                    566,
                    1044,
                    568,
                    2388,
                    574,
                    575
                ],
                "transpose": [
                    568,
                    566
                ],
                "expand_dims": [
                    1254,
                    1417,
                    1327,
                    1551,
                    625,
                    1552,
                    1331,
                    1553,
                    2167,
                    571
                ],
                "normalized_axis": [
                    577,
                    581,
                    573,
                    574,
                    575
                ],
                "normalized_axis.append": [
                    574,
                    575
                ],
                "_normalize_axis": [
                    864,
                    1474,
                    1155,
                    644,
                    1191,
                    1097,
                    651,
                    813,
                    1009,
                    2388,
                    630,
                    857,
                    637,
                    574,
                    575
                ],
                "C.swapaxes": [
                    579,
                    583,
                    1513,
                    593,
                    1496,
                    1497
                ],
                "squeeze": [
                    1064,
                    1065,
                    588,
                    1586,
                    1299,
                    1269,
                    1308,
                    1278
                ],
                "C.ops.gather": [
                    601
                ],
                "reference": [
                    601,
                    603,
                    605
                ],
                "indices": [
                    601,
                    604,
                    2093
                ],
                "num_classes": [
                    603,
                    604,
                    2093
                ],
                "reference.shape": [
                    603,
                    605
                ],
                "one_hot_matrix": [
                    604,
                    605
                ],
                "C.ops.one_hot": [
                    604
                ],
                "keepdims": [
                    640,
                    609,
                    832,
                    647,
                    654,
                    816,
                    658,
                    820,
                    664,
                    633,
                    668
                ],
                "axis": [
                    1024,
                    1026,
                    1048,
                    1049,
                    1053,
                    1054,
                    1057,
                    1058,
                    2047,
                    1096,
                    1097,
                    1098,
                    609,
                    612,
                    617,
                    619,
                    630,
                    631,
                    633,
                    637,
                    638,
                    640,
                    1153,
                    1154,
                    1155,
                    644,
                    645,
                    1156,
                    647,
                    651,
                    652,
                    654,
                    658,
                    662,
                    664,
                    668,
                    674,
                    1191,
                    1192,
                    1196,
                    1197,
                    686,
                    687,
                    688,
                    689,
                    1201,
                    694,
                    2298,
                    1787,
                    2299,
                    769,
                    770,
                    771,
                    772,
                    773,
                    774,
                    2307,
                    776,
                    1800,
                    794,
                    1829,
                    813,
                    814,
                    816,
                    1840,
                    820,
                    832,
                    856,
                    857,
                    858,
                    859,
                    863,
                    864,
                    865,
                    866,
                    1473,
                    1474,
                    1475,
                    2039,
                    2041,
                    1019,
                    1020,
                    2045,
                    1022,
                    1023
                ],
                "reduce_axes": [
                    611,
                    614,
                    615
                ],
                "C.Axis": [
                    2432,
                    613,
                    2471,
                    620,
                    2288,
                    786,
                    852,
                    2300,
                    2301
                ],
                "reduce_axes.append": [
                    614
                ],
                "_reshape_dummy_dim": [
                    866,
                    615,
                    1075,
                    1077,
                    1082,
                    859,
                    1087
                ],
                "has_seq": [
                    618,
                    621,
                    623
                ],
                "nones": [
                    673,
                    705,
                    739,
                    676,
                    741,
                    680,
                    744,
                    1256,
                    1262,
                    624,
                    625,
                    752,
                    758,
                    760,
                    701,
                    766
                ],
                "output": [
                    640,
                    1280,
                    1283,
                    645,
                    1273,
                    647,
                    1801,
                    1162,
                    1163,
                    652,
                    1164,
                    654,
                    1166,
                    1167,
                    1168,
                    1291,
                    1807,
                    917,
                    918,
                    919,
                    920,
                    1177,
                    1178,
                    1179,
                    1180,
                    1301,
                    1182,
                    1183,
                    1184,
                    1185,
                    1303,
                    1310,
                    1313,
                    1813,
                    1821,
                    1823,
                    1824,
                    1321,
                    1830,
                    2338,
                    1836,
                    1837,
                    814,
                    1839,
                    816,
                    1816,
                    1840,
                    2371,
                    2373,
                    2375,
                    2380,
                    2382,
                    847,
                    858,
                    859,
                    2334,
                    865,
                    866,
                    2336,
                    631,
                    633,
                    1271,
                    638
                ],
                "_reduce_on_axis": [
                    645,
                    652,
                    814,
                    631,
                    638
                ],
                "_remove_dims": [
                    640,
                    647,
                    654,
                    816,
                    633
                ],
                "log": [
                    658
                ],
                "exp": [
                    658
                ],
                "m": [
                    1436,
                    1437,
                    662,
                    663
                ],
                "devs_squared": [
                    664,
                    663
                ],
                "C.square": [
                    1056,
                    1475,
                    1060,
                    870,
                    663
                ],
                "C.sqrt": [
                    1089,
                    1475,
                    668,
                    878
                ],
                "var": [
                    1089,
                    1076,
                    1077,
                    1080,
                    668
                ],
                "index": [
                    801,
                    674,
                    675,
                    802,
                    805,
                    806,
                    1218,
                    680,
                    1222,
                    1214,
                    1213,
                    798
                ],
                "shape.insert": [
                    675
                ],
                "new_shape": [
                    705,
                    706,
                    707,
                    676,
                    677,
                    678,
                    679,
                    1217,
                    1218,
                    1219,
                    1220,
                    1133,
                    1134,
                    1135,
                    1136
                ],
                "C.reshape": [
                    1028,
                    1029,
                    1030,
                    1031,
                    2185,
                    1559,
                    1818,
                    799,
                    679,
                    809,
                    1839,
                    1595,
                    707,
                    1220,
                    1108,
                    1630,
                    2403,
                    1136,
                    1781
                ],
                "result._keras_shape": [
                    681
                ],
                "_axis": [
                    770,
                    772,
                    774,
                    776,
                    778,
                    779,
                    781,
                    782,
                    783,
                    785,
                    786,
                    788,
                    794,
                    798,
                    805,
                    693,
                    696,
                    698,
                    702
                ],
                "_axis.append": [
                    696
                ],
                "n": [
                    1221,
                    711,
                    712,
                    713,
                    714,
                    719,
                    720,
                    722,
                    726,
                    1211,
                    1437
                ],
                "num_dynamic_axis": [
                    1154,
                    2058,
                    2064,
                    2075,
                    2076,
                    2077,
                    2087,
                    2088,
                    2089,
                    717,
                    725,
                    1110,
                    727,
                    1112,
                    729,
                    1124,
                    2025,
                    2026,
                    1131,
                    2027,
                    1134,
                    1141,
                    1147
                ],
                "rep": [
                    728,
                    1198,
                    726
                ],
                "tmp": [
                    728,
                    729,
                    1197,
                    1199
                ],
                "C.splice": [
                    1222,
                    1098,
                    1201,
                    1332,
                    729,
                    2041,
                    2047
                ],
                "cntk_axis": [
                    749,
                    783,
                    753,
                    756,
                    761,
                    765,
                    766
                ],
                "dynamic_axis_index": [
                    750,
                    752,
                    753,
                    754,
                    756,
                    758,
                    760,
                    761,
                    763
                ],
                "cntk_axis.append": [
                    753,
                    756
                ],
                "C.Axis.all_axes": [
                    786,
                    852
                ],
                "shape.count": [
                    796
                ],
                "reduce_result": [
                    832,
                    834,
                    835,
                    836,
                    837,
                    820,
                    822,
                    823,
                    824,
                    825
                ],
                "any_matrix": [
                    826,
                    828,
                    821
                ],
                "ones_like": [
                    835,
                    1000,
                    1002,
                    823,
                    1080
                ],
                "reduce_result.shape": [
                    825,
                    837
                ],
                "C.reduce_sum": [
                    826,
                    1475,
                    1821,
                    838
                ],
                "prod": [
                    832
                ],
                "all_matrix": [
                    840,
                    833,
                    838
                ],
                "C.ops.reduce_mean": [
                    844
                ],
                "C.equal": [
                    845,
                    957
                ],
                "argmax": [
                    849,
                    846
                ],
                "target": [
                    1824,
                    1837,
                    1839,
                    1840,
                    850,
                    1814,
                    919,
                    1816
                ],
                "C.ops.argmax": [
                    858
                ],
                "C.ops.argmin": [
                    865
                ],
                "C.abs": [
                    874,
                    1795,
                    898
                ],
                "C.exp": [
                    882
                ],
                "C.log": [
                    1824,
                    886,
                    919
                ],
                "C.round": [
                    890
                ],
                "C.sigmoid": [
                    917,
                    894
                ],
                "C.pow": [
                    902
                ],
                "max_value": [
                    1763,
                    1764,
                    906,
                    907,
                    908,
                    909,
                    912
                ],
                "min_value": [
                    906,
                    907,
                    910,
                    911,
                    912
                ],
                "np.inf": [
                    909,
                    911
                ],
                "C.clip": [
                    1764,
                    1481,
                    912,
                    918,
                    1823
                ],
                "from_logits": [
                    1840,
                    916,
                    1815
                ],
                "epsilon": [
                    1089,
                    1038,
                    1013,
                    918,
                    1823
                ],
                "C.assign": [
                    928,
                    937,
                    932,
                    1861
                ],
                "new_x": [
                    928
                ],
                "momentum": [
                    932
                ],
                "increment": [
                    936
                ],
                "variables": [
                    2148,
                    2149,
                    2151,
                    946,
                    947,
                    949
                ],
                "grads": [
                    1872,
                    948,
                    1876,
                    951,
                    953
                ],
                "g": [
                    1879,
                    1876,
                    1877,
                    950,
                    951,
                    952,
                    1878,
                    1886
                ],
                "grads.append": [
                    951
                ],
                "C.not_equal": [
                    961
                ],
                "C.greater": [
                    2179,
                    965
                ],
                "C.greater_equal": [
                    969
                ],
                "C.less": [
                    973
                ],
                "C.less_equal": [
                    977
                ],
                "C.element_max": [
                    981
                ],
                "C.element_min": [
                    985
                ],
                "C.sin": [
                    989
                ],
                "C.cos": [
                    993
                ],
                "gamma": [
                    1089,
                    998,
                    1030,
                    1000,
                    1002,
                    1004,
                    1007,
                    1013,
                    1079,
                    1080,
                    1081,
                    1082,
                    1022,
                    1023
                ],
                "beta": [
                    1024,
                    1089,
                    999,
                    1031,
                    1002,
                    1003,
                    1005,
                    1007,
                    1013,
                    1084,
                    1085,
                    1086,
                    1087
                ],
                "variant": [
                    1040,
                    1009,
                    1029,
                    1013
                ],
                "_moments": [
                    1009
                ],
                "reduction_axes": [
                    1009,
                    1011,
                    1020
                ],
                "normalized": [
                    1032,
                    1040,
                    1012
                ],
                "batch_normalization": [
                    1032,
                    1012
                ],
                "target_shape": [
                    1026,
                    1028,
                    1029,
                    1030,
                    1031,
                    1016,
                    1021
                ],
                "target_shape.append": [
                    1026,
                    1021
                ],
                "C.reduce_mean": [
                    1024,
                    1058,
                    1049,
                    1054,
                    1023
                ],
                "broadcast_mean": [
                    1034,
                    1028
                ],
                "broadcast_var": [
                    1035,
                    1029
                ],
                "broadcast_gamma": [
                    1037,
                    1030
                ],
                "broadcast_beta": [
                    1036,
                    1031
                ],
                "_axes": [
                    1057,
                    1064,
                    1065,
                    1044,
                    1048,
                    1053
                ],
                "shift": [
                    1056,
                    1061,
                    1045,
                    1046,
                    1049,
                    1051,
                    1052
                ],
                "C.stop_gradient": [
                    1051,
                    2149,
                    2151
                ],
                "shifted_mean": [
                    1060,
                    1052,
                    1061,
                    1054
                ],
                "C.minus": [
                    1056,
                    1052,
                    1060
                ],
                "variance_mean": [
                    1056,
                    1058,
                    1060
                ],
                "variance": [
                    1065,
                    1067,
                    1060
                ],
                "C.plus": [
                    1061
                ],
                "keep_dims": [
                    1063
                ],
                "tensors": [
                    1097,
                    1098,
                    1093
                ],
                "_reshape_batch": [
                    1121
                ],
                "pattern": [
                    1153,
                    2053,
                    2043,
                    2055,
                    2057,
                    2059,
                    2060,
                    2037,
                    1142,
                    2039,
                    1147,
                    2045,
                    1151
                ],
                "current_layout": [
                    1145,
                    1147,
                    1143
                ],
                "interpolation": [
                    1160
                ],
                "repeat_elements": [
                    1184,
                    1162,
                    1163,
                    1166,
                    1167,
                    1177,
                    1178,
                    1179,
                    1182,
                    1183
                ],
                "height_factor": [
                    1183,
                    1162,
                    1166,
                    1178
                ],
                "width_factor": [
                    1184,
                    1163,
                    1179,
                    1167
                ],
                "depth_factor": [
                    1177,
                    1182
                ],
                "slices": [
                    1193,
                    1201,
                    1199
                ],
                "C.ops.slice": [
                    1197,
                    1297,
                    1267,
                    1307,
                    1277
                ],
                "slices.append": [
                    1199
                ],
                "new_shape.insert": [
                    1218
                ],
                "temp": [
                    1221,
                    1222
                ],
                "C.tanh": [
                    1226
                ],
                "inputs": [
                    1344,
                    2020,
                    1382,
                    1256,
                    1354,
                    1930,
                    1388,
                    1357,
                    1932,
                    2362,
                    1233,
                    1297,
                    1267,
                    1846,
                    2328,
                    1369,
                    1370,
                    2365
                ],
                "constants": [
                    1248,
                    1249,
                    1411,
                    1361,
                    1396,
                    1365,
                    1302,
                    1366,
                    1272
                ],
                "mask": [
                    1251,
                    1252,
                    1413,
                    1254,
                    1415,
                    1416,
                    1417,
                    1418,
                    1445,
                    1360,
                    1306,
                    1307,
                    1276,
                    1277
                ],
                "mask_shape": [
                    1252,
                    1253
                ],
                "states": [
                    1316,
                    1445,
                    1286,
                    1258,
                    1322,
                    1292,
                    1420,
                    1425,
                    1427,
                    1302,
                    1272,
                    1337
                ],
                "initial_states": [
                    1258,
                    1373,
                    1358,
                    1455
                ],
                "outputs": [
                    1282,
                    1291,
                    1309,
                    1312,
                    1321,
                    1327,
                    1328,
                    1329,
                    1968,
                    1331,
                    1970,
                    1333,
                    1851,
                    1852,
                    1892,
                    1893,
                    1894,
                    2020,
                    1897,
                    1260,
                    1906,
                    1910,
                    1911,
                    1279
                ],
                "time_axis": [
                    1262,
                    1297,
                    1267,
                    1332,
                    1269,
                    1307,
                    1277,
                    1278
                ],
                "go_backwards": [
                    1414,
                    1383,
                    1390,
                    1359,
                    1264
                ],
                "current": [
                    1297,
                    1299,
                    1267,
                    1269,
                    1302,
                    1272
                ],
                "new_states": [
                    1316,
                    1286,
                    1320,
                    1290,
                    1322,
                    1292,
                    1301,
                    1429,
                    1271,
                    1437,
                    1439
                ],
                "step_function": [
                    1356,
                    1301,
                    1429,
                    1271
                ],
                "mask_slice": [
                    1313,
                    1283,
                    1319,
                    1289,
                    1307,
                    1308,
                    1277,
                    1278
                ],
                "prev_output": [
                    1280,
                    1312,
                    1282,
                    1283,
                    1313,
                    1310
                ],
                "C.ops.element_select": [
                    1288,
                    1313,
                    1283,
                    1318
                ],
                "return_states": [
                    1315,
                    1317,
                    1285,
                    1287,
                    1320,
                    1290
                ],
                "n_s": [
                    1440,
                    1441,
                    1442,
                    1443,
                    1316,
                    1286,
                    1319,
                    1289,
                    1438
                ],
                "return_states.append": [
                    1317,
                    1287
                ],
                "outputs.append": [
                    1321,
                    1291
                ],
                "final_output": [
                    1445,
                    1446,
                    1450,
                    1465,
                    1452,
                    1327,
                    1332,
                    1337
                ],
                "last_output": [
                    1446,
                    1464,
                    1465,
                    1328,
                    1333,
                    1336,
                    1337
                ],
                "output_slice": [
                    1331,
                    1332
                ],
                "last_output._uses_learning_phase": [
                    1336,
                    1464
                ],
                "unroll": [
                    1354,
                    1362
                ],
                "_static_rnn": [
                    1355
                ],
                "input_length": [
                    1363
                ],
                "num_time_step": [
                    1451,
                    1452,
                    1368,
                    1369,
                    1370
                ],
                "has_seq_axis": [
                    1369,
                    1413,
                    1382
                ],
                "inputs.shape": [
                    1370
                ],
                "initial": [
                    1376,
                    1378,
                    1380,
                    1420,
                    1372
                ],
                "initial.append": [
                    1376,
                    1378,
                    1380
                ],
                "C.to_batch": [
                    1376,
                    2404
                ],
                "C.user_function": [
                    2129,
                    1378,
                    1460,
                    2406
                ],
                "ConvertToBatch": [
                    1378,
                    2468
                ],
                "need_convert": [
                    1449,
                    1389,
                    1382,
                    1383
                ],
                "rnn_inputs": [
                    1445,
                    1418,
                    1388,
                    1391,
                    1393,
                    1401,
                    1407
                ],
                "reverse": [
                    1415,
                    1391
                ],
                "C.to_sequence": [
                    1393
                ],
                "rnn_constants": [
                    1409,
                    1411,
                    1395,
                    1430,
                    1404,
                    1407
                ],
                "constant": [
                    1409,
                    1396,
                    1397,
                    1399,
                    1406,
                    1407
                ],
                "new_c": [
                    1401,
                    1403,
                    1404,
                    1398
                ],
                "c": [
                    1400,
                    1401,
                    1403,
                    1399
                ],
                "new_c.append": [
                    1401,
                    1403
                ],
                "C.sequence.broadcast_as": [
                    1401,
                    1407
                ],
                "C.sequence": [
                    2303,
                    1446,
                    1447,
                    1450,
                    1428,
                    1401,
                    2302,
                    1407
                ],
                "rnn_constants.append": [
                    1409,
                    1404,
                    1407
                ],
                "C.to_sequence_like": [
                    1418
                ],
                "C.default_options": [
                    1422
                ],
                "place_holders": [
                    1425,
                    1427,
                    1439
                ],
                "C.placeholder": [
                    1425
                ],
                "past_values": [
                    1426,
                    1428,
                    1437,
                    1430
                ],
                "past_values.append": [
                    1428
                ],
                "C.sequence.past_value": [
                    1428
                ],
                "new_output": [
                    1432,
                    1442,
                    1443,
                    1429
                ],
                "o": [
                    1440,
                    1996,
                    1997,
                    1969,
                    1970,
                    2002,
                    1439
                ],
                "n_s.append": [
                    1440
                ],
                "o.replace_placeholders": [
                    1440
                ],
                "o.output": [
                    1440
                ],
                "final_states": [
                    1445,
                    1447
                ],
                "_recurrence": [
                    1445
                ],
                "C.sequence.last": [
                    1446,
                    1447
                ],
                "last_states": [
                    1455,
                    1447
                ],
                "C.sequence.unpack": [
                    1450
                ],
                "_reshape_sequence": [
                    1452
                ],
                "f_stats": [
                    1454,
                    1458,
                    1460,
                    1462,
                    1465
                ],
                "l_s": [
                    1455,
                    1456,
                    1458,
                    1460,
                    1462
                ],
                "i_s": [
                    1456,
                    1460,
                    1455
                ],
                "f_stats.append": [
                    1458,
                    1460,
                    1462
                ],
                "C.unpack_batch": [
                    1458,
                    2402
                ],
                "ConvertToStatic": [
                    2498,
                    1460
                ],
                "i_s.shape": [
                    1460
                ],
                "norm": [
                    1475,
                    1476
                ],
                "padding": [
                    1669,
                    1681,
                    2194,
                    2070,
                    2071,
                    2072,
                    1562,
                    2077,
                    1567,
                    1696,
                    1697,
                    1698,
                    2081,
                    2082,
                    2083,
                    2084,
                    2209,
                    2210,
                    2089,
                    1580,
                    1708,
                    1718,
                    1725,
                    1598,
                    2242,
                    2243,
                    1604,
                    2244,
                    2245,
                    1735,
                    2247,
                    2248,
                    1489,
                    1617,
                    1745,
                    1493,
                    1752,
                    1499,
                    1632,
                    1509,
                    1637,
                    2024,
                    2027,
                    1647,
                    1523,
                    1658,
                    1533
                ],
                "left_pad": [
                    1491,
                    1492
                ],
                "dilation_rate": [
                    1670,
                    1544,
                    1545,
                    1555,
                    1564,
                    2212,
                    1573,
                    1600,
                    1610,
                    1616,
                    1491,
                    1501,
                    1503,
                    1633,
                    1510,
                    1640,
                    1646,
                    1525,
                    1528,
                    1660,
                    1534,
                    1663
                ],
                "kernel.shape": [
                    1491
                ],
                "kernel": [
                    1666,
                    1680,
                    2193,
                    2321,
                    1691,
                    2332,
                    2204,
                    2351,
                    2237,
                    2238,
                    2369,
                    1491,
                    1497,
                    2269,
                    2270,
                    1506,
                    1522,
                    1657,
                    1530
                ],
                "temporal_padding": [
                    1492
                ],
                "_preprocess_border_mode": [
                    1632,
                    1735,
                    1708,
                    1681,
                    2194,
                    1523,
                    1658,
                    1562,
                    1499,
                    1598
                ],
                "C.convolution": [
                    1505,
                    1569,
                    1602,
                    1635,
                    1665,
                    1606,
                    1578,
                    1645,
                    1582,
                    1615,
                    1618,
                    1530,
                    1565
                ],
                "strides": [
                    1668,
                    1542,
                    1543,
                    2320,
                    1554,
                    1682,
                    2195,
                    1693,
                    1566,
                    2206,
                    1576,
                    1579,
                    1709,
                    2349,
                    1717,
                    1724,
                    1601,
                    1603,
                    1613,
                    1744,
                    1751,
                    2391,
                    2392,
                    1634,
                    1508,
                    1636,
                    1643,
                    1532
                ],
                "_preprocess_conv2d_input": [
                    1711,
                    2192,
                    1521,
                    1557,
                    1593,
                    1628
                ],
                "_preprocess_conv2d_kernel": [
                    1629,
                    2193,
                    1522,
                    1558,
                    1561,
                    1594,
                    1597
                ],
                "_postprocess_conv2d_output": [
                    1536,
                    1728,
                    2213,
                    1585,
                    1649,
                    1621
                ],
                "spatial_start_dim": [
                    1586,
                    1548,
                    1550,
                    1551
                ],
                "depthwise_kernel": [
                    1602,
                    1635,
                    1629,
                    1578,
                    1645,
                    1615,
                    1552,
                    1558,
                    1559,
                    1560,
                    1594,
                    1595,
                    1596,
                    1565,
                    1630,
                    1631
                ],
                "pointwise_kernel": [
                    1569,
                    1606,
                    1582,
                    1553,
                    1618,
                    1561,
                    1597
                ],
                "depthwise_kernel.shape": [
                    1560,
                    1596,
                    1631
                ],
                "_preprocess_conv3d_input": [
                    1656,
                    1737,
                    1679
                ],
                "_preprocess_conv3d_kernel": [
                    1680,
                    1657
                ],
                "_postprocess_conv3d_output": [
                    1672,
                    1756,
                    1700
                ],
                "output_shape": [
                    1699,
                    2211,
                    2350,
                    1684,
                    2197,
                    1687,
                    2200
                ],
                "transpose_shape": [
                    2200,
                    1687
                ],
                "C.convolution_transpose": [
                    1690,
                    2203
                ],
                "pool_size": [
                    1710,
                    1743,
                    1716,
                    1750,
                    1723
                ],
                "pool_mode": [
                    1739,
                    1712,
                    1746,
                    1719,
                    1754,
                    1727
                ],
                "C.pooling": [
                    1720,
                    1713,
                    1747,
                    1740
                ],
                "C.MAX_POOLING": [
                    1715,
                    1742
                ],
                "C.AVG_POOLING": [
                    1722,
                    1749
                ],
                "alpha": [
                    1760,
                    2176,
                    2179,
                    1765,
                    1766
                ],
                "negative_part": [
                    1761,
                    1766
                ],
                "C.relu": [
                    1761,
                    1762
                ],
                "level": [
                    1771,
                    1773,
                    1774
                ],
                "C.dropout": [
                    1774
                ],
                "dim": [
                    2032,
                    1780,
                    1782
                ],
                "C.softmax": [
                    1787
                ],
                "C.softplus": [
                    1791
                ],
                "axis_without_batch": [
                    1829,
                    1831,
                    1800,
                    1802,
                    1834,
                    1805,
                    1837,
                    1838,
                    1809,
                    1810,
                    1811,
                    1812
                ],
                "output_dimensions": [
                    1830,
                    1831,
                    1801,
                    1802,
                    1809,
                    1810,
                    1811
                ],
                "output.shape": [
                    1830,
                    1801,
                    1836,
                    1837,
                    1807,
                    1839
                ],
                "C.cross_entropy_with_softmax": [
                    1816
                ],
                "C.one_hot": [
                    2093,
                    1837,
                    2183
                ],
                "categorical_crossentropy": [
                    1840
                ],
                "self.placeholders": [
                    1932,
                    1846
                ],
                "self": [
                    2435,
                    2436,
                    2440,
                    2441,
                    2443,
                    1932,
                    2448,
                    2449,
                    2453,
                    1944,
                    1953,
                    1955,
                    2468,
                    2474,
                    2475,
                    1964,
                    1965,
                    1969,
                    1972,
                    1846,
                    1847,
                    1848,
                    1849,
                    1974,
                    1852,
                    1986,
                    2498,
                    1988,
                    2499,
                    1990,
                    1991,
                    2504,
                    2505,
                    1994,
                    1996,
                    2002,
                    2005,
                    2007,
                    2521,
                    2522,
                    2524,
                    2015,
                    2529,
                    2530,
                    2531,
                    2534,
                    2535,
                    1896,
                    1898,
                    1903,
                    1905,
                    1906,
                    1907,
                    1911,
                    1912,
                    1914,
                    2427,
                    2428,
                    2429
                ],
                "self.trainer": [
                    1953,
                    1896,
                    1964,
                    1905,
                    1847
                ],
                "self.unrelated_updates": [
                    1986,
                    1903,
                    2005,
                    2007,
                    1848,
                    2015
                ],
                "self.updates": [
                    1849
                ],
                "updates": [
                    1856,
                    1849,
                    1850,
                    2020
                ],
                "self.loss": [
                    1955,
                    1852
                ],
                "u_ops": [
                    1866,
                    1899,
                    1900,
                    1870,
                    1854
                ],
                "unrelated_updates": [
                    1900,
                    1868,
                    1902,
                    1903,
                    1855
                ],
                "update": [
                    1856,
                    1857,
                    1858,
                    1861,
                    1863
                ],
                "u": [
                    1861,
                    1863,
                    1865,
                    1866,
                    1868,
                    1870
                ],
                "u.arguments": [
                    1865
                ],
                "u_ops.append": [
                    1866
                ],
                "unrelated_updates.append": [
                    1868
                ],
                "update_func": [
                    1872,
                    1889,
                    1870
                ],
                "C.combine": [
                    1912,
                    1907,
                    1870,
                    1903
                ],
                "u.output": [
                    1870
                ],
                "update_func.find_all_with_name": [
                    1872
                ],
                "u_list": [
                    1888,
                    1889,
                    1874,
                    1879
                ],
                "p_list": [
                    1889,
                    1875,
                    1878
                ],
                "p_list.append": [
                    1878
                ],
                "u_list.append": [
                    1879
                ],
                "learner": [
                    1889,
                    1897
                ],
                "C.cntk_py.universal_learner": [
                    1889
                ],
                "criterion": [
                    1897,
                    1898,
                    1891
                ],
                "C.trainer.Trainer": [
                    1896
                ],
                "C.trainer": [
                    1896
                ],
                "self.trainer_output": [
                    1969,
                    1898,
                    1965
                ],
                "f.output": [
                    1898,
                    1906,
                    1911
                ],
                "f": [
                    1898,
                    1906,
                    1911
                ],
                "unrelated_updates.extend": [
                    1900
                ],
                "_.output": [
                    1903
                ],
                "self.metrics_outputs": [
                    1996,
                    1906,
                    1907,
                    2002,
                    1911,
                    1912
                ],
                "self.metrics_func": [
                    1988,
                    1990,
                    1991,
                    1994,
                    1907,
                    1972,
                    1974,
                    1912,
                    1914
                ],
                "placeholder": [
                    1921,
                    1918,
                    1919
                ],
                "get_num_dynamic_axis": [
                    1919
                ],
                "input_shape": [
                    1920,
                    1922
                ],
                "input.shape": [
                    1920,
                    2499,
                    2428
                ],
                "placeholder_shape": [
                    1921,
                    1922
                ],
                "placeholder.shape": [
                    1921
                ],
                "feed_dict": [
                    1956,
                    1957,
                    1931,
                    2008,
                    1975,
                    1976,
                    2009,
                    1950
                ],
                "self._is_input_shape_compatible": [
                    1944
                ],
                "tensor.shape": [
                    1949
                ],
                "updated": [
                    1952,
                    2016,
                    1999,
                    1970,
                    2003
                ],
                "input_dict": [
                    1954,
                    1957,
                    1989,
                    1994,
                    1965,
                    1973,
                    2006,
                    1976,
                    2009,
                    2015
                ],
                "argument": [
                    1955,
                    1956,
                    1957,
                    2534,
                    2535,
                    2537,
                    1962,
                    2007,
                    2008,
                    1974,
                    1975,
                    1976,
                    2009,
                    1980,
                    2014
                ],
                "self.loss.arguments": [
                    1955
                ],
                "argument.name": [
                    1962,
                    1980,
                    2014
                ],
                "self.trainer.train_minibatch": [
                    1964
                ],
                "updated.append": [
                    1970,
                    2003,
                    1999
                ],
                "self.metrics_func.arguments": [
                    1974
                ],
                "output_values": [
                    1988,
                    1994,
                    1995,
                    1997,
                    2001
                ],
                "self.metrics_func.forward": [
                    1988
                ],
                "self.metrics_func.outputs": [
                    1990,
                    1991
                ],
                "self.metrics_func.eval": [
                    1994
                ],
                "value.asarray": [
                    1998
                ],
                "output_values.asarray": [
                    2001
                ],
                "self.unrelated_updates.arguments": [
                    2007
                ],
                "self.unrelated_updates.eval": [
                    2015
                ],
                "Function": [
                    2020
                ],
                "kwargs": [
                    2020
                ],
                "pad": [
                    2089,
                    2027,
                    2077
                ],
                "base_shape": [
                    2031,
                    2032,
                    2036,
                    2038,
                    2042,
                    2044
                ],
                "prefix_shape": [
                    2040,
                    2041,
                    2038,
                    2039
                ],
                "postfix_shape": [
                    2044,
                    2045,
                    2046,
                    2047
                ],
                "pad_info": [
                    2053,
                    2062
                ],
                "C.pad": [
                    2060
                ],
                "_padding": [
                    2063
                ],
                "xs": [
                    2362,
                    2353,
                    2324,
                    2328,
                    2330,
                    2365,
                    2109,
                    2367
                ],
                "result.append": [
                    2112,
                    2114
                ],
                "np.full": [
                    2122
                ],
                "LambdaFunc": [
                    2130,
                    2524
                ],
                "message": [
                    2132
                ],
                "t": [
                    2136,
                    2137,
                    2138
                ],
                "tuples": [
                    2136
                ],
                "np.ndarray": [
                    2139
                ],
                "ndim_cond": [
                    2155,
                    2157,
                    2161,
                    2163,
                    2165,
                    2168
                ],
                "condition": [
                    2168,
                    2169,
                    2155,
                    2167
                ],
                "ndim_expr": [
                    2156,
                    2157,
                    2162,
                    2163,
                    2165
                ],
                "then_expression": [
                    2164,
                    2170,
                    2156
                ],
                "shape_expr": [
                    2168,
                    2164
                ],
                "ndim_diff": [
                    2165,
                    2166
                ],
                "tile": [
                    2168
                ],
                "else_expression": [
                    2171
                ],
                "res": [
                    2177,
                    2179,
                    2175
                ],
                "C.elu": [
                    2175
                ],
                "_targets": [
                    2184,
                    2183
                ],
                "targets": [
                    2183
                ],
                "predictions.shape": [
                    2183
                ],
                "predictions": [
                    2184,
                    2183
                ],
                "C.classification_error": [
                    2184
                ],
                "k": [
                    2184
                ],
                "x.name": [
                    2218
                ],
                "C.alias": [
                    2219
                ],
                "dim_ordering": [
                    2274
                ],
                "C.Axis.default_dynamic_axis": [
                    2288
                ],
                "C.Axis.default_batch_axis": [
                    2432,
                    2301,
                    2471
                ],
                "reduce_fun_name": [
                    2305,
                    2307,
                    2302,
                    2303
                ],
                "tmp_shape": [
                    2312,
                    2313,
                    2314
                ],
                "time_step": [
                    2313
                ],
                "stride": [
                    2320,
                    2326,
                    2327
                ],
                "kernel_shape": [
                    2352,
                    2321,
                    2322,
                    2351
                ],
                "output_length": [
                    2322,
                    2325
                ],
                "feature_dim": [
                    2352,
                    2322,
                    2329,
                    2363,
                    2366
                ],
                "filters": [
                    2352,
                    2322,
                    2376
                ],
                "slice_length": [
                    2328,
                    2326
                ],
                "kernel_size": [
                    2360,
                    2358,
                    2327
                ],
                "xs.append": [
                    2328,
                    2362,
                    2365
                ],
                "x_aggregate": [
                    2330,
                    2371,
                    2334,
                    2367
                ],
                "concatenate": [
                    2330,
                    2367
                ],
                "weight": [
                    2369,
                    2371,
                    2332,
                    2334
                ],
                "permute_dimensions": [
                    2369,
                    2338,
                    2332,
                    2380
                ],
                "stride_row": [
                    2357,
                    2349,
                    2358
                ],
                "stride_col": [
                    2360,
                    2349,
                    2359
                ],
                "output_row": [
                    2376,
                    2355,
                    2350
                ],
                "output_col": [
                    2376,
                    2356,
                    2350
                ],
                "j": [
                    2360,
                    2356,
                    2359
                ],
                "slice_row": [
                    2362,
                    2365,
                    2357
                ],
                "slice_col": [
                    2362,
                    2365,
                    2359
                ],
                "cntk_axes": [
                    2388,
                    2389,
                    2390,
                    2391,
                    2392
                ],
                "begin_index": [
                    2392,
                    2389
                ],
                "end_index": [
                    2392,
                    2390
                ],
                "C.slice": [
                    2392
                ],
                "const_a": [
                    2402,
                    2403,
                    2404
                ],
                "ReshapeBatch": [
                    2427,
                    2406
                ],
                "version": [
                    2410,
                    2411,
                    2412,
                    2414,
                    2415,
                    2417
                ],
                "C.__version__": [
                    2410
                ],
                "version.endswith": [
                    2411
                ],
                "replace": [
                    2415
                ],
                "C.ops.functions.UserFunction": [
                    2456,
                    2425,
                    2515,
                    2485
                ],
                "__init__": [
                    2524,
                    2498,
                    2427,
                    2468
                ],
                "self.from_shape": [
                    2440,
                    2449,
                    2428,
                    2453
                ],
                "self.target_shape": [
                    2435,
                    2499,
                    2504,
                    2441,
                    2443,
                    2448,
                    2429
                ],
                "batch_axis": [
                    2432,
                    2476,
                    2437,
                    2471
                ],
                "C.output_variable": [
                    2528,
                    2473,
                    2434,
                    2503
                ],
                "self.inputs": [
                    2529,
                    2530,
                    2531,
                    2436,
                    2505,
                    2474,
                    2475
                ],
                "num_element": [
                    2440,
                    2442,
                    2448,
                    2450
                ],
                "arguments.shape": [
                    2440
                ],
                "arguments": [
                    2440,
                    2443,
                    2509,
                    2479
                ],
                "num_static_element": [
                    2441,
                    2442,
                    2449,
                    2450
                ],
                "num_batch": [
                    2442,
                    2443
                ],
                "as_shape": [
                    2443
                ],
                "arguments.data": [
                    2443,
                    2509,
                    2479
                ],
                "C.cntk_py.Value": [
                    2444,
                    2509,
                    2479,
                    2512,
                    2482,
                    2451
                ],
                "grad_array_view": [
                    2452,
                    2447
                ],
                "root_gradients.data": [
                    2512,
                    2482,
                    2447
                ],
                "root_gradients": [
                    2540,
                    2447,
                    2448,
                    2512,
                    2482
                ],
                "root_gradients.shape": [
                    2448
                ],
                "num_old_batch": [
                    2450,
                    2453
                ],
                "grad_array_view.as_shape": [
                    2452
                ],
                "batch_size": [
                    2499
                ],
                "arg": [
                    2524,
                    2519
                ],
                "self.when": [
                    2521,
                    2534
                ],
                "when": [
                    2521
                ],
                "self.execute": [
                    2522,
                    2535
                ],
                "execute": [
                    2522
                ],
                "dynamic_axes": [
                    2531
                ]
            }
        },
        "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_19/keras/layers/recurrent.py": {
            "buggy_functions": [
                {
                    "function_name": "__init__",
                    "function_code": "def __init__(self, cells, **kwargs):\n    for cell in cells:\n        if not hasattr(cell, 'call'):\n            raise ValueError('All cells must have a `call` method. '\n                             'received cells:', cells)\n        if not hasattr(cell, 'state_size'):\n            raise ValueError('All cells must have a '\n                             '`state_size` attribute. '\n                             'received cells:', cells)\n    self.cells = cells\n    super(StackedRNNCells, self).__init__(**kwargs)\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 47,
                    "end_line": 57,
                    "variables": {
                        "cell": [
                            48,
                            49,
                            52
                        ],
                        "cells": [
                            48,
                            56,
                            51,
                            55
                        ],
                        "hasattr": [
                            49,
                            52
                        ],
                        "ValueError": [
                            50,
                            53
                        ],
                        "self.cells": [
                            56
                        ],
                        "self": [
                            56,
                            57
                        ],
                        "__init__": [
                            57
                        ],
                        "super": [
                            57
                        ],
                        "StackedRNNCells": [
                            57
                        ],
                        "kwargs": [
                            57
                        ]
                    },
                    "filtered_variables": {
                        "cell": [
                            48,
                            49,
                            52
                        ],
                        "cells": [
                            48,
                            56,
                            51,
                            55
                        ],
                        "self.cells": [
                            56
                        ],
                        "self": [
                            56,
                            57
                        ],
                        "__init__": [
                            57
                        ],
                        "StackedRNNCells": [
                            57
                        ],
                        "kwargs": [
                            57
                        ]
                    },
                    "diff_line_number": 56,
                    "class_data": {
                        "signature": "class StackedRNNCells(Layer)",
                        "docstring": "Wrapper allowing a stack of RNN cells to behave as a single cell.\n\nUsed to implement efficient stacked RNNs.\n\n# Arguments\n    cells: List of RNN cell instances.\n\n# Examples\n\n```python\n    cells = [\n        keras.layers.LSTMCell(output_dim),\n        keras.layers.LSTMCell(output_dim),\n        keras.layers.LSTMCell(output_dim),\n    ]\n\n    inputs = keras.Input((timesteps, input_dim))\n    x = keras.layers.RNN(cells)(inputs)\n```",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, cells, **kwargs):\n    for cell in cells:\n        if not hasattr(cell, 'call'):\n            raise ValueError('All cells must have a `call` method. received cells:', cells)\n        if not hasattr(cell, 'state_size'):\n            raise ValueError('All cells must have a `state_size` attribute. received cells:', cells)\n    self.cells = cells\n    super(StackedRNNCells, self).__init__(**kwargs)",
                            "@property\ndef state_size(self):\n    state_size = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            state_size += list(cell.state_size)\n        else:\n            state_size.append(cell.state_size)\n    return tuple(state_size)",
                            "def call(self, inputs, states, constants=None, **kwargs):\n    nested_states = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            nested_states.append(states[:len(cell.state_size)])\n            states = states[len(cell.state_size):]\n        else:\n            nested_states.append([states[0]])\n            states = states[1:]\n    nested_states = nested_states[::-1]\n    new_nested_states = []\n    for cell, states in zip(self.cells, nested_states):\n        if has_arg(cell.call, 'constants'):\n            inputs, states = cell.call(inputs, states, constants=constants, **kwargs)\n        else:\n            inputs, states = cell.call(inputs, states, **kwargs)\n        new_nested_states.append(states)\n    states = []\n    for cell_states in new_nested_states[::-1]:\n        states += cell_states\n    return (inputs, states)",
                            "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        constants_shape = input_shape[1:]\n        input_shape = input_shape[0]\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            if has_arg(cell.call, 'constants'):\n                cell.build([input_shape] + constants_shape)\n            else:\n                cell.build(input_shape)\n        if hasattr(cell.state_size, '__len__'):\n            output_dim = cell.state_size[0]\n        else:\n            output_dim = cell.state_size\n        input_shape = (input_shape[0], output_dim)\n    self.built = True",
                            "def get_config(self):\n    cells = []\n    for cell in self.cells:\n        cells.append({'class_name': cell.__class__.__name__, 'config': cell.get_config()})\n    config = {'cells': cells}\n    base_config = super(StackedRNNCells, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                            "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cells = []\n    for cell_config in config.pop('cells'):\n        cells.append(deserialize_layer(cell_config, custom_objects=custom_objects))\n    return cls(cells, **config)",
                            "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.trainable_weights\n    return weights",
                            "@property\ndef non_trainable_weights(self):\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.non_trainable_weights\n    if not self.trainable:\n        trainable_weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                trainable_weights += cell.trainable_weights\n        return trainable_weights + weights\n    return weights",
                            "def get_weights(self):\n    \"\"\"Retrieves the weights of the model.\n\n    # Returns\n        A flat list of Numpy arrays.\n    \"\"\"\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.weights\n    return K.batch_get_value(weights)",
                            "def set_weights(self, weights):\n    \"\"\"Sets the weights of the model.\n\n    # Arguments\n        weights: A list of Numpy arrays with shapes and types matching\n            the output of `model.get_weights()`.\n    \"\"\"\n    tuples = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            num_param = len(cell.weights)\n            weights = weights[:num_param]\n            for sw, w in zip(cell.weights, weights):\n                tuples.append((sw, w))\n            weights = weights[num_param:]\n    K.batch_set_value(tuples)",
                            "@property\ndef losses(self):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.losses\n            losses += cell_losses\n    return losses",
                            "def get_losses_for(self, inputs=None):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.get_losses_for(inputs)\n            losses += cell_losses\n    return losses"
                        ],
                        "constructor_variables": [
                            "cells"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, cells, **kwargs)",
                            "state_size(self)",
                            "call(self, inputs, states, constants=None, **kwargs)",
                            "build(self, input_shape)",
                            "get_config(self)",
                            "from_config(cls, config, custom_objects=None)",
                            "trainable_weights(self)",
                            "non_trainable_weights(self)",
                            "get_weights(self)",
                            "set_weights(self, weights)",
                            "losses(self)",
                            "get_losses_for(self, inputs=None)"
                        ]
                    },
                    "variable_values": [
                        [
                            {
                                "cell": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12f31be90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12f49fd90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12f326f50>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "self.cells": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x12f326b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "StackedRNNCells": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                }
                            },
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {
                                "cell": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x1238f1e50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x1238d5510>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x1238fc090>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "self.cells": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x1238fc8d0>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "kwargs.pop": {
                                    "variable_value": "<built-in method pop of dict object at 0x123907a50>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                },
                                "warnings.warn": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "warnings": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "StackedRNNCells": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            },
                            {}
                        ],
                        [
                            {
                                "cell": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12d850f50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12d850690>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12d85d810>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "StackedRNNCells": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "__init__": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "RNN": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.RNN object at 0x12d85dd50>",
                                    "variable_type": "RNN",
                                    "variable_shape": null
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                },
                                "self.cell": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.go_backwards": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "go_backwards": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.stateful": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "stateful": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.unroll": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "unroll": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self.supports_masking": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.input_spec": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "InputSpec": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.state_spec": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self._states": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.constants_spec": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self._num_constants": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                }
                            },
                            {}
                        ],
                        [
                            {},
                            {}
                        ],
                        [
                            {},
                            {}
                        ],
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "state_size",
                    "function_code": "@property\ndef state_size(self):\n    # States are a flat list\n    # in reverse order of the cell stack.\n    # This allows to preserve the requirement\n    # `stack.state_size[0] == output_dim`.\n    # e.g. states of a 2-layer LSTM would be\n    # `[h2, c2, h1, c1]`\n    # (assuming one LSTM has states [h, c])\n    state_size = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            state_size += list(cell.state_size)\n        else:\n            state_size.append(cell.state_size)\n    return tuple(state_size)\n",
                    "decorators": [
                        "property"
                    ],
                    "docstring": null,
                    "start_line": 59,
                    "end_line": 74,
                    "variables": {
                        "state_size": [
                            73,
                            74,
                            68,
                            71
                        ],
                        "cell": [
                            73,
                            69,
                            70,
                            71
                        ],
                        "self.cells": [
                            69
                        ],
                        "self": [
                            69
                        ],
                        "hasattr": [
                            70
                        ],
                        "cell.state_size": [
                            73,
                            70,
                            71
                        ],
                        "list": [
                            71
                        ],
                        "state_size.append": [
                            73
                        ],
                        "tuple": [
                            74
                        ],
                        "property": [
                            59
                        ]
                    },
                    "filtered_variables": {
                        "state_size": [
                            73,
                            74,
                            68,
                            71
                        ],
                        "cell": [
                            73,
                            69,
                            70,
                            71
                        ],
                        "self.cells": [
                            69
                        ],
                        "self": [
                            69
                        ],
                        "cell.state_size": [
                            73,
                            70,
                            71
                        ],
                        "state_size.append": [
                            73
                        ]
                    },
                    "diff_line_number": 61,
                    "class_data": {
                        "signature": "class StackedRNNCells(Layer)",
                        "docstring": "Wrapper allowing a stack of RNN cells to behave as a single cell.\n\nUsed to implement efficient stacked RNNs.\n\n# Arguments\n    cells: List of RNN cell instances.\n\n# Examples\n\n```python\n    cells = [\n        keras.layers.LSTMCell(output_dim),\n        keras.layers.LSTMCell(output_dim),\n        keras.layers.LSTMCell(output_dim),\n    ]\n\n    inputs = keras.Input((timesteps, input_dim))\n    x = keras.layers.RNN(cells)(inputs)\n```",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, cells, **kwargs):\n    for cell in cells:\n        if not hasattr(cell, 'call'):\n            raise ValueError('All cells must have a `call` method. received cells:', cells)\n        if not hasattr(cell, 'state_size'):\n            raise ValueError('All cells must have a `state_size` attribute. received cells:', cells)\n    self.cells = cells\n    super(StackedRNNCells, self).__init__(**kwargs)",
                            "@property\ndef state_size(self):\n    state_size = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            state_size += list(cell.state_size)\n        else:\n            state_size.append(cell.state_size)\n    return tuple(state_size)",
                            "def call(self, inputs, states, constants=None, **kwargs):\n    nested_states = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            nested_states.append(states[:len(cell.state_size)])\n            states = states[len(cell.state_size):]\n        else:\n            nested_states.append([states[0]])\n            states = states[1:]\n    nested_states = nested_states[::-1]\n    new_nested_states = []\n    for cell, states in zip(self.cells, nested_states):\n        if has_arg(cell.call, 'constants'):\n            inputs, states = cell.call(inputs, states, constants=constants, **kwargs)\n        else:\n            inputs, states = cell.call(inputs, states, **kwargs)\n        new_nested_states.append(states)\n    states = []\n    for cell_states in new_nested_states[::-1]:\n        states += cell_states\n    return (inputs, states)",
                            "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        constants_shape = input_shape[1:]\n        input_shape = input_shape[0]\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            if has_arg(cell.call, 'constants'):\n                cell.build([input_shape] + constants_shape)\n            else:\n                cell.build(input_shape)\n        if hasattr(cell.state_size, '__len__'):\n            output_dim = cell.state_size[0]\n        else:\n            output_dim = cell.state_size\n        input_shape = (input_shape[0], output_dim)\n    self.built = True",
                            "def get_config(self):\n    cells = []\n    for cell in self.cells:\n        cells.append({'class_name': cell.__class__.__name__, 'config': cell.get_config()})\n    config = {'cells': cells}\n    base_config = super(StackedRNNCells, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                            "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cells = []\n    for cell_config in config.pop('cells'):\n        cells.append(deserialize_layer(cell_config, custom_objects=custom_objects))\n    return cls(cells, **config)",
                            "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.trainable_weights\n    return weights",
                            "@property\ndef non_trainable_weights(self):\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.non_trainable_weights\n    if not self.trainable:\n        trainable_weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                trainable_weights += cell.trainable_weights\n        return trainable_weights + weights\n    return weights",
                            "def get_weights(self):\n    \"\"\"Retrieves the weights of the model.\n\n    # Returns\n        A flat list of Numpy arrays.\n    \"\"\"\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.weights\n    return K.batch_get_value(weights)",
                            "def set_weights(self, weights):\n    \"\"\"Sets the weights of the model.\n\n    # Arguments\n        weights: A list of Numpy arrays with shapes and types matching\n            the output of `model.get_weights()`.\n    \"\"\"\n    tuples = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            num_param = len(cell.weights)\n            weights = weights[:num_param]\n            for sw, w in zip(cell.weights, weights):\n                tuples.append((sw, w))\n            weights = weights[num_param:]\n    K.batch_set_value(tuples)",
                            "@property\ndef losses(self):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.losses\n            losses += cell_losses\n    return losses",
                            "def get_losses_for(self, inputs=None):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.get_losses_for(inputs)\n            losses += cell_losses\n    return losses"
                        ],
                        "constructor_variables": [
                            "cells"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, cells, **kwargs)",
                            "state_size(self)",
                            "call(self, inputs, states, constants=None, **kwargs)",
                            "build(self, input_shape)",
                            "get_config(self)",
                            "from_config(cls, config, custom_objects=None)",
                            "trainable_weights(self)",
                            "non_trainable_weights(self)",
                            "get_weights(self)",
                            "set_weights(self, weights)",
                            "losses(self)",
                            "get_losses_for(self, inputs=None)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[32, 32, 16, 16, 8, 8]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12cf5eb90>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12cf5eb90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12cf55f10>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12cf69f50>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x12cf69b50>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "cell.state_size": {
                                    "variable_value": "(8, 8)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x12cf621e0>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[32, 32, 16, 16, 8, 8]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12cf5eb90>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12cf5eb90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12cf55f10>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12cf69f50>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x12cf69b50>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "cell.state_size": {
                                    "variable_value": "(8, 8)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x12cf83eb0>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[8, 8, 16, 16, 32, 32]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x127c08b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127befc90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127b3df50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "cell.state_size": {
                                    "variable_value": "(32, 32)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x127bf42d0>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[8, 8, 16, 16, 32, 32]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x127c08b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127befc90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127b3df50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "cell.state_size": {
                                    "variable_value": "(32, 32)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x127c1ab90>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[8, 8, 16, 16, 32, 32]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x127c08b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127befc90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127b3df50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "cell.state_size": {
                                    "variable_value": "(32, 32)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x127c183c0>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[8, 8, 16, 16, 32, 32]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x127c08b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127befc90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127b3df50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "cell.state_size": {
                                    "variable_value": "(32, 32)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x127c1dcd0>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[8, 8, 16, 16, 32, 32]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x127c08b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127befc90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127b3df50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "cell.state_size": {
                                    "variable_value": "(32, 32)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x127c1d730>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[8, 8, 16, 16, 32, 32]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x127c08b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127befc90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127b3df50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "cell.state_size": {
                                    "variable_value": "(32, 32)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x127c21190>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[8, 8, 16, 16, 32, 32]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x127c08b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127befc90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127b3df50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "cell.state_size": {
                                    "variable_value": "(32, 32)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x127c21500>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[8, 8, 16, 16, 32, 32]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x127c08b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127befc90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127b3df50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "cell.state_size": {
                                    "variable_value": "(32, 32)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x127c24910>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[8, 8, 16, 16, 32, 32]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x127c08b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127befc90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127b3df50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "cell.state_size": {
                                    "variable_value": "(32, 32)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x127c24a50>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ],
                        [
                            {},
                            {
                                "state_size": {
                                    "variable_value": "[8, 8, 16, 16, 32, 32]",
                                    "variable_type": "list",
                                    "variable_shape": "6"
                                },
                                "cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self.reverse_state_order": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x127c08b90>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self.cells": {
                                    "variable_value": "[<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127befc90>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127b3df50>, <recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x127bfd710>]",
                                    "variable_type": "list",
                                    "variable_shape": "3"
                                },
                                "cell.state_size": {
                                    "variable_value": "(32, 32)",
                                    "variable_type": "tuple",
                                    "variable_shape": "2"
                                },
                                "state_size.append": {
                                    "variable_value": "<built-in method append of list object at 0x127c214b0>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                }
                            }
                        ]
                    ]
                },
                {
                    "function_name": "call",
                    "function_code": "def call(self, inputs, states, constants=None, **kwargs):\n    # Recover per-cell states.\n    nested_states = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            nested_states.append(states[:len(cell.state_size)])\n            states = states[len(cell.state_size):]\n        else:\n            nested_states.append([states[0]])\n            states = states[1:]\n    nested_states = nested_states[::-1]\n\n    # Call the cells in order and store the returned states.\n    new_nested_states = []\n    for cell, states in zip(self.cells, nested_states):\n        if has_arg(cell.call, 'constants'):\n            inputs, states = cell.call(inputs, states,\n                                       constants=constants,\n                                       **kwargs)\n        else:\n            inputs, states = cell.call(inputs, states, **kwargs)\n        new_nested_states.append(states)\n\n    # Format the new states as a flat list\n    # in reverse cell order.\n    states = []\n    for cell_states in new_nested_states[::-1]:\n        states += cell_states\n    return inputs, states\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 76,
                    "end_line": 104,
                    "variables": {
                        "nested_states": [
                            78,
                            81,
                            84,
                            86,
                            90
                        ],
                        "cell": [
                            96,
                            79,
                            80,
                            81,
                            82,
                            90,
                            91,
                            92
                        ],
                        "self.cells": [
                            90,
                            79
                        ],
                        "self": [
                            90,
                            79
                        ],
                        "hasattr": [
                            80
                        ],
                        "cell.state_size": [
                            80,
                            81,
                            82
                        ],
                        "nested_states.append": [
                            81,
                            84
                        ],
                        "states": [
                            96,
                            97,
                            101,
                            103,
                            104,
                            81,
                            82,
                            84,
                            85,
                            90,
                            92
                        ],
                        "len": [
                            81,
                            82
                        ],
                        "new_nested_states": [
                            89,
                            102,
                            97
                        ],
                        "zip": [
                            90
                        ],
                        "has_arg": [
                            91
                        ],
                        "cell.call": [
                            96,
                            91,
                            92
                        ],
                        "inputs": [
                            96,
                            92,
                            104
                        ],
                        "constants": [
                            93
                        ],
                        "kwargs": [
                            96,
                            94
                        ],
                        "new_nested_states.append": [
                            97
                        ],
                        "cell_states": [
                            102,
                            103
                        ]
                    },
                    "filtered_variables": {
                        "nested_states": [
                            78,
                            81,
                            84,
                            86,
                            90
                        ],
                        "cell": [
                            96,
                            79,
                            80,
                            81,
                            82,
                            90,
                            91,
                            92
                        ],
                        "self.cells": [
                            90,
                            79
                        ],
                        "self": [
                            90,
                            79
                        ],
                        "cell.state_size": [
                            80,
                            81,
                            82
                        ],
                        "nested_states.append": [
                            81,
                            84
                        ],
                        "states": [
                            96,
                            97,
                            101,
                            103,
                            104,
                            81,
                            82,
                            84,
                            85,
                            90,
                            92
                        ],
                        "new_nested_states": [
                            89,
                            102,
                            97
                        ],
                        "has_arg": [
                            91
                        ],
                        "cell.call": [
                            96,
                            91,
                            92
                        ],
                        "inputs": [
                            96,
                            92,
                            104
                        ],
                        "constants": [
                            93
                        ],
                        "kwargs": [
                            96,
                            94
                        ],
                        "new_nested_states.append": [
                            97
                        ],
                        "cell_states": [
                            102,
                            103
                        ]
                    },
                    "diff_line_number": 76,
                    "class_data": {
                        "signature": "class StackedRNNCells(Layer)",
                        "docstring": "Wrapper allowing a stack of RNN cells to behave as a single cell.\n\nUsed to implement efficient stacked RNNs.\n\n# Arguments\n    cells: List of RNN cell instances.\n\n# Examples\n\n```python\n    cells = [\n        keras.layers.LSTMCell(output_dim),\n        keras.layers.LSTMCell(output_dim),\n        keras.layers.LSTMCell(output_dim),\n    ]\n\n    inputs = keras.Input((timesteps, input_dim))\n    x = keras.layers.RNN(cells)(inputs)\n```",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, cells, **kwargs):\n    for cell in cells:\n        if not hasattr(cell, 'call'):\n            raise ValueError('All cells must have a `call` method. received cells:', cells)\n        if not hasattr(cell, 'state_size'):\n            raise ValueError('All cells must have a `state_size` attribute. received cells:', cells)\n    self.cells = cells\n    super(StackedRNNCells, self).__init__(**kwargs)",
                            "@property\ndef state_size(self):\n    state_size = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            state_size += list(cell.state_size)\n        else:\n            state_size.append(cell.state_size)\n    return tuple(state_size)",
                            "def call(self, inputs, states, constants=None, **kwargs):\n    nested_states = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            nested_states.append(states[:len(cell.state_size)])\n            states = states[len(cell.state_size):]\n        else:\n            nested_states.append([states[0]])\n            states = states[1:]\n    nested_states = nested_states[::-1]\n    new_nested_states = []\n    for cell, states in zip(self.cells, nested_states):\n        if has_arg(cell.call, 'constants'):\n            inputs, states = cell.call(inputs, states, constants=constants, **kwargs)\n        else:\n            inputs, states = cell.call(inputs, states, **kwargs)\n        new_nested_states.append(states)\n    states = []\n    for cell_states in new_nested_states[::-1]:\n        states += cell_states\n    return (inputs, states)",
                            "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        constants_shape = input_shape[1:]\n        input_shape = input_shape[0]\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            if has_arg(cell.call, 'constants'):\n                cell.build([input_shape] + constants_shape)\n            else:\n                cell.build(input_shape)\n        if hasattr(cell.state_size, '__len__'):\n            output_dim = cell.state_size[0]\n        else:\n            output_dim = cell.state_size\n        input_shape = (input_shape[0], output_dim)\n    self.built = True",
                            "def get_config(self):\n    cells = []\n    for cell in self.cells:\n        cells.append({'class_name': cell.__class__.__name__, 'config': cell.get_config()})\n    config = {'cells': cells}\n    base_config = super(StackedRNNCells, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                            "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cells = []\n    for cell_config in config.pop('cells'):\n        cells.append(deserialize_layer(cell_config, custom_objects=custom_objects))\n    return cls(cells, **config)",
                            "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.trainable_weights\n    return weights",
                            "@property\ndef non_trainable_weights(self):\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.non_trainable_weights\n    if not self.trainable:\n        trainable_weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                trainable_weights += cell.trainable_weights\n        return trainable_weights + weights\n    return weights",
                            "def get_weights(self):\n    \"\"\"Retrieves the weights of the model.\n\n    # Returns\n        A flat list of Numpy arrays.\n    \"\"\"\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.weights\n    return K.batch_get_value(weights)",
                            "def set_weights(self, weights):\n    \"\"\"Sets the weights of the model.\n\n    # Arguments\n        weights: A list of Numpy arrays with shapes and types matching\n            the output of `model.get_weights()`.\n    \"\"\"\n    tuples = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            num_param = len(cell.weights)\n            weights = weights[:num_param]\n            for sw, w in zip(cell.weights, weights):\n                tuples.append((sw, w))\n            weights = weights[num_param:]\n    K.batch_set_value(tuples)",
                            "@property\ndef losses(self):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.losses\n            losses += cell_losses\n    return losses",
                            "def get_losses_for(self, inputs=None):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.get_losses_for(inputs)\n            losses += cell_losses\n    return losses"
                        ],
                        "constructor_variables": [
                            "cells"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, cells, **kwargs)",
                            "state_size(self)",
                            "call(self, inputs, states, constants=None, **kwargs)",
                            "build(self, input_shape)",
                            "get_config(self)",
                            "from_config(cls, config, custom_objects=None)",
                            "trainable_weights(self)",
                            "non_trainable_weights(self)",
                            "get_weights(self)",
                            "set_weights(self, weights)",
                            "losses(self)",
                            "get_losses_for(self, inputs=None)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "build",
                    "function_code": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        constants_shape = input_shape[1:]\n        input_shape = input_shape[0]\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            if has_arg(cell.call, 'constants'):\n                cell.build([input_shape] + constants_shape)\n            else:\n                cell.build(input_shape)\n        if hasattr(cell.state_size, '__len__'):\n            output_dim = cell.state_size[0]\n        else:\n            output_dim = cell.state_size\n        input_shape = (input_shape[0], output_dim)\n    self.built = True\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 106,
                    "end_line": 121,
                    "variables": {
                        "isinstance": [
                            107,
                            111
                        ],
                        "input_shape": [
                            107,
                            108,
                            109,
                            113,
                            115,
                            120
                        ],
                        "list": [
                            107
                        ],
                        "constants_shape": [
                            113,
                            108
                        ],
                        "cell": [
                            110,
                            111,
                            112,
                            113,
                            115,
                            116,
                            117,
                            119
                        ],
                        "self.cells": [
                            110
                        ],
                        "self": [
                            121,
                            110
                        ],
                        "Layer": [
                            111
                        ],
                        "has_arg": [
                            112
                        ],
                        "cell.call": [
                            112
                        ],
                        "cell.build": [
                            113,
                            115
                        ],
                        "hasattr": [
                            116
                        ],
                        "cell.state_size": [
                            116,
                            117,
                            119
                        ],
                        "output_dim": [
                            120,
                            117,
                            119
                        ],
                        "self.built": [
                            121
                        ]
                    },
                    "filtered_variables": {
                        "input_shape": [
                            107,
                            108,
                            109,
                            113,
                            115,
                            120
                        ],
                        "constants_shape": [
                            113,
                            108
                        ],
                        "cell": [
                            110,
                            111,
                            112,
                            113,
                            115,
                            116,
                            117,
                            119
                        ],
                        "self.cells": [
                            110
                        ],
                        "self": [
                            121,
                            110
                        ],
                        "Layer": [
                            111
                        ],
                        "has_arg": [
                            112
                        ],
                        "cell.call": [
                            112
                        ],
                        "cell.build": [
                            113,
                            115
                        ],
                        "cell.state_size": [
                            116,
                            117,
                            119
                        ],
                        "output_dim": [
                            120,
                            117,
                            119
                        ],
                        "self.built": [
                            121
                        ]
                    },
                    "diff_line_number": 116,
                    "class_data": {
                        "signature": "class StackedRNNCells(Layer)",
                        "docstring": "Wrapper allowing a stack of RNN cells to behave as a single cell.\n\nUsed to implement efficient stacked RNNs.\n\n# Arguments\n    cells: List of RNN cell instances.\n\n# Examples\n\n```python\n    cells = [\n        keras.layers.LSTMCell(output_dim),\n        keras.layers.LSTMCell(output_dim),\n        keras.layers.LSTMCell(output_dim),\n    ]\n\n    inputs = keras.Input((timesteps, input_dim))\n    x = keras.layers.RNN(cells)(inputs)\n```",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, cells, **kwargs):\n    for cell in cells:\n        if not hasattr(cell, 'call'):\n            raise ValueError('All cells must have a `call` method. received cells:', cells)\n        if not hasattr(cell, 'state_size'):\n            raise ValueError('All cells must have a `state_size` attribute. received cells:', cells)\n    self.cells = cells\n    super(StackedRNNCells, self).__init__(**kwargs)",
                            "@property\ndef state_size(self):\n    state_size = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            state_size += list(cell.state_size)\n        else:\n            state_size.append(cell.state_size)\n    return tuple(state_size)",
                            "def call(self, inputs, states, constants=None, **kwargs):\n    nested_states = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            nested_states.append(states[:len(cell.state_size)])\n            states = states[len(cell.state_size):]\n        else:\n            nested_states.append([states[0]])\n            states = states[1:]\n    nested_states = nested_states[::-1]\n    new_nested_states = []\n    for cell, states in zip(self.cells, nested_states):\n        if has_arg(cell.call, 'constants'):\n            inputs, states = cell.call(inputs, states, constants=constants, **kwargs)\n        else:\n            inputs, states = cell.call(inputs, states, **kwargs)\n        new_nested_states.append(states)\n    states = []\n    for cell_states in new_nested_states[::-1]:\n        states += cell_states\n    return (inputs, states)",
                            "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        constants_shape = input_shape[1:]\n        input_shape = input_shape[0]\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            if has_arg(cell.call, 'constants'):\n                cell.build([input_shape] + constants_shape)\n            else:\n                cell.build(input_shape)\n        if hasattr(cell.state_size, '__len__'):\n            output_dim = cell.state_size[0]\n        else:\n            output_dim = cell.state_size\n        input_shape = (input_shape[0], output_dim)\n    self.built = True",
                            "def get_config(self):\n    cells = []\n    for cell in self.cells:\n        cells.append({'class_name': cell.__class__.__name__, 'config': cell.get_config()})\n    config = {'cells': cells}\n    base_config = super(StackedRNNCells, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                            "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cells = []\n    for cell_config in config.pop('cells'):\n        cells.append(deserialize_layer(cell_config, custom_objects=custom_objects))\n    return cls(cells, **config)",
                            "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.trainable_weights\n    return weights",
                            "@property\ndef non_trainable_weights(self):\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.non_trainable_weights\n    if not self.trainable:\n        trainable_weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                trainable_weights += cell.trainable_weights\n        return trainable_weights + weights\n    return weights",
                            "def get_weights(self):\n    \"\"\"Retrieves the weights of the model.\n\n    # Returns\n        A flat list of Numpy arrays.\n    \"\"\"\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.weights\n    return K.batch_get_value(weights)",
                            "def set_weights(self, weights):\n    \"\"\"Sets the weights of the model.\n\n    # Arguments\n        weights: A list of Numpy arrays with shapes and types matching\n            the output of `model.get_weights()`.\n    \"\"\"\n    tuples = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            num_param = len(cell.weights)\n            weights = weights[:num_param]\n            for sw, w in zip(cell.weights, weights):\n                tuples.append((sw, w))\n            weights = weights[num_param:]\n    K.batch_set_value(tuples)",
                            "@property\ndef losses(self):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.losses\n            losses += cell_losses\n    return losses",
                            "def get_losses_for(self, inputs=None):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.get_losses_for(inputs)\n            losses += cell_losses\n    return losses"
                        ],
                        "constructor_variables": [
                            "cells"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, cells, **kwargs)",
                            "state_size(self)",
                            "call(self, inputs, states, constants=None, **kwargs)",
                            "build(self, input_shape)",
                            "get_config(self)",
                            "from_config(cls, config, custom_objects=None)",
                            "trainable_weights(self)",
                            "non_trainable_weights(self)",
                            "get_weights(self)",
                            "set_weights(self, weights)",
                            "losses(self)",
                            "get_losses_for(self, inputs=None)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "compute_output_shape",
                    "function_code": "def compute_output_shape(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = self.cell.state_size\n    else:\n        state_size = [self.cell.state_size]\n    output_dim = state_size[0]\n\n    if self.return_sequences:\n        output_shape = (input_shape[0], input_shape[1], output_dim)\n    else:\n        output_shape = (input_shape[0], output_dim)\n\n    if self.return_state:\n        state_shape = [(input_shape[0], dim) for dim in state_size]\n        return [output_shape] + state_shape\n    else:\n        return output_shape\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 409,
                    "end_line": 428,
                    "variables": {
                        "isinstance": [
                            410
                        ],
                        "input_shape": [
                            420,
                            422,
                            425,
                            410,
                            411
                        ],
                        "list": [
                            410
                        ],
                        "hasattr": [
                            413
                        ],
                        "self.cell.state_size": [
                            416,
                            413,
                            414
                        ],
                        "self.cell": [
                            416,
                            413,
                            414
                        ],
                        "self": [
                            416,
                            419,
                            424,
                            413,
                            414
                        ],
                        "state_size": [
                            416,
                            417,
                            425,
                            414
                        ],
                        "output_dim": [
                            417,
                            420,
                            422
                        ],
                        "self.return_sequences": [
                            419
                        ],
                        "output_shape": [
                            426,
                            428,
                            420,
                            422
                        ],
                        "self.return_state": [
                            424
                        ],
                        "state_shape": [
                            425,
                            426
                        ],
                        "dim": [
                            425
                        ]
                    },
                    "filtered_variables": {
                        "input_shape": [
                            420,
                            422,
                            425,
                            410,
                            411
                        ],
                        "self.cell.state_size": [
                            416,
                            413,
                            414
                        ],
                        "self.cell": [
                            416,
                            413,
                            414
                        ],
                        "self": [
                            416,
                            419,
                            424,
                            413,
                            414
                        ],
                        "state_size": [
                            416,
                            417,
                            425,
                            414
                        ],
                        "output_dim": [
                            417,
                            420,
                            422
                        ],
                        "self.return_sequences": [
                            419
                        ],
                        "output_shape": [
                            426,
                            428,
                            420,
                            422
                        ],
                        "self.return_state": [
                            424
                        ],
                        "state_shape": [
                            425,
                            426
                        ],
                        "dim": [
                            425
                        ]
                    },
                    "diff_line_number": 417,
                    "class_data": {
                        "signature": "class RNN(Layer)",
                        "docstring": "Base class for recurrent layers.\n\n# Arguments\n    cell: A RNN cell instance. A RNN cell is a class that has:\n        - a `call(input_at_t, states_at_t)` method, returning\n            `(output_at_t, states_at_t_plus_1)`. The call method of the\n            cell can also take the optional argument `constants`, see\n            section \"Note on passing external constants\" below.\n        - a `state_size` attribute. This can be a single integer\n            (single state) in which case it is\n            the size of the recurrent state\n            (which should be the same as the size of the cell output).\n            This can also be a list/tuple of integers\n            (one size per state). In this case, the first entry\n            (`state_size[0]`) should be the same as\n            the size of the cell output.\n        It is also possible for `cell` to be a list of RNN cell instances,\n        in which cases the cells get stacked on after the other in the RNN,\n        implementing an efficient stacked RNN.\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    unroll: Boolean (default False).\n        If True, the network will be unrolled,\n        else a symbolic loop will be used.\n        Unrolling can speed-up a RNN,\n        although it tends to be more memory-intensive.\n        Unrolling is only suitable for short sequences.\n    input_dim: dimensionality of the input (integer).\n        This argument (or alternatively,\n        the keyword argument `input_shape`)\n        is required when using this layer as the first layer in a model.\n    input_length: Length of input sequences, to be specified\n        when it is constant.\n        This argument is required if you are going to connect\n        `Flatten` then `Dense` layers upstream\n        (without it, the shape of the dense outputs cannot be computed).\n        Note that if the recurrent layer is not the first layer\n        in your model, you would need to specify the input length\n        at the level of the first layer\n        (e.g. via the `input_shape` argument)\n\n# Input shape\n    3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n# Output shape\n    - if `return_state`: a list of tensors. The first tensor is\n        the output. The remaining tensors are the last states,\n        each with shape `(batch_size, units)`.\n    - if `return_sequences`: 3D tensor with shape\n        `(batch_size, timesteps, units)`.\n    - else, 2D tensor with shape `(batch_size, units)`.\n\n# Masking\n    This layer supports masking for input data with a variable number\n    of timesteps. To introduce masks to your data,\n    use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n    set to `True`.\n\n# Note on using statefulness in RNNs\n    You can set RNN layers to be 'stateful', which means that the states\n    computed for the samples in one batch will be reused as initial states\n    for the samples in the next batch. This assumes a one-to-one mapping\n    between samples in different successive batches.\n\n    To enable statefulness:\n        - specify `stateful=True` in the layer constructor.\n        - specify a fixed batch size for your model, by passing\n            if sequential model:\n              `batch_input_shape=(...)` to the first layer in your model.\n            else for functional model with 1 or more Input layers:\n              `batch_shape=(...)` to all the first layers in your model.\n            This is the expected shape of your inputs\n            *including the batch size*.\n            It should be a tuple of integers, e.g. `(32, 10, 100)`.\n        - specify `shuffle=False` when calling fit().\n\n    To reset the states of your model, call `.reset_states()` on either\n    a specific layer, or on your entire model.\n\n# Note on specifying the initial state of RNNs\n    You can specify the initial state of RNN layers symbolically by\n    calling them with the keyword argument `initial_state`. The value of\n    `initial_state` should be a tensor or list of tensors representing\n    the initial state of the RNN layer.\n\n    You can specify the initial state of RNN layers numerically by\n    calling `reset_states` with the keyword argument `states`. The value of\n    `states` should be a numpy array or list of numpy arrays representing\n    the initial state of the RNN layer.\n\n# Note on passing external constants to RNNs\n    You can pass \"external\" constants to the cell using the `constants`\n    keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n    requires that the `cell.call` method accepts the same keyword argument\n    `constants`. Such constants can be used to condition the cell\n    transformation on additional static inputs (not changing over time),\n    a.k.a. an attention mechanism.\n\n# Examples\n\n```python\n    # First, let's define a RNN Cell, as a layer subclass.\n\n    class MinimalRNNCell(keras.layers.Layer):\n\n        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(MinimalRNNCell, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                          initializer='uniform',\n                                          name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.built = True\n\n        def call(self, inputs, states):\n            prev_output = states[0]\n            h = K.dot(inputs, self.kernel)\n            output = h + K.dot(prev_output, self.recurrent_kernel)\n            return output, [output]\n\n    # Let's use this cell in a RNN layer:\n\n    cell = MinimalRNNCell(32)\n    x = keras.Input((None, 5))\n    layer = RNN(cell)\n    y = layer(x)\n\n    # Here's how to use the cell to build a stacked RNN:\n\n    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n    x = keras.Input((None, 5))\n    layer = RNN(cells)\n    y = layer(x)\n```",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs):\n    if isinstance(cell, (list, tuple)):\n        cell = StackedRNNCells(cell)\n    if not hasattr(cell, 'call'):\n        raise ValueError('`cell` should have a `call` method. The RNN was passed:', cell)\n    if not hasattr(cell, 'state_size'):\n        raise ValueError('The RNN cell should have an attribute `state_size` (tuple of integers, one integer per RNN state).')\n    super(RNN, self).__init__(**kwargs)\n    self.cell = cell\n    self.return_sequences = return_sequences\n    self.return_state = return_state\n    self.go_backwards = go_backwards\n    self.stateful = stateful\n    self.unroll = unroll\n    self.supports_masking = True\n    self.input_spec = [InputSpec(ndim=3)]\n    self.state_spec = None\n    self._states = None\n    self.constants_spec = None\n    self._num_constants = None",
                            "@property\ndef states(self):\n    if self._states is None:\n        if isinstance(self.cell.state_size, int):\n            num_states = 1\n        else:\n            num_states = len(self.cell.state_size)\n        return [None for _ in range(num_states)]\n    return self._states",
                            "@states.setter\ndef states(self, states):\n    self._states = states",
                            "def compute_output_shape(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = self.cell.state_size\n    else:\n        state_size = [self.cell.state_size]\n    output_dim = state_size[0]\n    if self.return_sequences:\n        output_shape = (input_shape[0], input_shape[1], output_dim)\n    else:\n        output_shape = (input_shape[0], output_dim)\n    if self.return_state:\n        state_shape = [(input_shape[0], dim) for dim in state_size]\n        return [output_shape] + state_shape\n    else:\n        return output_shape",
                            "def compute_mask(self, inputs, mask):\n    if isinstance(mask, list):\n        mask = mask[0]\n    output_mask = mask if self.return_sequences else None\n    if self.return_state:\n        state_mask = [None for _ in self.states]\n        return [output_mask] + state_mask\n    else:\n        return output_mask",
                            "def build(self, input_shape):\n    if self._num_constants is not None:\n        constants_shape = input_shape[-self._num_constants:]\n    else:\n        constants_shape = None\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    batch_size = input_shape[0] if self.stateful else None\n    input_dim = input_shape[-1]\n    self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))\n    if isinstance(self.cell, Layer):\n        step_input_shape = (input_shape[0],) + input_shape[2:]\n        if constants_shape is not None:\n            self.cell.build([step_input_shape] + constants_shape)\n        else:\n            self.cell.build(step_input_shape)\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = list(self.cell.state_size)\n    else:\n        state_size = [self.cell.state_size]\n    if self.state_spec is not None:\n        if [spec.shape[-1] for spec in self.state_spec] != state_size:\n            raise ValueError('An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`={}; however `cell.state_size` is {}'.format(self.state_spec, self.cell.state_size))\n    else:\n        self.state_spec = [InputSpec(shape=(None, dim)) for dim in state_size]\n    if self.stateful:\n        self.reset_states()\n    self.built = True",
                            "def get_initial_state(self, inputs):\n    initial_state = K.zeros_like(inputs)\n    initial_state = K.sum(initial_state, axis=(1, 2))\n    initial_state = K.expand_dims(initial_state)\n    if hasattr(self.cell.state_size, '__len__'):\n        return [K.tile(initial_state, [1, dim]) for dim in self.cell.state_size]\n    else:\n        return [K.tile(initial_state, [1, self.cell.state_size])]",
                            "def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n    inputs, initial_state, constants = _standardize_args(inputs, initial_state, constants, self._num_constants)\n    if initial_state is None and constants is None:\n        return super(RNN, self).__call__(inputs, **kwargs)\n    additional_inputs = []\n    additional_specs = []\n    if initial_state is not None:\n        kwargs['initial_state'] = initial_state\n        additional_inputs += initial_state\n        self.state_spec = [InputSpec(shape=K.int_shape(state)) for state in initial_state]\n        additional_specs += self.state_spec\n    if constants is not None:\n        kwargs['constants'] = constants\n        additional_inputs += constants\n        self.constants_spec = [InputSpec(shape=K.int_shape(constant)) for constant in constants]\n        self._num_constants = len(constants)\n        additional_specs += self.constants_spec\n    is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n    for tensor in additional_inputs:\n        if K.is_keras_tensor(tensor) != is_keras_tensor:\n            raise ValueError('The initial state or constants of an RNN layer cannot be specified with a mix of Keras tensors and non-Keras tensors (a \"Keras tensor\" is a tensor that was returned by a Keras layer, or by `Input`)')\n    if is_keras_tensor:\n        full_input = [inputs] + additional_inputs\n        full_input_spec = self.input_spec + additional_specs\n        original_input_spec = self.input_spec\n        self.input_spec = full_input_spec\n        output = super(RNN, self).__call__(full_input, **kwargs)\n        self.input_spec = original_input_spec\n        return output\n    else:\n        return super(RNN, self).__call__(inputs, **kwargs)",
                            "def call(self, inputs, mask=None, training=None, initial_state=None, constants=None):\n    if isinstance(inputs, list):\n        if self._num_constants is None:\n            initial_state = inputs[1:]\n        else:\n            initial_state = inputs[1:-self._num_constants]\n        if len(initial_state) == 0:\n            initial_state = None\n        inputs = inputs[0]\n    if initial_state is not None:\n        pass\n    elif self.stateful:\n        initial_state = self.states\n    else:\n        initial_state = self.get_initial_state(inputs)\n    if isinstance(mask, list):\n        mask = mask[0]\n    if len(initial_state) != len(self.states):\n        raise ValueError('Layer has ' + str(len(self.states)) + ' states but was passed ' + str(len(initial_state)) + ' initial states.')\n    input_shape = K.int_shape(inputs)\n    timesteps = input_shape[1]\n    if self.unroll and timesteps in [None, 1]:\n        raise ValueError('Cannot unroll a RNN if the time dimension is undefined or equal to 1. \\n- If using a Sequential model, specify the time dimension by passing an `input_shape` or `batch_input_shape` argument to your first layer. If your first layer is an Embedding, you can also use the `input_length` argument.\\n- If using the functional API, specify the time dimension by passing a `shape` or `batch_shape` argument to your Input layer.')\n    kwargs = {}\n    if has_arg(self.cell.call, 'training'):\n        kwargs['training'] = training\n    if constants:\n        if not has_arg(self.cell.call, 'constants'):\n            raise ValueError('RNN cell does not support constants')\n\n        def step(inputs, states):\n            constants = states[-self._num_constants:]\n            states = states[:-self._num_constants]\n            return self.cell.call(inputs, states, constants=constants, **kwargs)\n    else:\n\n        def step(inputs, states):\n            return self.cell.call(inputs, states, **kwargs)\n    last_output, outputs, states = K.rnn(step, inputs, initial_state, constants=constants, go_backwards=self.go_backwards, mask=mask, unroll=self.unroll, input_length=timesteps)\n    if self.stateful:\n        updates = []\n        for i in range(len(states)):\n            updates.append((self.states[i], states[i]))\n        self.add_update(updates, inputs)\n    if self.return_sequences:\n        output = outputs\n    else:\n        output = last_output\n    if getattr(last_output, '_uses_learning_phase', False):\n        output._uses_learning_phase = True\n        for state in states:\n            state._uses_learning_phase = True\n    if self.return_state:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        else:\n            states = list(states)\n        return [output] + states\n    else:\n        return output",
                            "def reset_states(self, states=None):\n    if not self.stateful:\n        raise AttributeError('Layer must be stateful.')\n    batch_size = self.input_spec[0].shape[0]\n    if not batch_size:\n        raise ValueError('If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \\n- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.\\n- If using the functional API, specify the batch size by passing a `batch_shape` argument to your Input layer.')\n    if self.states[0] is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            self.states = [K.zeros((batch_size, dim)) for dim in self.cell.state_size]\n        else:\n            self.states = [K.zeros((batch_size, self.cell.state_size))]\n    elif states is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            for state, dim in zip(self.states, self.cell.state_size):\n                K.set_value(state, np.zeros((batch_size, dim)))\n        else:\n            K.set_value(self.states[0], np.zeros((batch_size, self.cell.state_size)))\n    else:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        if len(states) != len(self.states):\n            raise ValueError('Layer ' + self.name + ' expects ' + str(len(self.states)) + ' states, but it received ' + str(len(states)) + ' state values. Input received: ' + str(states))\n        for index, (value, state) in enumerate(zip(states, self.states)):\n            if hasattr(self.cell.state_size, '__len__'):\n                dim = self.cell.state_size[index]\n            else:\n                dim = self.cell.state_size\n            if value.shape != (batch_size, dim):\n                raise ValueError('State ' + str(index) + ' is incompatible with layer ' + self.name + ': expected shape=' + str((batch_size, dim)) + ', found shape=' + str(value.shape))\n            K.set_value(state, value)",
                            "def get_config(self):\n    config = {'return_sequences': self.return_sequences, 'return_state': self.return_state, 'go_backwards': self.go_backwards, 'stateful': self.stateful, 'unroll': self.unroll}\n    if self._num_constants is not None:\n        config['num_constants'] = self._num_constants\n    cell_config = self.cell.get_config()\n    config['cell'] = {'class_name': self.cell.__class__.__name__, 'config': cell_config}\n    base_config = super(RNN, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                            "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cell = deserialize_layer(config.pop('cell'), custom_objects=custom_objects)\n    num_constants = config.pop('num_constants', None)\n    layer = cls(cell, **config)\n    layer._num_constants = num_constants\n    return layer",
                            "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    if isinstance(self.cell, Layer):\n        return self.cell.trainable_weights\n    return []",
                            "@property\ndef non_trainable_weights(self):\n    if isinstance(self.cell, Layer):\n        if not self.trainable:\n            return self.cell.weights\n        return self.cell.non_trainable_weights\n    return []",
                            "@property\ndef losses(self):\n    layer_losses = super(RNN, self).losses\n    if isinstance(self.cell, Layer):\n        return self.cell.losses + layer_losses\n    return layer_losses",
                            "def get_losses_for(self, inputs=None):\n    if isinstance(self.cell, Layer):\n        cell_losses = self.cell.get_losses_for(inputs)\n        return cell_losses + super(RNN, self).get_losses_for(inputs)\n    return super(RNN, self).get_losses_for(inputs)",
                            "def step(inputs, states):\n    constants = states[-self._num_constants:]\n    states = states[:-self._num_constants]\n    return self.cell.call(inputs, states, constants=constants, **kwargs)",
                            "def step(inputs, states):\n    return self.cell.call(inputs, states, **kwargs)"
                        ],
                        "constructor_variables": [
                            "supports_masking",
                            "unroll",
                            "_states",
                            "constants_spec",
                            "_num_constants",
                            "return_state",
                            "return_sequences",
                            "state_spec",
                            "go_backwards",
                            "stateful",
                            "input_spec",
                            "cell"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
                            "states(self)",
                            "states(self, states)",
                            "compute_output_shape(self, input_shape)",
                            "compute_mask(self, inputs, mask)",
                            "build(self, input_shape)",
                            "get_initial_state(self, inputs)",
                            "__call__(self, inputs, initial_state=None, constants=None, **kwargs)",
                            "call(self, inputs, mask=None, training=None, initial_state=None, constants=None)",
                            "reset_states(self, states=None)",
                            "get_config(self)",
                            "from_config(cls, config, custom_objects=None)",
                            "trainable_weights(self)",
                            "non_trainable_weights(self)",
                            "losses(self)",
                            "get_losses_for(self, inputs=None)",
                            "step(inputs, states)",
                            "step(inputs, states)"
                        ]
                    },
                    "variable_values": [
                        [
                            {
                                "input_shape": {
                                    "variable_value": "(None, None, 5)",
                                    "variable_type": "tuple",
                                    "variable_shape": "3"
                                },
                                "self.cell.state_size": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.cell": {
                                    "variable_value": "<recurrent_test.test_minimal_rnn_cell_non_layer_multiple_states.<locals>.MinimalRNNCell object at 0x12e52b6d0>",
                                    "variable_type": "MinimalRNNCell",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.RNN object at 0x12e4c6090>",
                                    "variable_type": "RNN",
                                    "variable_shape": null
                                },
                                "state_size": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "output_dim": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "output_shape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "False",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "state_shape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "dim": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            },
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "__init__",
                    "function_code": "def __init__(self, units,\n             activation='tanh',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             **kwargs):\n    super(SimpleRNNCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.state_size = self.units\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 795,
                    "end_line": 831,
                    "variables": {
                        "__init__": [
                            810
                        ],
                        "super": [
                            810
                        ],
                        "SimpleRNNCell": [
                            810
                        ],
                        "self": [
                            810,
                            811,
                            812,
                            813,
                            815,
                            816,
                            817,
                            819,
                            820,
                            821,
                            823,
                            824,
                            825,
                            827,
                            828,
                            829,
                            830,
                            831
                        ],
                        "kwargs": [
                            810
                        ],
                        "self.units": [
                            811,
                            829
                        ],
                        "units": [
                            811
                        ],
                        "self.activation": [
                            812
                        ],
                        "activations.get": [
                            812
                        ],
                        "activations": [
                            812
                        ],
                        "activation": [
                            812
                        ],
                        "self.use_bias": [
                            813
                        ],
                        "use_bias": [
                            813
                        ],
                        "self.kernel_initializer": [
                            815
                        ],
                        "initializers.get": [
                            816,
                            817,
                            815
                        ],
                        "initializers": [
                            816,
                            817,
                            815
                        ],
                        "kernel_initializer": [
                            815
                        ],
                        "self.recurrent_initializer": [
                            816
                        ],
                        "recurrent_initializer": [
                            816
                        ],
                        "self.bias_initializer": [
                            817
                        ],
                        "bias_initializer": [
                            817
                        ],
                        "self.kernel_regularizer": [
                            819
                        ],
                        "regularizers.get": [
                            819,
                            820,
                            821
                        ],
                        "regularizers": [
                            819,
                            820,
                            821
                        ],
                        "kernel_regularizer": [
                            819
                        ],
                        "self.recurrent_regularizer": [
                            820
                        ],
                        "recurrent_regularizer": [
                            820
                        ],
                        "self.bias_regularizer": [
                            821
                        ],
                        "bias_regularizer": [
                            821
                        ],
                        "self.kernel_constraint": [
                            823
                        ],
                        "constraints.get": [
                            824,
                            825,
                            823
                        ],
                        "constraints": [
                            824,
                            825,
                            823
                        ],
                        "kernel_constraint": [
                            823
                        ],
                        "self.recurrent_constraint": [
                            824
                        ],
                        "recurrent_constraint": [
                            824
                        ],
                        "self.bias_constraint": [
                            825
                        ],
                        "bias_constraint": [
                            825
                        ],
                        "self.dropout": [
                            827
                        ],
                        "min": [
                            827,
                            828
                        ],
                        "max": [
                            827,
                            828
                        ],
                        "dropout": [
                            827
                        ],
                        "self.recurrent_dropout": [
                            828
                        ],
                        "recurrent_dropout": [
                            828
                        ],
                        "self.state_size": [
                            829
                        ],
                        "self._dropout_mask": [
                            830
                        ],
                        "self._recurrent_dropout_mask": [
                            831
                        ]
                    },
                    "filtered_variables": {
                        "__init__": [
                            810
                        ],
                        "SimpleRNNCell": [
                            810
                        ],
                        "self": [
                            810,
                            811,
                            812,
                            813,
                            815,
                            816,
                            817,
                            819,
                            820,
                            821,
                            823,
                            824,
                            825,
                            827,
                            828,
                            829,
                            830,
                            831
                        ],
                        "kwargs": [
                            810
                        ],
                        "self.units": [
                            811,
                            829
                        ],
                        "units": [
                            811
                        ],
                        "self.activation": [
                            812
                        ],
                        "activations.get": [
                            812
                        ],
                        "activations": [
                            812
                        ],
                        "activation": [
                            812
                        ],
                        "self.use_bias": [
                            813
                        ],
                        "use_bias": [
                            813
                        ],
                        "self.kernel_initializer": [
                            815
                        ],
                        "initializers.get": [
                            816,
                            817,
                            815
                        ],
                        "initializers": [
                            816,
                            817,
                            815
                        ],
                        "kernel_initializer": [
                            815
                        ],
                        "self.recurrent_initializer": [
                            816
                        ],
                        "recurrent_initializer": [
                            816
                        ],
                        "self.bias_initializer": [
                            817
                        ],
                        "bias_initializer": [
                            817
                        ],
                        "self.kernel_regularizer": [
                            819
                        ],
                        "regularizers.get": [
                            819,
                            820,
                            821
                        ],
                        "regularizers": [
                            819,
                            820,
                            821
                        ],
                        "kernel_regularizer": [
                            819
                        ],
                        "self.recurrent_regularizer": [
                            820
                        ],
                        "recurrent_regularizer": [
                            820
                        ],
                        "self.bias_regularizer": [
                            821
                        ],
                        "bias_regularizer": [
                            821
                        ],
                        "self.kernel_constraint": [
                            823
                        ],
                        "constraints.get": [
                            824,
                            825,
                            823
                        ],
                        "constraints": [
                            824,
                            825,
                            823
                        ],
                        "kernel_constraint": [
                            823
                        ],
                        "self.recurrent_constraint": [
                            824
                        ],
                        "recurrent_constraint": [
                            824
                        ],
                        "self.bias_constraint": [
                            825
                        ],
                        "bias_constraint": [
                            825
                        ],
                        "self.dropout": [
                            827
                        ],
                        "dropout": [
                            827
                        ],
                        "self.recurrent_dropout": [
                            828
                        ],
                        "recurrent_dropout": [
                            828
                        ],
                        "self.state_size": [
                            829
                        ],
                        "self._dropout_mask": [
                            830
                        ],
                        "self._recurrent_dropout_mask": [
                            831
                        ]
                    },
                    "diff_line_number": 829,
                    "class_data": {
                        "signature": "class SimpleRNNCell(Layer)",
                        "docstring": "Cell class for SimpleRNN.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, **kwargs):\n    super(SimpleRNNCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.use_bias = use_bias\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n    self.dropout = min(1.0, max(0.0, dropout))\n    self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n    self.state_size = self.units\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                            "def build(self, input_shape):\n    self.kernel = self.add_weight(shape=(input_shape[-1], self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight(shape=(self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
                            "def call(self, inputs, states, training=None):\n    prev_output = states[0]\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(K.ones_like(inputs), self.dropout, training=training)\n    if 0 < self.recurrent_dropout < 1 and self._recurrent_dropout_mask is None:\n        self._recurrent_dropout_mask = _generate_dropout_mask(K.ones_like(prev_output), self.recurrent_dropout, training=training)\n    dp_mask = self._dropout_mask\n    rec_dp_mask = self._recurrent_dropout_mask\n    if dp_mask is not None:\n        h = K.dot(inputs * dp_mask, self.kernel)\n    else:\n        h = K.dot(inputs, self.kernel)\n    if self.bias is not None:\n        h = K.bias_add(h, self.bias)\n    if rec_dp_mask is not None:\n        prev_output *= rec_dp_mask\n    output = h + K.dot(prev_output, self.recurrent_kernel)\n    if self.activation is not None:\n        output = self.activation(output)\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            output._uses_learning_phase = True\n    return (output, [output])",
                            "def get_config(self):\n    config = {'units': self.units, 'activation': activations.serialize(self.activation), 'use_bias': self.use_bias, 'kernel_initializer': initializers.serialize(self.kernel_initializer), 'recurrent_initializer': initializers.serialize(self.recurrent_initializer), 'bias_initializer': initializers.serialize(self.bias_initializer), 'kernel_regularizer': regularizers.serialize(self.kernel_regularizer), 'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer), 'bias_regularizer': regularizers.serialize(self.bias_regularizer), 'kernel_constraint': constraints.serialize(self.kernel_constraint), 'recurrent_constraint': constraints.serialize(self.recurrent_constraint), 'bias_constraint': constraints.serialize(self.bias_constraint), 'dropout': self.dropout, 'recurrent_dropout': self.recurrent_dropout}\n    base_config = super(SimpleRNNCell, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))"
                        ],
                        "constructor_variables": [
                            "bias_regularizer",
                            "dropout",
                            "_recurrent_dropout_mask",
                            "units",
                            "kernel_regularizer",
                            "state_size",
                            "recurrent_initializer",
                            "kernel_initializer",
                            "recurrent_regularizer",
                            "recurrent_constraint",
                            "recurrent_dropout",
                            "use_bias",
                            "kernel_constraint",
                            "_dropout_mask",
                            "activation",
                            "bias_initializer",
                            "bias_constraint"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, **kwargs)",
                            "build(self, input_shape)",
                            "call(self, inputs, states, training=None)",
                            "get_config(self)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "__init__",
                    "function_code": "def __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             reset_after=False,\n             **kwargs):\n    super(GRUCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.recurrent_activation = activations.get(recurrent_activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.implementation = implementation\n    self.reset_after = reset_after\n    self.state_size = self.units\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 1182,
                    "end_line": 1224,
                    "variables": {
                        "__init__": [
                            1200
                        ],
                        "super": [
                            1200
                        ],
                        "GRUCell": [
                            1200
                        ],
                        "self": [
                            1200,
                            1201,
                            1202,
                            1203,
                            1204,
                            1206,
                            1207,
                            1208,
                            1210,
                            1211,
                            1212,
                            1214,
                            1215,
                            1216,
                            1218,
                            1219,
                            1220,
                            1221,
                            1222,
                            1223,
                            1224
                        ],
                        "kwargs": [
                            1200
                        ],
                        "self.units": [
                            1201,
                            1222
                        ],
                        "units": [
                            1201
                        ],
                        "self.activation": [
                            1202
                        ],
                        "activations.get": [
                            1202,
                            1203
                        ],
                        "activations": [
                            1202,
                            1203
                        ],
                        "activation": [
                            1202
                        ],
                        "self.recurrent_activation": [
                            1203
                        ],
                        "recurrent_activation": [
                            1203
                        ],
                        "self.use_bias": [
                            1204
                        ],
                        "use_bias": [
                            1204
                        ],
                        "self.kernel_initializer": [
                            1206
                        ],
                        "initializers.get": [
                            1208,
                            1206,
                            1207
                        ],
                        "initializers": [
                            1208,
                            1206,
                            1207
                        ],
                        "kernel_initializer": [
                            1206
                        ],
                        "self.recurrent_initializer": [
                            1207
                        ],
                        "recurrent_initializer": [
                            1207
                        ],
                        "self.bias_initializer": [
                            1208
                        ],
                        "bias_initializer": [
                            1208
                        ],
                        "self.kernel_regularizer": [
                            1210
                        ],
                        "regularizers.get": [
                            1210,
                            1211,
                            1212
                        ],
                        "regularizers": [
                            1210,
                            1211,
                            1212
                        ],
                        "kernel_regularizer": [
                            1210
                        ],
                        "self.recurrent_regularizer": [
                            1211
                        ],
                        "recurrent_regularizer": [
                            1211
                        ],
                        "self.bias_regularizer": [
                            1212
                        ],
                        "bias_regularizer": [
                            1212
                        ],
                        "self.kernel_constraint": [
                            1214
                        ],
                        "constraints.get": [
                            1216,
                            1214,
                            1215
                        ],
                        "constraints": [
                            1216,
                            1214,
                            1215
                        ],
                        "kernel_constraint": [
                            1214
                        ],
                        "self.recurrent_constraint": [
                            1215
                        ],
                        "recurrent_constraint": [
                            1215
                        ],
                        "self.bias_constraint": [
                            1216
                        ],
                        "bias_constraint": [
                            1216
                        ],
                        "self.dropout": [
                            1218
                        ],
                        "min": [
                            1218,
                            1219
                        ],
                        "max": [
                            1218,
                            1219
                        ],
                        "dropout": [
                            1218
                        ],
                        "self.recurrent_dropout": [
                            1219
                        ],
                        "recurrent_dropout": [
                            1219
                        ],
                        "self.implementation": [
                            1220
                        ],
                        "implementation": [
                            1220
                        ],
                        "self.reset_after": [
                            1221
                        ],
                        "reset_after": [
                            1221
                        ],
                        "self.state_size": [
                            1222
                        ],
                        "self._dropout_mask": [
                            1223
                        ],
                        "self._recurrent_dropout_mask": [
                            1224
                        ]
                    },
                    "filtered_variables": {
                        "__init__": [
                            1200
                        ],
                        "GRUCell": [
                            1200
                        ],
                        "self": [
                            1200,
                            1201,
                            1202,
                            1203,
                            1204,
                            1206,
                            1207,
                            1208,
                            1210,
                            1211,
                            1212,
                            1214,
                            1215,
                            1216,
                            1218,
                            1219,
                            1220,
                            1221,
                            1222,
                            1223,
                            1224
                        ],
                        "kwargs": [
                            1200
                        ],
                        "self.units": [
                            1201,
                            1222
                        ],
                        "units": [
                            1201
                        ],
                        "self.activation": [
                            1202
                        ],
                        "activations.get": [
                            1202,
                            1203
                        ],
                        "activations": [
                            1202,
                            1203
                        ],
                        "activation": [
                            1202
                        ],
                        "self.recurrent_activation": [
                            1203
                        ],
                        "recurrent_activation": [
                            1203
                        ],
                        "self.use_bias": [
                            1204
                        ],
                        "use_bias": [
                            1204
                        ],
                        "self.kernel_initializer": [
                            1206
                        ],
                        "initializers.get": [
                            1208,
                            1206,
                            1207
                        ],
                        "initializers": [
                            1208,
                            1206,
                            1207
                        ],
                        "kernel_initializer": [
                            1206
                        ],
                        "self.recurrent_initializer": [
                            1207
                        ],
                        "recurrent_initializer": [
                            1207
                        ],
                        "self.bias_initializer": [
                            1208
                        ],
                        "bias_initializer": [
                            1208
                        ],
                        "self.kernel_regularizer": [
                            1210
                        ],
                        "regularizers.get": [
                            1210,
                            1211,
                            1212
                        ],
                        "regularizers": [
                            1210,
                            1211,
                            1212
                        ],
                        "kernel_regularizer": [
                            1210
                        ],
                        "self.recurrent_regularizer": [
                            1211
                        ],
                        "recurrent_regularizer": [
                            1211
                        ],
                        "self.bias_regularizer": [
                            1212
                        ],
                        "bias_regularizer": [
                            1212
                        ],
                        "self.kernel_constraint": [
                            1214
                        ],
                        "constraints.get": [
                            1216,
                            1214,
                            1215
                        ],
                        "constraints": [
                            1216,
                            1214,
                            1215
                        ],
                        "kernel_constraint": [
                            1214
                        ],
                        "self.recurrent_constraint": [
                            1215
                        ],
                        "recurrent_constraint": [
                            1215
                        ],
                        "self.bias_constraint": [
                            1216
                        ],
                        "bias_constraint": [
                            1216
                        ],
                        "self.dropout": [
                            1218
                        ],
                        "dropout": [
                            1218
                        ],
                        "self.recurrent_dropout": [
                            1219
                        ],
                        "recurrent_dropout": [
                            1219
                        ],
                        "self.implementation": [
                            1220
                        ],
                        "implementation": [
                            1220
                        ],
                        "self.reset_after": [
                            1221
                        ],
                        "reset_after": [
                            1221
                        ],
                        "self.state_size": [
                            1222
                        ],
                        "self._dropout_mask": [
                            1223
                        ],
                        "self._recurrent_dropout_mask": [
                            1224
                        ]
                    },
                    "diff_line_number": 1222,
                    "class_data": {
                        "signature": "class GRUCell(Layer)",
                        "docstring": "Cell class for the GRU layer.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step\n        (see [activations](../activations.md)).\n        Default: hard sigmoid (`hard_sigmoid`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.\n    implementation: Implementation mode, either 1 or 2.\n        Mode 1 will structure its operations as a larger number of\n        smaller dot products and additions, whereas mode 2 will\n        batch them into fewer, larger operations. These modes will\n        have different performance profiles on different hardware and\n        for different applications.\n    reset_after: GRU convention (whether to apply reset gate after or\n        before matrix multiplication). False = \"before\" (default),\n        True = \"after\" (CuDNN compatible).",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, reset_after=False, **kwargs):\n    super(GRUCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.recurrent_activation = activations.get(recurrent_activation)\n    self.use_bias = use_bias\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n    self.dropout = min(1.0, max(0.0, dropout))\n    self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n    self.implementation = implementation\n    self.reset_after = reset_after\n    self.state_size = self.units\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                            "def build(self, input_shape):\n    input_dim = input_shape[-1]\n    self.kernel = self.add_weight(shape=(input_dim, self.units * 3), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(shape=(self.units, self.units * 3), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        if not self.reset_after:\n            bias_shape = (3 * self.units,)\n        else:\n            bias_shape = (2, 3 * self.units)\n        self.bias = self.add_weight(shape=bias_shape, name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n        if not self.reset_after:\n            self.input_bias, self.recurrent_bias = (self.bias, None)\n        else:\n            self.input_bias = K.flatten(self.bias[0])\n            self.recurrent_bias = K.flatten(self.bias[1])\n    else:\n        self.bias = None\n    self.kernel_z = self.kernel[:, :self.units]\n    self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n    self.kernel_r = self.kernel[:, self.units:self.units * 2]\n    self.recurrent_kernel_r = self.recurrent_kernel[:, self.units:self.units * 2]\n    self.kernel_h = self.kernel[:, self.units * 2:]\n    self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n    if self.use_bias:\n        self.input_bias_z = self.input_bias[:self.units]\n        self.input_bias_r = self.input_bias[self.units:self.units * 2]\n        self.input_bias_h = self.input_bias[self.units * 2:]\n        if self.reset_after:\n            self.recurrent_bias_z = self.recurrent_bias[:self.units]\n            self.recurrent_bias_r = self.recurrent_bias[self.units:self.units * 2]\n            self.recurrent_bias_h = self.recurrent_bias[self.units * 2:]\n    else:\n        self.input_bias_z = None\n        self.input_bias_r = None\n        self.input_bias_h = None\n        if self.reset_after:\n            self.recurrent_bias_z = None\n            self.recurrent_bias_r = None\n            self.recurrent_bias_h = None\n    self.built = True",
                            "def call(self, inputs, states, training=None):\n    h_tm1 = states[0]\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(K.ones_like(inputs), self.dropout, training=training, count=3)\n    if 0 < self.recurrent_dropout < 1 and self._recurrent_dropout_mask is None:\n        self._recurrent_dropout_mask = _generate_dropout_mask(K.ones_like(h_tm1), self.recurrent_dropout, training=training, count=3)\n    dp_mask = self._dropout_mask\n    rec_dp_mask = self._recurrent_dropout_mask\n    if self.implementation == 1:\n        if 0.0 < self.dropout < 1.0:\n            inputs_z = inputs * dp_mask[0]\n            inputs_r = inputs * dp_mask[1]\n            inputs_h = inputs * dp_mask[2]\n        else:\n            inputs_z = inputs\n            inputs_r = inputs\n            inputs_h = inputs\n        x_z = K.dot(inputs_z, self.kernel_z)\n        x_r = K.dot(inputs_r, self.kernel_r)\n        x_h = K.dot(inputs_h, self.kernel_h)\n        if self.use_bias:\n            x_z = K.bias_add(x_z, self.input_bias_z)\n            x_r = K.bias_add(x_r, self.input_bias_r)\n            x_h = K.bias_add(x_h, self.input_bias_h)\n        if 0.0 < self.recurrent_dropout < 1.0:\n            h_tm1_z = h_tm1 * rec_dp_mask[0]\n            h_tm1_r = h_tm1 * rec_dp_mask[1]\n            h_tm1_h = h_tm1 * rec_dp_mask[2]\n        else:\n            h_tm1_z = h_tm1\n            h_tm1_r = h_tm1\n            h_tm1_h = h_tm1\n        recurrent_z = K.dot(h_tm1_z, self.recurrent_kernel_z)\n        recurrent_r = K.dot(h_tm1_r, self.recurrent_kernel_r)\n        if self.reset_after and self.use_bias:\n            recurrent_z = K.bias_add(recurrent_z, self.recurrent_bias_z)\n            recurrent_r = K.bias_add(recurrent_r, self.recurrent_bias_r)\n        z = self.recurrent_activation(x_z + recurrent_z)\n        r = self.recurrent_activation(x_r + recurrent_r)\n        if self.reset_after:\n            recurrent_h = K.dot(h_tm1_h, self.recurrent_kernel_h)\n            if self.use_bias:\n                recurrent_h = K.bias_add(recurrent_h, self.recurrent_bias_h)\n            recurrent_h = r * recurrent_h\n        else:\n            recurrent_h = K.dot(r * h_tm1_h, self.recurrent_kernel_h)\n        hh = self.activation(x_h + recurrent_h)\n    else:\n        if 0.0 < self.dropout < 1.0:\n            inputs *= dp_mask[0]\n        matrix_x = K.dot(inputs, self.kernel)\n        if self.use_bias:\n            matrix_x = K.bias_add(matrix_x, self.input_bias)\n        x_z = matrix_x[:, :self.units]\n        x_r = matrix_x[:, self.units:2 * self.units]\n        x_h = matrix_x[:, 2 * self.units:]\n        if 0.0 < self.recurrent_dropout < 1.0:\n            h_tm1 *= rec_dp_mask[0]\n        if self.reset_after:\n            matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n            if self.use_bias:\n                matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n        else:\n            matrix_inner = K.dot(h_tm1, self.recurrent_kernel[:, :2 * self.units])\n        recurrent_z = matrix_inner[:, :self.units]\n        recurrent_r = matrix_inner[:, self.units:2 * self.units]\n        z = self.recurrent_activation(x_z + recurrent_z)\n        r = self.recurrent_activation(x_r + recurrent_r)\n        if self.reset_after:\n            recurrent_h = r * matrix_inner[:, 2 * self.units:]\n        else:\n            recurrent_h = K.dot(r * h_tm1, self.recurrent_kernel[:, 2 * self.units:])\n        hh = self.activation(x_h + recurrent_h)\n    h = z * h_tm1 + (1 - z) * hh\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            h._uses_learning_phase = True\n    return (h, [h])",
                            "def get_config(self):\n    config = {'units': self.units, 'activation': activations.serialize(self.activation), 'recurrent_activation': activations.serialize(self.recurrent_activation), 'use_bias': self.use_bias, 'kernel_initializer': initializers.serialize(self.kernel_initializer), 'recurrent_initializer': initializers.serialize(self.recurrent_initializer), 'bias_initializer': initializers.serialize(self.bias_initializer), 'kernel_regularizer': regularizers.serialize(self.kernel_regularizer), 'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer), 'bias_regularizer': regularizers.serialize(self.bias_regularizer), 'kernel_constraint': constraints.serialize(self.kernel_constraint), 'recurrent_constraint': constraints.serialize(self.recurrent_constraint), 'bias_constraint': constraints.serialize(self.bias_constraint), 'dropout': self.dropout, 'recurrent_dropout': self.recurrent_dropout, 'implementation': self.implementation, 'reset_after': self.reset_after}\n    base_config = super(GRUCell, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))"
                        ],
                        "constructor_variables": [
                            "dropout",
                            "units",
                            "state_size",
                            "recurrent_initializer",
                            "_dropout_mask",
                            "activation",
                            "bias_initializer",
                            "bias_regularizer",
                            "recurrent_activation",
                            "_recurrent_dropout_mask",
                            "bias_constraint",
                            "recurrent_regularizer",
                            "recurrent_constraint",
                            "recurrent_dropout",
                            "use_bias",
                            "implementation",
                            "kernel_regularizer",
                            "kernel_initializer",
                            "kernel_constraint",
                            "reset_after"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, reset_after=False, **kwargs)",
                            "build(self, input_shape)",
                            "call(self, inputs, states, training=None)",
                            "get_config(self)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                },
                {
                    "function_name": "__init__",
                    "function_code": "def __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             unit_forget_bias=True,\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             **kwargs):\n    super(LSTMCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.recurrent_activation = activations.get(recurrent_activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.unit_forget_bias = unit_forget_bias\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.implementation = implementation\n    self.state_size = (self.units, self.units)\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 1757,
                    "end_line": 1799,
                    "variables": {
                        "__init__": [
                            1775
                        ],
                        "super": [
                            1775
                        ],
                        "LSTMCell": [
                            1775
                        ],
                        "self": [
                            1792,
                            1794,
                            1795,
                            1796,
                            1797,
                            1798,
                            1799,
                            1775,
                            1776,
                            1777,
                            1778,
                            1779,
                            1781,
                            1782,
                            1783,
                            1784,
                            1786,
                            1787,
                            1788,
                            1790,
                            1791
                        ],
                        "kwargs": [
                            1775
                        ],
                        "self.units": [
                            1776,
                            1797
                        ],
                        "units": [
                            1776
                        ],
                        "self.activation": [
                            1777
                        ],
                        "activations.get": [
                            1777,
                            1778
                        ],
                        "activations": [
                            1777,
                            1778
                        ],
                        "activation": [
                            1777
                        ],
                        "self.recurrent_activation": [
                            1778
                        ],
                        "recurrent_activation": [
                            1778
                        ],
                        "self.use_bias": [
                            1779
                        ],
                        "use_bias": [
                            1779
                        ],
                        "self.kernel_initializer": [
                            1781
                        ],
                        "initializers.get": [
                            1781,
                            1782,
                            1783
                        ],
                        "initializers": [
                            1781,
                            1782,
                            1783
                        ],
                        "kernel_initializer": [
                            1781
                        ],
                        "self.recurrent_initializer": [
                            1782
                        ],
                        "recurrent_initializer": [
                            1782
                        ],
                        "self.bias_initializer": [
                            1783
                        ],
                        "bias_initializer": [
                            1783
                        ],
                        "self.unit_forget_bias": [
                            1784
                        ],
                        "unit_forget_bias": [
                            1784
                        ],
                        "self.kernel_regularizer": [
                            1786
                        ],
                        "regularizers.get": [
                            1786,
                            1787,
                            1788
                        ],
                        "regularizers": [
                            1786,
                            1787,
                            1788
                        ],
                        "kernel_regularizer": [
                            1786
                        ],
                        "self.recurrent_regularizer": [
                            1787
                        ],
                        "recurrent_regularizer": [
                            1787
                        ],
                        "self.bias_regularizer": [
                            1788
                        ],
                        "bias_regularizer": [
                            1788
                        ],
                        "self.kernel_constraint": [
                            1790
                        ],
                        "constraints.get": [
                            1792,
                            1790,
                            1791
                        ],
                        "constraints": [
                            1792,
                            1790,
                            1791
                        ],
                        "kernel_constraint": [
                            1790
                        ],
                        "self.recurrent_constraint": [
                            1791
                        ],
                        "recurrent_constraint": [
                            1791
                        ],
                        "self.bias_constraint": [
                            1792
                        ],
                        "bias_constraint": [
                            1792
                        ],
                        "self.dropout": [
                            1794
                        ],
                        "min": [
                            1794,
                            1795
                        ],
                        "max": [
                            1794,
                            1795
                        ],
                        "dropout": [
                            1794
                        ],
                        "self.recurrent_dropout": [
                            1795
                        ],
                        "recurrent_dropout": [
                            1795
                        ],
                        "self.implementation": [
                            1796
                        ],
                        "implementation": [
                            1796
                        ],
                        "self.state_size": [
                            1797
                        ],
                        "self._dropout_mask": [
                            1798
                        ],
                        "self._recurrent_dropout_mask": [
                            1799
                        ]
                    },
                    "filtered_variables": {
                        "__init__": [
                            1775
                        ],
                        "LSTMCell": [
                            1775
                        ],
                        "self": [
                            1792,
                            1794,
                            1795,
                            1796,
                            1797,
                            1798,
                            1799,
                            1775,
                            1776,
                            1777,
                            1778,
                            1779,
                            1781,
                            1782,
                            1783,
                            1784,
                            1786,
                            1787,
                            1788,
                            1790,
                            1791
                        ],
                        "kwargs": [
                            1775
                        ],
                        "self.units": [
                            1776,
                            1797
                        ],
                        "units": [
                            1776
                        ],
                        "self.activation": [
                            1777
                        ],
                        "activations.get": [
                            1777,
                            1778
                        ],
                        "activations": [
                            1777,
                            1778
                        ],
                        "activation": [
                            1777
                        ],
                        "self.recurrent_activation": [
                            1778
                        ],
                        "recurrent_activation": [
                            1778
                        ],
                        "self.use_bias": [
                            1779
                        ],
                        "use_bias": [
                            1779
                        ],
                        "self.kernel_initializer": [
                            1781
                        ],
                        "initializers.get": [
                            1781,
                            1782,
                            1783
                        ],
                        "initializers": [
                            1781,
                            1782,
                            1783
                        ],
                        "kernel_initializer": [
                            1781
                        ],
                        "self.recurrent_initializer": [
                            1782
                        ],
                        "recurrent_initializer": [
                            1782
                        ],
                        "self.bias_initializer": [
                            1783
                        ],
                        "bias_initializer": [
                            1783
                        ],
                        "self.unit_forget_bias": [
                            1784
                        ],
                        "unit_forget_bias": [
                            1784
                        ],
                        "self.kernel_regularizer": [
                            1786
                        ],
                        "regularizers.get": [
                            1786,
                            1787,
                            1788
                        ],
                        "regularizers": [
                            1786,
                            1787,
                            1788
                        ],
                        "kernel_regularizer": [
                            1786
                        ],
                        "self.recurrent_regularizer": [
                            1787
                        ],
                        "recurrent_regularizer": [
                            1787
                        ],
                        "self.bias_regularizer": [
                            1788
                        ],
                        "bias_regularizer": [
                            1788
                        ],
                        "self.kernel_constraint": [
                            1790
                        ],
                        "constraints.get": [
                            1792,
                            1790,
                            1791
                        ],
                        "constraints": [
                            1792,
                            1790,
                            1791
                        ],
                        "kernel_constraint": [
                            1790
                        ],
                        "self.recurrent_constraint": [
                            1791
                        ],
                        "recurrent_constraint": [
                            1791
                        ],
                        "self.bias_constraint": [
                            1792
                        ],
                        "bias_constraint": [
                            1792
                        ],
                        "self.dropout": [
                            1794
                        ],
                        "dropout": [
                            1794
                        ],
                        "self.recurrent_dropout": [
                            1795
                        ],
                        "recurrent_dropout": [
                            1795
                        ],
                        "self.implementation": [
                            1796
                        ],
                        "implementation": [
                            1796
                        ],
                        "self.state_size": [
                            1797
                        ],
                        "self._dropout_mask": [
                            1798
                        ],
                        "self._recurrent_dropout_mask": [
                            1799
                        ]
                    },
                    "diff_line_number": 1797,
                    "class_data": {
                        "signature": "class LSTMCell(Layer)",
                        "docstring": "Cell class for the LSTM layer.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step\n        (see [activations](../activations.md)).\n        Default: hard sigmoid (`hard_sigmoid`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).x\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    unit_forget_bias: Boolean.\n        If True, add 1 to the bias of the forget gate at initialization.\n        Setting it to true will also force `bias_initializer=\"zeros\"`.\n        This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.\n    implementation: Implementation mode, either 1 or 2.\n        Mode 1 will structure its operations as a larger number of\n        smaller dot products and additions, whereas mode 2 will\n        batch them into fewer, larger operations. These modes will\n        have different performance profiles on different hardware and\n        for different applications.",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, **kwargs):\n    super(LSTMCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.recurrent_activation = activations.get(recurrent_activation)\n    self.use_bias = use_bias\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.unit_forget_bias = unit_forget_bias\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n    self.dropout = min(1.0, max(0.0, dropout))\n    self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n    self.implementation = implementation\n    self.state_size = (self.units, self.units)\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                            "def build(self, input_shape):\n    input_dim = input_shape[-1]\n    self.kernel = self.add_weight(shape=(input_dim, self.units * 4), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(shape=(self.units, self.units * 4), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        if self.unit_forget_bias:\n\n            def bias_initializer(_, *args, **kwargs):\n                return K.concatenate([self.bias_initializer((self.units,), *args, **kwargs), initializers.Ones()((self.units,), *args, **kwargs), self.bias_initializer((self.units * 2,), *args, **kwargs)])\n        else:\n            bias_initializer = self.bias_initializer\n        self.bias = self.add_weight(shape=(self.units * 4,), name='bias', initializer=bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.kernel_i = self.kernel[:, :self.units]\n    self.kernel_f = self.kernel[:, self.units:self.units * 2]\n    self.kernel_c = self.kernel[:, self.units * 2:self.units * 3]\n    self.kernel_o = self.kernel[:, self.units * 3:]\n    self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n    self.recurrent_kernel_f = self.recurrent_kernel[:, self.units:self.units * 2]\n    self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2:self.units * 3]\n    self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n    if self.use_bias:\n        self.bias_i = self.bias[:self.units]\n        self.bias_f = self.bias[self.units:self.units * 2]\n        self.bias_c = self.bias[self.units * 2:self.units * 3]\n        self.bias_o = self.bias[self.units * 3:]\n    else:\n        self.bias_i = None\n        self.bias_f = None\n        self.bias_c = None\n        self.bias_o = None\n    self.built = True",
                            "def call(self, inputs, states, training=None):\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(K.ones_like(inputs), self.dropout, training=training, count=4)\n    if 0 < self.recurrent_dropout < 1 and self._recurrent_dropout_mask is None:\n        self._recurrent_dropout_mask = _generate_dropout_mask(K.ones_like(states[0]), self.recurrent_dropout, training=training, count=4)\n    dp_mask = self._dropout_mask\n    rec_dp_mask = self._recurrent_dropout_mask\n    h_tm1 = states[0]\n    c_tm1 = states[1]\n    if self.implementation == 1:\n        if 0 < self.dropout < 1.0:\n            inputs_i = inputs * dp_mask[0]\n            inputs_f = inputs * dp_mask[1]\n            inputs_c = inputs * dp_mask[2]\n            inputs_o = inputs * dp_mask[3]\n        else:\n            inputs_i = inputs\n            inputs_f = inputs\n            inputs_c = inputs\n            inputs_o = inputs\n        x_i = K.dot(inputs_i, self.kernel_i)\n        x_f = K.dot(inputs_f, self.kernel_f)\n        x_c = K.dot(inputs_c, self.kernel_c)\n        x_o = K.dot(inputs_o, self.kernel_o)\n        if self.use_bias:\n            x_i = K.bias_add(x_i, self.bias_i)\n            x_f = K.bias_add(x_f, self.bias_f)\n            x_c = K.bias_add(x_c, self.bias_c)\n            x_o = K.bias_add(x_o, self.bias_o)\n        if 0 < self.recurrent_dropout < 1.0:\n            h_tm1_i = h_tm1 * rec_dp_mask[0]\n            h_tm1_f = h_tm1 * rec_dp_mask[1]\n            h_tm1_c = h_tm1 * rec_dp_mask[2]\n            h_tm1_o = h_tm1 * rec_dp_mask[3]\n        else:\n            h_tm1_i = h_tm1\n            h_tm1_f = h_tm1\n            h_tm1_c = h_tm1\n            h_tm1_o = h_tm1\n        i = self.recurrent_activation(x_i + K.dot(h_tm1_i, self.recurrent_kernel_i))\n        f = self.recurrent_activation(x_f + K.dot(h_tm1_f, self.recurrent_kernel_f))\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c, self.recurrent_kernel_c))\n        o = self.recurrent_activation(x_o + K.dot(h_tm1_o, self.recurrent_kernel_o))\n    else:\n        if 0.0 < self.dropout < 1.0:\n            inputs *= dp_mask[0]\n        z = K.dot(inputs, self.kernel)\n        if 0.0 < self.recurrent_dropout < 1.0:\n            h_tm1 *= rec_dp_mask[0]\n        z += K.dot(h_tm1, self.recurrent_kernel)\n        if self.use_bias:\n            z = K.bias_add(z, self.bias)\n        z0 = z[:, :self.units]\n        z1 = z[:, self.units:2 * self.units]\n        z2 = z[:, 2 * self.units:3 * self.units]\n        z3 = z[:, 3 * self.units:]\n        i = self.recurrent_activation(z0)\n        f = self.recurrent_activation(z1)\n        c = f * c_tm1 + i * self.activation(z2)\n        o = self.recurrent_activation(z3)\n    h = o * self.activation(c)\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            h._uses_learning_phase = True\n    return (h, [h, c])",
                            "def get_config(self):\n    config = {'units': self.units, 'activation': activations.serialize(self.activation), 'recurrent_activation': activations.serialize(self.recurrent_activation), 'use_bias': self.use_bias, 'kernel_initializer': initializers.serialize(self.kernel_initializer), 'recurrent_initializer': initializers.serialize(self.recurrent_initializer), 'bias_initializer': initializers.serialize(self.bias_initializer), 'unit_forget_bias': self.unit_forget_bias, 'kernel_regularizer': regularizers.serialize(self.kernel_regularizer), 'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer), 'bias_regularizer': regularizers.serialize(self.bias_regularizer), 'kernel_constraint': constraints.serialize(self.kernel_constraint), 'recurrent_constraint': constraints.serialize(self.recurrent_constraint), 'bias_constraint': constraints.serialize(self.bias_constraint), 'dropout': self.dropout, 'recurrent_dropout': self.recurrent_dropout, 'implementation': self.implementation}\n    base_config = super(LSTMCell, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                            "def bias_initializer(_, *args, **kwargs):\n    return K.concatenate([self.bias_initializer((self.units,), *args, **kwargs), initializers.Ones()((self.units,), *args, **kwargs), self.bias_initializer((self.units * 2,), *args, **kwargs)])"
                        ],
                        "constructor_variables": [
                            "dropout",
                            "units",
                            "state_size",
                            "recurrent_initializer",
                            "_dropout_mask",
                            "activation",
                            "bias_initializer",
                            "bias_regularizer",
                            "recurrent_activation",
                            "_recurrent_dropout_mask",
                            "bias_constraint",
                            "unit_forget_bias",
                            "recurrent_regularizer",
                            "recurrent_constraint",
                            "recurrent_dropout",
                            "use_bias",
                            "implementation",
                            "kernel_regularizer",
                            "kernel_initializer",
                            "kernel_constraint"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, **kwargs)",
                            "build(self, input_shape)",
                            "call(self, inputs, states, training=None)",
                            "get_config(self)",
                            "bias_initializer(_, *args, **kwargs)"
                        ]
                    },
                    "variable_values": [
                        [
                            {},
                            {}
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "(one size per state). In this case, the first entry\n                (`state_size[0]`) should be the same as\n                the size of the cell output.\n            It is also possible for `cell` to be a list of RNN cell instances,",
                    "start_line": 226,
                    "end_line": 229
                }
            ],
            "inscope_functions": [
                "def _generate_dropout_mask(ones, rate, training=None, count=1):\n    def dropped_inputs():\n        return K.dropout(ones, rate)\n\n    if count > 1:\n        return [K.in_train_phase(\n            dropped_inputs,\n            ones,\n            training=training) for _ in range(count)]\n    return K.in_train_phase(\n        dropped_inputs,\n        ones,\n        training=training)",
                "def _standardize_args(inputs, initial_state, constants, num_constants):\n    \"\"\"Standardize `__call__` to a single list of tensor inputs.\n\n    When running a model loaded from file, the input tensors\n    `initial_state` and `constants` can be passed to `RNN.__call__` as part\n    of `inputs` instead of by the dedicated keyword arguments. This method\n    makes sure the arguments are separated and that `initial_state` and\n    `constants` are lists of tensors (or None).\n\n    # Arguments\n        inputs: tensor or list/tuple of tensors\n        initial_state: tensor or list of tensors or None\n        constants: tensor or list of tensors or None\n\n    # Returns\n        inputs: tensor\n        initial_state: list of tensors or None\n        constants: list of tensors or None\n    \"\"\"\n    if isinstance(inputs, list):\n        assert initial_state is None and constants is None\n        if num_constants is not None:\n            constants = inputs[-num_constants:]\n            inputs = inputs[:-num_constants]\n        if len(inputs) > 1:\n            initial_state = inputs[1:]\n        inputs = inputs[0]\n\n    def to_list_or_none(x):\n        if x is None or isinstance(x, list):\n            return x\n        if isinstance(x, tuple):\n            return list(x)\n        return [x]\n\n    initial_state = to_list_or_none(initial_state)\n    constants = to_list_or_none(constants)\n\n    return inputs, initial_state, constants",
                "def __init__(self, cells, **kwargs):\n    for cell in cells:\n        if not hasattr(cell, 'call'):\n            raise ValueError('All cells must have a `call` method. '\n                             'received cells:', cells)\n        if not hasattr(cell, 'state_size'):\n            raise ValueError('All cells must have a '\n                             '`state_size` attribute. '\n                             'received cells:', cells)\n    self.cells = cells\n    super(StackedRNNCells, self).__init__(**kwargs)",
                "@property\ndef state_size(self):\n    # States are a flat list\n    # in reverse order of the cell stack.\n    # This allows to preserve the requirement\n    # `stack.state_size[0] == output_dim`.\n    # e.g. states of a 2-layer LSTM would be\n    # `[h2, c2, h1, c1]`\n    # (assuming one LSTM has states [h, c])\n    state_size = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            state_size += list(cell.state_size)\n        else:\n            state_size.append(cell.state_size)\n    return tuple(state_size)",
                "def call(self, inputs, states, constants=None, **kwargs):\n    # Recover per-cell states.\n    nested_states = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            nested_states.append(states[:len(cell.state_size)])\n            states = states[len(cell.state_size):]\n        else:\n            nested_states.append([states[0]])\n            states = states[1:]\n    nested_states = nested_states[::-1]\n\n    # Call the cells in order and store the returned states.\n    new_nested_states = []\n    for cell, states in zip(self.cells, nested_states):\n        if has_arg(cell.call, 'constants'):\n            inputs, states = cell.call(inputs, states,\n                                       constants=constants,\n                                       **kwargs)\n        else:\n            inputs, states = cell.call(inputs, states, **kwargs)\n        new_nested_states.append(states)\n\n    # Format the new states as a flat list\n    # in reverse cell order.\n    states = []\n    for cell_states in new_nested_states[::-1]:\n        states += cell_states\n    return inputs, states",
                "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        constants_shape = input_shape[1:]\n        input_shape = input_shape[0]\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            if has_arg(cell.call, 'constants'):\n                cell.build([input_shape] + constants_shape)\n            else:\n                cell.build(input_shape)\n        if hasattr(cell.state_size, '__len__'):\n            output_dim = cell.state_size[0]\n        else:\n            output_dim = cell.state_size\n        input_shape = (input_shape[0], output_dim)\n    self.built = True",
                "def get_config(self):\n    cells = []\n    for cell in self.cells:\n        cells.append({'class_name': cell.__class__.__name__,\n                      'config': cell.get_config()})\n    config = {'cells': cells}\n    base_config = super(StackedRNNCells, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cells = []\n    for cell_config in config.pop('cells'):\n        cells.append(deserialize_layer(cell_config,\n                                       custom_objects=custom_objects))\n    return cls(cells, **config)",
                "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.trainable_weights\n    return weights",
                "@property\ndef non_trainable_weights(self):\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.non_trainable_weights\n    if not self.trainable:\n        trainable_weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                trainable_weights += cell.trainable_weights\n        return trainable_weights + weights\n    return weights",
                "def get_weights(self):\n    \"\"\"Retrieves the weights of the model.\n\n    # Returns\n        A flat list of Numpy arrays.\n    \"\"\"\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.weights\n    return K.batch_get_value(weights)",
                "def set_weights(self, weights):\n    \"\"\"Sets the weights of the model.\n\n    # Arguments\n        weights: A list of Numpy arrays with shapes and types matching\n            the output of `model.get_weights()`.\n    \"\"\"\n    tuples = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            num_param = len(cell.weights)\n            weights = weights[:num_param]\n            for sw, w in zip(cell.weights, weights):\n                tuples.append((sw, w))\n            weights = weights[num_param:]\n    K.batch_set_value(tuples)",
                "@property\ndef losses(self):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.losses\n            losses += cell_losses\n    return losses",
                "def get_losses_for(self, inputs=None):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.get_losses_for(inputs)\n            losses += cell_losses\n    return losses",
                "def __init__(self, cell,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if isinstance(cell, (list, tuple)):\n        cell = StackedRNNCells(cell)\n    if not hasattr(cell, 'call'):\n        raise ValueError('`cell` should have a `call` method. '\n                         'The RNN was passed:', cell)\n    if not hasattr(cell, 'state_size'):\n        raise ValueError('The RNN cell should have '\n                         'an attribute `state_size` '\n                         '(tuple of integers, '\n                         'one integer per RNN state).')\n    super(RNN, self).__init__(**kwargs)\n    self.cell = cell\n    self.return_sequences = return_sequences\n    self.return_state = return_state\n    self.go_backwards = go_backwards\n    self.stateful = stateful\n    self.unroll = unroll\n\n    self.supports_masking = True\n    self.input_spec = [InputSpec(ndim=3)]\n    self.state_spec = None\n    self._states = None\n    self.constants_spec = None\n    self._num_constants = None",
                "@property\ndef states(self):\n    if self._states is None:\n        if isinstance(self.cell.state_size, int):\n            num_states = 1\n        else:\n            num_states = len(self.cell.state_size)\n        return [None for _ in range(num_states)]\n    return self._states",
                "@states.setter\ndef states(self, states):\n    self._states = states",
                "def compute_output_shape(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = self.cell.state_size\n    else:\n        state_size = [self.cell.state_size]\n    output_dim = state_size[0]\n\n    if self.return_sequences:\n        output_shape = (input_shape[0], input_shape[1], output_dim)\n    else:\n        output_shape = (input_shape[0], output_dim)\n\n    if self.return_state:\n        state_shape = [(input_shape[0], dim) for dim in state_size]\n        return [output_shape] + state_shape\n    else:\n        return output_shape",
                "def compute_mask(self, inputs, mask):\n    if isinstance(mask, list):\n        mask = mask[0]\n    output_mask = mask if self.return_sequences else None\n    if self.return_state:\n        state_mask = [None for _ in self.states]\n        return [output_mask] + state_mask\n    else:\n        return output_mask",
                "def build(self, input_shape):\n    # Note input_shape will be list of shapes of initial states and\n    # constants if these are passed in __call__.\n    if self._num_constants is not None:\n        constants_shape = input_shape[-self._num_constants:]\n    else:\n        constants_shape = None\n\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n\n    batch_size = input_shape[0] if self.stateful else None\n    input_dim = input_shape[-1]\n    self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))\n\n    # allow cell (if layer) to build before we set or validate state_spec\n    if isinstance(self.cell, Layer):\n        step_input_shape = (input_shape[0],) + input_shape[2:]\n        if constants_shape is not None:\n            self.cell.build([step_input_shape] + constants_shape)\n        else:\n            self.cell.build(step_input_shape)\n\n    # set or validate state_spec\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = list(self.cell.state_size)\n    else:\n        state_size = [self.cell.state_size]\n\n    if self.state_spec is not None:\n        # initial_state was passed in call, check compatibility\n        if [spec.shape[-1] for spec in self.state_spec] != state_size:\n            raise ValueError(\n                'An `initial_state` was passed that is not compatible with '\n                '`cell.state_size`. Received `state_spec`={}; '\n                'however `cell.state_size` is '\n                '{}'.format(self.state_spec, self.cell.state_size))\n    else:\n        self.state_spec = [InputSpec(shape=(None, dim))\n                           for dim in state_size]\n    if self.stateful:\n        self.reset_states()\n    self.built = True",
                "def get_initial_state(self, inputs):\n    # build an all-zero tensor of shape (samples, output_dim)\n    initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n    initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n    initial_state = K.expand_dims(initial_state)  # (samples, 1)\n    if hasattr(self.cell.state_size, '__len__'):\n        return [K.tile(initial_state, [1, dim])\n                for dim in self.cell.state_size]\n    else:\n        return [K.tile(initial_state, [1, self.cell.state_size])]",
                "def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n    inputs, initial_state, constants = _standardize_args(\n        inputs, initial_state, constants, self._num_constants)\n\n    if initial_state is None and constants is None:\n        return super(RNN, self).__call__(inputs, **kwargs)\n\n    # If any of `initial_state` or `constants` are specified and are Keras\n    # tensors, then add them to the inputs and temporarily modify the\n    # input_spec to include them.\n\n    additional_inputs = []\n    additional_specs = []\n    if initial_state is not None:\n        kwargs['initial_state'] = initial_state\n        additional_inputs += initial_state\n        self.state_spec = [InputSpec(shape=K.int_shape(state))\n                           for state in initial_state]\n        additional_specs += self.state_spec\n    if constants is not None:\n        kwargs['constants'] = constants\n        additional_inputs += constants\n        self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n                               for constant in constants]\n        self._num_constants = len(constants)\n        additional_specs += self.constants_spec\n    # at this point additional_inputs cannot be empty\n    is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n    for tensor in additional_inputs:\n        if K.is_keras_tensor(tensor) != is_keras_tensor:\n            raise ValueError('The initial state or constants of an RNN'\n                             ' layer cannot be specified with a mix of'\n                             ' Keras tensors and non-Keras tensors'\n                             ' (a \"Keras tensor\" is a tensor that was'\n                             ' returned by a Keras layer, or by `Input`)')\n\n    if is_keras_tensor:\n        # Compute the full input spec, including state and constants\n        full_input = [inputs] + additional_inputs\n        full_input_spec = self.input_spec + additional_specs\n        # Perform the call with temporarily replaced input_spec\n        original_input_spec = self.input_spec\n        self.input_spec = full_input_spec\n        output = super(RNN, self).__call__(full_input, **kwargs)\n        self.input_spec = original_input_spec\n        return output\n    else:\n        return super(RNN, self).__call__(inputs, **kwargs)",
                "def call(self,\n         inputs,\n         mask=None,\n         training=None,\n         initial_state=None,\n         constants=None):\n    # input shape: `(samples, time (padded with zeros), input_dim)`\n    # note that the .build() method of subclasses MUST define\n    # self.input_spec and self.state_spec with complete input shapes.\n    if isinstance(inputs, list):\n        # get initial_state from full input spec\n        # as they could be copied to multiple GPU.\n        if self._num_constants is None:\n            initial_state = inputs[1:]\n        else:\n            initial_state = inputs[1:-self._num_constants]\n        if len(initial_state) == 0:\n            initial_state = None\n        inputs = inputs[0]\n    if initial_state is not None:\n        pass\n    elif self.stateful:\n        initial_state = self.states\n    else:\n        initial_state = self.get_initial_state(inputs)\n\n    if isinstance(mask, list):\n        mask = mask[0]\n\n    if len(initial_state) != len(self.states):\n        raise ValueError('Layer has ' + str(len(self.states)) +\n                         ' states but was passed ' +\n                         str(len(initial_state)) +\n                         ' initial states.')\n    input_shape = K.int_shape(inputs)\n    timesteps = input_shape[1]\n    if self.unroll and timesteps in [None, 1]:\n        raise ValueError('Cannot unroll a RNN if the '\n                         'time dimension is undefined or equal to 1. \\n'\n                         '- If using a Sequential model, '\n                         'specify the time dimension by passing '\n                         'an `input_shape` or `batch_input_shape` '\n                         'argument to your first layer. If your '\n                         'first layer is an Embedding, you can '\n                         'also use the `input_length` argument.\\n'\n                         '- If using the functional API, specify '\n                         'the time dimension by passing a `shape` '\n                         'or `batch_shape` argument to your Input layer.')\n\n    kwargs = {}\n    if has_arg(self.cell.call, 'training'):\n        kwargs['training'] = training\n\n    if constants:\n        if not has_arg(self.cell.call, 'constants'):\n            raise ValueError('RNN cell does not support constants')\n\n        def step(inputs, states):\n            constants = states[-self._num_constants:]\n            states = states[:-self._num_constants]\n            return self.cell.call(inputs, states, constants=constants,\n                                  **kwargs)\n    else:\n        def step(inputs, states):\n            return self.cell.call(inputs, states, **kwargs)\n\n    last_output, outputs, states = K.rnn(step,\n                                         inputs,\n                                         initial_state,\n                                         constants=constants,\n                                         go_backwards=self.go_backwards,\n                                         mask=mask,\n                                         unroll=self.unroll,\n                                         input_length=timesteps)\n    if self.stateful:\n        updates = []\n        for i in range(len(states)):\n            updates.append((self.states[i], states[i]))\n        self.add_update(updates, inputs)\n\n    if self.return_sequences:\n        output = outputs\n    else:\n        output = last_output\n\n    # Properly set learning phase\n    if getattr(last_output, '_uses_learning_phase', False):\n        output._uses_learning_phase = True\n        for state in states:\n            state._uses_learning_phase = True\n\n    if self.return_state:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        else:\n            states = list(states)\n        return [output] + states\n    else:\n        return output",
                "def reset_states(self, states=None):\n    if not self.stateful:\n        raise AttributeError('Layer must be stateful.')\n    batch_size = self.input_spec[0].shape[0]\n    if not batch_size:\n        raise ValueError('If a RNN is stateful, it needs to know '\n                         'its batch size. Specify the batch size '\n                         'of your input tensors: \\n'\n                         '- If using a Sequential model, '\n                         'specify the batch size by passing '\n                         'a `batch_input_shape` '\n                         'argument to your first layer.\\n'\n                         '- If using the functional API, specify '\n                         'the batch size by passing a '\n                         '`batch_shape` argument to your Input layer.')\n    # initialize state if None\n    if self.states[0] is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            self.states = [K.zeros((batch_size, dim))\n                           for dim in self.cell.state_size]\n        else:\n            self.states = [K.zeros((batch_size, self.cell.state_size))]\n    elif states is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            for state, dim in zip(self.states, self.cell.state_size):\n                K.set_value(state, np.zeros((batch_size, dim)))\n        else:\n            K.set_value(self.states[0],\n                        np.zeros((batch_size, self.cell.state_size)))\n    else:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        if len(states) != len(self.states):\n            raise ValueError('Layer ' + self.name + ' expects ' +\n                             str(len(self.states)) + ' states, '\n                             'but it received ' + str(len(states)) +\n                             ' state values. Input received: ' +\n                             str(states))\n        for index, (value, state) in enumerate(zip(states, self.states)):\n            if hasattr(self.cell.state_size, '__len__'):\n                dim = self.cell.state_size[index]\n            else:\n                dim = self.cell.state_size\n            if value.shape != (batch_size, dim):\n                raise ValueError('State ' + str(index) +\n                                 ' is incompatible with layer ' +\n                                 self.name + ': expected shape=' +\n                                 str((batch_size, dim)) +\n                                 ', found shape=' + str(value.shape))\n            # TODO: consider batch calls to `set_value`.\n            K.set_value(state, value)",
                "def get_config(self):\n    config = {'return_sequences': self.return_sequences,\n              'return_state': self.return_state,\n              'go_backwards': self.go_backwards,\n              'stateful': self.stateful,\n              'unroll': self.unroll}\n    if self._num_constants is not None:\n        config['num_constants'] = self._num_constants\n\n    cell_config = self.cell.get_config()\n    config['cell'] = {'class_name': self.cell.__class__.__name__,\n                      'config': cell_config}\n    base_config = super(RNN, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cell = deserialize_layer(config.pop('cell'),\n                             custom_objects=custom_objects)\n    num_constants = config.pop('num_constants', None)\n    layer = cls(cell, **config)\n    layer._num_constants = num_constants\n    return layer",
                "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    if isinstance(self.cell, Layer):\n        return self.cell.trainable_weights\n    return []",
                "@property\ndef non_trainable_weights(self):\n    if isinstance(self.cell, Layer):\n        if not self.trainable:\n            return self.cell.weights\n        return self.cell.non_trainable_weights\n    return []",
                "@property\ndef losses(self):\n    layer_losses = super(RNN, self).losses\n    if isinstance(self.cell, Layer):\n        return self.cell.losses + layer_losses\n    return layer_losses",
                "def get_losses_for(self, inputs=None):\n    if isinstance(self.cell, Layer):\n        cell_losses = self.cell.get_losses_for(inputs)\n        return cell_losses + super(RNN, self).get_losses_for(inputs)\n    return super(RNN, self).get_losses_for(inputs)",
                "def __init__(self, units,\n             activation='tanh',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             **kwargs):\n    super(SimpleRNNCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.state_size = self.units\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                "def build(self, input_shape):\n    self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                  name='kernel',\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units),\n        name='recurrent_kernel',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight(shape=(self.units,),\n                                    name='bias',\n                                    initializer=self.bias_initializer,\n                                    regularizer=self.bias_regularizer,\n                                    constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
                "def call(self, inputs, states, training=None):\n    prev_output = states[0]\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(\n            K.ones_like(inputs),\n            self.dropout,\n            training=training)\n    if (0 < self.recurrent_dropout < 1 and\n            self._recurrent_dropout_mask is None):\n        self._recurrent_dropout_mask = _generate_dropout_mask(\n            K.ones_like(prev_output),\n            self.recurrent_dropout,\n            training=training)\n\n    dp_mask = self._dropout_mask\n    rec_dp_mask = self._recurrent_dropout_mask\n\n    if dp_mask is not None:\n        h = K.dot(inputs * dp_mask, self.kernel)\n    else:\n        h = K.dot(inputs, self.kernel)\n    if self.bias is not None:\n        h = K.bias_add(h, self.bias)\n\n    if rec_dp_mask is not None:\n        prev_output *= rec_dp_mask\n    output = h + K.dot(prev_output, self.recurrent_kernel)\n    if self.activation is not None:\n        output = self.activation(output)\n\n    # Properly set learning phase on output tensor.\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            output._uses_learning_phase = True\n    return output, [output]",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout}\n    base_config = super(SimpleRNNCell, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@interfaces.legacy_recurrent_support\ndef __init__(self, units,\n             activation='tanh',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             activity_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if 'implementation' in kwargs:\n        kwargs.pop('implementation')\n        warnings.warn('The `implementation` argument '\n                      'in `SimpleRNN` has been deprecated. '\n                      'Please remove it from your layer call.')\n    if K.backend() == 'theano' and (dropout or recurrent_dropout):\n        warnings.warn(\n            'RNN dropout is no longer supported with the Theano backend '\n            'due to technical limitations. '\n            'You can either set `dropout` and `recurrent_dropout` to 0, '\n            'or use the TensorFlow backend.')\n        dropout = 0.\n        recurrent_dropout = 0.\n\n    cell = SimpleRNNCell(units,\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         recurrent_initializer=recurrent_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         recurrent_regularizer=recurrent_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         recurrent_constraint=recurrent_constraint,\n                         bias_constraint=bias_constraint,\n                         dropout=dropout,\n                         recurrent_dropout=recurrent_dropout)\n    super(SimpleRNN, self).__init__(cell,\n                                    return_sequences=return_sequences,\n                                    return_state=return_state,\n                                    go_backwards=go_backwards,\n                                    stateful=stateful,\n                                    unroll=unroll,\n                                    **kwargs)\n    self.activity_regularizer = regularizers.get(activity_regularizer)",
                "def call(self, inputs, mask=None, training=None, initial_state=None):\n    self.cell._dropout_mask = None\n    self.cell._recurrent_dropout_mask = None\n    return super(SimpleRNN, self).call(inputs,\n                                       mask=mask,\n                                       training=training,\n                                       initial_state=initial_state)",
                "@property\ndef units(self):\n    return self.cell.units",
                "@property\ndef activation(self):\n    return self.cell.activation",
                "@property\ndef use_bias(self):\n    return self.cell.use_bias",
                "@property\ndef kernel_initializer(self):\n    return self.cell.kernel_initializer",
                "@property\ndef recurrent_initializer(self):\n    return self.cell.recurrent_initializer",
                "@property\ndef bias_initializer(self):\n    return self.cell.bias_initializer",
                "@property\ndef kernel_regularizer(self):\n    return self.cell.kernel_regularizer",
                "@property\ndef recurrent_regularizer(self):\n    return self.cell.recurrent_regularizer",
                "@property\ndef bias_regularizer(self):\n    return self.cell.bias_regularizer",
                "@property\ndef kernel_constraint(self):\n    return self.cell.kernel_constraint",
                "@property\ndef recurrent_constraint(self):\n    return self.cell.recurrent_constraint",
                "@property\ndef bias_constraint(self):\n    return self.cell.bias_constraint",
                "@property\ndef dropout(self):\n    return self.cell.dropout",
                "@property\ndef recurrent_dropout(self):\n    return self.cell.recurrent_dropout",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout}\n    base_config = super(SimpleRNN, self).get_config()\n    del base_config['cell']\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config):\n    if 'implementation' in config:\n        config.pop('implementation')\n    return cls(**config)",
                "def __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             reset_after=False,\n             **kwargs):\n    super(GRUCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.recurrent_activation = activations.get(recurrent_activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.implementation = implementation\n    self.reset_after = reset_after\n    self.state_size = self.units\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                "def build(self, input_shape):\n    input_dim = input_shape[-1]\n    self.kernel = self.add_weight(shape=(input_dim, self.units * 3),\n                                  name='kernel',\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units * 3),\n        name='recurrent_kernel',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n\n    if self.use_bias:\n        if not self.reset_after:\n            bias_shape = (3 * self.units,)\n        else:\n            # separate biases for input and recurrent kernels\n            # Note: the shape is intentionally different from CuDNNGRU biases\n            # `(2 * 3 * self.units,)`, so that we can distinguish the classes\n            # when loading and converting saved weights.\n            bias_shape = (2, 3 * self.units)\n        self.bias = self.add_weight(shape=bias_shape,\n                                    name='bias',\n                                    initializer=self.bias_initializer,\n                                    regularizer=self.bias_regularizer,\n                                    constraint=self.bias_constraint)\n        if not self.reset_after:\n            self.input_bias, self.recurrent_bias = self.bias, None\n        else:\n            # NOTE: need to flatten, since slicing in CNTK gives 2D array\n            self.input_bias = K.flatten(self.bias[0])\n            self.recurrent_bias = K.flatten(self.bias[1])\n    else:\n        self.bias = None\n\n    # update gate\n    self.kernel_z = self.kernel[:, :self.units]\n    self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n    # reset gate\n    self.kernel_r = self.kernel[:, self.units: self.units * 2]\n    self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                    self.units:\n                                                    self.units * 2]\n    # new gate\n    self.kernel_h = self.kernel[:, self.units * 2:]\n    self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n    if self.use_bias:\n        # bias for inputs\n        self.input_bias_z = self.input_bias[:self.units]\n        self.input_bias_r = self.input_bias[self.units: self.units * 2]\n        self.input_bias_h = self.input_bias[self.units * 2:]\n        # bias for hidden state - just for compatibility with CuDNN\n        if self.reset_after:\n            self.recurrent_bias_z = self.recurrent_bias[:self.units]\n            self.recurrent_bias_r = self.recurrent_bias[self.units: self.units * 2]\n            self.recurrent_bias_h = self.recurrent_bias[self.units * 2:]\n    else:\n        self.input_bias_z = None\n        self.input_bias_r = None\n        self.input_bias_h = None\n        if self.reset_after:\n            self.recurrent_bias_z = None\n            self.recurrent_bias_r = None\n            self.recurrent_bias_h = None\n    self.built = True",
                "def call(self, inputs, states, training=None):\n    h_tm1 = states[0]  # previous memory\n\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(\n            K.ones_like(inputs),\n            self.dropout,\n            training=training,\n            count=3)\n    if (0 < self.recurrent_dropout < 1 and\n            self._recurrent_dropout_mask is None):\n        self._recurrent_dropout_mask = _generate_dropout_mask(\n            K.ones_like(h_tm1),\n            self.recurrent_dropout,\n            training=training,\n            count=3)\n\n    # dropout matrices for input units\n    dp_mask = self._dropout_mask\n    # dropout matrices for recurrent units\n    rec_dp_mask = self._recurrent_dropout_mask\n\n    if self.implementation == 1:\n        if 0. < self.dropout < 1.:\n            inputs_z = inputs * dp_mask[0]\n            inputs_r = inputs * dp_mask[1]\n            inputs_h = inputs * dp_mask[2]\n        else:\n            inputs_z = inputs\n            inputs_r = inputs\n            inputs_h = inputs\n\n        x_z = K.dot(inputs_z, self.kernel_z)\n        x_r = K.dot(inputs_r, self.kernel_r)\n        x_h = K.dot(inputs_h, self.kernel_h)\n        if self.use_bias:\n            x_z = K.bias_add(x_z, self.input_bias_z)\n            x_r = K.bias_add(x_r, self.input_bias_r)\n            x_h = K.bias_add(x_h, self.input_bias_h)\n\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1_z = h_tm1 * rec_dp_mask[0]\n            h_tm1_r = h_tm1 * rec_dp_mask[1]\n            h_tm1_h = h_tm1 * rec_dp_mask[2]\n        else:\n            h_tm1_z = h_tm1\n            h_tm1_r = h_tm1\n            h_tm1_h = h_tm1\n\n        recurrent_z = K.dot(h_tm1_z, self.recurrent_kernel_z)\n        recurrent_r = K.dot(h_tm1_r, self.recurrent_kernel_r)\n        if self.reset_after and self.use_bias:\n            recurrent_z = K.bias_add(recurrent_z, self.recurrent_bias_z)\n            recurrent_r = K.bias_add(recurrent_r, self.recurrent_bias_r)\n\n        z = self.recurrent_activation(x_z + recurrent_z)\n        r = self.recurrent_activation(x_r + recurrent_r)\n\n        # reset gate applied after/before matrix multiplication\n        if self.reset_after:\n            recurrent_h = K.dot(h_tm1_h, self.recurrent_kernel_h)\n            if self.use_bias:\n                recurrent_h = K.bias_add(recurrent_h, self.recurrent_bias_h)\n            recurrent_h = r * recurrent_h\n        else:\n            recurrent_h = K.dot(r * h_tm1_h, self.recurrent_kernel_h)\n\n        hh = self.activation(x_h + recurrent_h)\n    else:\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n\n        # inputs projected by all gate matrices at once\n        matrix_x = K.dot(inputs, self.kernel)\n        if self.use_bias:\n            # biases: bias_z_i, bias_r_i, bias_h_i\n            matrix_x = K.bias_add(matrix_x, self.input_bias)\n        x_z = matrix_x[:, :self.units]\n        x_r = matrix_x[:, self.units: 2 * self.units]\n        x_h = matrix_x[:, 2 * self.units:]\n\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1 *= rec_dp_mask[0]\n\n        if self.reset_after:\n            # hidden state projected by all gate matrices at once\n            matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n            if self.use_bias:\n                matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n        else:\n            # hidden state projected separately for update/reset and new\n            matrix_inner = K.dot(h_tm1,\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n        recurrent_z = matrix_inner[:, :self.units]\n        recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n        z = self.recurrent_activation(x_z + recurrent_z)\n        r = self.recurrent_activation(x_r + recurrent_r)\n\n        if self.reset_after:\n            recurrent_h = r * matrix_inner[:, 2 * self.units:]\n        else:\n            recurrent_h = K.dot(r * h_tm1,\n                                self.recurrent_kernel[:, 2 * self.units:])\n\n        hh = self.activation(x_h + recurrent_h)\n\n    # previous and candidate state mixed by update gate\n    h = z * h_tm1 + (1 - z) * hh\n\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            h._uses_learning_phase = True\n\n    return h, [h]",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'recurrent_activation': activations.serialize(self.recurrent_activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout,\n              'implementation': self.implementation,\n              'reset_after': self.reset_after}\n    base_config = super(GRUCell, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@interfaces.legacy_recurrent_support\ndef __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             activity_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             reset_after=False,\n             **kwargs):\n    if implementation == 0:\n        warnings.warn('`implementation=0` has been deprecated, '\n                      'and now defaults to `implementation=1`.'\n                      'Please update your layer call.')\n    if K.backend() == 'theano' and (dropout or recurrent_dropout):\n        warnings.warn(\n            'RNN dropout is no longer supported with the Theano backend '\n            'due to technical limitations. '\n            'You can either set `dropout` and `recurrent_dropout` to 0, '\n            'or use the TensorFlow backend.')\n        dropout = 0.\n        recurrent_dropout = 0.\n\n    cell = GRUCell(units,\n                   activation=activation,\n                   recurrent_activation=recurrent_activation,\n                   use_bias=use_bias,\n                   kernel_initializer=kernel_initializer,\n                   recurrent_initializer=recurrent_initializer,\n                   bias_initializer=bias_initializer,\n                   kernel_regularizer=kernel_regularizer,\n                   recurrent_regularizer=recurrent_regularizer,\n                   bias_regularizer=bias_regularizer,\n                   kernel_constraint=kernel_constraint,\n                   recurrent_constraint=recurrent_constraint,\n                   bias_constraint=bias_constraint,\n                   dropout=dropout,\n                   recurrent_dropout=recurrent_dropout,\n                   implementation=implementation,\n                   reset_after=reset_after)\n    super(GRU, self).__init__(cell,\n                              return_sequences=return_sequences,\n                              return_state=return_state,\n                              go_backwards=go_backwards,\n                              stateful=stateful,\n                              unroll=unroll,\n                              **kwargs)\n    self.activity_regularizer = regularizers.get(activity_regularizer)",
                "def call(self, inputs, mask=None, training=None, initial_state=None):\n    self.cell._dropout_mask = None\n    self.cell._recurrent_dropout_mask = None\n    return super(GRU, self).call(inputs,\n                                 mask=mask,\n                                 training=training,\n                                 initial_state=initial_state)",
                "@property\ndef units(self):\n    return self.cell.units",
                "@property\ndef activation(self):\n    return self.cell.activation",
                "@property\ndef recurrent_activation(self):\n    return self.cell.recurrent_activation",
                "@property\ndef use_bias(self):\n    return self.cell.use_bias",
                "@property\ndef kernel_initializer(self):\n    return self.cell.kernel_initializer",
                "@property\ndef recurrent_initializer(self):\n    return self.cell.recurrent_initializer",
                "@property\ndef bias_initializer(self):\n    return self.cell.bias_initializer",
                "@property\ndef kernel_regularizer(self):\n    return self.cell.kernel_regularizer",
                "@property\ndef recurrent_regularizer(self):\n    return self.cell.recurrent_regularizer",
                "@property\ndef bias_regularizer(self):\n    return self.cell.bias_regularizer",
                "@property\ndef kernel_constraint(self):\n    return self.cell.kernel_constraint",
                "@property\ndef recurrent_constraint(self):\n    return self.cell.recurrent_constraint",
                "@property\ndef bias_constraint(self):\n    return self.cell.bias_constraint",
                "@property\ndef dropout(self):\n    return self.cell.dropout",
                "@property\ndef recurrent_dropout(self):\n    return self.cell.recurrent_dropout",
                "@property\ndef implementation(self):\n    return self.cell.implementation",
                "@property\ndef reset_after(self):\n    return self.cell.reset_after",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'recurrent_activation': activations.serialize(self.recurrent_activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout,\n              'implementation': self.implementation,\n              'reset_after': self.reset_after}\n    base_config = super(GRU, self).get_config()\n    del base_config['cell']\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config):\n    if 'implementation' in config and config['implementation'] == 0:\n        config['implementation'] = 1\n    return cls(**config)",
                "def __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             unit_forget_bias=True,\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             **kwargs):\n    super(LSTMCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.recurrent_activation = activations.get(recurrent_activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.unit_forget_bias = unit_forget_bias\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.implementation = implementation\n    self.state_size = (self.units, self.units)\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                "def build(self, input_shape):\n    input_dim = input_shape[-1]\n    self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n                                  name='kernel',\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units * 4),\n        name='recurrent_kernel',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n\n    if self.use_bias:\n        if self.unit_forget_bias:\n            def bias_initializer(_, *args, **kwargs):\n                return K.concatenate([\n                    self.bias_initializer((self.units,), *args, **kwargs),\n                    initializers.Ones()((self.units,), *args, **kwargs),\n                    self.bias_initializer((self.units * 2,), *args, **kwargs),\n                ])\n        else:\n            bias_initializer = self.bias_initializer\n        self.bias = self.add_weight(shape=(self.units * 4,),\n                                    name='bias',\n                                    initializer=bias_initializer,\n                                    regularizer=self.bias_regularizer,\n                                    constraint=self.bias_constraint)\n    else:\n        self.bias = None\n\n    self.kernel_i = self.kernel[:, :self.units]\n    self.kernel_f = self.kernel[:, self.units: self.units * 2]\n    self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n    self.kernel_o = self.kernel[:, self.units * 3:]\n\n    self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n    self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n    self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n    self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n    if self.use_bias:\n        self.bias_i = self.bias[:self.units]\n        self.bias_f = self.bias[self.units: self.units * 2]\n        self.bias_c = self.bias[self.units * 2: self.units * 3]\n        self.bias_o = self.bias[self.units * 3:]\n    else:\n        self.bias_i = None\n        self.bias_f = None\n        self.bias_c = None\n        self.bias_o = None\n    self.built = True",
                "def call(self, inputs, states, training=None):\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(\n            K.ones_like(inputs),\n            self.dropout,\n            training=training,\n            count=4)\n    if (0 < self.recurrent_dropout < 1 and\n            self._recurrent_dropout_mask is None):\n        self._recurrent_dropout_mask = _generate_dropout_mask(\n            K.ones_like(states[0]),\n            self.recurrent_dropout,\n            training=training,\n            count=4)\n\n    # dropout matrices for input units\n    dp_mask = self._dropout_mask\n    # dropout matrices for recurrent units\n    rec_dp_mask = self._recurrent_dropout_mask\n\n    h_tm1 = states[0]  # previous memory state\n    c_tm1 = states[1]  # previous carry state\n\n    if self.implementation == 1:\n        if 0 < self.dropout < 1.:\n            inputs_i = inputs * dp_mask[0]\n            inputs_f = inputs * dp_mask[1]\n            inputs_c = inputs * dp_mask[2]\n            inputs_o = inputs * dp_mask[3]\n        else:\n            inputs_i = inputs\n            inputs_f = inputs\n            inputs_c = inputs\n            inputs_o = inputs\n        x_i = K.dot(inputs_i, self.kernel_i)\n        x_f = K.dot(inputs_f, self.kernel_f)\n        x_c = K.dot(inputs_c, self.kernel_c)\n        x_o = K.dot(inputs_o, self.kernel_o)\n        if self.use_bias:\n            x_i = K.bias_add(x_i, self.bias_i)\n            x_f = K.bias_add(x_f, self.bias_f)\n            x_c = K.bias_add(x_c, self.bias_c)\n            x_o = K.bias_add(x_o, self.bias_o)\n\n        if 0 < self.recurrent_dropout < 1.:\n            h_tm1_i = h_tm1 * rec_dp_mask[0]\n            h_tm1_f = h_tm1 * rec_dp_mask[1]\n            h_tm1_c = h_tm1 * rec_dp_mask[2]\n            h_tm1_o = h_tm1 * rec_dp_mask[3]\n        else:\n            h_tm1_i = h_tm1\n            h_tm1_f = h_tm1\n            h_tm1_c = h_tm1\n            h_tm1_o = h_tm1\n        i = self.recurrent_activation(x_i + K.dot(h_tm1_i,\n                                                  self.recurrent_kernel_i))\n        f = self.recurrent_activation(x_f + K.dot(h_tm1_f,\n                                                  self.recurrent_kernel_f))\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,\n                                                        self.recurrent_kernel_c))\n        o = self.recurrent_activation(x_o + K.dot(h_tm1_o,\n                                                  self.recurrent_kernel_o))\n    else:\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n        z = K.dot(inputs, self.kernel)\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1 *= rec_dp_mask[0]\n        z += K.dot(h_tm1, self.recurrent_kernel)\n        if self.use_bias:\n            z = K.bias_add(z, self.bias)\n\n        z0 = z[:, :self.units]\n        z1 = z[:, self.units: 2 * self.units]\n        z2 = z[:, 2 * self.units: 3 * self.units]\n        z3 = z[:, 3 * self.units:]\n\n        i = self.recurrent_activation(z0)\n        f = self.recurrent_activation(z1)\n        c = f * c_tm1 + i * self.activation(z2)\n        o = self.recurrent_activation(z3)\n\n    h = o * self.activation(c)\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            h._uses_learning_phase = True\n    return h, [h, c]",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'recurrent_activation': activations.serialize(self.recurrent_activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'unit_forget_bias': self.unit_forget_bias,\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout,\n              'implementation': self.implementation}\n    base_config = super(LSTMCell, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                "@interfaces.legacy_recurrent_support\ndef __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             unit_forget_bias=True,\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             activity_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if implementation == 0:\n        warnings.warn('`implementation=0` has been deprecated, '\n                      'and now defaults to `implementation=1`.'\n                      'Please update your layer call.')\n    if K.backend() == 'theano' and (dropout or recurrent_dropout):\n        warnings.warn(\n            'RNN dropout is no longer supported with the Theano backend '\n            'due to technical limitations. '\n            'You can either set `dropout` and `recurrent_dropout` to 0, '\n            'or use the TensorFlow backend.')\n        dropout = 0.\n        recurrent_dropout = 0.\n\n    cell = LSTMCell(units,\n                    activation=activation,\n                    recurrent_activation=recurrent_activation,\n                    use_bias=use_bias,\n                    kernel_initializer=kernel_initializer,\n                    recurrent_initializer=recurrent_initializer,\n                    unit_forget_bias=unit_forget_bias,\n                    bias_initializer=bias_initializer,\n                    kernel_regularizer=kernel_regularizer,\n                    recurrent_regularizer=recurrent_regularizer,\n                    bias_regularizer=bias_regularizer,\n                    kernel_constraint=kernel_constraint,\n                    recurrent_constraint=recurrent_constraint,\n                    bias_constraint=bias_constraint,\n                    dropout=dropout,\n                    recurrent_dropout=recurrent_dropout,\n                    implementation=implementation)\n    super(LSTM, self).__init__(cell,\n                               return_sequences=return_sequences,\n                               return_state=return_state,\n                               go_backwards=go_backwards,\n                               stateful=stateful,\n                               unroll=unroll,\n                               **kwargs)\n    self.activity_regularizer = regularizers.get(activity_regularizer)",
                "def call(self, inputs, mask=None, training=None, initial_state=None):\n    self.cell._dropout_mask = None\n    self.cell._recurrent_dropout_mask = None\n    return super(LSTM, self).call(inputs,\n                                  mask=mask,\n                                  training=training,\n                                  initial_state=initial_state)",
                "@property\ndef units(self):\n    return self.cell.units",
                "@property\ndef activation(self):\n    return self.cell.activation",
                "@property\ndef recurrent_activation(self):\n    return self.cell.recurrent_activation",
                "@property\ndef use_bias(self):\n    return self.cell.use_bias",
                "@property\ndef kernel_initializer(self):\n    return self.cell.kernel_initializer",
                "@property\ndef recurrent_initializer(self):\n    return self.cell.recurrent_initializer",
                "@property\ndef bias_initializer(self):\n    return self.cell.bias_initializer",
                "@property\ndef unit_forget_bias(self):\n    return self.cell.unit_forget_bias",
                "@property\ndef kernel_regularizer(self):\n    return self.cell.kernel_regularizer",
                "@property\ndef recurrent_regularizer(self):\n    return self.cell.recurrent_regularizer",
                "@property\ndef bias_regularizer(self):\n    return self.cell.bias_regularizer",
                "@property\ndef kernel_constraint(self):\n    return self.cell.kernel_constraint",
                "@property\ndef recurrent_constraint(self):\n    return self.cell.recurrent_constraint",
                "@property\ndef bias_constraint(self):\n    return self.cell.bias_constraint",
                "@property\ndef dropout(self):\n    return self.cell.dropout",
                "@property\ndef recurrent_dropout(self):\n    return self.cell.recurrent_dropout",
                "@property\ndef implementation(self):\n    return self.cell.implementation",
                "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'recurrent_activation': activations.serialize(self.recurrent_activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'unit_forget_bias': self.unit_forget_bias,\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout,\n              'implementation': self.implementation}\n    base_config = super(LSTM, self).get_config()\n    del base_config['cell']\n    return dict(list(base_config.items()) + list(config.items()))",
                "@classmethod\ndef from_config(cls, config):\n    if 'implementation' in config and config['implementation'] == 0:\n        config['implementation'] = 1\n    return cls(**config)",
                "def dropped_inputs():\n    return K.dropout(ones, rate)",
                "def to_list_or_none(x):\n    if x is None or isinstance(x, list):\n        return x\n    if isinstance(x, tuple):\n        return list(x)\n    return [x]",
                "def step(inputs, states):\n    constants = states[-self._num_constants:]\n    states = states[:-self._num_constants]\n    return self.cell.call(inputs, states, constants=constants,\n                          **kwargs)",
                "def step(inputs, states):\n    return self.cell.call(inputs, states, **kwargs)",
                "def bias_initializer(_, *args, **kwargs):\n    return K.concatenate([\n        self.bias_initializer((self.units,), *args, **kwargs),\n        initializers.Ones()((self.units,), *args, **kwargs),\n        self.bias_initializer((self.units * 2,), *args, **kwargs),\n    ])"
            ],
            "inscope_function_signatures": [
                "_generate_dropout_mask(ones, rate, training=None, count=1)",
                "_standardize_args(inputs, initial_state, constants, num_constants)",
                "__init__(self, cells, **kwargs)",
                "state_size(self)",
                "call(self, inputs, states, constants=None, **kwargs)",
                "build(self, input_shape)",
                "get_config(self)",
                "from_config(cls, config, custom_objects=None)",
                "trainable_weights(self)",
                "non_trainable_weights(self)",
                "get_weights(self)",
                "set_weights(self, weights)",
                "losses(self)",
                "get_losses_for(self, inputs=None)",
                "__init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
                "states(self)",
                "states(self, states)",
                "compute_output_shape(self, input_shape)",
                "compute_mask(self, inputs, mask)",
                "build(self, input_shape)",
                "get_initial_state(self, inputs)",
                "__call__(self, inputs, initial_state=None, constants=None, **kwargs)",
                "call(self, inputs, mask=None, training=None, initial_state=None, constants=None)",
                "reset_states(self, states=None)",
                "get_config(self)",
                "from_config(cls, config, custom_objects=None)",
                "trainable_weights(self)",
                "non_trainable_weights(self)",
                "losses(self)",
                "get_losses_for(self, inputs=None)",
                "__init__(self, units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, **kwargs)",
                "build(self, input_shape)",
                "call(self, inputs, states, training=None)",
                "get_config(self)",
                "__init__(self, units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
                "call(self, inputs, mask=None, training=None, initial_state=None)",
                "units(self)",
                "activation(self)",
                "use_bias(self)",
                "kernel_initializer(self)",
                "recurrent_initializer(self)",
                "bias_initializer(self)",
                "kernel_regularizer(self)",
                "recurrent_regularizer(self)",
                "bias_regularizer(self)",
                "kernel_constraint(self)",
                "recurrent_constraint(self)",
                "bias_constraint(self)",
                "dropout(self)",
                "recurrent_dropout(self)",
                "get_config(self)",
                "from_config(cls, config)",
                "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, reset_after=False, **kwargs)",
                "build(self, input_shape)",
                "call(self, inputs, states, training=None)",
                "get_config(self)",
                "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False, **kwargs)",
                "call(self, inputs, mask=None, training=None, initial_state=None)",
                "units(self)",
                "activation(self)",
                "recurrent_activation(self)",
                "use_bias(self)",
                "kernel_initializer(self)",
                "recurrent_initializer(self)",
                "bias_initializer(self)",
                "kernel_regularizer(self)",
                "recurrent_regularizer(self)",
                "bias_regularizer(self)",
                "kernel_constraint(self)",
                "recurrent_constraint(self)",
                "bias_constraint(self)",
                "dropout(self)",
                "recurrent_dropout(self)",
                "implementation(self)",
                "reset_after(self)",
                "get_config(self)",
                "from_config(cls, config)",
                "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, **kwargs)",
                "build(self, input_shape)",
                "call(self, inputs, states, training=None)",
                "get_config(self)",
                "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
                "call(self, inputs, mask=None, training=None, initial_state=None)",
                "units(self)",
                "activation(self)",
                "recurrent_activation(self)",
                "use_bias(self)",
                "kernel_initializer(self)",
                "recurrent_initializer(self)",
                "bias_initializer(self)",
                "unit_forget_bias(self)",
                "kernel_regularizer(self)",
                "recurrent_regularizer(self)",
                "bias_regularizer(self)",
                "kernel_constraint(self)",
                "recurrent_constraint(self)",
                "bias_constraint(self)",
                "dropout(self)",
                "recurrent_dropout(self)",
                "implementation(self)",
                "get_config(self)",
                "from_config(cls, config)",
                "dropped_inputs()",
                "to_list_or_none(x)",
                "step(inputs, states)",
                "step(inputs, states)",
                "bias_initializer(_, *args, **kwargs)"
            ],
            "variables_in_file": {
                "Layer": [
                    160,
                    1697,
                    1123,
                    740,
                    198,
                    456,
                    745,
                    173,
                    206,
                    111,
                    751,
                    147,
                    212,
                    725,
                    25,
                    186,
                    155,
                    731
                ],
                "cell": [
                    146,
                    147,
                    148,
                    154,
                    155,
                    156,
                    1023,
                    1565,
                    159,
                    160,
                    161,
                    2088,
                    172,
                    173,
                    174,
                    1582,
                    48,
                    49,
                    52,
                    185,
                    186,
                    187,
                    2105,
                    189,
                    69,
                    70,
                    71,
                    197,
                    73,
                    198,
                    199,
                    714,
                    205,
                    206,
                    79,
                    80,
                    81,
                    82,
                    207,
                    717,
                    1009,
                    90,
                    91,
                    92,
                    375,
                    96,
                    110,
                    111,
                    112,
                    113,
                    370,
                    115,
                    116,
                    117,
                    371,
                    119,
                    372,
                    374,
                    381,
                    125,
                    126,
                    127
                ],
                "cells": [
                    128,
                    135,
                    137,
                    139,
                    48,
                    51,
                    55,
                    56,
                    124,
                    126
                ],
                "hasattr": [
                    70,
                    489,
                    683,
                    80,
                    49,
                    464,
                    52,
                    116,
                    372,
                    375,
                    661,
                    667,
                    413
                ],
                "ValueError": [
                    581,
                    677,
                    472,
                    649,
                    525,
                    688,
                    50,
                    373,
                    53,
                    599,
                    376,
                    574
                ],
                "self.cells": [
                    69,
                    197,
                    154,
                    172,
                    205,
                    110,
                    79,
                    146,
                    56,
                    185,
                    90,
                    125,
                    159
                ],
                "self": [
                    56,
                    57,
                    2105,
                    2112,
                    2115,
                    2116,
                    69,
                    2117,
                    2124,
                    79,
                    2128,
                    2132,
                    2136,
                    90,
                    2140,
                    2144,
                    2148,
                    2152,
                    2156,
                    110,
                    2160,
                    2164,
                    2168,
                    121,
                    2172,
                    125,
                    2176,
                    129,
                    2180,
                    2184,
                    2188,
                    143,
                    2191,
                    2192,
                    146,
                    2193,
                    2194,
                    2195,
                    2196,
                    2197,
                    2198,
                    2199,
                    154,
                    2200,
                    2201,
                    157,
                    2202,
                    159,
                    2203,
                    2204,
                    2205,
                    2206,
                    2207,
                    2208,
                    2209,
                    172,
                    185,
                    197,
                    205,
                    380,
                    381,
                    382,
                    383,
                    384,
                    385,
                    386,
                    388,
                    389,
                    390,
                    391,
                    392,
                    393,
                    397,
                    398,
                    401,
                    403,
                    407,
                    413,
                    414,
                    416,
                    419,
                    424,
                    433,
                    434,
                    435,
                    443,
                    444,
                    451,
                    453,
                    456,
                    459,
                    461,
                    464,
                    465,
                    467,
                    469,
                    471,
                    476,
                    478,
                    480,
                    481,
                    482,
                    489,
                    491,
                    493,
                    497,
                    500,
                    511,
                    513,
                    517,
                    519,
                    520,
                    534,
                    536,
                    537,
                    538,
                    539,
                    542,
                    556,
                    559,
                    565,
                    566,
                    568,
                    573,
                    574,
                    580,
                    594,
                    598,
                    602,
                    603,
                    604,
                    608,
                    614,
                    616,
                    618,
                    621,
                    622,
                    624,
                    635,
                    645,
                    647,
                    660,
                    661,
                    662,
                    663,
                    665,
                    667,
                    668,
                    671,
                    672,
                    676,
                    677,
                    678,
                    682,
                    683,
                    684,
                    686,
                    690,
                    697,
                    698,
                    699,
                    700,
                    701,
                    702,
                    703,
                    705,
                    706,
                    708,
                    723,
                    725,
                    726,
                    731,
                    732,
                    733,
                    734,
                    739,
                    740,
                    741,
                    745,
                    746,
                    747,
                    748,
                    810,
                    811,
                    812,
                    813,
                    815,
                    816,
                    817,
                    819,
                    820,
                    821,
                    823,
                    824,
                    825,
                    827,
                    828,
                    829,
                    830,
                    831,
                    834,
                    836,
                    837,
                    838,
                    839,
                    840,
                    842,
                    843,
                    844,
                    845,
                    846,
                    848,
                    849,
                    850,
                    852,
                    853,
                    857,
                    858,
                    860,
                    862,
                    863,
                    864,
                    866,
                    869,
                    870,
                    873,
                    875,
                    876,
                    877,
                    881,
                    882,
                    883,
                    886,
                    892,
                    893,
                    894,
                    895,
                    896,
                    897,
                    898,
                    899,
                    900,
                    901,
                    902,
                    903,
                    904,
                    905,
                    906,
                    1023,
                    1030,
                    1033,
                    1034,
                    1035,
                    1042,
                    1046,
                    1050,
                    1054,
                    1058,
                    1062,
                    1066,
                    1070,
                    1074,
                    1078,
                    1082,
                    1086,
                    1090,
                    1094,
                    1097,
                    1098,
                    1099,
                    1100,
                    1101,
                    1102,
                    1103,
                    1104,
                    1105,
                    1106,
                    1107,
                    1108,
                    1109,
                    1110,
                    1111,
                    1112,
                    1200,
                    1201,
                    1202,
                    1203,
                    1204,
                    1206,
                    1207,
                    1208,
                    1210,
                    1211,
                    1212,
                    1214,
                    1215,
                    1216,
                    1218,
                    1219,
                    1220,
                    1221,
                    1222,
                    1223,
                    1224,
                    1228,
                    1230,
                    1231,
                    1232,
                    1233,
                    1234,
                    1236,
                    1237,
                    1238,
                    1240,
                    1241,
                    1242,
                    1248,
                    1249,
                    1251,
                    1252,
                    1253,
                    1254,
                    1255,
                    1258,
                    1259,
                    1261,
                    1264,
                    1265,
                    1267,
                    1268,
                    1269,
                    1270,
                    1272,
                    1273,
                    1275,
                    1277,
                    1278,
                    1279,
                    1281,
                    1282,
                    1283,
                    1284,
                    1286,
                    1287,
                    1288,
                    1289,
                    1290,
                    1291,
                    1292,
                    1293,
                    1298,
                    1299,
                    1301,
                    1304,
                    1305,
                    1306,
                    1308,
                    1313,
                    1315,
                    1317,
                    1318,
                    1327,
                    1328,
                    1329,
                    1330,
                    1331,
                    1332,
                    1333,
                    1335,
                    1344,
                    1345,
                    1346,
                    1347,
                    1348,
                    1350,
                    1351,
                    1354,
                    1355,
                    1356,
                    1357,
                    1360,
                    1362,
                    1364,
                    1368,
                    1369,
                    1371,
                    1372,
                    1373,
                    1374,
                    1376,
                    1379,
                    1381,
                    1382,
                    1383,
                    1387,
                    1389,
                    1390,
                    1392,
                    1393,
                    1395,
                    1396,
                    1399,
                    1401,
                    1406,
                    1413,
                    1414,
                    1415,
                    1416,
                    1417,
                    1418,
                    1419,
                    1420,
                    1421,
                    1422,
                    1423,
                    1424,
                    1425,
                    1426,
                    1427,
                    1428,
                    1429,
                    1430,
                    1582,
                    1589,
                    1592,
                    1593,
                    1594,
                    1601,
                    1605,
                    1609,
                    1613,
                    1617,
                    1621,
                    1625,
                    1629,
                    1633,
                    1637,
                    1641,
                    1645,
                    1649,
                    1653,
                    1657,
                    1661,
                    1665,
                    1668,
                    1669,
                    1670,
                    1671,
                    1672,
                    1673,
                    1674,
                    1675,
                    1676,
                    1677,
                    1678,
                    1679,
                    1680,
                    1681,
                    1682,
                    1683,
                    1684,
                    1685,
                    1686,
                    1775,
                    1776,
                    1777,
                    1778,
                    1779,
                    1781,
                    1782,
                    1783,
                    1784,
                    1786,
                    1787,
                    1788,
                    1790,
                    1791,
                    1792,
                    1794,
                    1795,
                    1796,
                    1797,
                    1798,
                    1799,
                    1803,
                    1805,
                    1806,
                    1807,
                    1808,
                    1809,
                    1811,
                    1812,
                    1813,
                    1815,
                    1816,
                    1819,
                    1820,
                    1821,
                    1824,
                    1825,
                    1828,
                    1829,
                    1831,
                    1833,
                    1834,
                    1835,
                    1836,
                    1838,
                    1839,
                    1840,
                    1841,
                    1843,
                    1844,
                    1845,
                    1846,
                    1847,
                    1849,
                    1850,
                    1851,
                    1852,
                    1853,
                    1856,
                    1857,
                    1859,
                    1862,
                    1863,
                    1864,
                    1866,
                    1871,
                    1873,
                    1878,
                    1879,
                    1889,
                    1890,
                    1891,
                    1892,
                    1893,
                    1894,
                    1895,
                    1896,
                    1897,
                    1899,
                    1909,
                    1910,
                    1911,
                    1912,
                    1913,
                    1914,
                    1915,
                    1916,
                    1918,
                    1920,
                    1921,
                    1923,
                    1924,
                    1925,
                    1927,
                    1928,
                    1929,
                    1930,
                    1932,
                    1933,
                    1934,
                    1935,
                    1937,
                    1938,
                    1944,
                    1945,
                    1946,
                    1947,
                    1948,
                    1949,
                    1950,
                    1951,
                    1952,
                    1953,
                    1954,
                    1955,
                    1956,
                    1957,
                    1958,
                    1959,
                    1960,
                    1961
                ],
                "__init__": [
                    810,
                    1582,
                    1775,
                    1200,
                    57,
                    380,
                    2105,
                    1023
                ],
                "super": [
                    129,
                    906,
                    1035,
                    1430,
                    1686,
                    538,
                    542,
                    2209,
                    1961,
                    810,
                    1582,
                    1200,
                    57,
                    1594,
                    2105,
                    708,
                    2117,
                    1112,
                    739,
                    747,
                    748,
                    1775,
                    500,
                    380,
                    1023
                ],
                "StackedRNNCells": [
                    57,
                    129,
                    371
                ],
                "kwargs": [
                    515,
                    1029,
                    538,
                    1819,
                    1820,
                    1821,
                    542,
                    810,
                    1200,
                    1588,
                    57,
                    2111,
                    593,
                    595,
                    605,
                    94,
                    96,
                    608,
                    995,
                    996,
                    1775,
                    500,
                    380,
                    509
                ],
                "state_size": [
                    416,
                    417,
                    68,
                    71,
                    73,
                    74,
                    425,
                    465,
                    467,
                    471,
                    414,
                    479
                ],
                "cell.state_size": [
                    70,
                    71,
                    73,
                    80,
                    81,
                    82,
                    116,
                    117,
                    119
                ],
                "list": [
                    130,
                    907,
                    1431,
                    1688,
                    410,
                    674,
                    2211,
                    553,
                    1962,
                    431,
                    570,
                    448,
                    709,
                    71,
                    2254,
                    465,
                    2264,
                    1114,
                    2267,
                    107,
                    370,
                    636,
                    639
                ],
                "state_size.append": [
                    73
                ],
                "tuple": [
                    674,
                    74,
                    370,
                    2266,
                    636
                ],
                "property": [
                    2178,
                    2182,
                    2186,
                    395,
                    141,
                    1040,
                    1044,
                    151,
                    1048,
                    1052,
                    1056,
                    1060,
                    1064,
                    1068,
                    1072,
                    1076,
                    1080,
                    59,
                    1084,
                    1599,
                    1088,
                    194,
                    1603,
                    1092,
                    1607,
                    2122,
                    1611,
                    2126,
                    1615,
                    721,
                    2130,
                    1619,
                    2134,
                    1623,
                    729,
                    2138,
                    1627,
                    2142,
                    1631,
                    737,
                    2146,
                    1635,
                    2150,
                    1639,
                    2154,
                    1643,
                    2158,
                    1647,
                    2162,
                    1651,
                    2166,
                    1655,
                    2170,
                    1659,
                    2174,
                    1663
                ],
                "nested_states": [
                    78,
                    81,
                    84,
                    86,
                    90
                ],
                "nested_states.append": [
                    81,
                    84
                ],
                "states": [
                    640,
                    1296,
                    405,
                    407,
                    666,
                    674,
                    675,
                    676,
                    679,
                    681,
                    682,
                    1865,
                    81,
                    82,
                    1875,
                    84,
                    85,
                    1876,
                    856,
                    602,
                    90,
                    603,
                    92,
                    604,
                    96,
                    97,
                    608,
                    610,
                    101,
                    103,
                    104,
                    620,
                    621,
                    632,
                    636,
                    637,
                    639
                ],
                "len": [
                    576,
                    676,
                    678,
                    519,
                    679,
                    620,
                    560,
                    401,
                    81,
                    82,
                    2259,
                    187,
                    573,
                    574
                ],
                "new_nested_states": [
                    89,
                    102,
                    97
                ],
                "zip": [
                    682,
                    90,
                    668,
                    189
                ],
                "has_arg": [
                    112,
                    594,
                    91,
                    598
                ],
                "cell.call": [
                    96,
                    91,
                    92,
                    112
                ],
                "inputs": [
                    1920,
                    1035,
                    1300,
                    533,
                    542,
                    1319,
                    1320,
                    553,
                    1321,
                    1323,
                    1324,
                    557,
                    1325,
                    559,
                    2273,
                    562,
                    568,
                    1594,
                    578,
                    1858,
                    2117,
                    2254,
                    207,
                    2257,
                    2258,
                    2259,
                    2260,
                    1365,
                    2261,
                    1368,
                    1880,
                    1881,
                    859,
                    92,
                    604,
                    1882,
                    1883,
                    96,
                    608,
                    1885,
                    611,
                    1886,
                    1887,
                    486,
                    1888,
                    104,
                    873,
                    746,
                    747,
                    748,
                    875,
                    622,
                    496,
                    497,
                    500,
                    1919
                ],
                "constants": [
                    2273,
                    514,
                    515,
                    516,
                    613,
                    518,
                    519,
                    2255,
                    496,
                    497,
                    2257,
                    499,
                    597,
                    602,
                    604,
                    93,
                    2271
                ],
                "new_nested_states.append": [
                    97
                ],
                "cell_states": [
                    102,
                    103
                ],
                "isinstance": [
                    398,
                    147,
                    410,
                    155,
                    160,
                    674,
                    553,
                    173,
                    431,
                    570,
                    186,
                    448,
                    198,
                    456,
                    206,
                    2254,
                    725,
                    2264,
                    2266,
                    731,
                    740,
                    745,
                    107,
                    111,
                    370,
                    636
                ],
                "input_shape": [
                    1802,
                    410,
                    411,
                    420,
                    422,
                    425,
                    444,
                    448,
                    449,
                    578,
                    451,
                    452,
                    579,
                    834,
                    457,
                    1227,
                    107,
                    108,
                    109,
                    113,
                    115,
                    120
                ],
                "constants_shape": [
                    458,
                    459,
                    108,
                    113,
                    444,
                    446
                ],
                "cell.build": [
                    113,
                    115
                ],
                "output_dim": [
                    417,
                    420,
                    422,
                    117,
                    119,
                    120
                ],
                "self.built": [
                    482,
                    1293,
                    853,
                    121,
                    1853
                ],
                "cells.append": [
                    137,
                    126
                ],
                "cell.__class__.__name__": [
                    126
                ],
                "cell.__class__": [
                    126
                ],
                "cell.get_config": [
                    127
                ],
                "config": [
                    128,
                    130,
                    1668,
                    1413,
                    136,
                    139,
                    907,
                    2191,
                    1431,
                    1688,
                    1944,
                    1692,
                    1693,
                    1694,
                    2211,
                    2215,
                    2216,
                    2217,
                    1962,
                    697,
                    703,
                    706,
                    709,
                    1097,
                    714,
                    716,
                    717,
                    1114,
                    1118,
                    1119,
                    1120,
                    892
                ],
                "base_config": [
                    129,
                    130,
                    906,
                    907,
                    1430,
                    1687,
                    1688,
                    1431,
                    1686,
                    2209,
                    2210,
                    2211,
                    1961,
                    1962,
                    708,
                    709,
                    1112,
                    1113,
                    1114
                ],
                "get_config": [
                    129,
                    2209,
                    708,
                    1961,
                    906,
                    1430,
                    1686,
                    1112
                ],
                "dict": [
                    130,
                    2211,
                    709,
                    1962,
                    907,
                    1431,
                    1688,
                    1114
                ],
                "base_config.items": [
                    130,
                    2211,
                    709,
                    1962,
                    907,
                    1431,
                    1688,
                    1114
                ],
                "config.items": [
                    130,
                    2211,
                    709,
                    1962,
                    907,
                    1431,
                    1688,
                    1114
                ],
                "cell_config": [
                    136,
                    137,
                    707,
                    705
                ],
                "config.pop": [
                    136,
                    714,
                    716,
                    1119
                ],
                "deserialize_layer": [
                    137,
                    714
                ],
                "custom_objects": [
                    138,
                    715
                ],
                "cls": [
                    1120,
                    2217,
                    139,
                    717,
                    1694
                ],
                "classmethod": [
                    132,
                    2213,
                    711,
                    1690,
                    1116
                ],
                "self.trainable": [
                    723,
                    732,
                    157,
                    143
                ],
                "weights": [
                    162,
                    163,
                    171,
                    174,
                    175,
                    145,
                    188,
                    148,
                    149,
                    153,
                    156,
                    189,
                    191
                ],
                "cell.trainable_weights": [
                    161,
                    148
                ],
                "cell.non_trainable_weights": [
                    156
                ],
                "trainable_weights": [
                    161,
                    162,
                    158
                ],
                "cell.weights": [
                    187,
                    189,
                    174
                ],
                "K.batch_get_value": [
                    175
                ],
                "K": [
                    1920,
                    1923,
                    517,
                    1925,
                    522,
                    524,
                    1300,
                    1556,
                    662,
                    665,
                    1818,
                    1307,
                    669,
                    671,
                    2079,
                    1891,
                    2222,
                    175,
                    1327,
                    1328,
                    1329,
                    1331,
                    1332,
                    1333,
                    694,
                    1383,
                    1892,
                    2225,
                    1894,
                    2229,
                    1895,
                    192,
                    1344,
                    578,
                    1345,
                    1347,
                    1348,
                    1858,
                    1896,
                    1865,
                    1897,
                    1355,
                    1357,
                    1360,
                    1368,
                    859,
                    1371,
                    865,
                    610,
                    1889,
                    1890,
                    1381,
                    486,
                    487,
                    488,
                    873,
                    490,
                    875,
                    1000,
                    493,
                    877,
                    1258,
                    1259,
                    881,
                    1386,
                    1909,
                    1398,
                    1911,
                    1913,
                    1915,
                    511
                ],
                "tuples": [
                    184,
                    190,
                    192
                ],
                "num_param": [
                    187,
                    188,
                    191
                ],
                "sw": [
                    189,
                    190
                ],
                "w": [
                    189,
                    190
                ],
                "tuples.append": [
                    190
                ],
                "K.batch_set_value": [
                    192
                ],
                "losses": [
                    739,
                    196,
                    200,
                    201,
                    204,
                    208,
                    209
                ],
                "cell_losses": [
                    199,
                    200,
                    746,
                    747,
                    207,
                    208
                ],
                "cell.losses": [
                    199
                ],
                "cell.get_losses_for": [
                    207
                ],
                "RNN": [
                    739,
                    708,
                    747,
                    748,
                    1965,
                    910,
                    1434,
                    500,
                    538,
                    380,
                    542
                ],
                "self.cell": [
                    1033,
                    1034,
                    1042,
                    1046,
                    1050,
                    1054,
                    1058,
                    1062,
                    1066,
                    1070,
                    1074,
                    1078,
                    1592,
                    1593,
                    1082,
                    1086,
                    1601,
                    1090,
                    2115,
                    2116,
                    1605,
                    1094,
                    1609,
                    2124,
                    1613,
                    2128,
                    1617,
                    594,
                    2132,
                    1621,
                    598,
                    2136,
                    1625,
                    604,
                    1629,
                    2140,
                    608,
                    1633,
                    2144,
                    2148,
                    1637,
                    2152,
                    1641,
                    2156,
                    1645,
                    2160,
                    1649,
                    2164,
                    1653,
                    2168,
                    1657,
                    2172,
                    1661,
                    2176,
                    1665,
                    2180,
                    2184,
                    2188,
                    661,
                    663,
                    665,
                    667,
                    668,
                    672,
                    683,
                    684,
                    686,
                    705,
                    706,
                    725,
                    726,
                    731,
                    733,
                    734,
                    740,
                    741,
                    745,
                    746,
                    381,
                    398,
                    401,
                    413,
                    414,
                    416,
                    456,
                    459,
                    461,
                    464,
                    465,
                    467,
                    476,
                    489,
                    491,
                    493
                ],
                "self.return_sequences": [
                    419,
                    624,
                    433,
                    697,
                    382
                ],
                "return_sequences": [
                    1024,
                    2106,
                    382,
                    1583
                ],
                "self.return_state": [
                    424,
                    434,
                    698,
                    635,
                    383
                ],
                "return_state": [
                    1584,
                    1025,
                    2107,
                    383
                ],
                "self.go_backwards": [
                    384,
                    699,
                    614
                ],
                "go_backwards": [
                    384,
                    1585,
                    1026,
                    2108
                ],
                "self.stateful": [
                    480,
                    385,
                    451,
                    645,
                    618,
                    565,
                    700
                ],
                "stateful": [
                    385,
                    1586,
                    1027,
                    2109
                ],
                "self.unroll": [
                    616,
                    386,
                    580,
                    701
                ],
                "unroll": [
                    386,
                    1587,
                    1028,
                    2110
                ],
                "self.supports_masking": [
                    388
                ],
                "self.input_spec": [
                    453,
                    389,
                    647,
                    534,
                    536,
                    537,
                    539
                ],
                "InputSpec": [
                    453,
                    517,
                    389,
                    478,
                    511
                ],
                "self.state_spec": [
                    513,
                    390,
                    469,
                    471,
                    476,
                    478,
                    511
                ],
                "self._states": [
                    407,
                    403,
                    397,
                    391
                ],
                "self.constants_spec": [
                    392,
                    520,
                    517
                ],
                "self._num_constants": [
                    603,
                    519,
                    393,
                    556,
                    559,
                    497,
                    602,
                    443,
                    444,
                    702,
                    703
                ],
                "self.cell.state_size": [
                    398,
                    401,
                    661,
                    663,
                    665,
                    667,
                    668,
                    413,
                    414,
                    416,
                    672,
                    683,
                    684,
                    686,
                    464,
                    465,
                    467,
                    476,
                    489,
                    491,
                    493
                ],
                "int": [
                    398
                ],
                "num_states": [
                    401,
                    402,
                    399
                ],
                "_": [
                    402,
                    435,
                    2228
                ],
                "range": [
                    402,
                    620,
                    2228
                ],
                "states.setter": [
                    405
                ],
                "output_shape": [
                    426,
                    428,
                    420,
                    422
                ],
                "state_shape": [
                    425,
                    426
                ],
                "dim": [
                    425,
                    490,
                    491,
                    684,
                    686,
                    687,
                    691,
                    662,
                    663,
                    668,
                    669,
                    478,
                    479
                ],
                "mask": [
                    2118,
                    615,
                    1036,
                    431,
                    432,
                    433,
                    1595,
                    570,
                    571
                ],
                "output_mask": [
                    433,
                    436,
                    438
                ],
                "state_mask": [
                    435,
                    436
                ],
                "self.states": [
                    676,
                    678,
                    682,
                    621,
                    435,
                    660,
                    566,
                    662,
                    665,
                    668,
                    573,
                    574,
                    671
                ],
                "batch_size": [
                    672,
                    451,
                    453,
                    647,
                    648,
                    687,
                    691,
                    662,
                    665,
                    669
                ],
                "input_dim": [
                    452,
                    453,
                    1802,
                    1227,
                    1228,
                    1803
                ],
                "step_input_shape": [
                    457,
                    459,
                    461
                ],
                "self.cell.build": [
                    459,
                    461
                ],
                "spec.shape": [
                    471
                ],
                "spec": [
                    471
                ],
                "format": [
                    473
                ],
                "self.reset_states": [
                    481
                ],
                "initial_state": [
                    512,
                    1038,
                    557,
                    559,
                    560,
                    561,
                    563,
                    566,
                    568,
                    573,
                    1597,
                    576,
                    2120,
                    2255,
                    2260,
                    2270,
                    2273,
                    612,
                    486,
                    487,
                    488,
                    490,
                    493,
                    496,
                    497,
                    499,
                    508,
                    509,
                    510
                ],
                "K.zeros_like": [
                    486
                ],
                "K.sum": [
                    487
                ],
                "K.expand_dims": [
                    488
                ],
                "K.tile": [
                    490,
                    493
                ],
                "_standardize_args": [
                    496
                ],
                "__call__": [
                    538,
                    500,
                    542
                ],
                "additional_inputs": [
                    516,
                    522,
                    523,
                    533,
                    506,
                    510
                ],
                "additional_specs": [
                    520,
                    513,
                    507,
                    534
                ],
                "K.int_shape": [
                    578,
                    517,
                    511
                ],
                "state": [
                    512,
                    682,
                    694,
                    632,
                    633,
                    668,
                    669,
                    511
                ],
                "constant": [
                    517,
                    518
                ],
                "is_keras_tensor": [
                    522,
                    531,
                    524
                ],
                "K.is_keras_tensor": [
                    522,
                    524
                ],
                "tensor": [
                    523,
                    524
                ],
                "full_input": [
                    538,
                    533
                ],
                "full_input_spec": [
                    537,
                    534
                ],
                "original_input_spec": [
                    536,
                    539
                ],
                "output": [
                    640,
                    642,
                    625,
                    881,
                    627,
                    883,
                    631,
                    888,
                    889,
                    538,
                    540
                ],
                "self.get_initial_state": [
                    568
                ],
                "str": [
                    576,
                    678,
                    679,
                    681,
                    688,
                    691,
                    692,
                    574
                ],
                "timesteps": [
                    617,
                    579,
                    580
                ],
                "self.cell.call": [
                    608,
                    594,
                    604,
                    598
                ],
                "training": [
                    867,
                    1860,
                    2119,
                    1867,
                    1037,
                    595,
                    1939,
                    2228,
                    1302,
                    887,
                    2232,
                    1309,
                    1596,
                    861,
                    1407
                ],
                "last_output": [
                    610,
                    627,
                    630
                ],
                "outputs": [
                    625,
                    610
                ],
                "K.rnn": [
                    610
                ],
                "step": [
                    610
                ],
                "updates": [
                    619,
                    621,
                    622
                ],
                "i": [
                    620,
                    621,
                    1932,
                    1934,
                    1909,
                    1913
                ],
                "updates.append": [
                    621
                ],
                "self.add_update": [
                    622
                ],
                "getattr": [
                    630
                ],
                "output._uses_learning_phase": [
                    888,
                    631
                ],
                "state._uses_learning_phase": [
                    633
                ],
                "AttributeError": [
                    646
                ],
                "shape": [
                    647
                ],
                "K.zeros": [
                    665,
                    662
                ],
                "K.set_value": [
                    669,
                    694,
                    671
                ],
                "np.zeros": [
                    672,
                    669
                ],
                "np": [
                    672,
                    669
                ],
                "self.name": [
                    690,
                    677
                ],
                "index": [
                    688,
                    682,
                    684
                ],
                "value": [
                    682,
                    692,
                    694,
                    687
                ],
                "enumerate": [
                    682
                ],
                "value.shape": [
                    692,
                    687
                ],
                "self.cell.get_config": [
                    705
                ],
                "self.cell.__class__.__name__": [
                    706
                ],
                "self.cell.__class__": [
                    706
                ],
                "num_constants": [
                    716,
                    718,
                    2256,
                    2257,
                    2258
                ],
                "layer": [
                    717,
                    718,
                    719
                ],
                "layer._num_constants": [
                    718
                ],
                "self.cell.trainable_weights": [
                    726
                ],
                "self.cell.weights": [
                    733
                ],
                "self.cell.non_trainable_weights": [
                    734
                ],
                "layer_losses": [
                    739,
                    741,
                    742
                ],
                "self.cell.losses": [
                    741
                ],
                "self.cell.get_losses_for": [
                    746
                ],
                "get_losses_for": [
                    747,
                    748
                ],
                "SimpleRNNCell": [
                    1009,
                    810,
                    906
                ],
                "self.units": [
                    1282,
                    1283,
                    1284,
                    1413,
                    1668,
                    1797,
                    1927,
                    1928,
                    1929,
                    1803,
                    1930,
                    2191,
                    1809,
                    1944,
                    1819,
                    1820,
                    1821,
                    1825,
                    1833,
                    1834,
                    811,
                    1835,
                    1836,
                    1838,
                    1839,
                    1840,
                    1201,
                    1841,
                    1844,
                    1845,
                    1846,
                    1847,
                    829,
                    834,
                    1222,
                    840,
                    1097,
                    1228,
                    846,
                    1234,
                    1242,
                    1372,
                    1373,
                    1374,
                    1248,
                    1387,
                    1389,
                    1390,
                    1264,
                    1265,
                    1776,
                    1267,
                    1396,
                    1269,
                    1270,
                    1399,
                    1272,
                    1273,
                    892,
                    1277,
                    1278,
                    1279
                ],
                "units": [
                    2088,
                    811,
                    1776,
                    1009,
                    1201,
                    1565
                ],
                "self.activation": [
                    1669,
                    1414,
                    1098,
                    812,
                    1934,
                    2192,
                    1777,
                    882,
                    883,
                    1202,
                    1362,
                    1937,
                    1401,
                    1945,
                    893,
                    1913
                ],
                "activations.get": [
                    812,
                    1777,
                    1202,
                    1203,
                    1778
                ],
                "activations": [
                    1669,
                    1414,
                    1415,
                    1670,
                    1098,
                    812,
                    2192,
                    1777,
                    1202,
                    1203,
                    1778,
                    2193,
                    1945,
                    1946,
                    893
                ],
                "activation": [
                    2089,
                    812,
                    1777,
                    1202,
                    1010,
                    1566
                ],
                "self.use_bias": [
                    1924,
                    1671,
                    1416,
                    2194,
                    1815,
                    1947,
                    813,
                    1330,
                    1843,
                    1204,
                    1346,
                    1099,
                    1356,
                    845,
                    1240,
                    1369,
                    1893,
                    1382,
                    1779,
                    1275,
                    894
                ],
                "use_bias": [
                    1568,
                    2091,
                    813,
                    1011,
                    1204,
                    1779
                ],
                "self.kernel_initializer": [
                    836,
                    1672,
                    1417,
                    1100,
                    1805,
                    1230,
                    815,
                    2195,
                    1781,
                    1206,
                    1948,
                    895
                ],
                "initializers.get": [
                    1783,
                    815,
                    816,
                    817,
                    1781,
                    1206,
                    1207,
                    1208,
                    1782
                ],
                "initializers": [
                    896,
                    897,
                    1672,
                    1417,
                    1418,
                    1419,
                    1673,
                    1674,
                    2195,
                    2196,
                    2197,
                    1820,
                    1948,
                    1949,
                    1950,
                    815,
                    816,
                    817,
                    1206,
                    1207,
                    1208,
                    1100,
                    1101,
                    1102,
                    1781,
                    1782,
                    1783,
                    895
                ],
                "kernel_initializer": [
                    1569,
                    2092,
                    815,
                    1012,
                    1781,
                    1206
                ],
                "self.recurrent_initializer": [
                    896,
                    1673,
                    842,
                    1418,
                    1101,
                    816,
                    1811,
                    1236,
                    2196,
                    1782,
                    1207,
                    1949
                ],
                "recurrent_initializer": [
                    1570,
                    2093,
                    816,
                    1013,
                    1782,
                    1207
                ],
                "self.bias_initializer": [
                    1824,
                    897,
                    1251,
                    1674,
                    1419,
                    1102,
                    848,
                    817,
                    2197,
                    1783,
                    1208,
                    1819,
                    1821,
                    1950
                ],
                "bias_initializer": [
                    1824,
                    1571,
                    1827,
                    2095,
                    817,
                    1014,
                    1783,
                    1208
                ],
                "self.kernel_regularizer": [
                    1952,
                    898,
                    1786,
                    837,
                    1675,
                    1420,
                    1806,
                    1103,
                    1231,
                    819,
                    2199,
                    1210
                ],
                "regularizers.get": [
                    2112,
                    1786,
                    1788,
                    1030,
                    819,
                    820,
                    821,
                    1589,
                    1210,
                    1211,
                    1212,
                    1787
                ],
                "regularizers": [
                    898,
                    899,
                    900,
                    1030,
                    1675,
                    1420,
                    1421,
                    1422,
                    1676,
                    1677,
                    1678,
                    2199,
                    2200,
                    2201,
                    2202,
                    1952,
                    1953,
                    1954,
                    819,
                    820,
                    821,
                    1589,
                    1210,
                    1211,
                    1212,
                    2112,
                    1103,
                    1104,
                    1105,
                    1106,
                    1786,
                    1787,
                    1788
                ],
                "kernel_regularizer": [
                    1572,
                    1786,
                    2096,
                    819,
                    1015,
                    1210
                ],
                "self.recurrent_regularizer": [
                    1953,
                    899,
                    843,
                    1676,
                    1421,
                    1104,
                    820,
                    1237,
                    1812,
                    2200,
                    1211,
                    1787
                ],
                "recurrent_regularizer": [
                    1573,
                    2097,
                    820,
                    1016,
                    1211,
                    1787
                ],
                "self.bias_regularizer": [
                    1954,
                    900,
                    1252,
                    1788,
                    1828,
                    1677,
                    1422,
                    1105,
                    849,
                    821,
                    2201,
                    1212
                ],
                "bias_regularizer": [
                    1788,
                    1574,
                    2098,
                    821,
                    1017,
                    1212
                ],
                "self.kernel_constraint": [
                    1955,
                    901,
                    838,
                    1790,
                    1423,
                    1232,
                    1679,
                    1807,
                    1107,
                    823,
                    2203,
                    1214
                ],
                "constraints.get": [
                    1216,
                    1792,
                    1790,
                    1791,
                    823,
                    824,
                    825,
                    1214,
                    1215
                ],
                "constraints": [
                    1792,
                    901,
                    902,
                    903,
                    1423,
                    1424,
                    1425,
                    1679,
                    1680,
                    1681,
                    2203,
                    2204,
                    2205,
                    1955,
                    1956,
                    1957,
                    823,
                    824,
                    825,
                    1214,
                    1215,
                    1216,
                    1107,
                    1108,
                    1109,
                    1790,
                    1791
                ],
                "kernel_constraint": [
                    1575,
                    2099,
                    823,
                    1018,
                    1214,
                    1790
                ],
                "self.recurrent_constraint": [
                    1956,
                    902,
                    844,
                    1424,
                    1680,
                    1791,
                    1108,
                    1813,
                    1238,
                    824,
                    2204,
                    1215
                ],
                "recurrent_constraint": [
                    1215,
                    1576,
                    2100,
                    824,
                    1019,
                    1791
                ],
                "self.bias_constraint": [
                    1216,
                    1792,
                    1253,
                    1829,
                    903,
                    1957,
                    1425,
                    850,
                    1681,
                    1109,
                    825,
                    2205
                ],
                "bias_constraint": [
                    1216,
                    1792,
                    1577,
                    2101,
                    825,
                    1020
                ],
                "self.dropout": [
                    1794,
                    904,
                    1298,
                    1426,
                    1682,
                    1301,
                    1938,
                    2206,
                    1406,
                    1318,
                    1958,
                    827,
                    1856,
                    1218,
                    1859,
                    1364,
                    1110,
                    1879,
                    857,
                    860,
                    886,
                    1918
                ],
                "min": [
                    1794,
                    1218,
                    1219,
                    1795,
                    827,
                    828
                ],
                "max": [
                    1794,
                    1218,
                    1219,
                    1795,
                    827,
                    828
                ],
                "dropout": [
                    1218,
                    1794,
                    2085,
                    1000,
                    1578,
                    1006,
                    1556,
                    2102,
                    1562,
                    827,
                    1021,
                    2079
                ],
                "self.recurrent_dropout": [
                    1921,
                    1795,
                    905,
                    1938,
                    1427,
                    1683,
                    1304,
                    1308,
                    2207,
                    1959,
                    1335,
                    828,
                    1219,
                    1862,
                    1866,
                    1111,
                    862,
                    1376,
                    866,
                    1899,
                    886,
                    1406
                ],
                "recurrent_dropout": [
                    1219,
                    1795,
                    2086,
                    1000,
                    1579,
                    1007,
                    1556,
                    2103,
                    1563,
                    828,
                    1022,
                    2079
                ],
                "self.state_size": [
                    1797,
                    829,
                    1222
                ],
                "self._dropout_mask": [
                    1856,
                    1313,
                    1857,
                    869,
                    1798,
                    1223,
                    1871,
                    1298,
                    1299,
                    857,
                    858,
                    830
                ],
                "self._recurrent_dropout_mask": [
                    864,
                    1315,
                    870,
                    1799,
                    1224,
                    1863,
                    1864,
                    1873,
                    831,
                    1305,
                    1306,
                    863
                ],
                "self.kernel": [
                    1920,
                    834,
                    1368,
                    873,
                    1833,
                    875,
                    1228,
                    1803,
                    1834,
                    1835,
                    1264,
                    1836,
                    1267,
                    1272
                ],
                "self.add_weight": [
                    1249,
                    834,
                    1825,
                    839,
                    1803,
                    1228,
                    846,
                    1808,
                    1233
                ],
                "self.recurrent_kernel": [
                    1923,
                    1381,
                    839,
                    1387,
                    1838,
                    1839,
                    1808,
                    881,
                    1233,
                    1265,
                    1268,
                    1840,
                    1841,
                    1399,
                    1273
                ],
                "self.bias": [
                    1249,
                    1825,
                    1925,
                    1255,
                    1831,
                    1258,
                    1259,
                    876,
                    877,
                    846,
                    1261,
                    1847,
                    852,
                    1844,
                    1845,
                    1846
                ],
                "prev_output": [
                    856,
                    865,
                    880,
                    881
                ],
                "_generate_dropout_mask": [
                    864,
                    1857,
                    1864,
                    1299,
                    1306,
                    858
                ],
                "K.ones_like": [
                    865,
                    1858,
                    859,
                    1865,
                    1300,
                    1307
                ],
                "dp_mask": [
                    1313,
                    869,
                    1319,
                    872,
                    873,
                    1320,
                    1321,
                    1871,
                    1365,
                    1880,
                    1881,
                    1882,
                    1883,
                    1919
                ],
                "rec_dp_mask": [
                    1377,
                    1922,
                    1315,
                    870,
                    1900,
                    1901,
                    1902,
                    879,
                    880,
                    1873,
                    1903,
                    1336,
                    1337,
                    1338
                ],
                "h": [
                    1408,
                    1410,
                    873,
                    875,
                    877,
                    881,
                    1937,
                    1940,
                    1941,
                    1404
                ],
                "K.dot": [
                    1920,
                    1923,
                    1327,
                    1328,
                    1329,
                    1344,
                    1345,
                    1355,
                    1360,
                    1368,
                    1889,
                    1890,
                    1891,
                    1892,
                    1381,
                    873,
                    1386,
                    875,
                    881,
                    1909,
                    1398,
                    1911,
                    1913,
                    1915
                ],
                "K.bias_add": [
                    1347,
                    1348,
                    1925,
                    1894,
                    1383,
                    1895,
                    1896,
                    1897,
                    877,
                    1357,
                    1331,
                    1332,
                    1333,
                    1371
                ],
                "activations.serialize": [
                    1669,
                    1414,
                    1415,
                    1670,
                    1098,
                    2192,
                    2193,
                    1945,
                    1946,
                    893
                ],
                "initializers.serialize": [
                    896,
                    897,
                    1672,
                    1417,
                    1418,
                    1419,
                    1100,
                    1101,
                    1102,
                    1673,
                    1674,
                    2195,
                    2196,
                    2197,
                    1948,
                    1949,
                    1950,
                    895
                ],
                "regularizers.serialize": [
                    898,
                    899,
                    900,
                    1675,
                    1420,
                    1421,
                    1422,
                    1676,
                    1677,
                    1678,
                    2199,
                    2200,
                    2201,
                    2202,
                    1952,
                    1953,
                    1954,
                    1103,
                    1104,
                    1105,
                    1106
                ],
                "constraints.serialize": [
                    1955,
                    1956,
                    901,
                    902,
                    903,
                    1957,
                    1423,
                    1424,
                    1425,
                    1679,
                    1107,
                    1108,
                    1109,
                    1680,
                    1681,
                    2203,
                    2204,
                    2205
                ],
                "kwargs.pop": [
                    996
                ],
                "warnings.warn": [
                    2080,
                    997,
                    1001,
                    1553,
                    1557,
                    2076
                ],
                "warnings": [
                    2080,
                    997,
                    1001,
                    1553,
                    1557,
                    2076
                ],
                "K.backend": [
                    1000,
                    1556,
                    2079
                ],
                "SimpleRNN": [
                    1112,
                    1035,
                    1023
                ],
                "self.activity_regularizer": [
                    2112,
                    1030,
                    1678,
                    1106,
                    1589,
                    2202
                ],
                "activity_regularizer": [
                    2112,
                    1589,
                    1030
                ],
                "interfaces.legacy_recurrent_support": [
                    2050,
                    973,
                    1527
                ],
                "interfaces": [
                    2050,
                    973,
                    1527
                ],
                "self.cell._dropout_mask": [
                    1592,
                    1033,
                    2115
                ],
                "self.cell._recurrent_dropout_mask": [
                    1593,
                    1034,
                    2116
                ],
                "call": [
                    1594,
                    1035,
                    2117
                ],
                "self.cell.units": [
                    1601,
                    1042,
                    2124
                ],
                "self.cell.activation": [
                    2128,
                    1605,
                    1046
                ],
                "self.cell.use_bias": [
                    2136,
                    1050,
                    1613
                ],
                "self.cell.kernel_initializer": [
                    1617,
                    2140,
                    1054
                ],
                "self.cell.recurrent_initializer": [
                    2144,
                    1058,
                    1621
                ],
                "self.cell.bias_initializer": [
                    1625,
                    2148,
                    1062
                ],
                "self.cell.kernel_regularizer": [
                    1066,
                    2156,
                    1629
                ],
                "self.cell.recurrent_regularizer": [
                    2160,
                    1633,
                    1070
                ],
                "self.cell.bias_regularizer": [
                    1074,
                    2164,
                    1637
                ],
                "self.cell.kernel_constraint": [
                    2168,
                    1641,
                    1078
                ],
                "self.cell.recurrent_constraint": [
                    1082,
                    2172,
                    1645
                ],
                "self.cell.bias_constraint": [
                    2176,
                    1649,
                    1086
                ],
                "self.cell.dropout": [
                    1090,
                    2180,
                    1653
                ],
                "self.cell.recurrent_dropout": [
                    2184,
                    1657,
                    1094
                ],
                "GRUCell": [
                    1200,
                    1565,
                    1430
                ],
                "self.recurrent_activation": [
                    1350,
                    1351,
                    1415,
                    1670,
                    1932,
                    1933,
                    1935,
                    1392,
                    1393,
                    1778,
                    1203,
                    2193,
                    1909,
                    1911,
                    1946,
                    1915
                ],
                "recurrent_activation": [
                    1778,
                    1203,
                    2090,
                    1567
                ],
                "self.implementation": [
                    2208,
                    1796,
                    1220,
                    1317,
                    1960,
                    1428,
                    1684,
                    1878
                ],
                "implementation": [
                    1220,
                    1796,
                    1580,
                    1552,
                    2104,
                    2075
                ],
                "self.reset_after": [
                    1281,
                    1346,
                    1379,
                    1221,
                    1254,
                    1289,
                    1354,
                    1395,
                    1429,
                    1685,
                    1241
                ],
                "reset_after": [
                    1581,
                    1221
                ],
                "bias_shape": [
                    1248,
                    1249,
                    1242
                ],
                "self.input_bias": [
                    1255,
                    1258,
                    1371,
                    1277,
                    1278,
                    1279
                ],
                "self.recurrent_bias": [
                    1282,
                    1283,
                    1284,
                    1255,
                    1383,
                    1259
                ],
                "K.flatten": [
                    1258,
                    1259
                ],
                "self.kernel_z": [
                    1264,
                    1327
                ],
                "self.recurrent_kernel_z": [
                    1344,
                    1265
                ],
                "self.kernel_r": [
                    1328,
                    1267
                ],
                "self.recurrent_kernel_r": [
                    1345,
                    1268
                ],
                "self.kernel_h": [
                    1272,
                    1329
                ],
                "self.recurrent_kernel_h": [
                    1360,
                    1273,
                    1355
                ],
                "self.input_bias_z": [
                    1331,
                    1277,
                    1286
                ],
                "self.input_bias_r": [
                    1332,
                    1278,
                    1287
                ],
                "self.input_bias_h": [
                    1288,
                    1333,
                    1279
                ],
                "self.recurrent_bias_z": [
                    1282,
                    1290,
                    1347
                ],
                "self.recurrent_bias_r": [
                    1291,
                    1283,
                    1348
                ],
                "self.recurrent_bias_h": [
                    1284,
                    1292,
                    1357
                ],
                "h_tm1": [
                    1922,
                    1923,
                    1296,
                    1307,
                    1336,
                    1337,
                    1338,
                    1340,
                    1341,
                    1342,
                    1875,
                    1377,
                    1381,
                    1386,
                    1900,
                    1901,
                    1902,
                    1903,
                    1905,
                    1906,
                    1907,
                    1908,
                    1398,
                    1404
                ],
                "inputs_z": [
                    1323,
                    1327,
                    1319
                ],
                "inputs_r": [
                    1320,
                    1328,
                    1324
                ],
                "inputs_h": [
                    1321,
                    1325,
                    1329
                ],
                "x_z": [
                    1350,
                    1327,
                    1392,
                    1331,
                    1372
                ],
                "x_r": [
                    1351,
                    1328,
                    1393,
                    1332,
                    1373
                ],
                "x_h": [
                    1329,
                    1362,
                    1333,
                    1401,
                    1374
                ],
                "h_tm1_z": [
                    1336,
                    1344,
                    1340
                ],
                "h_tm1_r": [
                    1345,
                    1337,
                    1341
                ],
                "h_tm1_h": [
                    1360,
                    1338,
                    1355,
                    1342
                ],
                "recurrent_z": [
                    1344,
                    1347,
                    1350,
                    1389,
                    1392
                ],
                "recurrent_r": [
                    1345,
                    1348,
                    1351,
                    1390,
                    1393
                ],
                "z": [
                    1920,
                    1923,
                    1925,
                    1350,
                    1927,
                    1928,
                    1929,
                    1930,
                    1392,
                    1404
                ],
                "r": [
                    1351,
                    1358,
                    1360,
                    1393,
                    1396,
                    1398
                ],
                "recurrent_h": [
                    1355,
                    1357,
                    1358,
                    1360,
                    1362,
                    1396,
                    1398,
                    1401
                ],
                "hh": [
                    1401,
                    1362,
                    1404
                ],
                "matrix_x": [
                    1368,
                    1371,
                    1372,
                    1373,
                    1374
                ],
                "matrix_inner": [
                    1381,
                    1383,
                    1386,
                    1389,
                    1390,
                    1396
                ],
                "h._uses_learning_phase": [
                    1408,
                    1940
                ],
                "GRU": [
                    1594,
                    1686,
                    1582
                ],
                "self.cell.recurrent_activation": [
                    1609,
                    2132
                ],
                "self.cell.implementation": [
                    2188,
                    1661
                ],
                "self.cell.reset_after": [
                    1665
                ],
                "LSTMCell": [
                    2088,
                    1961,
                    1775
                ],
                "self.unit_forget_bias": [
                    1784,
                    1816,
                    2198,
                    1951
                ],
                "unit_forget_bias": [
                    1784,
                    2094
                ],
                "K.concatenate": [
                    1818
                ],
                "args": [
                    1819,
                    1820,
                    1821
                ],
                "initializers.Ones": [
                    1820
                ],
                "self.kernel_i": [
                    1833,
                    1889
                ],
                "self.kernel_f": [
                    1834,
                    1890
                ],
                "self.kernel_c": [
                    1891,
                    1835
                ],
                "self.kernel_o": [
                    1892,
                    1836
                ],
                "self.recurrent_kernel_i": [
                    1910,
                    1838
                ],
                "self.recurrent_kernel_f": [
                    1912,
                    1839
                ],
                "self.recurrent_kernel_c": [
                    1840,
                    1914
                ],
                "self.recurrent_kernel_o": [
                    1841,
                    1916
                ],
                "self.bias_i": [
                    1849,
                    1844,
                    1894
                ],
                "self.bias_f": [
                    1850,
                    1845,
                    1895
                ],
                "self.bias_c": [
                    1896,
                    1851,
                    1846
                ],
                "self.bias_o": [
                    1897,
                    1852,
                    1847
                ],
                "c_tm1": [
                    1913,
                    1876,
                    1934
                ],
                "inputs_i": [
                    1880,
                    1889,
                    1885
                ],
                "inputs_f": [
                    1881,
                    1890,
                    1886
                ],
                "inputs_c": [
                    1882,
                    1891,
                    1887
                ],
                "inputs_o": [
                    1888,
                    1883,
                    1892
                ],
                "x_i": [
                    1889,
                    1909,
                    1894
                ],
                "x_f": [
                    1890,
                    1911,
                    1895
                ],
                "x_c": [
                    1896,
                    1913,
                    1891
                ],
                "x_o": [
                    1897,
                    1915,
                    1892
                ],
                "h_tm1_i": [
                    1905,
                    1900,
                    1909
                ],
                "h_tm1_f": [
                    1906,
                    1901,
                    1911
                ],
                "h_tm1_c": [
                    1913,
                    1907,
                    1902
                ],
                "h_tm1_o": [
                    1915,
                    1908,
                    1903
                ],
                "f": [
                    1913,
                    1933,
                    1934,
                    1911
                ],
                "c": [
                    1913,
                    1937,
                    1941,
                    1934
                ],
                "o": [
                    1937,
                    1915,
                    1935
                ],
                "z0": [
                    1932,
                    1927
                ],
                "z1": [
                    1928,
                    1933
                ],
                "z2": [
                    1929,
                    1934
                ],
                "z3": [
                    1930,
                    1935
                ],
                "LSTM": [
                    2105,
                    2209,
                    2117
                ],
                "self.cell.unit_forget_bias": [
                    2152
                ],
                "K.dropout": [
                    2222
                ],
                "ones": [
                    2227,
                    2222,
                    2231
                ],
                "rate": [
                    2222
                ],
                "count": [
                    2224,
                    2228
                ],
                "K.in_train_phase": [
                    2225,
                    2229
                ],
                "dropped_inputs": [
                    2226,
                    2230
                ],
                "x": [
                    2264,
                    2265,
                    2266,
                    2267,
                    2268
                ],
                "to_list_or_none": [
                    2270,
                    2271
                ]
            },
            "filtered_variables_in_file": {
                "Layer": [
                    160,
                    1697,
                    1123,
                    740,
                    198,
                    456,
                    745,
                    173,
                    206,
                    111,
                    751,
                    147,
                    212,
                    725,
                    25,
                    186,
                    155,
                    731
                ],
                "cell": [
                    146,
                    147,
                    148,
                    154,
                    155,
                    156,
                    1023,
                    1565,
                    159,
                    160,
                    161,
                    2088,
                    172,
                    173,
                    174,
                    1582,
                    48,
                    49,
                    52,
                    185,
                    186,
                    187,
                    2105,
                    189,
                    69,
                    70,
                    71,
                    197,
                    73,
                    198,
                    199,
                    714,
                    205,
                    206,
                    79,
                    80,
                    81,
                    82,
                    207,
                    717,
                    1009,
                    90,
                    91,
                    92,
                    375,
                    96,
                    110,
                    111,
                    112,
                    113,
                    370,
                    115,
                    116,
                    117,
                    371,
                    119,
                    372,
                    374,
                    381,
                    125,
                    126,
                    127
                ],
                "cells": [
                    128,
                    135,
                    137,
                    139,
                    48,
                    51,
                    55,
                    56,
                    124,
                    126
                ],
                "self.cells": [
                    69,
                    197,
                    154,
                    172,
                    205,
                    110,
                    79,
                    146,
                    56,
                    185,
                    90,
                    125,
                    159
                ],
                "self": [
                    56,
                    57,
                    2105,
                    2112,
                    2115,
                    2116,
                    69,
                    2117,
                    2124,
                    79,
                    2128,
                    2132,
                    2136,
                    90,
                    2140,
                    2144,
                    2148,
                    2152,
                    2156,
                    110,
                    2160,
                    2164,
                    2168,
                    121,
                    2172,
                    125,
                    2176,
                    129,
                    2180,
                    2184,
                    2188,
                    143,
                    2191,
                    2192,
                    146,
                    2193,
                    2194,
                    2195,
                    2196,
                    2197,
                    2198,
                    2199,
                    154,
                    2200,
                    2201,
                    157,
                    2202,
                    159,
                    2203,
                    2204,
                    2205,
                    2206,
                    2207,
                    2208,
                    2209,
                    172,
                    185,
                    197,
                    205,
                    380,
                    381,
                    382,
                    383,
                    384,
                    385,
                    386,
                    388,
                    389,
                    390,
                    391,
                    392,
                    393,
                    397,
                    398,
                    401,
                    403,
                    407,
                    413,
                    414,
                    416,
                    419,
                    424,
                    433,
                    434,
                    435,
                    443,
                    444,
                    451,
                    453,
                    456,
                    459,
                    461,
                    464,
                    465,
                    467,
                    469,
                    471,
                    476,
                    478,
                    480,
                    481,
                    482,
                    489,
                    491,
                    493,
                    497,
                    500,
                    511,
                    513,
                    517,
                    519,
                    520,
                    534,
                    536,
                    537,
                    538,
                    539,
                    542,
                    556,
                    559,
                    565,
                    566,
                    568,
                    573,
                    574,
                    580,
                    594,
                    598,
                    602,
                    603,
                    604,
                    608,
                    614,
                    616,
                    618,
                    621,
                    622,
                    624,
                    635,
                    645,
                    647,
                    660,
                    661,
                    662,
                    663,
                    665,
                    667,
                    668,
                    671,
                    672,
                    676,
                    677,
                    678,
                    682,
                    683,
                    684,
                    686,
                    690,
                    697,
                    698,
                    699,
                    700,
                    701,
                    702,
                    703,
                    705,
                    706,
                    708,
                    723,
                    725,
                    726,
                    731,
                    732,
                    733,
                    734,
                    739,
                    740,
                    741,
                    745,
                    746,
                    747,
                    748,
                    810,
                    811,
                    812,
                    813,
                    815,
                    816,
                    817,
                    819,
                    820,
                    821,
                    823,
                    824,
                    825,
                    827,
                    828,
                    829,
                    830,
                    831,
                    834,
                    836,
                    837,
                    838,
                    839,
                    840,
                    842,
                    843,
                    844,
                    845,
                    846,
                    848,
                    849,
                    850,
                    852,
                    853,
                    857,
                    858,
                    860,
                    862,
                    863,
                    864,
                    866,
                    869,
                    870,
                    873,
                    875,
                    876,
                    877,
                    881,
                    882,
                    883,
                    886,
                    892,
                    893,
                    894,
                    895,
                    896,
                    897,
                    898,
                    899,
                    900,
                    901,
                    902,
                    903,
                    904,
                    905,
                    906,
                    1023,
                    1030,
                    1033,
                    1034,
                    1035,
                    1042,
                    1046,
                    1050,
                    1054,
                    1058,
                    1062,
                    1066,
                    1070,
                    1074,
                    1078,
                    1082,
                    1086,
                    1090,
                    1094,
                    1097,
                    1098,
                    1099,
                    1100,
                    1101,
                    1102,
                    1103,
                    1104,
                    1105,
                    1106,
                    1107,
                    1108,
                    1109,
                    1110,
                    1111,
                    1112,
                    1200,
                    1201,
                    1202,
                    1203,
                    1204,
                    1206,
                    1207,
                    1208,
                    1210,
                    1211,
                    1212,
                    1214,
                    1215,
                    1216,
                    1218,
                    1219,
                    1220,
                    1221,
                    1222,
                    1223,
                    1224,
                    1228,
                    1230,
                    1231,
                    1232,
                    1233,
                    1234,
                    1236,
                    1237,
                    1238,
                    1240,
                    1241,
                    1242,
                    1248,
                    1249,
                    1251,
                    1252,
                    1253,
                    1254,
                    1255,
                    1258,
                    1259,
                    1261,
                    1264,
                    1265,
                    1267,
                    1268,
                    1269,
                    1270,
                    1272,
                    1273,
                    1275,
                    1277,
                    1278,
                    1279,
                    1281,
                    1282,
                    1283,
                    1284,
                    1286,
                    1287,
                    1288,
                    1289,
                    1290,
                    1291,
                    1292,
                    1293,
                    1298,
                    1299,
                    1301,
                    1304,
                    1305,
                    1306,
                    1308,
                    1313,
                    1315,
                    1317,
                    1318,
                    1327,
                    1328,
                    1329,
                    1330,
                    1331,
                    1332,
                    1333,
                    1335,
                    1344,
                    1345,
                    1346,
                    1347,
                    1348,
                    1350,
                    1351,
                    1354,
                    1355,
                    1356,
                    1357,
                    1360,
                    1362,
                    1364,
                    1368,
                    1369,
                    1371,
                    1372,
                    1373,
                    1374,
                    1376,
                    1379,
                    1381,
                    1382,
                    1383,
                    1387,
                    1389,
                    1390,
                    1392,
                    1393,
                    1395,
                    1396,
                    1399,
                    1401,
                    1406,
                    1413,
                    1414,
                    1415,
                    1416,
                    1417,
                    1418,
                    1419,
                    1420,
                    1421,
                    1422,
                    1423,
                    1424,
                    1425,
                    1426,
                    1427,
                    1428,
                    1429,
                    1430,
                    1582,
                    1589,
                    1592,
                    1593,
                    1594,
                    1601,
                    1605,
                    1609,
                    1613,
                    1617,
                    1621,
                    1625,
                    1629,
                    1633,
                    1637,
                    1641,
                    1645,
                    1649,
                    1653,
                    1657,
                    1661,
                    1665,
                    1668,
                    1669,
                    1670,
                    1671,
                    1672,
                    1673,
                    1674,
                    1675,
                    1676,
                    1677,
                    1678,
                    1679,
                    1680,
                    1681,
                    1682,
                    1683,
                    1684,
                    1685,
                    1686,
                    1775,
                    1776,
                    1777,
                    1778,
                    1779,
                    1781,
                    1782,
                    1783,
                    1784,
                    1786,
                    1787,
                    1788,
                    1790,
                    1791,
                    1792,
                    1794,
                    1795,
                    1796,
                    1797,
                    1798,
                    1799,
                    1803,
                    1805,
                    1806,
                    1807,
                    1808,
                    1809,
                    1811,
                    1812,
                    1813,
                    1815,
                    1816,
                    1819,
                    1820,
                    1821,
                    1824,
                    1825,
                    1828,
                    1829,
                    1831,
                    1833,
                    1834,
                    1835,
                    1836,
                    1838,
                    1839,
                    1840,
                    1841,
                    1843,
                    1844,
                    1845,
                    1846,
                    1847,
                    1849,
                    1850,
                    1851,
                    1852,
                    1853,
                    1856,
                    1857,
                    1859,
                    1862,
                    1863,
                    1864,
                    1866,
                    1871,
                    1873,
                    1878,
                    1879,
                    1889,
                    1890,
                    1891,
                    1892,
                    1893,
                    1894,
                    1895,
                    1896,
                    1897,
                    1899,
                    1909,
                    1910,
                    1911,
                    1912,
                    1913,
                    1914,
                    1915,
                    1916,
                    1918,
                    1920,
                    1921,
                    1923,
                    1924,
                    1925,
                    1927,
                    1928,
                    1929,
                    1930,
                    1932,
                    1933,
                    1934,
                    1935,
                    1937,
                    1938,
                    1944,
                    1945,
                    1946,
                    1947,
                    1948,
                    1949,
                    1950,
                    1951,
                    1952,
                    1953,
                    1954,
                    1955,
                    1956,
                    1957,
                    1958,
                    1959,
                    1960,
                    1961
                ],
                "__init__": [
                    810,
                    1582,
                    1775,
                    1200,
                    57,
                    380,
                    2105,
                    1023
                ],
                "StackedRNNCells": [
                    57,
                    129,
                    371
                ],
                "kwargs": [
                    515,
                    1029,
                    538,
                    1819,
                    1820,
                    1821,
                    542,
                    810,
                    1200,
                    1588,
                    57,
                    2111,
                    593,
                    595,
                    605,
                    94,
                    96,
                    608,
                    995,
                    996,
                    1775,
                    500,
                    380,
                    509
                ],
                "state_size": [
                    416,
                    417,
                    68,
                    71,
                    73,
                    74,
                    425,
                    465,
                    467,
                    471,
                    414,
                    479
                ],
                "cell.state_size": [
                    70,
                    71,
                    73,
                    80,
                    81,
                    82,
                    116,
                    117,
                    119
                ],
                "state_size.append": [
                    73
                ],
                "nested_states": [
                    78,
                    81,
                    84,
                    86,
                    90
                ],
                "nested_states.append": [
                    81,
                    84
                ],
                "states": [
                    640,
                    1296,
                    405,
                    407,
                    666,
                    674,
                    675,
                    676,
                    679,
                    681,
                    682,
                    1865,
                    81,
                    82,
                    1875,
                    84,
                    85,
                    1876,
                    856,
                    602,
                    90,
                    603,
                    92,
                    604,
                    96,
                    97,
                    608,
                    610,
                    101,
                    103,
                    104,
                    620,
                    621,
                    632,
                    636,
                    637,
                    639
                ],
                "new_nested_states": [
                    89,
                    102,
                    97
                ],
                "has_arg": [
                    112,
                    594,
                    91,
                    598
                ],
                "cell.call": [
                    96,
                    91,
                    92,
                    112
                ],
                "inputs": [
                    1920,
                    1035,
                    1300,
                    533,
                    542,
                    1319,
                    1320,
                    553,
                    1321,
                    1323,
                    1324,
                    557,
                    1325,
                    559,
                    2273,
                    562,
                    568,
                    1594,
                    578,
                    1858,
                    2117,
                    2254,
                    207,
                    2257,
                    2258,
                    2259,
                    2260,
                    1365,
                    2261,
                    1368,
                    1880,
                    1881,
                    859,
                    92,
                    604,
                    1882,
                    1883,
                    96,
                    608,
                    1885,
                    611,
                    1886,
                    1887,
                    486,
                    1888,
                    104,
                    873,
                    746,
                    747,
                    748,
                    875,
                    622,
                    496,
                    497,
                    500,
                    1919
                ],
                "constants": [
                    2273,
                    514,
                    515,
                    516,
                    613,
                    518,
                    519,
                    2255,
                    496,
                    497,
                    2257,
                    499,
                    597,
                    602,
                    604,
                    93,
                    2271
                ],
                "new_nested_states.append": [
                    97
                ],
                "cell_states": [
                    102,
                    103
                ],
                "input_shape": [
                    1802,
                    410,
                    411,
                    420,
                    422,
                    425,
                    444,
                    448,
                    449,
                    578,
                    451,
                    452,
                    579,
                    834,
                    457,
                    1227,
                    107,
                    108,
                    109,
                    113,
                    115,
                    120
                ],
                "constants_shape": [
                    458,
                    459,
                    108,
                    113,
                    444,
                    446
                ],
                "cell.build": [
                    113,
                    115
                ],
                "output_dim": [
                    417,
                    420,
                    422,
                    117,
                    119,
                    120
                ],
                "self.built": [
                    482,
                    1293,
                    853,
                    121,
                    1853
                ],
                "cells.append": [
                    137,
                    126
                ],
                "cell.__class__.__name__": [
                    126
                ],
                "cell.__class__": [
                    126
                ],
                "cell.get_config": [
                    127
                ],
                "config": [
                    128,
                    130,
                    1668,
                    1413,
                    136,
                    139,
                    907,
                    2191,
                    1431,
                    1688,
                    1944,
                    1692,
                    1693,
                    1694,
                    2211,
                    2215,
                    2216,
                    2217,
                    1962,
                    697,
                    703,
                    706,
                    709,
                    1097,
                    714,
                    716,
                    717,
                    1114,
                    1118,
                    1119,
                    1120,
                    892
                ],
                "base_config": [
                    129,
                    130,
                    906,
                    907,
                    1430,
                    1687,
                    1688,
                    1431,
                    1686,
                    2209,
                    2210,
                    2211,
                    1961,
                    1962,
                    708,
                    709,
                    1112,
                    1113,
                    1114
                ],
                "get_config": [
                    129,
                    2209,
                    708,
                    1961,
                    906,
                    1430,
                    1686,
                    1112
                ],
                "base_config.items": [
                    130,
                    2211,
                    709,
                    1962,
                    907,
                    1431,
                    1688,
                    1114
                ],
                "config.items": [
                    130,
                    2211,
                    709,
                    1962,
                    907,
                    1431,
                    1688,
                    1114
                ],
                "cell_config": [
                    136,
                    137,
                    707,
                    705
                ],
                "config.pop": [
                    136,
                    714,
                    716,
                    1119
                ],
                "deserialize_layer": [
                    137,
                    714
                ],
                "custom_objects": [
                    138,
                    715
                ],
                "cls": [
                    1120,
                    2217,
                    139,
                    717,
                    1694
                ],
                "self.trainable": [
                    723,
                    732,
                    157,
                    143
                ],
                "weights": [
                    162,
                    163,
                    171,
                    174,
                    175,
                    145,
                    188,
                    148,
                    149,
                    153,
                    156,
                    189,
                    191
                ],
                "cell.trainable_weights": [
                    161,
                    148
                ],
                "cell.non_trainable_weights": [
                    156
                ],
                "trainable_weights": [
                    161,
                    162,
                    158
                ],
                "cell.weights": [
                    187,
                    189,
                    174
                ],
                "K.batch_get_value": [
                    175
                ],
                "K": [
                    1920,
                    1923,
                    517,
                    1925,
                    522,
                    524,
                    1300,
                    1556,
                    662,
                    665,
                    1818,
                    1307,
                    669,
                    671,
                    2079,
                    1891,
                    2222,
                    175,
                    1327,
                    1328,
                    1329,
                    1331,
                    1332,
                    1333,
                    694,
                    1383,
                    1892,
                    2225,
                    1894,
                    2229,
                    1895,
                    192,
                    1344,
                    578,
                    1345,
                    1347,
                    1348,
                    1858,
                    1896,
                    1865,
                    1897,
                    1355,
                    1357,
                    1360,
                    1368,
                    859,
                    1371,
                    865,
                    610,
                    1889,
                    1890,
                    1381,
                    486,
                    487,
                    488,
                    873,
                    490,
                    875,
                    1000,
                    493,
                    877,
                    1258,
                    1259,
                    881,
                    1386,
                    1909,
                    1398,
                    1911,
                    1913,
                    1915,
                    511
                ],
                "tuples": [
                    184,
                    190,
                    192
                ],
                "num_param": [
                    187,
                    188,
                    191
                ],
                "sw": [
                    189,
                    190
                ],
                "w": [
                    189,
                    190
                ],
                "tuples.append": [
                    190
                ],
                "K.batch_set_value": [
                    192
                ],
                "losses": [
                    739,
                    196,
                    200,
                    201,
                    204,
                    208,
                    209
                ],
                "cell_losses": [
                    199,
                    200,
                    746,
                    747,
                    207,
                    208
                ],
                "cell.losses": [
                    199
                ],
                "cell.get_losses_for": [
                    207
                ],
                "RNN": [
                    739,
                    708,
                    747,
                    748,
                    1965,
                    910,
                    1434,
                    500,
                    538,
                    380,
                    542
                ],
                "self.cell": [
                    1033,
                    1034,
                    1042,
                    1046,
                    1050,
                    1054,
                    1058,
                    1062,
                    1066,
                    1070,
                    1074,
                    1078,
                    1592,
                    1593,
                    1082,
                    1086,
                    1601,
                    1090,
                    2115,
                    2116,
                    1605,
                    1094,
                    1609,
                    2124,
                    1613,
                    2128,
                    1617,
                    594,
                    2132,
                    1621,
                    598,
                    2136,
                    1625,
                    604,
                    1629,
                    2140,
                    608,
                    1633,
                    2144,
                    2148,
                    1637,
                    2152,
                    1641,
                    2156,
                    1645,
                    2160,
                    1649,
                    2164,
                    1653,
                    2168,
                    1657,
                    2172,
                    1661,
                    2176,
                    1665,
                    2180,
                    2184,
                    2188,
                    661,
                    663,
                    665,
                    667,
                    668,
                    672,
                    683,
                    684,
                    686,
                    705,
                    706,
                    725,
                    726,
                    731,
                    733,
                    734,
                    740,
                    741,
                    745,
                    746,
                    381,
                    398,
                    401,
                    413,
                    414,
                    416,
                    456,
                    459,
                    461,
                    464,
                    465,
                    467,
                    476,
                    489,
                    491,
                    493
                ],
                "self.return_sequences": [
                    419,
                    624,
                    433,
                    697,
                    382
                ],
                "return_sequences": [
                    1024,
                    2106,
                    382,
                    1583
                ],
                "self.return_state": [
                    424,
                    434,
                    698,
                    635,
                    383
                ],
                "return_state": [
                    1584,
                    1025,
                    2107,
                    383
                ],
                "self.go_backwards": [
                    384,
                    699,
                    614
                ],
                "go_backwards": [
                    384,
                    1585,
                    1026,
                    2108
                ],
                "self.stateful": [
                    480,
                    385,
                    451,
                    645,
                    618,
                    565,
                    700
                ],
                "stateful": [
                    385,
                    1586,
                    1027,
                    2109
                ],
                "self.unroll": [
                    616,
                    386,
                    580,
                    701
                ],
                "unroll": [
                    386,
                    1587,
                    1028,
                    2110
                ],
                "self.supports_masking": [
                    388
                ],
                "self.input_spec": [
                    453,
                    389,
                    647,
                    534,
                    536,
                    537,
                    539
                ],
                "InputSpec": [
                    453,
                    517,
                    389,
                    478,
                    511
                ],
                "self.state_spec": [
                    513,
                    390,
                    469,
                    471,
                    476,
                    478,
                    511
                ],
                "self._states": [
                    407,
                    403,
                    397,
                    391
                ],
                "self.constants_spec": [
                    392,
                    520,
                    517
                ],
                "self._num_constants": [
                    603,
                    519,
                    393,
                    556,
                    559,
                    497,
                    602,
                    443,
                    444,
                    702,
                    703
                ],
                "self.cell.state_size": [
                    398,
                    401,
                    661,
                    663,
                    665,
                    667,
                    668,
                    413,
                    414,
                    416,
                    672,
                    683,
                    684,
                    686,
                    464,
                    465,
                    467,
                    476,
                    489,
                    491,
                    493
                ],
                "num_states": [
                    401,
                    402,
                    399
                ],
                "_": [
                    402,
                    435,
                    2228
                ],
                "states.setter": [
                    405
                ],
                "output_shape": [
                    426,
                    428,
                    420,
                    422
                ],
                "state_shape": [
                    425,
                    426
                ],
                "dim": [
                    425,
                    490,
                    491,
                    684,
                    686,
                    687,
                    691,
                    662,
                    663,
                    668,
                    669,
                    478,
                    479
                ],
                "mask": [
                    2118,
                    615,
                    1036,
                    431,
                    432,
                    433,
                    1595,
                    570,
                    571
                ],
                "output_mask": [
                    433,
                    436,
                    438
                ],
                "state_mask": [
                    435,
                    436
                ],
                "self.states": [
                    676,
                    678,
                    682,
                    621,
                    435,
                    660,
                    566,
                    662,
                    665,
                    668,
                    573,
                    574,
                    671
                ],
                "batch_size": [
                    672,
                    451,
                    453,
                    647,
                    648,
                    687,
                    691,
                    662,
                    665,
                    669
                ],
                "input_dim": [
                    452,
                    453,
                    1802,
                    1227,
                    1228,
                    1803
                ],
                "step_input_shape": [
                    457,
                    459,
                    461
                ],
                "self.cell.build": [
                    459,
                    461
                ],
                "spec.shape": [
                    471
                ],
                "spec": [
                    471
                ],
                "self.reset_states": [
                    481
                ],
                "initial_state": [
                    512,
                    1038,
                    557,
                    559,
                    560,
                    561,
                    563,
                    566,
                    568,
                    573,
                    1597,
                    576,
                    2120,
                    2255,
                    2260,
                    2270,
                    2273,
                    612,
                    486,
                    487,
                    488,
                    490,
                    493,
                    496,
                    497,
                    499,
                    508,
                    509,
                    510
                ],
                "K.zeros_like": [
                    486
                ],
                "K.sum": [
                    487
                ],
                "K.expand_dims": [
                    488
                ],
                "K.tile": [
                    490,
                    493
                ],
                "_standardize_args": [
                    496
                ],
                "__call__": [
                    538,
                    500,
                    542
                ],
                "additional_inputs": [
                    516,
                    522,
                    523,
                    533,
                    506,
                    510
                ],
                "additional_specs": [
                    520,
                    513,
                    507,
                    534
                ],
                "K.int_shape": [
                    578,
                    517,
                    511
                ],
                "state": [
                    512,
                    682,
                    694,
                    632,
                    633,
                    668,
                    669,
                    511
                ],
                "constant": [
                    517,
                    518
                ],
                "is_keras_tensor": [
                    522,
                    531,
                    524
                ],
                "K.is_keras_tensor": [
                    522,
                    524
                ],
                "tensor": [
                    523,
                    524
                ],
                "full_input": [
                    538,
                    533
                ],
                "full_input_spec": [
                    537,
                    534
                ],
                "original_input_spec": [
                    536,
                    539
                ],
                "output": [
                    640,
                    642,
                    625,
                    881,
                    627,
                    883,
                    631,
                    888,
                    889,
                    538,
                    540
                ],
                "self.get_initial_state": [
                    568
                ],
                "timesteps": [
                    617,
                    579,
                    580
                ],
                "self.cell.call": [
                    608,
                    594,
                    604,
                    598
                ],
                "training": [
                    867,
                    1860,
                    2119,
                    1867,
                    1037,
                    595,
                    1939,
                    2228,
                    1302,
                    887,
                    2232,
                    1309,
                    1596,
                    861,
                    1407
                ],
                "last_output": [
                    610,
                    627,
                    630
                ],
                "outputs": [
                    625,
                    610
                ],
                "K.rnn": [
                    610
                ],
                "step": [
                    610
                ],
                "updates": [
                    619,
                    621,
                    622
                ],
                "i": [
                    620,
                    621,
                    1932,
                    1934,
                    1909,
                    1913
                ],
                "updates.append": [
                    621
                ],
                "self.add_update": [
                    622
                ],
                "output._uses_learning_phase": [
                    888,
                    631
                ],
                "state._uses_learning_phase": [
                    633
                ],
                "shape": [
                    647
                ],
                "K.zeros": [
                    665,
                    662
                ],
                "K.set_value": [
                    669,
                    694,
                    671
                ],
                "np.zeros": [
                    672,
                    669
                ],
                "np": [
                    672,
                    669
                ],
                "self.name": [
                    690,
                    677
                ],
                "index": [
                    688,
                    682,
                    684
                ],
                "value": [
                    682,
                    692,
                    694,
                    687
                ],
                "value.shape": [
                    692,
                    687
                ],
                "self.cell.get_config": [
                    705
                ],
                "self.cell.__class__.__name__": [
                    706
                ],
                "self.cell.__class__": [
                    706
                ],
                "num_constants": [
                    716,
                    718,
                    2256,
                    2257,
                    2258
                ],
                "layer": [
                    717,
                    718,
                    719
                ],
                "layer._num_constants": [
                    718
                ],
                "self.cell.trainable_weights": [
                    726
                ],
                "self.cell.weights": [
                    733
                ],
                "self.cell.non_trainable_weights": [
                    734
                ],
                "layer_losses": [
                    739,
                    741,
                    742
                ],
                "self.cell.losses": [
                    741
                ],
                "self.cell.get_losses_for": [
                    746
                ],
                "get_losses_for": [
                    747,
                    748
                ],
                "SimpleRNNCell": [
                    1009,
                    810,
                    906
                ],
                "self.units": [
                    1282,
                    1283,
                    1284,
                    1413,
                    1668,
                    1797,
                    1927,
                    1928,
                    1929,
                    1803,
                    1930,
                    2191,
                    1809,
                    1944,
                    1819,
                    1820,
                    1821,
                    1825,
                    1833,
                    1834,
                    811,
                    1835,
                    1836,
                    1838,
                    1839,
                    1840,
                    1201,
                    1841,
                    1844,
                    1845,
                    1846,
                    1847,
                    829,
                    834,
                    1222,
                    840,
                    1097,
                    1228,
                    846,
                    1234,
                    1242,
                    1372,
                    1373,
                    1374,
                    1248,
                    1387,
                    1389,
                    1390,
                    1264,
                    1265,
                    1776,
                    1267,
                    1396,
                    1269,
                    1270,
                    1399,
                    1272,
                    1273,
                    892,
                    1277,
                    1278,
                    1279
                ],
                "units": [
                    2088,
                    811,
                    1776,
                    1009,
                    1201,
                    1565
                ],
                "self.activation": [
                    1669,
                    1414,
                    1098,
                    812,
                    1934,
                    2192,
                    1777,
                    882,
                    883,
                    1202,
                    1362,
                    1937,
                    1401,
                    1945,
                    893,
                    1913
                ],
                "activations.get": [
                    812,
                    1777,
                    1202,
                    1203,
                    1778
                ],
                "activations": [
                    1669,
                    1414,
                    1415,
                    1670,
                    1098,
                    812,
                    2192,
                    1777,
                    1202,
                    1203,
                    1778,
                    2193,
                    1945,
                    1946,
                    893
                ],
                "activation": [
                    2089,
                    812,
                    1777,
                    1202,
                    1010,
                    1566
                ],
                "self.use_bias": [
                    1924,
                    1671,
                    1416,
                    2194,
                    1815,
                    1947,
                    813,
                    1330,
                    1843,
                    1204,
                    1346,
                    1099,
                    1356,
                    845,
                    1240,
                    1369,
                    1893,
                    1382,
                    1779,
                    1275,
                    894
                ],
                "use_bias": [
                    1568,
                    2091,
                    813,
                    1011,
                    1204,
                    1779
                ],
                "self.kernel_initializer": [
                    836,
                    1672,
                    1417,
                    1100,
                    1805,
                    1230,
                    815,
                    2195,
                    1781,
                    1206,
                    1948,
                    895
                ],
                "initializers.get": [
                    1783,
                    815,
                    816,
                    817,
                    1781,
                    1206,
                    1207,
                    1208,
                    1782
                ],
                "initializers": [
                    896,
                    897,
                    1672,
                    1417,
                    1418,
                    1419,
                    1673,
                    1674,
                    2195,
                    2196,
                    2197,
                    1820,
                    1948,
                    1949,
                    1950,
                    815,
                    816,
                    817,
                    1206,
                    1207,
                    1208,
                    1100,
                    1101,
                    1102,
                    1781,
                    1782,
                    1783,
                    895
                ],
                "kernel_initializer": [
                    1569,
                    2092,
                    815,
                    1012,
                    1781,
                    1206
                ],
                "self.recurrent_initializer": [
                    896,
                    1673,
                    842,
                    1418,
                    1101,
                    816,
                    1811,
                    1236,
                    2196,
                    1782,
                    1207,
                    1949
                ],
                "recurrent_initializer": [
                    1570,
                    2093,
                    816,
                    1013,
                    1782,
                    1207
                ],
                "self.bias_initializer": [
                    1824,
                    897,
                    1251,
                    1674,
                    1419,
                    1102,
                    848,
                    817,
                    2197,
                    1783,
                    1208,
                    1819,
                    1821,
                    1950
                ],
                "bias_initializer": [
                    1824,
                    1571,
                    1827,
                    2095,
                    817,
                    1014,
                    1783,
                    1208
                ],
                "self.kernel_regularizer": [
                    1952,
                    898,
                    1786,
                    837,
                    1675,
                    1420,
                    1806,
                    1103,
                    1231,
                    819,
                    2199,
                    1210
                ],
                "regularizers.get": [
                    2112,
                    1786,
                    1788,
                    1030,
                    819,
                    820,
                    821,
                    1589,
                    1210,
                    1211,
                    1212,
                    1787
                ],
                "regularizers": [
                    898,
                    899,
                    900,
                    1030,
                    1675,
                    1420,
                    1421,
                    1422,
                    1676,
                    1677,
                    1678,
                    2199,
                    2200,
                    2201,
                    2202,
                    1952,
                    1953,
                    1954,
                    819,
                    820,
                    821,
                    1589,
                    1210,
                    1211,
                    1212,
                    2112,
                    1103,
                    1104,
                    1105,
                    1106,
                    1786,
                    1787,
                    1788
                ],
                "kernel_regularizer": [
                    1572,
                    1786,
                    2096,
                    819,
                    1015,
                    1210
                ],
                "self.recurrent_regularizer": [
                    1953,
                    899,
                    843,
                    1676,
                    1421,
                    1104,
                    820,
                    1237,
                    1812,
                    2200,
                    1211,
                    1787
                ],
                "recurrent_regularizer": [
                    1573,
                    2097,
                    820,
                    1016,
                    1211,
                    1787
                ],
                "self.bias_regularizer": [
                    1954,
                    900,
                    1252,
                    1788,
                    1828,
                    1677,
                    1422,
                    1105,
                    849,
                    821,
                    2201,
                    1212
                ],
                "bias_regularizer": [
                    1788,
                    1574,
                    2098,
                    821,
                    1017,
                    1212
                ],
                "self.kernel_constraint": [
                    1955,
                    901,
                    838,
                    1790,
                    1423,
                    1232,
                    1679,
                    1807,
                    1107,
                    823,
                    2203,
                    1214
                ],
                "constraints.get": [
                    1216,
                    1792,
                    1790,
                    1791,
                    823,
                    824,
                    825,
                    1214,
                    1215
                ],
                "constraints": [
                    1792,
                    901,
                    902,
                    903,
                    1423,
                    1424,
                    1425,
                    1679,
                    1680,
                    1681,
                    2203,
                    2204,
                    2205,
                    1955,
                    1956,
                    1957,
                    823,
                    824,
                    825,
                    1214,
                    1215,
                    1216,
                    1107,
                    1108,
                    1109,
                    1790,
                    1791
                ],
                "kernel_constraint": [
                    1575,
                    2099,
                    823,
                    1018,
                    1214,
                    1790
                ],
                "self.recurrent_constraint": [
                    1956,
                    902,
                    844,
                    1424,
                    1680,
                    1791,
                    1108,
                    1813,
                    1238,
                    824,
                    2204,
                    1215
                ],
                "recurrent_constraint": [
                    1215,
                    1576,
                    2100,
                    824,
                    1019,
                    1791
                ],
                "self.bias_constraint": [
                    1216,
                    1792,
                    1253,
                    1829,
                    903,
                    1957,
                    1425,
                    850,
                    1681,
                    1109,
                    825,
                    2205
                ],
                "bias_constraint": [
                    1216,
                    1792,
                    1577,
                    2101,
                    825,
                    1020
                ],
                "self.dropout": [
                    1794,
                    904,
                    1298,
                    1426,
                    1682,
                    1301,
                    1938,
                    2206,
                    1406,
                    1318,
                    1958,
                    827,
                    1856,
                    1218,
                    1859,
                    1364,
                    1110,
                    1879,
                    857,
                    860,
                    886,
                    1918
                ],
                "dropout": [
                    1218,
                    1794,
                    2085,
                    1000,
                    1578,
                    1006,
                    1556,
                    2102,
                    1562,
                    827,
                    1021,
                    2079
                ],
                "self.recurrent_dropout": [
                    1921,
                    1795,
                    905,
                    1938,
                    1427,
                    1683,
                    1304,
                    1308,
                    2207,
                    1959,
                    1335,
                    828,
                    1219,
                    1862,
                    1866,
                    1111,
                    862,
                    1376,
                    866,
                    1899,
                    886,
                    1406
                ],
                "recurrent_dropout": [
                    1219,
                    1795,
                    2086,
                    1000,
                    1579,
                    1007,
                    1556,
                    2103,
                    1563,
                    828,
                    1022,
                    2079
                ],
                "self.state_size": [
                    1797,
                    829,
                    1222
                ],
                "self._dropout_mask": [
                    1856,
                    1313,
                    1857,
                    869,
                    1798,
                    1223,
                    1871,
                    1298,
                    1299,
                    857,
                    858,
                    830
                ],
                "self._recurrent_dropout_mask": [
                    864,
                    1315,
                    870,
                    1799,
                    1224,
                    1863,
                    1864,
                    1873,
                    831,
                    1305,
                    1306,
                    863
                ],
                "self.kernel": [
                    1920,
                    834,
                    1368,
                    873,
                    1833,
                    875,
                    1228,
                    1803,
                    1834,
                    1835,
                    1264,
                    1836,
                    1267,
                    1272
                ],
                "self.add_weight": [
                    1249,
                    834,
                    1825,
                    839,
                    1803,
                    1228,
                    846,
                    1808,
                    1233
                ],
                "self.recurrent_kernel": [
                    1923,
                    1381,
                    839,
                    1387,
                    1838,
                    1839,
                    1808,
                    881,
                    1233,
                    1265,
                    1268,
                    1840,
                    1841,
                    1399,
                    1273
                ],
                "self.bias": [
                    1249,
                    1825,
                    1925,
                    1255,
                    1831,
                    1258,
                    1259,
                    876,
                    877,
                    846,
                    1261,
                    1847,
                    852,
                    1844,
                    1845,
                    1846
                ],
                "prev_output": [
                    856,
                    865,
                    880,
                    881
                ],
                "_generate_dropout_mask": [
                    864,
                    1857,
                    1864,
                    1299,
                    1306,
                    858
                ],
                "K.ones_like": [
                    865,
                    1858,
                    859,
                    1865,
                    1300,
                    1307
                ],
                "dp_mask": [
                    1313,
                    869,
                    1319,
                    872,
                    873,
                    1320,
                    1321,
                    1871,
                    1365,
                    1880,
                    1881,
                    1882,
                    1883,
                    1919
                ],
                "rec_dp_mask": [
                    1377,
                    1922,
                    1315,
                    870,
                    1900,
                    1901,
                    1902,
                    879,
                    880,
                    1873,
                    1903,
                    1336,
                    1337,
                    1338
                ],
                "h": [
                    1408,
                    1410,
                    873,
                    875,
                    877,
                    881,
                    1937,
                    1940,
                    1941,
                    1404
                ],
                "K.dot": [
                    1920,
                    1923,
                    1327,
                    1328,
                    1329,
                    1344,
                    1345,
                    1355,
                    1360,
                    1368,
                    1889,
                    1890,
                    1891,
                    1892,
                    1381,
                    873,
                    1386,
                    875,
                    881,
                    1909,
                    1398,
                    1911,
                    1913,
                    1915
                ],
                "K.bias_add": [
                    1347,
                    1348,
                    1925,
                    1894,
                    1383,
                    1895,
                    1896,
                    1897,
                    877,
                    1357,
                    1331,
                    1332,
                    1333,
                    1371
                ],
                "activations.serialize": [
                    1669,
                    1414,
                    1415,
                    1670,
                    1098,
                    2192,
                    2193,
                    1945,
                    1946,
                    893
                ],
                "initializers.serialize": [
                    896,
                    897,
                    1672,
                    1417,
                    1418,
                    1419,
                    1100,
                    1101,
                    1102,
                    1673,
                    1674,
                    2195,
                    2196,
                    2197,
                    1948,
                    1949,
                    1950,
                    895
                ],
                "regularizers.serialize": [
                    898,
                    899,
                    900,
                    1675,
                    1420,
                    1421,
                    1422,
                    1676,
                    1677,
                    1678,
                    2199,
                    2200,
                    2201,
                    2202,
                    1952,
                    1953,
                    1954,
                    1103,
                    1104,
                    1105,
                    1106
                ],
                "constraints.serialize": [
                    1955,
                    1956,
                    901,
                    902,
                    903,
                    1957,
                    1423,
                    1424,
                    1425,
                    1679,
                    1107,
                    1108,
                    1109,
                    1680,
                    1681,
                    2203,
                    2204,
                    2205
                ],
                "kwargs.pop": [
                    996
                ],
                "warnings.warn": [
                    2080,
                    997,
                    1001,
                    1553,
                    1557,
                    2076
                ],
                "warnings": [
                    2080,
                    997,
                    1001,
                    1553,
                    1557,
                    2076
                ],
                "K.backend": [
                    1000,
                    1556,
                    2079
                ],
                "SimpleRNN": [
                    1112,
                    1035,
                    1023
                ],
                "self.activity_regularizer": [
                    2112,
                    1030,
                    1678,
                    1106,
                    1589,
                    2202
                ],
                "activity_regularizer": [
                    2112,
                    1589,
                    1030
                ],
                "interfaces.legacy_recurrent_support": [
                    2050,
                    973,
                    1527
                ],
                "interfaces": [
                    2050,
                    973,
                    1527
                ],
                "self.cell._dropout_mask": [
                    1592,
                    1033,
                    2115
                ],
                "self.cell._recurrent_dropout_mask": [
                    1593,
                    1034,
                    2116
                ],
                "call": [
                    1594,
                    1035,
                    2117
                ],
                "self.cell.units": [
                    1601,
                    1042,
                    2124
                ],
                "self.cell.activation": [
                    2128,
                    1605,
                    1046
                ],
                "self.cell.use_bias": [
                    2136,
                    1050,
                    1613
                ],
                "self.cell.kernel_initializer": [
                    1617,
                    2140,
                    1054
                ],
                "self.cell.recurrent_initializer": [
                    2144,
                    1058,
                    1621
                ],
                "self.cell.bias_initializer": [
                    1625,
                    2148,
                    1062
                ],
                "self.cell.kernel_regularizer": [
                    1066,
                    2156,
                    1629
                ],
                "self.cell.recurrent_regularizer": [
                    2160,
                    1633,
                    1070
                ],
                "self.cell.bias_regularizer": [
                    1074,
                    2164,
                    1637
                ],
                "self.cell.kernel_constraint": [
                    2168,
                    1641,
                    1078
                ],
                "self.cell.recurrent_constraint": [
                    1082,
                    2172,
                    1645
                ],
                "self.cell.bias_constraint": [
                    2176,
                    1649,
                    1086
                ],
                "self.cell.dropout": [
                    1090,
                    2180,
                    1653
                ],
                "self.cell.recurrent_dropout": [
                    2184,
                    1657,
                    1094
                ],
                "GRUCell": [
                    1200,
                    1565,
                    1430
                ],
                "self.recurrent_activation": [
                    1350,
                    1351,
                    1415,
                    1670,
                    1932,
                    1933,
                    1935,
                    1392,
                    1393,
                    1778,
                    1203,
                    2193,
                    1909,
                    1911,
                    1946,
                    1915
                ],
                "recurrent_activation": [
                    1778,
                    1203,
                    2090,
                    1567
                ],
                "self.implementation": [
                    2208,
                    1796,
                    1220,
                    1317,
                    1960,
                    1428,
                    1684,
                    1878
                ],
                "implementation": [
                    1220,
                    1796,
                    1580,
                    1552,
                    2104,
                    2075
                ],
                "self.reset_after": [
                    1281,
                    1346,
                    1379,
                    1221,
                    1254,
                    1289,
                    1354,
                    1395,
                    1429,
                    1685,
                    1241
                ],
                "reset_after": [
                    1581,
                    1221
                ],
                "bias_shape": [
                    1248,
                    1249,
                    1242
                ],
                "self.input_bias": [
                    1255,
                    1258,
                    1371,
                    1277,
                    1278,
                    1279
                ],
                "self.recurrent_bias": [
                    1282,
                    1283,
                    1284,
                    1255,
                    1383,
                    1259
                ],
                "K.flatten": [
                    1258,
                    1259
                ],
                "self.kernel_z": [
                    1264,
                    1327
                ],
                "self.recurrent_kernel_z": [
                    1344,
                    1265
                ],
                "self.kernel_r": [
                    1328,
                    1267
                ],
                "self.recurrent_kernel_r": [
                    1345,
                    1268
                ],
                "self.kernel_h": [
                    1272,
                    1329
                ],
                "self.recurrent_kernel_h": [
                    1360,
                    1273,
                    1355
                ],
                "self.input_bias_z": [
                    1331,
                    1277,
                    1286
                ],
                "self.input_bias_r": [
                    1332,
                    1278,
                    1287
                ],
                "self.input_bias_h": [
                    1288,
                    1333,
                    1279
                ],
                "self.recurrent_bias_z": [
                    1282,
                    1290,
                    1347
                ],
                "self.recurrent_bias_r": [
                    1291,
                    1283,
                    1348
                ],
                "self.recurrent_bias_h": [
                    1284,
                    1292,
                    1357
                ],
                "h_tm1": [
                    1922,
                    1923,
                    1296,
                    1307,
                    1336,
                    1337,
                    1338,
                    1340,
                    1341,
                    1342,
                    1875,
                    1377,
                    1381,
                    1386,
                    1900,
                    1901,
                    1902,
                    1903,
                    1905,
                    1906,
                    1907,
                    1908,
                    1398,
                    1404
                ],
                "inputs_z": [
                    1323,
                    1327,
                    1319
                ],
                "inputs_r": [
                    1320,
                    1328,
                    1324
                ],
                "inputs_h": [
                    1321,
                    1325,
                    1329
                ],
                "x_z": [
                    1350,
                    1327,
                    1392,
                    1331,
                    1372
                ],
                "x_r": [
                    1351,
                    1328,
                    1393,
                    1332,
                    1373
                ],
                "x_h": [
                    1329,
                    1362,
                    1333,
                    1401,
                    1374
                ],
                "h_tm1_z": [
                    1336,
                    1344,
                    1340
                ],
                "h_tm1_r": [
                    1345,
                    1337,
                    1341
                ],
                "h_tm1_h": [
                    1360,
                    1338,
                    1355,
                    1342
                ],
                "recurrent_z": [
                    1344,
                    1347,
                    1350,
                    1389,
                    1392
                ],
                "recurrent_r": [
                    1345,
                    1348,
                    1351,
                    1390,
                    1393
                ],
                "z": [
                    1920,
                    1923,
                    1925,
                    1350,
                    1927,
                    1928,
                    1929,
                    1930,
                    1392,
                    1404
                ],
                "r": [
                    1351,
                    1358,
                    1360,
                    1393,
                    1396,
                    1398
                ],
                "recurrent_h": [
                    1355,
                    1357,
                    1358,
                    1360,
                    1362,
                    1396,
                    1398,
                    1401
                ],
                "hh": [
                    1401,
                    1362,
                    1404
                ],
                "matrix_x": [
                    1368,
                    1371,
                    1372,
                    1373,
                    1374
                ],
                "matrix_inner": [
                    1381,
                    1383,
                    1386,
                    1389,
                    1390,
                    1396
                ],
                "h._uses_learning_phase": [
                    1408,
                    1940
                ],
                "GRU": [
                    1594,
                    1686,
                    1582
                ],
                "self.cell.recurrent_activation": [
                    1609,
                    2132
                ],
                "self.cell.implementation": [
                    2188,
                    1661
                ],
                "self.cell.reset_after": [
                    1665
                ],
                "LSTMCell": [
                    2088,
                    1961,
                    1775
                ],
                "self.unit_forget_bias": [
                    1784,
                    1816,
                    2198,
                    1951
                ],
                "unit_forget_bias": [
                    1784,
                    2094
                ],
                "K.concatenate": [
                    1818
                ],
                "args": [
                    1819,
                    1820,
                    1821
                ],
                "initializers.Ones": [
                    1820
                ],
                "self.kernel_i": [
                    1833,
                    1889
                ],
                "self.kernel_f": [
                    1834,
                    1890
                ],
                "self.kernel_c": [
                    1891,
                    1835
                ],
                "self.kernel_o": [
                    1892,
                    1836
                ],
                "self.recurrent_kernel_i": [
                    1910,
                    1838
                ],
                "self.recurrent_kernel_f": [
                    1912,
                    1839
                ],
                "self.recurrent_kernel_c": [
                    1840,
                    1914
                ],
                "self.recurrent_kernel_o": [
                    1841,
                    1916
                ],
                "self.bias_i": [
                    1849,
                    1844,
                    1894
                ],
                "self.bias_f": [
                    1850,
                    1845,
                    1895
                ],
                "self.bias_c": [
                    1896,
                    1851,
                    1846
                ],
                "self.bias_o": [
                    1897,
                    1852,
                    1847
                ],
                "c_tm1": [
                    1913,
                    1876,
                    1934
                ],
                "inputs_i": [
                    1880,
                    1889,
                    1885
                ],
                "inputs_f": [
                    1881,
                    1890,
                    1886
                ],
                "inputs_c": [
                    1882,
                    1891,
                    1887
                ],
                "inputs_o": [
                    1888,
                    1883,
                    1892
                ],
                "x_i": [
                    1889,
                    1909,
                    1894
                ],
                "x_f": [
                    1890,
                    1911,
                    1895
                ],
                "x_c": [
                    1896,
                    1913,
                    1891
                ],
                "x_o": [
                    1897,
                    1915,
                    1892
                ],
                "h_tm1_i": [
                    1905,
                    1900,
                    1909
                ],
                "h_tm1_f": [
                    1906,
                    1901,
                    1911
                ],
                "h_tm1_c": [
                    1913,
                    1907,
                    1902
                ],
                "h_tm1_o": [
                    1915,
                    1908,
                    1903
                ],
                "f": [
                    1913,
                    1933,
                    1934,
                    1911
                ],
                "c": [
                    1913,
                    1937,
                    1941,
                    1934
                ],
                "o": [
                    1937,
                    1915,
                    1935
                ],
                "z0": [
                    1932,
                    1927
                ],
                "z1": [
                    1928,
                    1933
                ],
                "z2": [
                    1929,
                    1934
                ],
                "z3": [
                    1930,
                    1935
                ],
                "LSTM": [
                    2105,
                    2209,
                    2117
                ],
                "self.cell.unit_forget_bias": [
                    2152
                ],
                "K.dropout": [
                    2222
                ],
                "ones": [
                    2227,
                    2222,
                    2231
                ],
                "rate": [
                    2222
                ],
                "count": [
                    2224,
                    2228
                ],
                "K.in_train_phase": [
                    2225,
                    2229
                ],
                "dropped_inputs": [
                    2226,
                    2230
                ],
                "x": [
                    2264,
                    2265,
                    2266,
                    2267,
                    2268
                ],
                "to_list_or_none": [
                    2270,
                    2271
                ]
            }
        },
        "test_data": [
            {
                "test_path": "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_19/tests/keras/layers/recurrent_test.py",
                "test_function": "test_inconsistent_output_state_size",
                "test_function_code": "@keras_test\n@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\ndef test_inconsistent_output_state_size():\n\n    class PlusOneRNNCell(keras.layers.Layer):\n        \"\"\"Add one to the input and state.\n\n        This cell is used for testing state_size and output_size.\"\"\"\n\n        def __init__(self, num_unit, **kwargs):\n            self.state_size = num_unit\n            super(PlusOneRNNCell, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            self.output_size = input_shape[-1]\n\n        def call(self, inputs, states):\n            return inputs + 1, [states[0] + 1]\n\n    batch = 32\n    time_step = 4\n    state_size = 5\n    input_size = 6\n    cell = PlusOneRNNCell(state_size)\n    x = keras.Input((None, input_size))\n    layer = recurrent.RNN(cell)\n    y = layer(x)\n\n    assert cell.state_size == state_size\n    init_state = layer.get_initial_state(x)\n    assert len(init_state) == 1\n    if K.backend() != 'theano':\n        # theano does not support static shape inference.\n        assert K.int_shape(init_state[0]) == (None, state_size)\n\n    model = keras.models.Model(x, y)\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.train_on_batch(\n        np.zeros((batch, time_step, input_size)),\n        np.zeros((batch, input_size)))\n    assert model.output_shape == (None, input_size)",
                "test_error": "ValueError: Error when checking target: expected rnn_1 to have shape (5,) but got array with shape (6,)",
                "full_test_error": "@keras_test\n    @pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n    def test_inconsistent_output_state_size():\n    \n        class PlusOneRNNCell(keras.layers.Layer):\n            \"\"\"Add one to the input and state.\n    \n            This cell is used for testing state_size and output_size.\"\"\"\n    \n            def __init__(self, num_unit, **kwargs):\n                self.state_size = num_unit\n                super(PlusOneRNNCell, self).__init__(**kwargs)\n    \n            def build(self, input_shape):\n                self.output_size = input_shape[-1]\n    \n            def call(self, inputs, states):\n                return inputs + 1, [states[0] + 1]\n    \n        batch = 32\n        time_step = 4\n        state_size = 5\n        input_size = 6\n        cell = PlusOneRNNCell(state_size)\n        x = keras.Input((None, input_size))\n        layer = recurrent.RNN(cell)\n        y = layer(x)\n    \n        assert cell.state_size == state_size\n        init_state = layer.get_initial_state(x)\n        assert len(init_state) == 1\n        if K.backend() != 'theano':\n            # theano does not support static shape inference.\n            assert K.int_shape(init_state[0]) == (None, state_size)\n    \n        model = keras.models.Model(x, y)\n        model.compile(optimizer='rmsprop', loss='mse')\n        model.train_on_batch(\n            np.zeros((batch, time_step, input_size)),\n>           np.zeros((batch, input_size)))\n\ntests/keras/layers/recurrent_test.py:962: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nkeras/engine/training.py:1217: in train_on_batch\n    class_weight=class_weight)\nkeras/engine/training.py:795: in _standardize_user_data\n    exception_prefix='target')\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ndata = [array([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0...., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]])]\nnames = ['rnn_1'], shapes = [(None, 5)], check_batch_axis = False\nexception_prefix = 'target'\n\n    def standardize_input_data(data,\n                               names,\n                               shapes=None,\n                               check_batch_axis=True,\n                               exception_prefix=''):\n        \"\"\"Normalizes inputs and targets provided by users.\n    \n        Users may pass data as a list of arrays, dictionary of arrays,\n        or as a single array. We normalize this to an ordered list of\n        arrays (same order as `names`), while checking that the provided\n        arrays have shapes that match the network's expectations.\n    \n        # Arguments\n            data: User-provided input data (polymorphic).\n            names: List of expected array names.\n            shapes: Optional list of expected array shapes.\n            check_batch_axis: Boolean; whether to check that\n                the batch axis of the arrays matches the expected\n                value found in `shapes`.\n            exception_prefix: String prefix used for exception formatting.\n    \n        # Returns\n            List of standardized input arrays (one array per model input).\n    \n        # Raises\n            ValueError: in case of improperly formatted user-provided data.\n        \"\"\"\n        if not names:\n            if data is not None and hasattr(data, '__len__') and len(data):\n                raise ValueError('Error when checking model ' +\n                                 exception_prefix + ': '\n                                 'expected no data, but got:', data)\n            return []\n        if data is None:\n            return [None for _ in range(len(names))]\n    \n        if isinstance(data, dict):\n            try:\n                data = [\n                    data[x].values\n                    if data[x].__class__.__name__ == 'DataFrame' else data[x]\n                    for x in names\n                ]\n            except KeyError as e:\n                raise ValueError('No data provided for \"' + e.args[0] +\n                                 '\". Need data '\n                                 'for each key in: ' + str(names))\n        elif isinstance(data, list):\n            if isinstance(data[0], list):\n                data = [np.asarray(d) for d in data]\n            elif len(names) == 1 and isinstance(data[0], (float, int)):\n                data = [np.asarray(data)]\n            else:\n                data = [\n                    x.values if x.__class__.__name__ == 'DataFrame'\n                    else x for x in data\n                ]\n        else:\n            data = data.values if data.__class__.__name__ == 'DataFrame' else data\n            data = [data]\n        data = [standardize_single_array(x) for x in data]\n    \n        if len(data) != len(names):\n            if data and hasattr(data[0], 'shape'):\n                raise ValueError(\n                    'Error when checking model ' + exception_prefix +\n                    ': the list of Numpy arrays that you are passing to '\n                    'your model is not the size the model expected. '\n                    'Expected to see ' + str(len(names)) + ' array(s), '\n                    'but instead got the following list of ' +\n                    str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n            elif len(names) > 1:\n                raise ValueError(\n                    'Error when checking model ' + exception_prefix +\n                    ': you are passing a list as input to your model, '\n                    'but the model expects a list of ' + str(len(names)) +\n                    ' Numpy arrays instead. '\n                    'The list you passed was: ' + str(data)[:200])\n            elif len(data) == 1 and not hasattr(data[0], 'shape'):\n                raise TypeError('Error when checking model ' + exception_prefix +\n                                ': data should be a Numpy array, or list/dict of '\n                                'Numpy arrays. Found: ' + str(data)[:200] + '...')\n            elif len(names) == 1:\n                data = [np.asarray(data)]\n    \n        # Check shapes compatibility.\n        if shapes:\n            for i in range(len(names)):\n                if shapes[i] is not None and not K.is_tensor(data[i]):\n                    data_shape = data[i].shape\n                    shape = shapes[i]\n                    if data[i].ndim != len(shape):\n                        raise ValueError(\n                            'Error when checking ' + exception_prefix +\n                            ': expected ' + names[i] + ' to have ' +\n                            str(len(shape)) + ' dimensions, but got array '\n                            'with shape ' + str(data_shape))\n                    if not check_batch_axis:\n                        data_shape = data_shape[1:]\n                        shape = shape[1:]\n                    for dim, ref_dim in zip(data_shape, shape):\n                        if ref_dim != dim and ref_dim:\n                            raise ValueError(\n                                'Error when checking ' + exception_prefix +\n                                ': expected ' + names[i] + ' to have shape ' +\n                                str(shape) + ' but got array with shape ' +\n>                               str(data_shape))\nE                           ValueError: Error when checking target: expected rnn_1 to have shape (5,) but got array with shape (6,)\n\nkeras/engine/training_utils.py:138: ValueError",
                "traceback": "keras/engine/training.py:1217: in train_on_batch\n    class_weight=class_weight)\nkeras/engine/training.py:795: in _standardize_user_data\n    exception_prefix='target')",
                "test_error_location": "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ndata = [array([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0...., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]])]\nnames = ['rnn_1'], shapes = [(None, 5)], check_batch_axis = False\nexception_prefix = 'target'\n\n    def standardize_input_data(data,\n                               names,\n                               shapes=None,\n                               check_batch_axis=True,\n                               exception_prefix=''):\n        \"\"\"Normalizes inputs and targets provided by users.\n    \n        Users may pass data as a list of arrays, dictionary of arrays,\n        or as a single array. We normalize this to an ordered list of\n        arrays (same order as `names`), while checking that the provided\n        arrays have shapes that match the network's expectations.\n    \n        # Arguments\n            data: User-provided input data (polymorphic).\n            names: List of expected array names.\n            shapes: Optional list of expected array shapes.\n            check_batch_axis: Boolean; whether to check that\n                the batch axis of the arrays matches the expected\n                value found in `shapes`.\n            exception_prefix: String prefix used for exception formatting.\n    \n        # Returns\n            List of standardized input arrays (one array per model input).\n    \n        # Raises\n            ValueError: in case of improperly formatted user-provided data.\n        \"\"\"\n        if not names:\n            if data is not None and hasattr(data, '__len__') and len(data):\n                raise ValueError('Error when checking model ' +\n                                 exception_prefix + ': '\n                                 'expected no data, but got:', data)\n            return []\n        if data is None:\n            return [None for _ in range(len(names))]\n    \n        if isinstance(data, dict):\n            try:\n                data = [\n                    data[x].values\n                    if data[x].__class__.__name__ == 'DataFrame' else data[x]\n                    for x in names\n                ]\n            except KeyError as e:\n                raise ValueError('No data provided for \"' + e.args[0] +\n                                 '\". Need data '\n                                 'for each key in: ' + str(names))\n        elif isinstance(data, list):\n            if isinstance(data[0], list):\n                data = [np.asarray(d) for d in data]\n            elif len(names) == 1 and isinstance(data[0], (float, int)):\n                data = [np.asarray(data)]\n            else:\n                data = [\n                    x.values if x.__class__.__name__ == 'DataFrame'\n                    else x for x in data\n                ]\n        else:\n            data = data.values if data.__class__.__name__ == 'DataFrame' else data\n            data = [data]\n        data = [standardize_single_array(x) for x in data]\n    \n        if len(data) != len(names):\n            if data and hasattr(data[0], 'shape'):\n                raise ValueError(\n                    'Error when checking model ' + exception_prefix +\n                    ': the list of Numpy arrays that you are passing to '\n                    'your model is not the size the model expected. '\n                    'Expected to see ' + str(len(names)) + ' array(s), '\n                    'but instead got the following list of ' +\n                    str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n            elif len(names) > 1:\n                raise ValueError(\n                    'Error when checking model ' + exception_prefix +\n                    ': you are passing a list as input to your model, '\n                    'but the model expects a list of ' + str(len(names)) +\n                    ' Numpy arrays instead. '\n                    'The list you passed was: ' + str(data)[:200])\n            elif len(data) == 1 and not hasattr(data[0], 'shape'):\n                raise TypeError('Error when checking model ' + exception_prefix +\n                                ': data should be a Numpy array, or list/dict of '\n                                'Numpy arrays. Found: ' + str(data)[:200] + '...')\n            elif len(names) == 1:\n                data = [np.asarray(data)]\n    \n        # Check shapes compatibility.\n        if shapes:\n            for i in range(len(names)):\n                if shapes[i] is not None and not K.is_tensor(data[i]):\n                    data_shape = data[i].shape\n                    shape = shapes[i]\n                    if data[i].ndim != len(shape):\n                        raise ValueError(\n                            'Error when checking ' + exception_prefix +\n                            ': expected ' + names[i] + ' to have ' +\n                            str(len(shape)) + ' dimensions, but got array '\n                            'with shape ' + str(data_shape))\n                    if not check_batch_axis:\n                        data_shape = data_shape[1:]\n                        shape = shape[1:]\n                    for dim, ref_dim in zip(data_shape, shape):\n                        if ref_dim != dim and ref_dim:\n                            raise ValueError(\n                                'Error when checking ' + exception_prefix +\n                                ': expected ' + names[i] + ' to have shape ' +\n                                str(shape) + ' but got array with shape ' +\n>                               str(data_shape))\nE                           ValueError: Error when checking target: expected rnn_1 to have shape (5,) but got array with shape (6,)\n\nkeras/engine/training_utils.py:138: ValueError",
                "test_function_decorators": [
                    "keras_test",
                    "pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')"
                ]
            },
            {
                "test_path": "/Volumes/SSD2T/bgp_envs_non_pandas/repos/keras_19/tests/keras/layers/recurrent_test.py",
                "test_function": "test_minimal_rnn_cell_non_layer_multiple_states",
                "test_function_code": "@keras_test\ndef test_minimal_rnn_cell_non_layer_multiple_states():\n\n    class MinimalRNNCell(object):\n\n        def __init__(self, units, input_dim):\n            self.units = units\n            self.state_size = (units, units)\n            self.kernel = keras.backend.variable(\n                np.random.random((input_dim, units)))\n\n        def call(self, inputs, states):\n            prev_output_1 = states[0]\n            prev_output_2 = states[1]\n            output = keras.backend.dot(inputs, self.kernel)\n            output += prev_output_1\n            output -= prev_output_2\n            return output, [output * 2, output * 3]\n\n    # Basic test case.\n    cell = MinimalRNNCell(32, 5)\n    x = keras.Input((None, 5))\n    layer = recurrent.RNN(cell)\n    y = layer(x)\n    model = keras.models.Model(x, y)\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n\n    # Test stacking.\n    cells = [MinimalRNNCell(8, 5),\n             MinimalRNNCell(16, 8),\n             MinimalRNNCell(32, 16)]\n    layer = recurrent.RNN(cells)\n    assert layer.cell.state_size == (8, 8, 16, 16, 32, 32)\n    y = layer(x)\n    model = keras.models.Model(x, y)\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))",
                "test_error": "assert (32, 32, 16, 16, 8, 8) == (8, 8, 16, 16, 32, 32)   At index 0 diff: 32 != 8   Full diff:   - (8, 8, 16, 16, 32, 32)   + (32, 32, 16, 16, 8, 8)",
                "full_test_error": "@keras_test\n    def test_minimal_rnn_cell_non_layer_multiple_states():\n    \n        class MinimalRNNCell(object):\n    \n            def __init__(self, units, input_dim):\n                self.units = units\n                self.state_size = (units, units)\n                self.kernel = keras.backend.variable(\n                    np.random.random((input_dim, units)))\n    \n            def call(self, inputs, states):\n                prev_output_1 = states[0]\n                prev_output_2 = states[1]\n                output = keras.backend.dot(inputs, self.kernel)\n                output += prev_output_1\n                output -= prev_output_2\n                return output, [output * 2, output * 3]\n    \n        # Basic test case.\n        cell = MinimalRNNCell(32, 5)\n        x = keras.Input((None, 5))\n        layer = recurrent.RNN(cell)\n        y = layer(x)\n        model = keras.models.Model(x, y)\n        model.compile(optimizer='rmsprop', loss='mse')\n        model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n    \n        # Test stacking.\n        cells = [MinimalRNNCell(8, 5),\n                 MinimalRNNCell(16, 8),\n                 MinimalRNNCell(32, 16)]\n        layer = recurrent.RNN(cells)\n>       assert layer.cell.state_size == (8, 8, 16, 16, 32, 32)\nE       assert (32, 32, 16, 16, 8, 8) == (8, 8, 16, 16, 32, 32)\nE         At index 0 diff: 32 != 8\nE         Full diff:\nE         - (8, 8, 16, 16, 32, 32)\nE         + (32, 32, 16, 16, 8, 8)\n\ntests/keras/layers/recurrent_test.py:502: AssertionError",
                "traceback": null,
                "test_error_location": null,
                "test_function_decorators": [
                    "keras_test"
                ]
            }
        ]
    }
}