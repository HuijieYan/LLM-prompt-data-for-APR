{
    "luigi": [
        {
            "bugID": 11,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "start_line": 818,
            "file_name": "luigi/scheduler.py",
            "replace_code": "def get_work(self, host=None, assistant=False, current_tasks=None, worker=None, **kwargs):\n    # TODO: remove any expired nodes\n    \n\n    if self._config.prune_on_get_work:\n        self.prune()\n    \n    assert worker is not None\n    worker_id = worker\n    # Return remaining tasks that have no FAILED descendants\n    self.update(worker_id, {'host': host}, get_work=True)\n    if assistant:\n        self.add_worker(worker_id, [('assistant', assistant)])\n    \n    ct_set = set(current_tasks) if current_tasks is not None else set()\n    best_task = None\n    for task in sorted(self._state.get_running_tasks(), key=self._rank):\n        if task.worker_running == worker_id and task.id not in ct_set:\n            best_task = task\n            break\n    \n    if current_tasks is not None:\n        # batch running tasks that weren't claimed since the last get_work go back in the pool\n        self._reset_orphaned_batch_running_tasks(worker_id)\n    \n    locally_pending_tasks = 0\n    running_tasks = []\n    n_unique_pending = 0\n    \n    worker = self._state.get_worker(worker_id)\n    if worker.is_trivial_worker(self._state):\n        relevant_tasks = worker.get_pending_tasks(self._state)\n    else:\n        relevant_tasks = self._state.get_pending_tasks()\n    \n    retrieved_tasks = []\n    for task in relevant_tasks:\n        if task.status == PENDING and (assistant or worker_id in task.workers):\n            upstream_status = self._upstream_status(task.id)\n            if upstream_status != UPSTREAM_DISABLED:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1 and not assistant:\n                    n_unique_pending += 1\n            retrieved_tasks.append(task)\n    \n    batched_tasks = self._scheduler_resources.retrieve_batchable_tasks(relevant_tasks, len(retrieved_tasks))\n    if assistant:\n        self._add_to_worker(key=worker_id, value='assistant')"
        }
    ]
}