```python
def run_job(self, job):
    arglist = []
    if not job.jar() or not job.jar().exists():
        raise HadoopJarJobError("job jar does not exist")

    ssh_config = job.ssh()
    if ssh_config:
        host = ssh_config.get("host")
        key_file = ssh_config.get("key_file")
        username = ssh_config.get("username")
        if not all([host, key_file, username, job.jar()]):
            raise HadoopJarJobError("missing some config for HadoopRemoteJarJobRunner")
        
        arglist = ['ssh', '-i', key_file, '-o', 'BatchMode=yes']  # no password prompts etc
        if ssh_config.get("no_host_key_check", False):
            arglist += ['-o', 'UserKnownHostsFile=/dev/null', '-o', 'StrictHostKeyChecking=no']
        arglist.append('{}@{}'.format(username, host))

    hadoop_arglist = load_hadoop_cmd() + ['jar', job.jar()]
    if job.main():
        hadoop_arglist.append(job.main())

    jobconfs = job.jobconfs()

    for jc in jobconfs:
        hadoop_arglist += ['-D' + jc]

    (tmp_files, job_args) = fix_paths(job)

    hadoop_arglist += job_args
    arglist.extend(hadoop_arglist)

    run_and_track_hadoop_job(arglist)

    for a, b in tmp_files:
        a.move(b)
```