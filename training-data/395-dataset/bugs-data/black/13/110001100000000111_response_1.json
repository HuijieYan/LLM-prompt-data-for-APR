{
    "black": [
        {
            "bugID": 13,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 1,
                "4": 0,
                "5": 0,
                "6": 1,
                "7": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 1,
                "4": 0,
                "5": 0,
                "6": 1,
                "7": 1
            },
            "start_line": 337,
            "file_name": "/blib2to3/pgen2/tokenize.py",
            "replace_code": "def generate_tokens(readline):\n    import re\n    lnum = parenlev = continued = 0\n    numchars = '0123456789'\n    contstr, needcont = '', 0\n    contline = None\n    indents = [0]\n    \n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n    \n    # Regular expressions for token matching\n    endprog = re.compile(r\"(\\r|\\n|\\r\\n)$\")\n    pseudoprog = re.compile(r\"(\\r|\\n|\\r\\n)|([^ \\f\\t\\[\\](){}#,;\\\\\\000-\\040]+)\")\n    single_quoted = {'\\'', '\"\"\"', \"'''\", 'r\"\"\"', 'r\\'\\'\\'', \"r'''\", 'f\"\"\"', 'f\\'\\'\\''}\n    triple_quoted = {\"'''\", '\"\"\"'}\n    endprogs = {'\\'': re.compile(r\"(\\\\\\\\|\\\\'|[^'])*'\"), '\"\"\"': re.compile(r'(\\\\\\\\\"\"\"|[^\"])*\"\"\"'),\n                \"r'''\": re.compile(r\"(\\\\\\\\|\\\\'|[^'])*'''\"), 'r\"\"\"': re.compile(r'(\\\\\\\\\"\"\"|[^\"])*\"\"\"'),\n                \"f'''\": re.compile(r\"(\\\\\\\\|\\\\'|[^'])*'''\"), 'f\"\"\"': re.compile(r'(\\\\\\\\\"\"\"|[^\"])*\"\"\"')}\n    \n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = ''\n        lnum = lnum + 1\n        pos, max = 0, len(line)"
        }
    ]
}