```python
# Corrected function
def write(
    self, df, path, compression="snappy", index=None, partition_cols=None, **kwargs
):
    self.validate_dataframe(df)

    if "partition_on" in kwargs:
        if partition_cols is not None:
            raise ValueError(
                "Cannot use both partition_on and "
                "partition_cols. Use partition_cols for "
                "partitioning data"
            )
        else:
            partition_cols = kwargs.pop("partition_on")

    if partition_cols is not None:
        kwargs["file_scheme"] = "hive"

    if is_s3_url(path):
        path, _, _, _ = get_filepath_or_buffer(path, mode="wb")  # Changed 'wb' mode to support writing to a not-yet-existent GCS Parquet file
        kwargs["open_with"] = lambda path, _: path
    else:
        path, _, _, _ = get_filepath_or_buffer(path)

    with warnings.catch_warnings(record=True):  # Changed 'catch_warnings' to 'warnings.catch_warnings' to correctly handle warnings
        self.api.write(
            path,
            df,
            compression=compression,
            write_index=index,
            partition_on=partition_cols,
            **kwargs
        )

```