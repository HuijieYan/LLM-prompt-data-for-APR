{
    "pandas:149": {
        "/Volumes/JerrySSD/bgp_envs/repos/pandas_149/pandas/io/parquet.py": {
            "buggy_functions": [
                {
                    "function_name": "write",
                    "function_code": "def write(\n    self, df, path, compression=\"snappy\", index=None, partition_cols=None, **kwargs\n):\n    self.validate_dataframe(df)\n    # thriftpy/protocol/compact.py:339:\n    # DeprecationWarning: tostring() is deprecated.\n    # Use tobytes() instead.\n\n    if \"partition_on\" in kwargs and partition_cols is not None:\n        raise ValueError(\n            \"Cannot use both partition_on and \"\n            \"partition_cols. Use partition_cols for \"\n            \"partitioning data\"\n        )\n    elif \"partition_on\" in kwargs:\n        partition_cols = kwargs.pop(\"partition_on\")\n\n    if partition_cols is not None:\n        kwargs[\"file_scheme\"] = \"hive\"\n\n    if is_s3_url(path):\n        # path is s3:// so we need to open the s3file in 'wb' mode.\n        # TODO: Support 'ab'\n\n        path, _, _, _ = get_filepath_or_buffer(path, mode=\"wb\")\n        # And pass the opened s3file to the fastparquet internal impl.\n        kwargs[\"open_with\"] = lambda path, _: path\n    else:\n        path, _, _, _ = get_filepath_or_buffer(path)\n\n    with catch_warnings(record=True):\n        self.api.write(\n            path,\n            df,\n            compression=compression,\n            write_index=index,\n            partition_on=partition_cols,\n            **kwargs\n        )\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 142,
                    "end_line": 180,
                    "variables": {
                        "self.validate_dataframe": [
                            145
                        ],
                        "self": [
                            145,
                            173
                        ],
                        "df": [
                            145,
                            175
                        ],
                        "kwargs": [
                            160,
                            168,
                            179,
                            150,
                            156,
                            157
                        ],
                        "partition_cols": [
                            178,
                            157,
                            150,
                            159
                        ],
                        "ValueError": [
                            151
                        ],
                        "kwargs.pop": [
                            157
                        ],
                        "is_s3_url": [
                            162
                        ],
                        "path": [
                            162,
                            166,
                            168,
                            170,
                            174
                        ],
                        "_": [
                            170,
                            166
                        ],
                        "get_filepath_or_buffer": [
                            170,
                            166
                        ],
                        "catch_warnings": [
                            172
                        ],
                        "self.api.write": [
                            173
                        ],
                        "self.api": [
                            173
                        ],
                        "compression": [
                            176
                        ],
                        "index": [
                            177
                        ]
                    },
                    "filtered_variables": {
                        "self.validate_dataframe": [
                            145
                        ],
                        "self": [
                            145,
                            173
                        ],
                        "df": [
                            145,
                            175
                        ],
                        "kwargs": [
                            160,
                            168,
                            179,
                            150,
                            156,
                            157
                        ],
                        "partition_cols": [
                            178,
                            157,
                            150,
                            159
                        ],
                        "kwargs.pop": [
                            157
                        ],
                        "is_s3_url": [
                            162
                        ],
                        "path": [
                            162,
                            166,
                            168,
                            170,
                            174
                        ],
                        "_": [
                            170,
                            166
                        ],
                        "get_filepath_or_buffer": [
                            170,
                            166
                        ],
                        "catch_warnings": [
                            172
                        ],
                        "self.api.write": [
                            173
                        ],
                        "self.api": [
                            173
                        ],
                        "compression": [
                            176
                        ],
                        "index": [
                            177
                        ]
                    },
                    "diff_line_number": 162,
                    "class_data": {
                        "signature": "class FastParquetImpl(BaseImpl)",
                        "docstring": null,
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self):\n    fastparquet = import_optional_dependency('fastparquet', extra='fastparquet is required for parquet support.')\n    self.api = fastparquet",
                            "def write(self, df, path, compression='snappy', index=None, partition_cols=None, **kwargs):\n    self.validate_dataframe(df)\n    if 'partition_on' in kwargs and partition_cols is not None:\n        raise ValueError('Cannot use both partition_on and partition_cols. Use partition_cols for partitioning data')\n    elif 'partition_on' in kwargs:\n        partition_cols = kwargs.pop('partition_on')\n    if partition_cols is not None:\n        kwargs['file_scheme'] = 'hive'\n    if is_s3_url(path):\n        path, _, _, _ = get_filepath_or_buffer(path, mode='wb')\n        kwargs['open_with'] = lambda path, _: path\n    else:\n        path, _, _, _ = get_filepath_or_buffer(path)\n    with catch_warnings(record=True):\n        self.api.write(path, df, compression=compression, write_index=index, partition_on=partition_cols, **kwargs)",
                            "def read(self, path, columns=None, **kwargs):\n    if is_s3_url(path):\n        from pandas.io.s3 import get_file_and_filesystem\n        s3, filesystem = get_file_and_filesystem(path)\n        try:\n            parquet_file = self.api.ParquetFile(path, open_with=filesystem.open)\n        finally:\n            s3.close()\n    else:\n        path, _, _, _ = get_filepath_or_buffer(path)\n        parquet_file = self.api.ParquetFile(path)\n    return parquet_file.to_pandas(columns=columns, **kwargs)"
                        ],
                        "constructor_variables": [
                            "api",
                            "fastparquet"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self)",
                            "write(self, df, path, compression='snappy', index=None, partition_cols=None, **kwargs)",
                            "read(self, path, columns=None, **kwargs)"
                        ]
                    },
                    "variable_values": [
                        [
                            {
                                "self.validate_dataframe": {
                                    "variable_value": "<function BaseImpl.validate_dataframe at 0x11912d700>",
                                    "variable_type": "function",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<pandas.io.parquet.FastParquetImpl object at 0x11dd9e460>",
                                    "variable_type": "FastParquetImpl",
                                    "variable_shape": null
                                },
                                "df": {
                                    "variable_value": "   int  float str         dt\n0    1    2.0   t 2018-06-18\n1    3    NaN   s 2018-06-19",
                                    "variable_type": "DataFrame",
                                    "variable_shape": "(2, 4)"
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                },
                                "partition_cols": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "kwargs.pop": {
                                    "variable_value": "<built-in method pop of dict object at 0x126eff940>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                },
                                "is_s3_url": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "path": {
                                    "variable_value": "'gs://test/test.csv'",
                                    "variable_type": "str",
                                    "variable_shape": "18"
                                },
                                "_": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "get_filepath_or_buffer": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "catch_warnings": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.api.write": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.api": {
                                    "variable_value": "<module 'fastparquet' from '/Volumes/JerrySSD/bgp_envs/envs/pandas_149/lib/python3.8/site-packages/fastparquet/__init__.py'>",
                                    "variable_type": "module",
                                    "variable_shape": null
                                },
                                "compression": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "index": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                }
                            },
                            {}
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {
                                "self.validate_dataframe": {
                                    "variable_value": "<function BaseImpl.validate_dataframe at 0x11d6dc790>",
                                    "variable_type": "function",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<pandas.io.parquet.FastParquetImpl object at 0x12237a430>",
                                    "variable_type": "FastParquetImpl",
                                    "variable_shape": null
                                },
                                "df": {
                                    "variable_value": "   int  float str         dt\n0    1    2.0   t 2018-06-18\n1    3    NaN   s 2018-06-19",
                                    "variable_type": "DataFrame",
                                    "variable_shape": "(2, 4)"
                                },
                                "kwargs": {
                                    "variable_value": "{}",
                                    "variable_type": "dict",
                                    "variable_shape": "0"
                                },
                                "partition_cols": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "kwargs.pop": {
                                    "variable_value": "<built-in method pop of dict object at 0x12b4efa80>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                },
                                "is_s3_url": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "path": {
                                    "variable_value": "'gs://test/test.csv'",
                                    "variable_type": "str",
                                    "variable_shape": "18"
                                },
                                "is_gcs_url": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "_": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "get_filepath_or_buffer": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "catch_warnings": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.api.write": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.api": {
                                    "variable_value": "<module 'fastparquet' from '/Volumes/JerrySSD/bgp_envs/envs/pandas_149/lib/python3.8/site-packages/fastparquet/__init__.py'>",
                                    "variable_type": "module",
                                    "variable_shape": null
                                },
                                "compression": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "index": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                }
                            },
                            {
                                "self.validate_dataframe": {
                                    "variable_value": "<function BaseImpl.validate_dataframe at 0x11d6dc790>",
                                    "variable_type": "function",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<pandas.io.parquet.FastParquetImpl object at 0x12237a430>",
                                    "variable_type": "FastParquetImpl",
                                    "variable_shape": null
                                },
                                "df": {
                                    "variable_value": "   int  float str         dt\n0    1    2.0   t 2018-06-18\n1    3    NaN   s 2018-06-19",
                                    "variable_type": "DataFrame",
                                    "variable_shape": "(2, 4)"
                                },
                                "kwargs": {
                                    "variable_value": "{'open_with': <function FastParquetImpl.write.<locals>.<lambda> at 0x12b5098b0>}",
                                    "variable_type": "dict",
                                    "variable_shape": "1"
                                },
                                "partition_cols": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "kwargs.pop": {
                                    "variable_value": "<built-in method pop of dict object at 0x12b4efa80>",
                                    "variable_type": "builtin_function_or_method",
                                    "variable_shape": null
                                },
                                "is_s3_url": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "path": {
                                    "variable_value": "<_io.BufferedWriter name='/private/var/folders/ng/72llsm517x12c2p18htksyjc0000gn/T/pytest-of-jerry/pytest-1547/test_to_parquet_gcs_new_file0/test.parquet'>",
                                    "variable_type": "BufferedWriter",
                                    "variable_shape": null
                                },
                                "is_gcs_url": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "_": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "get_filepath_or_buffer": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "catch_warnings": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.api.write": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.api": {
                                    "variable_value": "<module 'fastparquet' from '/Volumes/JerrySSD/bgp_envs/envs/pandas_149/lib/python3.8/site-packages/fastparquet/__init__.py'>",
                                    "variable_type": "module",
                                    "variable_shape": null
                                },
                                "compression": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "index": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                }
                            }
                        ]
                    ]
                }
            ],
            "snippets": [
                {
                    "snippet_code": "from pandas.io.common import get_filepath_or_buffer, is_s3_url",
                    "start_line": 10,
                    "end_line": 11
                }
            ],
            "inscope_functions": [
                "def get_engine(engine):\n    \"\"\" return our implementation \"\"\"\n\n    if engine == \"auto\":\n        engine = get_option(\"io.parquet.engine\")\n\n    if engine == \"auto\":\n        # try engines in this order\n        try:\n            return PyArrowImpl()\n        except ImportError:\n            pass\n\n        try:\n            return FastParquetImpl()\n        except ImportError:\n            pass\n\n        raise ImportError(\n            \"Unable to find a usable engine; \"\n            \"tried using: 'pyarrow', 'fastparquet'.\\n\"\n            \"pyarrow or fastparquet is required for parquet \"\n            \"support\"\n        )\n\n    if engine not in [\"pyarrow\", \"fastparquet\"]:\n        raise ValueError(\"engine must be one of 'pyarrow', 'fastparquet'\")\n\n    if engine == \"pyarrow\":\n        return PyArrowImpl()\n    elif engine == \"fastparquet\":\n        return FastParquetImpl()",
                "def to_parquet(\n    df,\n    path,\n    engine=\"auto\",\n    compression=\"snappy\",\n    index=None,\n    partition_cols=None,\n    **kwargs\n):\n    \"\"\"\n    Write a DataFrame to the parquet format.\n\n    Parameters\n    ----------\n    path : str\n        File path or Root Directory path. Will be used as Root Directory path\n        while writing a partitioned dataset.\n\n        .. versionchanged:: 0.24.0\n\n    engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n        Parquet library to use. If 'auto', then the option\n        ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n        behavior is to try 'pyarrow', falling back to 'fastparquet' if\n        'pyarrow' is unavailable.\n    compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\n        Name of the compression to use. Use ``None`` for no compression.\n    index : bool, default None\n        If ``True``, include the dataframe's index(es) in the file output. If\n        ``False``, they will not be written to the file.\n        If ``None``, similar to ``True`` the dataframe's index(es)\n        will be saved. However, instead of being saved as values,\n        the RangeIndex will be stored as a range in the metadata so it\n        doesn't require much space and is faster. Other indexes will\n        be included as columns in the file output.\n\n        .. versionadded:: 0.24.0\n\n    partition_cols : list, optional, default None\n        Column names by which to partition the dataset\n        Columns are partitioned in the order they are given\n\n        .. versionadded:: 0.24.0\n\n    kwargs\n        Additional keyword arguments passed to the engine\n    \"\"\"\n    impl = get_engine(engine)\n    return impl.write(\n        df,\n        path,\n        compression=compression,\n        index=index,\n        partition_cols=partition_cols,\n        **kwargs\n    )",
                "def read_parquet(path, engine=\"auto\", columns=None, **kwargs):\n    \"\"\"\n    Load a parquet object from the file path, returning a DataFrame.\n\n    .. versionadded:: 0.21.0\n\n    Parameters\n    ----------\n    path : str, path object or file-like object\n        Any valid string path is acceptable. The string could be a URL. Valid\n        URL schemes include http, ftp, s3, and file. For file URLs, a host is\n        expected. A local file could be:\n        ``file://localhost/path/to/table.parquet``.\n\n        If you want to pass in a path object, pandas accepts any\n        ``os.PathLike``.\n\n        By file-like object, we refer to objects with a ``read()`` method,\n        such as a file handler (e.g. via builtin ``open`` function)\n        or ``StringIO``.\n    engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n        Parquet library to use. If 'auto', then the option\n        ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n        behavior is to try 'pyarrow', falling back to 'fastparquet' if\n        'pyarrow' is unavailable.\n    columns : list, default=None\n        If not None, only these columns will be read from the file.\n\n        .. versionadded:: 0.21.1\n    **kwargs\n        Any additional kwargs are passed to the engine.\n\n    Returns\n    -------\n    DataFrame\n    \"\"\"\n\n    impl = get_engine(engine)\n    return impl.read(path, columns=columns, **kwargs)",
                "@staticmethod\ndef validate_dataframe(df):\n\n    if not isinstance(df, DataFrame):\n        raise ValueError(\"to_parquet only supports IO with DataFrames\")\n\n    # must have value column names (strings only)\n    if df.columns.inferred_type not in {\"string\", \"unicode\", \"empty\"}:\n        raise ValueError(\"parquet must have string column names\")\n\n    # index level names must be strings\n    valid_names = all(\n        isinstance(name, str) for name in df.index.names if name is not None\n    )\n    if not valid_names:\n        raise ValueError(\"Index level names must be strings\")",
                "def write(self, df, path, compression, **kwargs):\n    raise AbstractMethodError(self)",
                "def read(self, path, columns=None, **kwargs):\n    raise AbstractMethodError(self)",
                "def __init__(self):\n    pyarrow = import_optional_dependency(\n        \"pyarrow\", extra=\"pyarrow is required for parquet support.\"\n    )\n    import pyarrow.parquet\n\n    self.api = pyarrow",
                "def write(\n    self,\n    df,\n    path,\n    compression=\"snappy\",\n    coerce_timestamps=\"ms\",\n    index=None,\n    partition_cols=None,\n    **kwargs\n):\n    self.validate_dataframe(df)\n    path, _, _, _ = get_filepath_or_buffer(path, mode=\"wb\")\n\n    if index is None:\n        from_pandas_kwargs = {}\n    else:\n        from_pandas_kwargs = {\"preserve_index\": index}\n    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n    if partition_cols is not None:\n        self.api.parquet.write_to_dataset(\n            table,\n            path,\n            compression=compression,\n            coerce_timestamps=coerce_timestamps,\n            partition_cols=partition_cols,\n            **kwargs\n        )\n    else:\n        self.api.parquet.write_table(\n            table,\n            path,\n            compression=compression,\n            coerce_timestamps=coerce_timestamps,\n            **kwargs\n        )",
                "def read(self, path, columns=None, **kwargs):\n    path, _, _, should_close = get_filepath_or_buffer(path)\n\n    kwargs[\"use_pandas_metadata\"] = True\n    result = self.api.parquet.read_table(\n        path, columns=columns, **kwargs\n    ).to_pandas()\n    if should_close:\n        path.close()\n\n    return result",
                "def __init__(self):\n    # since pandas is a dependency of fastparquet\n    # we need to import on first use\n    fastparquet = import_optional_dependency(\n        \"fastparquet\", extra=\"fastparquet is required for parquet support.\"\n    )\n    self.api = fastparquet",
                "def write(\n    self, df, path, compression=\"snappy\", index=None, partition_cols=None, **kwargs\n):\n    self.validate_dataframe(df)\n    # thriftpy/protocol/compact.py:339:\n    # DeprecationWarning: tostring() is deprecated.\n    # Use tobytes() instead.\n\n    if \"partition_on\" in kwargs and partition_cols is not None:\n        raise ValueError(\n            \"Cannot use both partition_on and \"\n            \"partition_cols. Use partition_cols for \"\n            \"partitioning data\"\n        )\n    elif \"partition_on\" in kwargs:\n        partition_cols = kwargs.pop(\"partition_on\")\n\n    if partition_cols is not None:\n        kwargs[\"file_scheme\"] = \"hive\"\n\n    if is_s3_url(path):\n        # path is s3:// so we need to open the s3file in 'wb' mode.\n        # TODO: Support 'ab'\n\n        path, _, _, _ = get_filepath_or_buffer(path, mode=\"wb\")\n        # And pass the opened s3file to the fastparquet internal impl.\n        kwargs[\"open_with\"] = lambda path, _: path\n    else:\n        path, _, _, _ = get_filepath_or_buffer(path)\n\n    with catch_warnings(record=True):\n        self.api.write(\n            path,\n            df,\n            compression=compression,\n            write_index=index,\n            partition_on=partition_cols,\n            **kwargs\n        )",
                "def read(self, path, columns=None, **kwargs):\n    if is_s3_url(path):\n        from pandas.io.s3 import get_file_and_filesystem\n\n        # When path is s3:// an S3File is returned.\n        # We need to retain the original path(str) while also\n        # pass the S3File().open function to fsatparquet impl.\n        s3, filesystem = get_file_and_filesystem(path)\n        try:\n            parquet_file = self.api.ParquetFile(path, open_with=filesystem.open)\n        finally:\n            s3.close()\n    else:\n        path, _, _, _ = get_filepath_or_buffer(path)\n        parquet_file = self.api.ParquetFile(path)\n\n    return parquet_file.to_pandas(columns=columns, **kwargs)"
            ],
            "inscope_function_signatures": [
                "get_engine(engine)",
                "to_parquet(df, path, engine='auto', compression='snappy', index=None, partition_cols=None, **kwargs)",
                "read_parquet(path, engine='auto', columns=None, **kwargs)",
                "validate_dataframe(df)",
                "write(self, df, path, compression, **kwargs)",
                "read(self, path, columns=None, **kwargs)",
                "__init__(self)",
                "write(self, df, path, compression='snappy', coerce_timestamps='ms', index=None, partition_cols=None, **kwargs)",
                "read(self, path, columns=None, **kwargs)",
                "__init__(self)",
                "write(self, df, path, compression='snappy', index=None, partition_cols=None, **kwargs)",
                "read(self, path, columns=None, **kwargs)"
            ],
            "variables_in_file": {
                "engine": [
                    38,
                    296,
                    41,
                    43,
                    16,
                    17,
                    19,
                    248
                ],
                "get_option": [
                    17
                ],
                "PyArrowImpl": [
                    42,
                    22
                ],
                "ImportError": [
                    28,
                    31,
                    23
                ],
                "FastParquetImpl": [
                    27,
                    44
                ],
                "ValueError": [
                    66,
                    39,
                    55,
                    151,
                    59
                ],
                "api": [
                    49
                ],
                "isinstance": [
                    54,
                    63
                ],
                "df": [
                    101,
                    250,
                    175,
                    145,
                    54,
                    58,
                    94,
                    63
                ],
                "DataFrame": [
                    54
                ],
                "df.columns.inferred_type": [
                    58
                ],
                "df.columns": [
                    58
                ],
                "valid_names": [
                    65,
                    62
                ],
                "all": [
                    62
                ],
                "name": [
                    63
                ],
                "str": [
                    63
                ],
                "df.index.names": [
                    63
                ],
                "df.index": [
                    63
                ],
                "staticmethod": [
                    51
                ],
                "AbstractMethodError": [
                    72,
                    69
                ],
                "self": [
                    196,
                    101,
                    69,
                    103,
                    72,
                    140,
                    173,
                    112,
                    145,
                    82,
                    124,
                    94,
                    191
                ],
                "BaseImpl": [
                    75,
                    133
                ],
                "pyarrow": [
                    82,
                    77
                ],
                "import_optional_dependency": [
                    137,
                    77
                ],
                "self.api": [
                    196,
                    101,
                    103,
                    140,
                    173,
                    112,
                    82,
                    124,
                    191
                ],
                "self.validate_dataframe": [
                    145,
                    94
                ],
                "path": [
                    128,
                    191,
                    162,
                    195,
                    196,
                    166,
                    168,
                    105,
                    170,
                    297,
                    174,
                    114,
                    189,
                    183,
                    121,
                    251,
                    125,
                    95
                ],
                "_": [
                    195,
                    166,
                    170,
                    121,
                    95
                ],
                "get_filepath_or_buffer": [
                    195,
                    166,
                    170,
                    121,
                    95
                ],
                "index": [
                    97,
                    177,
                    100,
                    253
                ],
                "from_pandas_kwargs": [
                    98,
                    100,
                    101
                ],
                "table": [
                    104,
                    113,
                    101
                ],
                "self.api.Table.from_pandas": [
                    101
                ],
                "self.api.Table": [
                    101
                ],
                "partition_cols": [
                    102,
                    108,
                    178,
                    150,
                    157,
                    254,
                    159
                ],
                "self.api.parquet.write_to_dataset": [
                    103
                ],
                "self.api.parquet": [
                    112,
                    124,
                    103
                ],
                "compression": [
                    176,
                    106,
                    115,
                    252
                ],
                "coerce_timestamps": [
                    107,
                    116
                ],
                "kwargs": [
                    160,
                    198,
                    168,
                    297,
                    109,
                    179,
                    117,
                    150,
                    157,
                    123,
                    156,
                    125,
                    255
                ],
                "self.api.parquet.write_table": [
                    112
                ],
                "should_close": [
                    121,
                    127
                ],
                "result": [
                    130,
                    124
                ],
                "to_pandas": [
                    124
                ],
                "self.api.parquet.read_table": [
                    124
                ],
                "columns": [
                    297,
                    125,
                    198
                ],
                "path.close": [
                    128
                ],
                "fastparquet": [
                    137,
                    140
                ],
                "kwargs.pop": [
                    157
                ],
                "is_s3_url": [
                    162,
                    183
                ],
                "catch_warnings": [
                    172
                ],
                "self.api.write": [
                    173
                ],
                "s3": [
                    193,
                    189
                ],
                "filesystem": [
                    189,
                    191
                ],
                "get_file_and_filesystem": [
                    189
                ],
                "parquet_file": [
                    196,
                    198,
                    191
                ],
                "self.api.ParquetFile": [
                    196,
                    191
                ],
                "filesystem.open": [
                    191
                ],
                "s3.close": [
                    193
                ],
                "parquet_file.to_pandas": [
                    198
                ],
                "impl": [
                    248,
                    249,
                    296,
                    297
                ],
                "get_engine": [
                    248,
                    296
                ],
                "impl.write": [
                    249
                ],
                "impl.read": [
                    297
                ]
            },
            "filtered_variables_in_file": {
                "engine": [
                    38,
                    296,
                    41,
                    43,
                    16,
                    17,
                    19,
                    248
                ],
                "get_option": [
                    17
                ],
                "PyArrowImpl": [
                    42,
                    22
                ],
                "FastParquetImpl": [
                    27,
                    44
                ],
                "api": [
                    49
                ],
                "df": [
                    101,
                    250,
                    175,
                    145,
                    54,
                    58,
                    94,
                    63
                ],
                "DataFrame": [
                    54
                ],
                "df.columns.inferred_type": [
                    58
                ],
                "df.columns": [
                    58
                ],
                "valid_names": [
                    65,
                    62
                ],
                "name": [
                    63
                ],
                "df.index.names": [
                    63
                ],
                "df.index": [
                    63
                ],
                "AbstractMethodError": [
                    72,
                    69
                ],
                "self": [
                    196,
                    101,
                    69,
                    103,
                    72,
                    140,
                    173,
                    112,
                    145,
                    82,
                    124,
                    94,
                    191
                ],
                "BaseImpl": [
                    75,
                    133
                ],
                "pyarrow": [
                    82,
                    77
                ],
                "import_optional_dependency": [
                    137,
                    77
                ],
                "self.api": [
                    196,
                    101,
                    103,
                    140,
                    173,
                    112,
                    82,
                    124,
                    191
                ],
                "self.validate_dataframe": [
                    145,
                    94
                ],
                "path": [
                    128,
                    191,
                    162,
                    195,
                    196,
                    166,
                    168,
                    105,
                    170,
                    297,
                    174,
                    114,
                    189,
                    183,
                    121,
                    251,
                    125,
                    95
                ],
                "_": [
                    195,
                    166,
                    170,
                    121,
                    95
                ],
                "get_filepath_or_buffer": [
                    195,
                    166,
                    170,
                    121,
                    95
                ],
                "index": [
                    97,
                    177,
                    100,
                    253
                ],
                "from_pandas_kwargs": [
                    98,
                    100,
                    101
                ],
                "table": [
                    104,
                    113,
                    101
                ],
                "self.api.Table.from_pandas": [
                    101
                ],
                "self.api.Table": [
                    101
                ],
                "partition_cols": [
                    102,
                    108,
                    178,
                    150,
                    157,
                    254,
                    159
                ],
                "self.api.parquet.write_to_dataset": [
                    103
                ],
                "self.api.parquet": [
                    112,
                    124,
                    103
                ],
                "compression": [
                    176,
                    106,
                    115,
                    252
                ],
                "coerce_timestamps": [
                    107,
                    116
                ],
                "kwargs": [
                    160,
                    198,
                    168,
                    297,
                    109,
                    179,
                    117,
                    150,
                    157,
                    123,
                    156,
                    125,
                    255
                ],
                "self.api.parquet.write_table": [
                    112
                ],
                "should_close": [
                    121,
                    127
                ],
                "result": [
                    130,
                    124
                ],
                "to_pandas": [
                    124
                ],
                "self.api.parquet.read_table": [
                    124
                ],
                "columns": [
                    297,
                    125,
                    198
                ],
                "path.close": [
                    128
                ],
                "fastparquet": [
                    137,
                    140
                ],
                "kwargs.pop": [
                    157
                ],
                "is_s3_url": [
                    162,
                    183
                ],
                "catch_warnings": [
                    172
                ],
                "self.api.write": [
                    173
                ],
                "s3": [
                    193,
                    189
                ],
                "filesystem": [
                    189,
                    191
                ],
                "get_file_and_filesystem": [
                    189
                ],
                "parquet_file": [
                    196,
                    198,
                    191
                ],
                "self.api.ParquetFile": [
                    196,
                    191
                ],
                "filesystem.open": [
                    191
                ],
                "s3.close": [
                    193
                ],
                "parquet_file.to_pandas": [
                    198
                ],
                "impl": [
                    248,
                    249,
                    296,
                    297
                ],
                "get_engine": [
                    248,
                    296
                ],
                "impl.write": [
                    249
                ],
                "impl.read": [
                    297
                ]
            }
        },
        "test_data": [
            {
                "test_path": "/Volumes/JerrySSD/bgp_envs/repos/pandas_149/pandas/tests/io/test_gcs.py",
                "test_function": "test_to_parquet_gcs_new_file",
                "test_function_code": "@td.skip_if_no(\"fastparquet\")\n@td.skip_if_no(\"gcsfs\")\ndef test_to_parquet_gcs_new_file(monkeypatch, tmpdir):\n    \"\"\"Regression test for writing to a not-yet-existent GCS Parquet file.\"\"\"\n    df1 = DataFrame(\n        {\n            \"int\": [1, 3],\n            \"float\": [2.0, np.nan],\n            \"str\": [\"t\", \"s\"],\n            \"dt\": date_range(\"2018-06-18\", periods=2),\n        }\n    )\n\n    class MockGCSFileSystem:\n        def open(self, path, mode=\"r\", *args):\n            if \"w\" not in mode:\n                raise FileNotFoundError\n            return open(os.path.join(tmpdir, \"test.parquet\"), mode)\n\n    monkeypatch.setattr(\"gcsfs.GCSFileSystem\", MockGCSFileSystem)\n    df1.to_parquet(\n        \"gs://test/test.csv\", index=True, engine=\"fastparquet\", compression=None\n    )",
                "test_error": "FileNotFoundError",
                "full_test_error": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x123d3be50>\ntmpdir = local('/private/var/folders/ng/72llsm517x12c2p18htksyjc0000gn/T/pytest-of-jerry/pytest-1545/test_to_parquet_gcs_new_file0')\n\n    @td.skip_if_no(\"fastparquet\")\n    @td.skip_if_no(\"gcsfs\")\n    def test_to_parquet_gcs_new_file(monkeypatch, tmpdir):\n        \"\"\"Regression test for writing to a not-yet-existent GCS Parquet file.\"\"\"\n        df1 = DataFrame(\n            {\n                \"int\": [1, 3],\n                \"float\": [2.0, np.nan],\n                \"str\": [\"t\", \"s\"],\n                \"dt\": date_range(\"2018-06-18\", periods=2),\n            }\n        )\n    \n        class MockGCSFileSystem:\n            def open(self, path, mode=\"r\", *args):\n                if \"w\" not in mode:\n                    raise FileNotFoundError\n                return open(os.path.join(tmpdir, \"test.parquet\"), mode)\n    \n        monkeypatch.setattr(\"gcsfs.GCSFileSystem\", MockGCSFileSystem)\n>       df1.to_parquet(\n            \"gs://test/test.csv\", index=True, engine=\"fastparquet\", compression=None\n        )\n\npandas/tests/io/test_gcs.py:84: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \npandas/core/frame.py:2155: in to_parquet\n    to_parquet(\npandas/io/parquet.py:249: in to_parquet\n    return impl.write(\npandas/io/parquet.py:170: in write\n    path, _, _, _ = get_filepath_or_buffer(path)\npandas/io/common.py:243: in get_filepath_or_buffer\n    return gcs.get_filepath_or_buffer(\npandas/io/gcs.py:17: in get_filepath_or_buffer\n    filepath_or_buffer = fs.open(filepath_or_buffer, mode)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <pandas.tests.io.test_gcs.test_to_parquet_gcs_new_file.<locals>.MockGCSFileSystem object at 0x123d45760>\npath = 'gs://test/test.csv', mode = 'rb', args = ()\n\n    def open(self, path, mode=\"r\", *args):\n        if \"w\" not in mode:\n>           raise FileNotFoundError\nE           FileNotFoundError\n\npandas/tests/io/test_gcs.py:80: FileNotFoundError",
                "traceback": "pandas/core/frame.py:2155: in to_parquet\n    to_parquet(\npandas/io/parquet.py:249: in to_parquet\n    return impl.write(\npandas/io/parquet.py:170: in write\n    path, _, _, _ = get_filepath_or_buffer(path)\npandas/io/common.py:243: in get_filepath_or_buffer\n    return gcs.get_filepath_or_buffer(\npandas/io/gcs.py:17: in get_filepath_or_buffer\n    filepath_or_buffer = fs.open(filepath_or_buffer, mode)",
                "test_error_location": "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <pandas.tests.io.test_gcs.test_to_parquet_gcs_new_file.<locals>.MockGCSFileSystem object at 0x123d45760>\npath = 'gs://test/test.csv', mode = 'rb', args = ()\n\n    def open(self, path, mode=\"r\", *args):\n        if \"w\" not in mode:\n>           raise FileNotFoundError\nE           FileNotFoundError\n\npandas/tests/io/test_gcs.py:80: FileNotFoundError",
                "test_function_decorators": [
                    "td.skip_if_no('fastparquet')",
                    "td.skip_if_no('gcsfs')"
                ]
            }
        ]
    }
}