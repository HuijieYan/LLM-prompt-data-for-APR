```python
# Corrected function
def write(self, df, path, compression="snappy", index=None, partition_cols=None, **kwargs):
    self.validate_dataframe(df)

    if "partition_on" in kwargs and partition_cols is not None:
        raise ValueError(
            "Cannot use both partition_on and "
            "partition_cols. Use partition_cols for "
            "partitioning data"
        )
    elif "partition_on" in kwargs:
        partition_cols = kwargs.pop("partition_on")

    if partition_cols is not None:
        kwargs["file_scheme"] = "hive"

    if is_s3_url(path):
        with open(path, mode="wb") as f:
            # path is s3:// so we need to open the s3file in 'wb' mode.
            # TODO: Support 'ab'
            # And pass the opened s3file to the fastparquet internal impl.
            kwargs["open_with"] = lambda path, _: f
    else:
        with open(path, mode="wb") as f:
            # path is not s3:// so we need to open the file in 'wb' mode.
            # And pass the opened file to the fastparquet internal impl.
            kwargs["open_with"] = lambda path, _: f

    with catch_warnings(record=True):
        self.api.write(
            path, df, compression=compression, write_index=index, partition_on=partition_cols, **kwargs
        )
```