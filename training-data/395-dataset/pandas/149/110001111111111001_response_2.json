{
    "pandas": [
        {
            "bugID": 149,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 1,
                "4": 1,
                "5": 1,
                "6": 0,
                "7": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 1,
                "4": 1,
                "5": 1,
                "6": 0,
                "7": 1
            },
            "start_line": 142,
            "file_name": "pandas/io/parquet.py",
            "replace_code": "def write(\n    self, df, path, compression=\"snappy\", index=None, partition_cols=None, **kwargs\n):\n    self.validate_dataframe(df)\n\n    if \"partition_on\" in kwargs and partition_cols is not None:\n        raise ValueError(\n            \"Cannot use both partition_on and \"\n            \"partition_cols. Use partition_cols for \"\n            \"partitioning data\"\n        )\n    elif \"partition_on\" in kwargs:\n        partition_cols = kwargs.pop(\"partition_on\")\n\n    if partition_cols is not None:\n        kwargs[\"file_scheme\"] = \"hive\"\n\n    if is_s3_url(path):\n        # path is s3:// so we need to open the s3file in 'wb' mode.\n        # TODO: Support 'ab'\n\n        path, _, _, _ = get_filepath_or_buffer(path, mode=\"wb\")\n        # And pass the opened s3file to the fastparquet internal impl.\n        kwargs[\"open_with\"] = lambda path, _: path\n    else:\n        # Handle the file path and open the file in 'wb' mode if it doesn't exist\n        with catch_warnings(record=True):\n            try:\n                with open(path, 'rb') as file:\n                    # File exists - append to existing Parquet file\n                    existing_df = fastparquet.ParquetFile(file).to_pandas()\n                    df = existing_df.append(df, ignore_index=True)\n            except FileNotFoundError:\n                # File doesn't exist - create and write the Parquet file\n                with open(path, 'wb') as file:\n                    # Write the Parquet file\n                    fastparquet.write(file, df, compression=compression, write_index=index, partition_on=partition_cols, **kwargs)",
            "imports": []
        }
    ]
}