```python
# file name: /Volumes/JerrySSD/bgp_envs/repos/pandas_149/pandas/io/parquet.py

# This is the corrected function

def write(self, df, path, compression="snappy", index=None, partition_cols=None, **kwargs):
    validate_dataframe(df)

    if "partition_on" in kwargs and partition_cols is not None:
        raise ValueError(
            "Cannot use both partition_on and partition_cols. Use partition_cols for partitioning data"
        )
    elif "partition_on" in kwargs:
        partition_cols = kwargs.pop("partition_on")

    if partition_cols is not None:
        kwargs["file_scheme"] = "hive"

    if is_s3_url(path):
        with open(s3fs.S3File(path, "wb")) as s3file:
            open_with = lambda path, _: s3file
            write = open_with
    else:
        write = s3fs.S3File(path).open

    with catch_warnings(record=True):
        self.api.write(
            path,
            df,
            compression=compression,
            write_index=index,
            partition_on=partition_cols,
            open_with=write,
            **kwargs
        )
```