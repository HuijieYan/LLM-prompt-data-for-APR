{
    "pandas": [
        {
            "bugID": 149,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 0,
                "6": 0,
                "7": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 0,
                "6": 0,
                "7": 1
            },
            "start_line": 142,
            "file_name": "pandas/io/parquet.py",
            "replace_code": "def write(\n    self, df, path, compression=\"snappy\", index=None, partition_cols=None, **kwargs\n):\n    self.validate_dataframe(df)\n\n    if \"partition_on\" in kwargs and partition_cols is not None:\n        raise ValueError(\n            \"Cannot use both partition_on and \"\n            \"partition_cols. Use partition_cols for \"\n            \"partitioning data\"\n        )\n    elif \"partition_on\" in kwargs:\n        partition_cols = kwargs.pop(\"partition_on\")\n\n    if partition_cols is not None:\n        kwargs[\"file_scheme\"] = \"hive\"\n\n    if is_s3_url(path):\n        # path is s3:// so we need to open the s3file in 'wb' mode.\n        # TODO: Support 'ab'\n        fs = gcsfs.GCSFileSystem()\n        parquet_file = gcsfs.GCSMap(path, fs=fs)\n\n        gcs_path = os.path.join(parquet_file.root, \"data.parquet\")\n        # And pass the opened s3file to the fastparquet internal impl.\n        with warnings.catch_warnings(record=True):\n            fastparquet_write(gcs_path, df, compression=compression, file_scheme=\"hive\", write_index=index, partition_on=partition_cols, **kwargs)\n    else:\n        path, _, _, _ = ParquetFile.get_filepath_or_buffer(path)\n\n        with warnings.catch_warnings(record=True):\n            fastparquet_write(path, df, compression=compression, file_scheme=\"hive\", write_index=index, partition_on=partition_cols, **kwargs)",
            "import_list": [
                "import warnings",
                "import os",
                "import gcsfs",
                "from pandas.api.types import is_s3_url",
                "from fastparquet import write as fastparquet_write",
                "from fastparquet import ParquetFile"
            ]
        }
    ]
}