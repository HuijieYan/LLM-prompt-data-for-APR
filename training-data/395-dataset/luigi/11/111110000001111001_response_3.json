{
    "luigi": [
        {
            "bugID": 11,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 1,
                "3": 0,
                "4": 0,
                "5": 1,
                "6": 0,
                "7": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 1,
                "3": 0,
                "4": 0,
                "5": 1,
                "6": 0,
                "7": 1
            },
            "start_line": 818,
            "file_name": "luigi/scheduler.py",
            "replace_code": "def get_work(self, host=None, assistant=False, current_tasks=None, worker=None, **kwargs):\n    if self._config.prune_on_get_work:\n        self.prune()\n\n    assert worker is not None\n    worker_id = worker\n\n    self.update(worker_id, {'host': host}, get_work=True)\n    if assistant:\n        self.add_worker(worker_id, [('assistant', assistant)])\n\n    batched_params, unbatched_params, batched_tasks, max_batch_size = None, None, [], 1\n    best_task = None\n\n    tasks = sorted(self._state.get_running_tasks(), key=self._rank, reverse=True)\n\n    locally_pending_tasks = self._state.count_locally_pending_tasks(worker_id, assistant)\n    running_tasks = self._state.get_running_task_info(only_these_tasks=current_tasks or [])\n    if current_tasks is not None:\n        self._reset_orphaned_batch_running_tasks(worker_id)\n\n    greedy_resources = self._state.greedy_resources()\n    n_unique_pending = self._state.count_unique_pending()\n\n    worker = self._state.get_worker(worker_id)\n    tasks = self._state.get_pending_tasks()\n    tasks.sort(key=self._rank, reverse=True)\n\n    for task in tasks:\n        in_workers = (assistant and getattr(task, 'runnable', bool(task.workers))) or worker_id in task.workers\n        if task.status == RUNNING and in_workers:\n            running_tasks.append({\n                'task_id': task.id,\n                'worker': str(self._state.ensure_one_worker(task.worker_running).info)\n            })\n\n        if task.status == PENDING and in_workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1 and not assistant:\n                n_unique_pending += 1\n\n        if current_tasks is not None:\n            best_task = self._state.task_by_random_batch()\n            continue\n\n        if self._schedulable(task) and self._state.has_all_resources(used_resources, greedy_resources):\n            if in_workers and self._state.has_all_resources(task.resources, used_resources):\n                best_task = task\n                batch_param_names, max_batch_size = self._state.get_batcher(worker_id, task.family)\n                if batch_param_names and task.is_batchable():\n                    batched_params, unbatched_params, batched_tasks = self._state.batch_tasks(\n                        task, batched_params, unbatched_params, batched_tasks, batch_param_names, max_batch_size\n                    )\n            else:\n                self._state.schedule_task(task, worker_id, greedy_resources)\n\n    reply = {\n        'n_pending_tasks': locally_pending_tasks,\n        'running_tasks': running_tasks,\n        'task_id': None,\n        'n_unique_pending': n_unique_pending\n    }\n\n    if len(batched_tasks) > 1:\n        combined_params, batch_id = self._state.setup_batch(batched_tasks, best_task, worker_id)\n        reply.update({\n            'task_id': None,\n            'task_family': best_task.family,\n            'task_module': getattr(best_task, 'module', None),\n            'task_params': combined_params,\n            'batch_id': batch_id,\n            'batch_task_ids': [task.id for task in batched_tasks]\n        })\n    elif best_task:\n        self._state.run_task(best_task, worker_id, host)\n        reply.update({\n            'task_id': best_task.id,\n            'task_family': best_task.family,\n            'task_module': getattr(best_task, 'module', None),\n            'task_params': best_task.params\n        })\n\n    return reply",
            "import_list": []
        }
    ]
}