{
    "luigi": [
        {
            "bugID": 11,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 1,
                "3": 0,
                "4": 0,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 1,
                "3": 0,
                "4": 0,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "start_line": 818,
            "file_name": "luigi/scheduler.py",
            "replace_code": "def get_work(self, host=None, assistant=False, current_tasks=None, worker=None, **kwargs):\n\t    # TODO: remove any expired nodes\n\t    \n\t    # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n\t    # resources.\n\t    \n\t    # Resource checking looks both at currently available resources and at which resources would\n\t    # be available if all running tasks died and we rescheduled all workers greedily. We do both\n\t    # checks in order to prevent a worker with many low-priority tasks from starving other\n\t    # workers with higher priority tasks that share the same resources.\n\t    \n\t    # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n\t    # nothing it can wait for\n\t    \n\t    if self._config.prune_on_get_work:\n\t        self.prune()\n\t    \n\t    assert worker is not None\n\t    worker_id = worker\n\t    # Return remaining tasks that have no FAILED descendants\n\t    self.update(worker_id, {'host': host}, get_work=True)\n\t    if assistant:\n\t        self.add_worker(worker_id, [('assistant', assistant)])\n\t    \n\t    batched_params, unbatched_params, batched_tasks, max_batch_size = None, None, [], 1\n\t    best_task = None\n\t    if current_tasks is not None:\n\t        ct_set = set(current_tasks)\n\t        for task in sorted(self._state.get_running_tasks(), key=self._rank):\n\t            if task.worker_running == worker_id and task.id not in ct_set:\n\t                best_task = task\n\t    \n\t    if current_tasks is not None:\n\t        # batch running tasks that weren't claimed since the last get_work go back in the pool\n\t        self._reset_orphaned_batch_running_tasks(worker_id)\n\t    \n\t    locally_pending_tasks = 0\n\t    running_tasks = []\n\t    upstream_table = {}\n\t    \n\t    greedy_resources = collections.defaultdict(int)\n\t    n_unique_pending = 0\n\t    \n\t    worker = self._state.get_worker(worker_id)\n\t    if worker.is_trivial_worker(self._state):\n\t        relevant_tasks = worker.get_pending_tasks(self._state)\n\t        used_resources = collections.defaultdict(int)\n\t        greedy_workers = dict()  # If there's no resources, then they can grab any task\n\t    else:\n\t        relevant_tasks = self._state.get_pending_tasks()\n\t        used_resources = self._used_resources()\n\t        activity_limit = time.time() - self._config.worker_disconnect_delay\n\t        active_workers = self._state.get_active_workers(last_get_work_gt=activity_limit)\n\t        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n\t                              for worker in active_workers)\n\t    tasks = list(relevant_tasks)\n\t    tasks.sort(key=self._rank, reverse=True)\n\t    \n\t    for task in tasks:\n\t        in_workers = (assistant and getattr(task, 'runnable', bool(task.workers))) or worker_id in task.workers\n\t        if task.status == RUNNING and in_workers:\n\t            # Return a list of currently running tasks to the client,\n\t            # makes it easier to troubleshoot\n\t            other_worker = self._state.get_worker(task.worker_running)\n\t            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n\t            if other_worker is not None:\n\t                more_info.update(other_worker.info)\n\t                running_tasks.append(more_info)\n\t    \n\t        if task.status == PENDING and in_workers:\n\t            upstream_status = self._upstream_status(task.id, upstream_table)\n\t            if upstream_status != UPSTREAM_DISABLED:\n\t                locally_pending_tasks += 1\n\t                if len(task.workers) == 1 and not assistant:\n\t                    n_unique_pending += 1\n\t    \n\t        if (best_task and batched_params and task.family == best_task.family and\n\t                len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n\t                task.params.get(name) == value for name, value in unbatched_params.items())):\n\t            for name, params in batched_params.items():\n\t                params.append(task.params.get(name))\n\t            batched_tasks.append(task)\n\t        if best_task:\n\t            continue\n\t    \n\t        if task.status == RUNNING and (task.worker_running in greedy_workers):\n\t            greedy_workers[task.worker_running] -= 1\n\t            for resource, amount in six.iteritems((task.resources or {})):\n\t                greedy_resources[resource] += amount\n\t    \n\t        if self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n\t            if in_workers and self._has_resources(task.resources, used_resources):\n\t                best_task = task\n\t                batch_param_names, max_batch_size = self._state.get_batcher(\n\t                    worker_id, task.family)\n\t                if batch_param_names and task.is_batchable():\n\t                    try:\n\t                        batched_params = {\n\t                            name: [task.params[name]] for name in batch_param_names\n\t                        }\n\t                        unbatched_params = {\n\t                            name: value for name, value in task.params.items()\n\t                            if name not in batched_params\n\t                        }\n\t                        batched_tasks.append(task)\n\t                    except KeyError:\n\t                        batched_params, unbatched_params = None, None\n\t            else:\n\t                workers = itertools.chain(task.workers, [worker_id]) if assistant else task.workers\n\t                for task_worker in workers:\n\t                    if greedy_workers.get(task_worker, 0) > 0:\n\t                        # use up a worker\n\t                        greedy_workers[task_worker] -= 1\n\t    \n\t                        # keep track of the resources used in greedy scheduling\n\t                        for resource, amount in six.iteritems((task.resources or {})):\n\t                            greedy_resources[resource] += amount\n\t    \n\t                        break\n\t    \n\t    reply = {'n_pending_tasks': locally_pending_tasks,\n\t             'running_tasks': running_tasks,\n\t             'task_id': None,\n\t             'n_unique_pending': n_unique_pending}\n\t    \n\t    if len(batched_tasks) > 1:\n\t        batch_string = '|'.join(task.id for task in batched_tasks)\n\t        batch_id = hashlib.md5(batch_string.encode('utf-8')).hexdigest()\n\t        for task in batched_tasks:\n\t            self._state.set_batch_running(task, batch_id, worker_id)\n\t    \n\t        combined_params = best_task.params.copy()\n\t        combined_params.update(batched_params)\n\t    \n\t        reply['task_id'] = None\n\t        reply['task_family'] = best_task.family\n\t        reply['task_module'] = getattr(best_task, 'module', None)\n\t        reply['task_params'] = combined_params\n\t        reply['batch_id'] = batch_id\n\t        reply['batch_task_ids'] = [task.id for task in batched_tasks]\n\t    \n\t    elif best_task:\n\t        self._state.set_status(best_task, RUNNING, self._config)\n\t        best_task.worker_running = worker_id\n\t        best_task.time_running = time.time()\n\t        self._update_task_history(best_task, RUNNING, host=host)\n\t    \n\t        reply['task_id'] = best_task.id\n\t        reply['task_family'] = best_task.family\n\t        reply['task_module'] = getattr(best_task, 'module', None)\n\t        reply['task_params'] = best_task.params\n\t    \n\t    return reply",
            "imports": [
                "import collections"
            ]
        }
    ]
}