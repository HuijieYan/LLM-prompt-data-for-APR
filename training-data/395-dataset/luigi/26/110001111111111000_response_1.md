```python
# Fix for the buggy function
def run_job(self, job):
        ssh_config = job.ssh() or {}
        host = ssh_config.get("host", None)
        key_file = ssh_config.get("key_file", None)
        username = ssh_config.get("username", None)
        jar_file = job.jar()

        if not host or not key_file or not username or not jar_file:
            raise HadoopJarJobError("missing some config for HadoopRemoteJarJobRunner")

        arglist = ['ssh', '-i', key_file, '-o', 'BatchMode=yes']  # no password prompts etc

        if ssh_config.get("no_host_key_check", False):
            arglist += ['-o', 'UserKnownHostsFile=/dev/null','-o', 'StrictHostKeyChecking=no']
        
        arglist.append('{}@{}'.format(username, host))
        hadoop_arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', jar_file]

        main_file = job.main()
        if main_file:
            hadoop_arglist.append(main_file)

        jobconfs = job.jobconfs() or []

        for jc in jobconfs:
            hadoop_arglist += ['-D' + jc]

        (tmp_files, job_args) = fix_paths(job)

        hadoop_arglist += job_args
        arglist.extend(hadoop_arglist)

        luigi.contrib.hadoop.run_and_track_hadoop_job(arglist)

        for a, b in tmp_files:
            a.move(b)
```