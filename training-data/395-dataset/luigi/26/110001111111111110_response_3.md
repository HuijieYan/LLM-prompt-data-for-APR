```python
def run_job(self, job):
        arglist = []
          
        # Added try except block to catch missing jar error
        try:
            if not job.ssh() or not job.jar() or job.ssh().get("host", None) is None or job.ssh().get("key_file", None) is None or job.ssh().get("username", None) is None:
                raise HadoopJarJobError("missing some config for HadoopRemoteJarJobRunner")
            else:
                ssh_config = job.ssh()
                host = ssh_config.get("host", None)
                key_file = ssh_config.get("key_file", None)
                username = ssh_config.get("username", None)
                arglist = ['ssh', '-i', key_file,
                           '-o', 'BatchMode=yes']  # no password prompts etc
                if ssh_config.get("no_host_key_check", False):
                    arglist += ['-o', 'UserKnownHostsFile=/dev/null',
                                '-o', 'StrictHostKeyChecking=no']
                arglist.append('{}@{}'.format(username, host))
        
        except AttributeError:
            logger.error("Can't find jar: %s, full path %s", job.jar(), os.path.abspath(job.jar()))
            raise HadoopJarJobError("job jar does not exist")
            
        
        hadoop_arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', job.jar()]
        if job.main():
            hadoop_arglist.append(job.main())
        
        jobconfs = job.jobconfs()
        
        for jc in jobconfs:
            hadoop_arglist += ['-D' + jc]
        
        (tmp_files, job_args) = fix_paths(job)
        
        hadoop_arglist += job_args
        arglist.extend(hadoop_arglist)
        
        luigi.contrib.hadoop.run_and_track_hadoop_job(arglist)
        
        for a, b in tmp_files:
            a.move(b)
```