```python
# corrected run_job method

import os
import luigi.contrib.hdfs
import luigi.contrib.hadoop
from luigi.contrib.hadoop_jar import HadoopJarJobError

def run_job(self, job):
    ssh_config = job.ssh()
    arglist = []
    if not job.jar() or not os.path.exists(job.jar()):
        raise HadoopJarJobError("job jar does not exist")
    else:
        hadoop_arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', job.jar()]
        if job.main():
            hadoop_arglist.append(job.main())

        jobconfs = job.jobconfs()

        for jc in jobconfs:
            hadoop_arglist += ['-D' + jc]

        (tmp_files, job_args) = fix_paths(job)

        hadoop_arglist += job_args
        arglist.extend(hadoop_arglist)

        luigi.contrib.hadoop.run_and_track_hadoop_job(arglist)

        for a, b in tmp_files:
            a.move(b)
```