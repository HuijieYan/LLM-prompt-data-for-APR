```python
def generate_tokens(readline):
    import re
    
    # function to convert bytes to string
    def decode_utf8(data):
        return data.decode("utf-8")

    from tokenize import *

    lnum = parenlev = continued = 0
    numchars = '0123456789'
    contstr, needcont = '', 0
    contline = None
    indents = [0]

    stashed = None
    async_def = False
    async_def_indent = 0
    async_def_nl = False

    #...

    while True:                                  # loop over lines in stream
        try:
            #line = readline()
            b = readline()
            line = decode_utf8(b)
        except StopIteration:
            line = ''
        lnum += 1
        pos, max = 0, len(line)
        
        #...
```