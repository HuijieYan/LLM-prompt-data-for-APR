The error occurs in the `ctc_batch_cost` function when it tries to convert the `label_length` tensor to an integer using `tf.squeeze` and `tf.to_int32`. The issue arises from the `label_length` tensor having a shape of (2, 1) in the first test case and (1, 1) in the second test case, causing unexpected behavior during the conversion.

To address this issue, the `label_length` tensor should be flattened to a 1D tensor before converting it to an integer.

Here's the corrected code for the `ctc_batch_cost` function:

```python
import tensorflow as tf

def ctc_batch_cost(y_true, y_pred, input_length, label_length):
    label_length = tf.cast(tf.reshape(label_length, [-1]), tf.int32)  # Flatten and cast to int32
    input_length = tf.cast(tf.reshape(input_length, [-1]), tf.int32)  # Flatten and cast to int32
    sparse_labels = tf.cast(tf.contrib.layers.dense_to_sparse(y_true, label_length), tf.int32)

    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + tf.keras.backend.epsilon())

    return tf.expand_dims(tf.contrib.ctc.ctc_loss(inputs=y_pred,
                                                  labels=sparse_labels,
                                                  sequence_length=input_length), 1)
```

The changes made include flattening the `label_length` and `input_length` tensors using `tf.reshape` and then casting them to `tf.int32` using `tf.cast`. Additionally, `ctc_label_dense_to_sparse` is replaced with `tf.contrib.layers.dense_to_sparse` and `ctc.ctc_loss` is replaced with `tf.contrib.ctc.ctc_loss` to align with TensorFlow's recommended API usage.