The error seems to be occurring in the `ctc_batch_cost` function due to the handling of tensor data types and shapes. The function is trying to squeeze the `label_length` and `input_length` tensors, which are causing issues when using online training with a batch size of 1.

The `tf.squeeze` function is removing dimensions of size 1, which is causing the shape mismatch error. When using a batch size of 1, the dimensions should not be squeezed.

To fix the bug, we need to modify the `ctc_batch_cost` function to handle the case where the batch size is 1 without squeezing the dimensions.

Here's the corrected code for the `ctc_batch_cost` function:

```python
def ctc_batch_cost(y_true, y_pred, input_length, label_length):
    """Runs CTC loss algorithm on each batch element.

    # Arguments
        y_true: tensor `(samples, max_string_length)`
            containing the truth labels.
        y_pred: tensor `(samples, time_steps, num_categories)`
            containing the prediction, or output of the softmax.
        input_length: tensor `(samples, 1)` containing the sequence length for
            each batch item in `y_pred`.
        label_length: tensor `(samples, 1)` containing the sequence length for
            each batch item in `y_true`.

    # Returns
        Tensor with shape (samples,1) containing the
            CTC loss of each element.
    """
    label_length = tf.cond(tf.equal(tf.shape(label_length)[0], 1),
                           lambda: label_length,
                           lambda: tf.squeeze(label_length))
    input_length = tf.cond(tf.equal(tf.shape(input_length)[0], 1),
                           lambda: input_length,
                           lambda: tf.squeeze(input_length))
    sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))

    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())

    return tf.expand_dims(ctc.ctc_loss(inputs=y_pred,
                                       labels=sparse_labels,
                                       sequence_length=input_length), 1)
```

In the corrected code, we use `tf.cond` to check if the batch size is 1. If it is, we do not squeeze the dimensions, otherwise, we apply the squeeze operation. This will ensure that the function works correctly for both batch sizes.