{
    "1.1.1": "def fit_generator(model,\n                  generator,\n                  steps_per_epoch=None,\n                  epochs=1,\n                  verbose=1,\n                  callbacks=None,\n                  validation_data=None,\n                  validation_steps=None,\n                  class_weight=None,\n                  max_queue_size=10,\n                  workers=1,\n                  use_multiprocessing=False,\n                  shuffle=True,\n                  initial_epoch=0):\n    \n    wait_time = 0.01  # in seconds\n    epoch = initial_epoch\n\n    do_validation = bool(validation_data)\n    model._make_train_function()\n    if do_validation:\n        model._make_test_function()\n\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n        warnings.warn(\n            UserWarning('Using a generator with `use_multiprocessing=True`'\n                        ' and multiple workers may duplicate your data.'\n                        ' Please consider using the`keras.utils.Sequence'\n                        ' class.'))\n    if steps_per_epoch is None:\n        if is_sequence:\n            steps_per_epoch = len(generator)\n        else:\n            raise ValueError('`steps_per_epoch=None` is only valid for a'\n                             ' generator based on the '\n                             '`keras.utils.Sequence`'\n                             ' class. Please specify `steps_per_epoch` '\n                             'or use the `keras.utils.Sequence` class.')\n\n    # python 2 has 'next', 3 has '__next__'\n    # avoid any explicit version checks\n    val_gen = (hasattr(validation_data, 'next') or\n               hasattr(validation_data, '__next__') or\n               isinstance(validation_data, Sequence))\n    if (val_gen and not isinstance(validation_data, Sequence) and\n            not validation_steps):\n        raise ValueError('`validation_steps=None` is only valid for a'\n                         ' generator based on the `keras.utils.Sequence`'\n                         ' class. Please specify `validation_steps` or use'\n                         ' the `keras.utils.Sequence` class.')\n\n    # Prepare display labels.\n    out_labels = model.metrics_names\n    callback_metrics = out_labels + ['val_' + n for n in out_labels]\n\n    # prepare callbacks\n    model.history = cbks.History()\n    _callbacks = [cbks.BaseLogger(\n        stateful_metrics=model.stateful_metric_names)]\n    if verbose:\n        _callbacks.append(\n            cbks.ProgbarLogger(\n                count_mode='steps',\n                stateful_metrics=model.stateful_metric_names))\n    _callbacks += (callbacks or []) + [model.history]\n    callbacks = cbks.CallbackList(_callbacks)\n\n    # it's possible to callback a different model than self:\n    if hasattr(model, 'callback_model') and model.callback_model:\n        callback_model = model.callback_model\n    else:\n        callback_model = model\n    callbacks.set_model(callback_model)\n    callbacks.set_params({\n        'epochs': epochs,\n        'steps': steps_per_epoch,\n        'verbose': verbose,\n        'do_validation': do_validation,\n        'metrics': callback_metrics,\n    })\n    callbacks.on_train_begin()\n\n    enqueuer = None\n    val_enqueuer = None\n\n    try:\n        if do_validation:\n            if val_gen and workers > 0:\n                # Create an Enqueuer that can be reused\n                val_data = validation_data\n                if isinstance(val_data, Sequence):\n                    val_enqueuer = OrderedEnqueuer(\n                        val_data,\n                        use_multiprocessing=use_multiprocessing)\n                    validation_steps = validation_steps or len(val_data)\n                else:\n                    val_enqueuer = GeneratorEnqueuer(\n                        val_data,\n                        use_multiprocessing=use_multiprocessing)\n                val_enqueuer.start(workers=workers,\n                                   max_queue_size=max_queue_size)\n                val_enqueuer_gen = val_enqueuer.get()\n            elif val_gen:\n                val_data = validation_data\n                if isinstance(val_data, Sequence):\n                    val_enqueuer_gen = iter_sequence_infinite(generator)\n                else:\n                    val_enqueuer_gen = val_data\n            else:\n                # Prepare data for validation\n                if len(validation_data) == 2:\n                    val_x, val_y = validation_data\n                    val_sample_weight = None\n                elif len(validation_data) == 3:\n                    val_x, val_y, val_sample_weight = validation_data\n                else:\n                    raise ValueError('`validation_data` should be a tuple '\n                                     '`(val_x, val_y, val_sample_weight)` '\n                                     'or `(val_x, val_y)`. Found: ' +\n                                     str(validation_data))\n                val_x, val_y, val_sample_weights = model._standardize_user_data(\n                    val_x, val_y, val_sample_weight)\n                val_data = val_x + val_y + val_sample_weights\n                if model.uses_learning_phase and not isinstance(K.learning_phase(),\n                                                                int):\n                    val_data += [0.]\n                for cbk in callbacks:\n                    cbk.validation_data = val_data\n\n        if workers > 0:\n            if is_sequence:\n                enqueuer = OrderedEnqueuer(\n                    generator,\n                    use_multiprocessing=use_multiprocessing,\n                    shuffle=shuffle)\n            else:\n                enqueuer = GeneratorEnqueuer(\n                    generator,\n                    use_multiprocessing=use_multiprocessing,\n                    wait_time=wait_time)\n            enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n            output_generator = enqueuer.get()\n        else:\n            if is_sequence:\n                output_generator = iter_sequence_infinite(generator)\n            else:\n                output_generator = generator\n\n        callback_model.stop_training = False\n        # Construct epoch logs.\n        epoch_logs = {}\n        while epoch < epochs:\n            for m in model.stateful_metric_functions:\n                m.reset_states()\n            callbacks.on_epoch_begin(epoch)\n            steps_done = 0\n            batch_index = 0\n            while steps_done < steps_per_epoch:\n                generator_output = next(output_generator)\n\n                if not hasattr(generator_output, '__len__'):\n                    raise ValueError('Output of generator should be '\n                                     'a tuple `(x, y, sample_weight)` '\n                                     'or `(x, y)`. Found: ' +\n                                     str(generator_output))\n\n                if len(generator_output) == 2:\n                    x, y = generator_output\n                    sample_weight = None\n                elif len(generator_output) == 3:\n                    x, y, sample_weight = generator_output\n                else:\n                    raise ValueError('Output of generator should be '\n                                     'a tuple `(x, y, sample_weight)` '\n                                     'or `(x, y)`. Found: ' +\n                                     str(generator_output))\n                # build batch logs\n                batch_logs = {}\n                if x is None or len(x) == 0:\n                    # Handle data tensors support when no input given\n                    # step-size = 1 for data tensors\n                    batch_size = 1\n                elif isinstance(x, list):\n                    batch_size = x[0].shape[0]\n                elif isinstance(x, dict):\n                    batch_size = list(x.values())[0].shape[0]\n                else:\n                    batch_size = x.shape[0]\n                batch_logs['batch'] = batch_index\n                batch_logs['size'] = batch_size\n                callbacks.on_batch_begin(batch_index, batch_logs)\n\n                outs = model.train_on_batch(x, y,\n                                            sample_weight=sample_weight,\n                                            class_weight=class_weight)\n\n                outs = to_list(outs)\n                for l, o in zip(out_labels, outs):\n                    batch_logs[l] = o\n\n                callbacks.on_batch_end(batch_index, batch_logs)\n\n                batch_index += 1\n                steps_done += 1\n\n                # Epoch finished.\n                if steps_done >= steps_per_epoch and do_validation:\n                    if val_gen:\n                        val_outs = model.evaluate_generator(\n                            val_enqueuer_gen,\n                            validation_steps,\n                            workers=0)\n                    else:\n                        # No need for try/except because\n                        # data has already been validated.\n                        val_outs = model.evaluate(\n                            val_x, val_y,\n                            batch_size=batch_size,\n                            sample_weight=val_sample_weights,\n                            verbose=0)\n                    val_outs = to_list(val_outs)\n                    # Same labels assumed.\n                    for l, o in zip(out_labels, val_outs):\n                        epoch_logs['val_' + l] = o\n\n                if callback_model.stop_training:\n                    break\n\n            callbacks.on_epoch_end(epoch, epoch_logs)\n            epoch += 1\n            if callback_model.stop_training:\n                break\n\n    finally:\n        try:\n            if enqueuer is not None:\n                enqueuer.stop()\n        finally:\n            if val_enqueuer is not None:\n                val_enqueuer.stop()\n\n    callbacks.on_train_end()\n    return model.history\n",
    "1.1.2": "See docstring for `Model.fit_generator`.",
    "1.2.1": null,
    "1.2.2": null,
    "1.2.3": null,
    "1.3.1": "/Volumes/SSD2T/bgp_envs/repos/keras_13/keras/engine/training_generator.py",
    "1.3.2": [
        "evaluate_generator(model, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)"
    ],
    "1.4.1": [
        "def test_model_methods():\n    a = Input(shape=(3,), name='input_a')\n    b = Input(shape=(3,), name='input_b')\n\n    a_2 = Dense(4, name='dense_1')(a)\n    dp = Dropout(0.5, name='dropout')\n    b_2 = dp(b)\n\n    model = Model([a, b], [a_2, b_2])\n\n    optimizer = 'rmsprop'\n    loss = 'mse'\n    loss_weights = [1., 0.5]\n\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n\n    output_a_np = np.random.random((10, 4))\n    output_b_np = np.random.random((10, 3))\n\n    # training/testing doesn't work before compiling.\n    with pytest.raises(RuntimeError):\n        model.train_on_batch([input_a_np, input_b_np],\n                             [output_a_np, output_b_np])\n\n    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                  sample_weight_mode=None)\n\n    # test train_on_batch\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n    out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                               [output_a_np, output_b_np])\n    out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                               {'dense_1': output_a_np, 'dropout': output_b_np})\n\n    # test fit\n    out = model.fit([input_a_np, input_b_np],\n                    [output_a_np, output_b_np], epochs=1, batch_size=4)\n    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                    [output_a_np, output_b_np], epochs=1, batch_size=4)\n    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                    {'dense_1': output_a_np, 'dropout': output_b_np},\n                    epochs=1, batch_size=4)\n\n    # test validation_split\n    out = model.fit([input_a_np, input_b_np],\n                    [output_a_np, output_b_np],\n                    epochs=1, batch_size=4, validation_split=0.5)\n    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                    [output_a_np, output_b_np],\n                    epochs=1, batch_size=4, validation_split=0.5)\n\n    # test validation data\n    out = model.fit([input_a_np, input_b_np],\n                    [output_a_np, output_b_np],\n                    epochs=1, batch_size=4,\n                    validation_data=([input_a_np, input_b_np],\n                                     [output_a_np, output_b_np]))\n    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                    [output_a_np, output_b_np],\n                    epochs=1, batch_size=4, validation_split=0.5,\n                    validation_data=({'input_a': input_a_np,\n                                      'input_b': input_b_np},\n                                     [output_a_np, output_b_np]))\n    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                    {'dense_1': output_a_np, 'dropout': output_b_np},\n                    epochs=1, batch_size=4, validation_split=0.5,\n                    validation_data=(\n                        {'input_a': input_a_np, 'input_b': input_b_np},\n                        {'dense_1': output_a_np, 'dropout': output_b_np}))\n\n    # test_on_batch\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np])\n    out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                              [output_a_np, output_b_np])\n    out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                              {'dense_1': output_a_np, 'dropout': output_b_np})\n\n    # predict_on_batch\n    out = model.predict_on_batch([input_a_np, input_b_np])\n    out = model.predict_on_batch({'input_a': input_a_np,\n                                  'input_b': input_b_np})\n\n    # predict, evaluate\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n\n    output_a_np = np.random.random((10, 4))\n    output_b_np = np.random.random((10, 3))\n\n    out = model.evaluate([input_a_np, input_b_np],\n                         [output_a_np, output_b_np],\n                         batch_size=4)\n    out = model.predict([input_a_np, input_b_np], batch_size=4)\n\n    # with sample_weight\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n\n    output_a_np = np.random.random((10, 4))\n    output_b_np = np.random.random((10, 3))\n\n    sample_weight = [None, np.random.random((10,))]\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np],\n                               sample_weight=sample_weight)\n\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np],\n                              sample_weight=sample_weight)\n\n    # test accuracy metric\n    model.compile(optimizer, loss, metrics=['acc'],\n                  sample_weight_mode=None)\n\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n    assert len(out) == 5\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np])\n    assert len(out) == 5\n\n    # this should also work\n    model.compile(optimizer, loss, metrics={'dense_1': 'acc'},\n                  sample_weight_mode=None)\n\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n    assert len(out) == 4\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np])\n    assert len(out) == 4\n\n    # and this as well\n    model.compile(optimizer, loss, metrics={'dense_1': ['acc']},\n                  sample_weight_mode=None)\n\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n    assert len(out) == 4\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np])\n    assert len(out) == 4\n\n    # test starting from non-zero initial epoch\n    trained_epochs = []\n    trained_batches = []\n\n    # define tracer callback\n    def on_epoch_begin(epoch, logs):\n        trained_epochs.append(epoch)\n\n    def on_batch_begin(batch, logs):\n        trained_batches.append(batch)\n\n    tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin,\n                                on_batch_begin=on_batch_begin)\n\n    out = model.fit([input_a_np, input_b_np],\n                    [output_a_np, output_b_np], epochs=5, batch_size=4,\n                    initial_epoch=2, callbacks=[tracker_cb])\n    assert trained_epochs == [2, 3, 4]\n\n    # test starting from non-zero initial epoch for generator too\n    trained_epochs = []\n\n    @threadsafe_generator\n    def gen_data(batch_sz):\n        while True:\n            yield ([np.random.random((batch_sz, 3)),\n                    np.random.random((batch_sz, 3))],\n                   [np.random.random((batch_sz, 4)),\n                    np.random.random((batch_sz, 3))])\n\n    out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,\n                              initial_epoch=2, callbacks=[tracker_cb])\n    assert trained_epochs == [2, 3, 4]\n\n    # test with a custom metric function\n    def mse(y_true, y_pred):\n        return K.mean(K.pow(y_true - y_pred, 2))\n\n    model.compile(optimizer, loss, metrics=[mse],\n                  sample_weight_mode=None)\n\n    out = model.train_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n    out_len = 1 + 2 * (1 + 1)  # total loss + 2 outputs * (loss + metric)\n    assert len(out) == out_len\n    out = model.test_on_batch([input_a_np, input_b_np],\n                              [output_a_np, output_b_np])\n    assert len(out) == out_len\n\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n\n    output_a_np = np.random.random((10, 4))\n    output_b_np = np.random.random((10, 3))\n\n    out = model.fit([input_a_np, input_b_np],\n                    [output_a_np, output_b_np],\n                    batch_size=4, epochs=1)\n    out = model.evaluate([input_a_np, input_b_np],\n                         [output_a_np, output_b_np],\n                         batch_size=4)\n    out = model.predict([input_a_np, input_b_np], batch_size=4)\n\n    # enable verbose for evaluate_generator\n    out = model.evaluate_generator(gen_data(4), steps=3, verbose=1)\n\n    # empty batch\n    with pytest.raises(ValueError):\n        @threadsafe_generator\n        def gen_data():\n            while True:\n                yield (np.asarray([]), np.asarray([]))\n\n        out = model.evaluate_generator(gen_data(), steps=1)\n\n    # x is not a list of numpy arrays.\n    with pytest.raises(ValueError):\n        out = model.predict([None])\n\n    # x does not match _feed_input_names.\n    with pytest.raises(ValueError):\n        out = model.predict([input_a_np, None, input_b_np])\n    with pytest.raises(ValueError):\n        out = model.predict([None, input_a_np, input_b_np])\n\n    # all input/output/weight arrays should have the same number of samples.\n    with pytest.raises(ValueError):\n        out = model.train_on_batch([input_a_np, input_b_np[:2]],\n                                   [output_a_np, output_b_np],\n                                   sample_weight=sample_weight)\n    with pytest.raises(ValueError):\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np[:2]],\n                                   sample_weight=sample_weight)\n    with pytest.raises(ValueError):\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np],\n                                   sample_weight=[sample_weight[1],\n                                                  sample_weight[1][:2]])\n\n    # `sample_weight` is neither a dict nor a list.\n    with pytest.raises(TypeError):\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np],\n                                   sample_weight=tuple(sample_weight))\n\n    # `validation_data` is neither a tuple nor a triple.\n    with pytest.raises(ValueError):\n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np],\n                        epochs=1, batch_size=4,\n                        validation_data=([input_a_np, input_b_np],))\n\n    # `loss` does not match outputs.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss=['mse', 'mae', 'mape'])\n\n    # `loss_weights` does not match output_names.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss='mse', loss_weights={'lstm': 0.5})\n\n    # `loss_weights` does not match outputs.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss='mse', loss_weights=[0.5])\n\n    # `loss_weights` is invalid type.\n    with pytest.raises(TypeError):\n        model.compile(optimizer, loss='mse', loss_weights=(0.5, 0.5))\n\n    # `sample_weight_mode` does not match output_names.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss='mse',\n                      sample_weight_mode={'lstm': 'temporal'})\n\n    # `sample_weight_mode` does not match output_names.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss='mse', sample_weight_mode=['temporal'])\n\n    # `sample_weight_mode` matches output_names partially.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss='mse',\n                      sample_weight_mode={'dense_1': 'temporal'})\n\n    # `loss` does not exist.\n    with pytest.raises(ValueError):\n        model.compile(optimizer, loss=[])\n\n    model.compile(optimizer, loss=['mse', 'mae'])\n    model.compile(optimizer, loss='mse', loss_weights={'dense_1': 0.2,\n                                                       'dropout': 0.8})\n    model.compile(optimizer, loss='mse', loss_weights=[0.2, 0.8])\n\n    # the rank of weight arrays should be 1.\n    with pytest.raises(ValueError):\n        out = model.train_on_batch(\n            [input_a_np, input_b_np],\n            [output_a_np, output_b_np],\n            sample_weight=[None, np.random.random((10, 20, 30))])\n\n    model.compile(optimizer, loss='mse',\n                  sample_weight_mode={'dense_1': None, 'dropout': 'temporal'})\n    model.compile(optimizer, loss='mse', sample_weight_mode=[None, 'temporal'])\n\n    # the rank of output arrays should be at least 3D.\n    with pytest.raises(ValueError):\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np],\n                                   sample_weight=sample_weight)\n\n    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                  sample_weight_mode=None)\n    trained_epochs = []\n    trained_batches = []\n    val_seq = RandomSequence(4)\n    out = model.fit_generator(generator=RandomSequence(3),\n                              steps_per_epoch=3,\n                              epochs=5,\n                              initial_epoch=0,\n                              validation_data=val_seq,\n                              validation_steps=3,\n                              max_queue_size=1,\n                              callbacks=[tracker_cb])\n    assert trained_epochs == [0, 1, 2, 3, 4]\n    assert trained_batches == list(range(3)) * 5\n    assert len(val_seq.logs) <= 4 * 5\n\n    # steps_per_epoch will be equal to len of sequence if it's unspecified\n    trained_epochs = []\n    trained_batches = []\n    val_seq = RandomSequence(4)\n    out = model.fit_generator(generator=RandomSequence(3),\n                              epochs=5,\n                              initial_epoch=0,\n                              validation_data=val_seq,\n                              callbacks=[tracker_cb])\n    assert trained_epochs == [0, 1, 2, 3, 4]\n    assert trained_batches == list(range(12)) * 5\n    assert len(val_seq.logs) == 12 * 5\n\n    # test for workers = 0\n    trained_epochs = []\n    trained_batches = []\n    val_seq = RandomSequence(4)\n    out = model.fit_generator(generator=RandomSequence(3),\n                              epochs=5,\n                              validation_data=val_seq,\n                              callbacks=[tracker_cb],\n                              workers=0)\n    assert trained_epochs == [0, 1, 2, 3, 4]\n    assert trained_batches == list(range(12)) * 5\n    assert len(val_seq.logs) == 12 * 5\n\n    # fit_generator will throw an exception\n    # if steps is unspecified for regular generator\n    with pytest.raises(ValueError):\n        @threadsafe_generator\n        def gen_data():\n            while True:\n                yield (np.asarray([]), np.asarray([]))\n\n        out = model.fit_generator(generator=gen_data(), epochs=5,\n                                  initial_epoch=0, validation_data=gen_data(),\n                                  callbacks=[tracker_cb])\n\n    # Check if generator is only accessed an expected number of times\n    gen_counters = [0, 0]\n\n    @threadsafe_generator\n    def gen_data(i):\n        while True:\n            gen_counters[i] += 1\n            yield ([np.random.random((1, 3)), np.random.random((1, 3))],\n                   [np.random.random((1, 4)), np.random.random((1, 3))])\n    out = model.fit_generator(generator=gen_data(0), epochs=3,\n                              steps_per_epoch=2,\n                              validation_data=gen_data(1),\n                              validation_steps=1,\n                              max_queue_size=2,\n                              workers=2)\n\n    # Need range check here as filling\n    # of the queue depends on sleep in the enqueuers\n    max_train = 3 * 2 + 2 * 2\n    min_train = 2 * 3\n    assert min_train <= gen_counters[0] <= max_train\n    # 12 = (epoch * workers * validation steps * max_queue_size)\n    assert 3 <= gen_counters[1] <= 12\n\n    gen_counters = [0]\n    out = model.fit_generator(generator=RandomSequence(3), epochs=3,\n                              validation_data=gen_data(0),\n                              validation_steps=1,\n                              max_queue_size=2,\n                              workers=2)\n\n    # 12 = (epoch * workers * validation steps * max_queue_size)\n    # Need range check here as filling\n    # of the queue depends on sleep in the enqueuers\n    assert 3 <= gen_counters[0] <= 12\n\n    # predict_generator output shape behavior should be consistent\n    def expected_shape(batch_size, n_batches):\n        return (batch_size * n_batches, 4), (batch_size * n_batches, 3)\n\n    # Multiple outputs and one step.\n    batch_size = 5\n    sequence_length = 1\n    shape_0, shape_1 = expected_shape(batch_size, sequence_length)\n    out = model.predict_generator(\n        RandomSequence(batch_size, sequence_length=sequence_length))\n    assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n\n    # Multiple outputs and multiple steps.\n    batch_size = 5\n    sequence_length = 2\n    shape_0, shape_1 = expected_shape(batch_size, sequence_length)\n    out = model.predict_generator(\n        RandomSequence(batch_size, sequence_length=sequence_length))\n    assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n\n    # Create a model with a single output.\n    single_output_model = Model([a, b], a_2)\n    single_output_model.compile(optimizer, loss,\n                                metrics=[], sample_weight_mode=None)\n\n    # Single output and one step.\n    batch_size = 5\n    sequence_length = 1\n    shape_0, _ = expected_shape(batch_size, sequence_length)\n    out = single_output_model.predict_generator(\n        RandomSequence(batch_size, sequence_length=sequence_length))\n    assert np.shape(out) == shape_0\n\n    # Single output and multiple steps.\n    batch_size = 5\n    sequence_length = 2\n    shape_0, _ = expected_shape(batch_size, sequence_length)\n    out = single_output_model.predict_generator(\n        RandomSequence(batch_size, sequence_length=sequence_length))\n    assert np.shape(out) == shape_0"
    ],
    "1.4.2": [
        "/Volumes/SSD2T/bgp_envs/repos/keras_13/tests/keras/engine/test_training.py"
    ],
    "2.1.1": [
        [
            "E               ValueError: `steps=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps` or use the `keras.utils.Sequence` class."
        ]
    ],
    "2.1.2": [
        [
            "def test_model_methods():\n        a = Input(shape=(3,), name='input_a')\n        b = Input(shape=(3,), name='input_b')\n    \n        a_2 = Dense(4, name='dense_1')(a)\n        dp = Dropout(0.5, name='dropout')\n        b_2 = dp(b)\n    \n        model = Model([a, b], [a_2, b_2])\n    \n        optimizer = 'rmsprop'\n        loss = 'mse'\n        loss_weights = [1., 0.5]\n    \n        input_a_np = np.random.random((10, 3))\n        input_b_np = np.random.random((10, 3))\n    \n        output_a_np = np.random.random((10, 4))\n        output_b_np = np.random.random((10, 3))\n    \n        # training/testing doesn't work before compiling.\n        with pytest.raises(RuntimeError):\n            model.train_on_batch([input_a_np, input_b_np],\n                                 [output_a_np, output_b_np])\n    \n        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                      sample_weight_mode=None)\n    \n        # test train_on_batch\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np])\n        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                                   [output_a_np, output_b_np])\n        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                                   {'dense_1': output_a_np, 'dropout': output_b_np})\n    \n        # test fit\n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np], epochs=1, batch_size=4)\n        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                        [output_a_np, output_b_np], epochs=1, batch_size=4)\n        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                        {'dense_1': output_a_np, 'dropout': output_b_np},\n                        epochs=1, batch_size=4)\n    \n        # test validation_split\n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np],\n                        epochs=1, batch_size=4, validation_split=0.5)\n        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                        [output_a_np, output_b_np],\n                        epochs=1, batch_size=4, validation_split=0.5)\n    \n        # test validation data\n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np],\n                        epochs=1, batch_size=4,\n                        validation_data=([input_a_np, input_b_np],\n                                         [output_a_np, output_b_np]))\n        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                        [output_a_np, output_b_np],\n                        epochs=1, batch_size=4, validation_split=0.5,\n                        validation_data=({'input_a': input_a_np,\n                                          'input_b': input_b_np},\n                                         [output_a_np, output_b_np]))\n        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                        {'dense_1': output_a_np, 'dropout': output_b_np},\n                        epochs=1, batch_size=4, validation_split=0.5,\n                        validation_data=(\n                            {'input_a': input_a_np, 'input_b': input_b_np},\n                            {'dense_1': output_a_np, 'dropout': output_b_np}))\n    \n        # test_on_batch\n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np])\n        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                                  [output_a_np, output_b_np])\n        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                                  {'dense_1': output_a_np, 'dropout': output_b_np})\n    \n        # predict_on_batch\n        out = model.predict_on_batch([input_a_np, input_b_np])\n        out = model.predict_on_batch({'input_a': input_a_np,\n                                      'input_b': input_b_np})\n    \n        # predict, evaluate\n        input_a_np = np.random.random((10, 3))\n        input_b_np = np.random.random((10, 3))\n    \n        output_a_np = np.random.random((10, 4))\n        output_b_np = np.random.random((10, 3))\n    \n        out = model.evaluate([input_a_np, input_b_np],\n                             [output_a_np, output_b_np],\n                             batch_size=4)\n        out = model.predict([input_a_np, input_b_np], batch_size=4)\n    \n        # with sample_weight\n        input_a_np = np.random.random((10, 3))\n        input_b_np = np.random.random((10, 3))\n    \n        output_a_np = np.random.random((10, 4))\n        output_b_np = np.random.random((10, 3))\n    \n        sample_weight = [None, np.random.random((10,))]\n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np],\n                                   sample_weight=sample_weight)\n    \n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np],\n                                  sample_weight=sample_weight)\n    \n        # test accuracy metric\n        model.compile(optimizer, loss, metrics=['acc'],\n                      sample_weight_mode=None)\n    \n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np])\n        assert len(out) == 5\n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np])\n        assert len(out) == 5\n    \n        # this should also work\n        model.compile(optimizer, loss, metrics={'dense_1': 'acc'},\n                      sample_weight_mode=None)\n    \n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np])\n        assert len(out) == 4\n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np])\n        assert len(out) == 4\n    \n        # and this as well\n        model.compile(optimizer, loss, metrics={'dense_1': ['acc']},\n                      sample_weight_mode=None)\n    \n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np])\n        assert len(out) == 4\n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np])\n        assert len(out) == 4\n    \n        # test starting from non-zero initial epoch\n        trained_epochs = []\n        trained_batches = []\n    \n        # define tracer callback\n        def on_epoch_begin(epoch, logs):\n            trained_epochs.append(epoch)\n    \n        def on_batch_begin(batch, logs):\n            trained_batches.append(batch)\n    \n        tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin,\n                                    on_batch_begin=on_batch_begin)\n    \n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np], epochs=5, batch_size=4,\n                        initial_epoch=2, callbacks=[tracker_cb])\n        assert trained_epochs == [2, 3, 4]\n    \n        # test starting from non-zero initial epoch for generator too\n        trained_epochs = []\n    \n        @threadsafe_generator\n        def gen_data(batch_sz):\n            while True:\n                yield ([np.random.random((batch_sz, 3)),\n                        np.random.random((batch_sz, 3))],\n                       [np.random.random((batch_sz, 4)),\n                        np.random.random((batch_sz, 3))])\n    \n        out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,\n                                  initial_epoch=2, callbacks=[tracker_cb])\n        assert trained_epochs == [2, 3, 4]\n    \n        # test with a custom metric function\n        def mse(y_true, y_pred):\n            return K.mean(K.pow(y_true - y_pred, 2))\n    \n        model.compile(optimizer, loss, metrics=[mse],\n                      sample_weight_mode=None)\n    \n        out = model.train_on_batch([input_a_np, input_b_np],\n                                   [output_a_np, output_b_np])\n        out_len = 1 + 2 * (1 + 1)  # total loss + 2 outputs * (loss + metric)\n        assert len(out) == out_len\n        out = model.test_on_batch([input_a_np, input_b_np],\n                                  [output_a_np, output_b_np])\n        assert len(out) == out_len\n    \n        input_a_np = np.random.random((10, 3))\n        input_b_np = np.random.random((10, 3))\n    \n        output_a_np = np.random.random((10, 4))\n        output_b_np = np.random.random((10, 3))\n    \n        out = model.fit([input_a_np, input_b_np],\n                        [output_a_np, output_b_np],\n                        batch_size=4, epochs=1)\n        out = model.evaluate([input_a_np, input_b_np],\n                             [output_a_np, output_b_np],\n                             batch_size=4)\n        out = model.predict([input_a_np, input_b_np], batch_size=4)\n    \n        # enable verbose for evaluate_generator\n        out = model.evaluate_generator(gen_data(4), steps=3, verbose=1)\n    \n        # empty batch\n        with pytest.raises(ValueError):\n            @threadsafe_generator\n            def gen_data():\n                while True:\n                    yield (np.asarray([]), np.asarray([]))\n    \n            out = model.evaluate_generator(gen_data(), steps=1)\n    \n        # x is not a list of numpy arrays.\n        with pytest.raises(ValueError):\n            out = model.predict([None])\n    \n        # x does not match _feed_input_names.\n        with pytest.raises(ValueError):\n            out = model.predict([input_a_np, None, input_b_np])\n        with pytest.raises(ValueError):\n            out = model.predict([None, input_a_np, input_b_np])\n    \n        # all input/output/weight arrays should have the same number of samples.\n        with pytest.raises(ValueError):\n            out = model.train_on_batch([input_a_np, input_b_np[:2]],\n                                       [output_a_np, output_b_np],\n                                       sample_weight=sample_weight)\n        with pytest.raises(ValueError):\n            out = model.train_on_batch([input_a_np, input_b_np],\n                                       [output_a_np, output_b_np[:2]],\n                                       sample_weight=sample_weight)\n        with pytest.raises(ValueError):\n            out = model.train_on_batch([input_a_np, input_b_np],\n                                       [output_a_np, output_b_np],\n                                       sample_weight=[sample_weight[1],\n                                                      sample_weight[1][:2]])\n    \n        # `sample_weight` is neither a dict nor a list.\n        with pytest.raises(TypeError):\n            out = model.train_on_batch([input_a_np, input_b_np],\n                                       [output_a_np, output_b_np],\n                                       sample_weight=tuple(sample_weight))\n    \n        # `validation_data` is neither a tuple nor a triple.\n        with pytest.raises(ValueError):\n            out = model.fit([input_a_np, input_b_np],\n                            [output_a_np, output_b_np],\n                            epochs=1, batch_size=4,\n                            validation_data=([input_a_np, input_b_np],))\n    \n        # `loss` does not match outputs.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss=['mse', 'mae', 'mape'])\n    \n        # `loss_weights` does not match output_names.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss='mse', loss_weights={'lstm': 0.5})\n    \n        # `loss_weights` does not match outputs.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss='mse', loss_weights=[0.5])\n    \n        # `loss_weights` is invalid type.\n        with pytest.raises(TypeError):\n            model.compile(optimizer, loss='mse', loss_weights=(0.5, 0.5))\n    \n        # `sample_weight_mode` does not match output_names.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss='mse',\n                          sample_weight_mode={'lstm': 'temporal'})\n    \n        # `sample_weight_mode` does not match output_names.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss='mse', sample_weight_mode=['temporal'])\n    \n        # `sample_weight_mode` matches output_names partially.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss='mse',\n                          sample_weight_mode={'dense_1': 'temporal'})\n    \n        # `loss` does not exist.\n        with pytest.raises(ValueError):\n            model.compile(optimizer, loss=[])\n    \n        model.compile(optimizer, loss=['mse', 'mae'])\n        model.compile(optimizer, loss='mse', loss_weights={'dense_1': 0.2,\n                                                           'dropout': 0.8})\n        model.compile(optimizer, loss='mse', loss_weights=[0.2, 0.8])\n    \n        # the rank of weight arrays should be 1.\n        with pytest.raises(ValueError):\n            out = model.train_on_batch(\n                [input_a_np, input_b_np],\n                [output_a_np, output_b_np],\n                sample_weight=[None, np.random.random((10, 20, 30))])\n    \n        model.compile(optimizer, loss='mse',\n                      sample_weight_mode={'dense_1': None, 'dropout': 'temporal'})\n        model.compile(optimizer, loss='mse', sample_weight_mode=[None, 'temporal'])\n    \n        # the rank of output arrays should be at least 3D.\n        with pytest.raises(ValueError):\n            out = model.train_on_batch([input_a_np, input_b_np],\n                                       [output_a_np, output_b_np],\n                                       sample_weight=sample_weight)\n    \n        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                      sample_weight_mode=None)\n        trained_epochs = []\n        trained_batches = []\n        val_seq = RandomSequence(4)\n        out = model.fit_generator(generator=RandomSequence(3),\n                                  steps_per_epoch=3,\n                                  epochs=5,\n                                  initial_epoch=0,\n                                  validation_data=val_seq,\n                                  validation_steps=3,\n                                  max_queue_size=1,\n                                  callbacks=[tracker_cb])\n        assert trained_epochs == [0, 1, 2, 3, 4]\n        assert trained_batches == list(range(3)) * 5\n        assert len(val_seq.logs) <= 4 * 5\n    \n        # steps_per_epoch will be equal to len of sequence if it's unspecified\n        trained_epochs = []\n        trained_batches = []\n        val_seq = RandomSequence(4)\n        out = model.fit_generator(generator=RandomSequence(3),\n                                  epochs=5,\n                                  initial_epoch=0,\n                                  validation_data=val_seq,\n                                  callbacks=[tracker_cb])\n        assert trained_epochs == [0, 1, 2, 3, 4]\n        assert trained_batches == list(range(12)) * 5\n        assert len(val_seq.logs) == 12 * 5\n    \n        # test for workers = 0\n        trained_epochs = []\n        trained_batches = []\n        val_seq = RandomSequence(4)\n        out = model.fit_generator(generator=RandomSequence(3),\n                                  epochs=5,\n                                  validation_data=val_seq,\n                                  callbacks=[tracker_cb],\n>                                 workers=0)\n\ntests/keras/engine/test_training.py:479: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nkeras/legacy/interfaces.py:91: in wrapper\n    return func(*args, **kwargs)\nkeras/engine/training.py:1418: in fit_generator\n    initial_epoch=initial_epoch)\nkeras/engine/training_generator.py:233: in fit_generator\n    workers=0)\nkeras/legacy/interfaces.py:91: in wrapper\n    return func(*args, **kwargs)\nkeras/engine/training.py:1472: in evaluate_generator\n    verbose=verbose)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmodel = <keras.engine.training.Model object at 0x12c191610>\ngenerator = <generator object iter_sequence_infinite at 0x12bff2a50>\nsteps = None, max_queue_size = 10, workers = 0, use_multiprocessing = False\nverbose = 0\n\n    def evaluate_generator(model, generator,\n                           steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,\n                           verbose=0):\n        \"\"\"See docstring for `Model.evaluate_generator`.\"\"\"\n        model._make_test_function()\n    \n        if hasattr(model, 'metrics'):\n            for m in model.stateful_metric_functions:\n                m.reset_states()\n            stateful_metric_indices = [\n                i for i, name in enumerate(model.metrics_names)\n                if str(name) in model.stateful_metric_names]\n        else:\n            stateful_metric_indices = []\n    \n        steps_done = 0\n        wait_time = 0.01\n        outs_per_batch = []\n        batch_sizes = []\n        is_sequence = isinstance(generator, Sequence)\n        if not is_sequence and use_multiprocessing and workers > 1:\n            warnings.warn(\n                UserWarning('Using a generator with `use_multiprocessing=True`'\n                            ' and multiple workers may duplicate your data.'\n                            ' Please consider using the`keras.utils.Sequence'\n                            ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n>               raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')",
            "\nkeras/engine/training_generator.py:300: ValueError"
        ]
    ],
    "2.1.3": [
        [
            {
                "initial_epoch": "2",
                "model": "<keras.engine.training.Model object at 0x130524150>",
                "generator": "<test_training.threadsafe_iter object at 0x130606fd0>",
                "use_multiprocessing": "False",
                "workers": "1",
                "steps_per_epoch": "3",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc']",
                "model.history": "<keras.callbacks.History object at 0x130cfb750>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x130bc6610>]",
                "epochs": "5",
                "max_queue_size": "10",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "False",
                "is_sequence": "False",
                "val_gen": "False",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss', 'val_dense_1_acc']",
                "model.history": "<keras.callbacks.History object at 0x130d08f10>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x130d08f50>, <keras.callbacks.ProgbarLogger object at 0x130d08f90>, <keras.callbacks.LambdaCallback object at 0x130bc6610>, <keras.callbacks.History object at 0x130d08f10>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x130d08fd0>",
                "callback_model": "<keras.engine.training.Model object at 0x130524150>",
                "enqueuer": "<keras.utils.data_utils.GeneratorEnqueuer object at 0x130d08dd0>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{}",
                "steps_done": "3",
                "batch_index": "3",
                "generator_output": "array of shape 2",
                "x": "[array([[0.70560202, 0.03502665, 0.99396007],\n       [0.93737624, 0.99473772, 0.3143665 ],\n       [0.26949476, 0.58382203, 0.41114939],\n       [0.20101167, 0.06166991, 0.94371909]]), array([[0.28151745, 0.04512048, 0.91251266],\n       [0.31295743, 0.60146303, 0.50820282],\n       [0.94899923, 0.35552213, 0.43446912],\n       [0.08184676, 0.0415875 , 0.01843863]])]",
                "y": "[array([[0.96267404, 0.56637982, 0.40370915, 0.7745561 ],\n       [0.00542247, 0.31163886, 0.49050717, 0.93155077],\n       [0.82940025, 0.56867303, 0.36493209, 0.50146491],\n       [0.60343683, 0.02759473, 0.23236803, 0.3385533 ]]), array([[0.26276965, 0.45719085, 0.86533381],\n       [0.51278779, 0.54317465, 0.03604988],\n       [0.73416093, 0.42201705, 0.5149987 ],\n       [0.43039684, 0.11384768, 0.74848362]])]",
                "batch_logs": "{'batch': 2, 'size': 4, 'loss': 1.1875378, 'dense_1_loss': 0.8395252, 'dropout_loss': 0.34801254, 'dense_1_acc': 0.0}",
                "batch_size": "4",
                "outs": "[1.1875378, 0.8395252, 0.34801254, 0.0]",
                "l": "'dense_1_acc'",
                "o": "0.0"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.RandomSequence object at 0x130dd8310>",
                "model": "<keras.engine.training.Model object at 0x130524150>",
                "generator": "<test_training.RandomSequence object at 0x130c9aa50>",
                "use_multiprocessing": "False",
                "workers": "1",
                "steps_per_epoch": "3",
                "validation_steps": "3",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x130dc0d50>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x130bc6610>]",
                "epochs": "5",
                "max_queue_size": "1",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "True",
                "is_sequence": "True",
                "val_gen": "True",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x130edcd90>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x130dc0d50>, <keras.callbacks.ProgbarLogger object at 0x130d43110>, <keras.callbacks.LambdaCallback object at 0x130bc6610>, <keras.callbacks.History object at 0x130edcd90>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x130d43090>",
                "callback_model": "<keras.engine.training.Model object at 0x130524150>",
                "enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x130f46810>",
                "val_enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x130d4ead0>",
                "val_data": "<test_training.RandomSequence object at 0x130dd8310>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.672760804494222, 'val_dense_1_loss': 0.5759458939234415, 'val_dropout_loss': 0.1936297888557116, 'loss': 0.7760198712348938, 'dense_1_loss': 0.5148996412754059, 'dropout_loss': 0.5222404301166534}",
                "steps_done": "3",
                "batch_index": "3",
                "generator_output": "array of shape 2",
                "x": "[array([[0.9551607 , 0.21997659, 0.31312019],\n       [0.8257608 , 0.91069558, 0.31975746],\n       [0.6868846 , 0.48377696, 0.00331938]]), array([[0.02211417, 0.36025267, 0.10503865],\n       [0.88670425, 0.22115492, 0.19514562],\n       [0.28848067, 0.8411662 , 0.25203354]])]",
                "y": "[array([[0.67234586, 0.12659832, 0.36003322, 0.70160536],\n       [0.34567297, 0.5801905 , 0.20159728, 0.89761009],\n       [0.51926778, 0.64416019, 0.01469451, 0.90194565]]), array([[0.50824249, 0.97062615, 0.33777743],\n       [0.96869128, 0.40506393, 0.82482664],\n       [0.42715922, 0.6483067 , 0.76311285]])]",
                "batch_logs": "{'batch': 2, 'size': 3, 'loss': 1.0448643, 'dense_1_loss': 0.82710916, 'dropout_loss': 0.43551028}",
                "batch_size": "3",
                "outs": "[1.0448643, 0.82710916, 0.43551028]",
                "l": "'dropout_loss'",
                "o": "0.1936297888557116",
                "val_outs": "[0.672760804494222, 0.5759458939234415, 0.1936297888557116]"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.RandomSequence object at 0x130dcad10>",
                "model": "<keras.engine.training.Model object at 0x130524150>",
                "generator": "<test_training.RandomSequence object at 0x130dd8310>",
                "use_multiprocessing": "False",
                "workers": "1",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x130edcd90>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x130bc6610>]",
                "epochs": "5",
                "max_queue_size": "10",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "True",
                "is_sequence": "True",
                "steps_per_epoch": "12",
                "val_gen": "True",
                "validation_steps": "12",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x130edc610>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x130edc950>, <keras.callbacks.ProgbarLogger object at 0x130edc8d0>, <keras.callbacks.LambdaCallback object at 0x130bc6610>, <keras.callbacks.History object at 0x130edc610>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x130edc710>",
                "callback_model": "<keras.engine.training.Model object at 0x130524150>",
                "enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x130f7d390>",
                "val_enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x130edc450>",
                "val_data": "<test_training.RandomSequence object at 0x130dcad10>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.3978737145662308, 'val_dense_1_loss': 0.3038039095699787, 'val_dropout_loss': 0.18813961123426756, 'loss': 0.5984019289414088, 'dense_1_loss': 0.38865052660306293, 'dropout_loss': 0.4195027860502402}",
                "steps_done": "12",
                "batch_index": "12",
                "generator_output": "array of shape 2",
                "x": "[array([[0.2918524 , 0.04898133, 0.56056091],\n       [0.42719609, 0.48475626, 0.13245272],\n       [0.64816722, 0.39141932, 0.52332865]]), array([[0.72233783, 0.71757599, 0.34009609],\n       [0.51980414, 0.75335939, 0.33579904],\n       [0.17670878, 0.4273658 , 0.94005961]])]",
                "y": "[array([[0.89836574, 0.46204427, 0.46312981, 0.30773725],\n       [0.36889117, 0.05811424, 0.02782399, 0.98136876],\n       [0.52806411, 0.09992129, 0.44264417, 0.08113702]]), array([[0.47422872, 0.63064254, 0.02481319],\n       [0.70115999, 0.08939141, 0.04847445],\n       [0.33906673, 0.2958472 , 0.95285547]])]",
                "batch_logs": "{'batch': 11, 'size': 3, 'loss': 0.65309227, 'dense_1_loss': 0.3997902, 'dropout_loss': 0.5066041}",
                "batch_size": "3",
                "outs": "[0.65309227, 0.3997902, 0.5066041]",
                "l": "'dropout_loss'",
                "o": "0.18813961123426756",
                "val_outs": "[0.3978737145662308, 0.3038039095699787, 0.18813961123426756]"
            }
        ]
    ],
    "2.1.4": [
        [
            {
                "initial_epoch": "int",
                "model": "Model",
                "generator": "threadsafe_iter",
                "use_multiprocessing": "bool",
                "workers": "int",
                "steps_per_epoch": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "GeneratorEnqueuer",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float32"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "RandomSequence",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "steps_per_epoch": "int",
                "validation_steps": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "OrderedEnqueuer",
                "val_enqueuer": "OrderedEnqueuer",
                "val_data": "RandomSequence",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "RandomSequence",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "steps_per_epoch": "int",
                "val_gen": "bool",
                "validation_steps": "int",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "OrderedEnqueuer",
                "val_enqueuer": "OrderedEnqueuer",
                "val_data": "RandomSequence",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ]
    ],
    "2.1.5": [
        [
            {
                "initial_epoch": "2",
                "model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "generator": "<test_training.threadsafe_iter object at 0x12a39eb90>",
                "use_multiprocessing": "False",
                "workers": "1",
                "steps_per_epoch": "3",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc']",
                "model.history": "<keras.callbacks.History object at 0x12aac95d0>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x12a36ded0>]",
                "epochs": "5",
                "max_queue_size": "10",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "False",
                "is_sequence": "False",
                "val_gen": "False",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'dense_1_acc', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss', 'val_dense_1_acc']",
                "model.history": "<keras.callbacks.History object at 0x12aad6ed0>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x12aad6f10>, <keras.callbacks.ProgbarLogger object at 0x12aadc050>, <keras.callbacks.LambdaCallback object at 0x12a36ded0>, <keras.callbacks.History object at 0x12aad6ed0>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x12aadc090>",
                "callback_model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "enqueuer": "<keras.utils.data_utils.GeneratorEnqueuer object at 0x12aadc0d0>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{}",
                "steps_done": "3",
                "batch_index": "3",
                "generator_output": "array of shape 2",
                "x": "[array([[0.33750873, 0.5136167 , 0.53577781],\n       [0.48760268, 0.01984002, 0.17521617],\n       [0.75278174, 0.99175478, 0.14639423],\n       [0.90686009, 0.37647084, 0.02102236]]), array([[0.83349624, 0.31948218, 0.20625704],\n       [0.08638254, 0.14220823, 0.37508703],\n       [0.19639428, 0.0528124 , 0.58414799],\n       [0.23966877, 0.74459003, 0.4079947 ]])]",
                "y": "[array([[0.32758379, 0.2964852 , 0.29471192, 0.68558195],\n       [0.1046377 , 0.76900004, 0.89396773, 0.51878667],\n       [0.88434257, 0.55727208, 0.78375848, 0.46058295],\n       [0.92437945, 0.80318954, 0.02108712, 0.34031867]]), array([[0.73767033, 0.85921857, 0.04067537],\n       [0.46481044, 0.55554165, 0.09640836],\n       [0.67821121, 0.84811522, 0.39642351],\n       [0.21904532, 0.89785596, 0.15404764]])]",
                "batch_logs": "{'batch': 2, 'size': 4, 'loss': 0.5378475, 'dense_1_loss': 0.1754938, 'dropout_loss': 0.3623537, 'dense_1_acc': 0.5}",
                "batch_size": "4",
                "outs": "[0.5378475, 0.1754938, 0.3623537, 0.5]",
                "l": "'dense_1_acc'",
                "o": "0.5"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.RandomSequence object at 0x12aad6ed0>",
                "model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "generator": "<test_training.RandomSequence object at 0x12aba9610>",
                "use_multiprocessing": "False",
                "workers": "1",
                "steps_per_epoch": "3",
                "validation_steps": "3",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12ab8f310>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x12a36ded0>]",
                "epochs": "5",
                "max_queue_size": "1",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "True",
                "is_sequence": "True",
                "val_gen": "True",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12acad3d0>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x12ab8f310>, <keras.callbacks.ProgbarLogger object at 0x12ab23790>, <keras.callbacks.LambdaCallback object at 0x12a36ded0>, <keras.callbacks.History object at 0x12acad3d0>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x12ab23ed0>",
                "callback_model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x12ad11a50>",
                "val_enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x12ab0ccd0>",
                "val_data": "<test_training.RandomSequence object at 0x12aad6ed0>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.4053945144017537, 'val_dense_1_loss': 0.30581893026828766, 'val_dropout_loss': 0.19915116826693216, 'loss': 0.5663792888323466, 'dense_1_loss': 0.2560502042373021, 'dropout_loss': 0.6206581989924113}",
                "steps_done": "3",
                "batch_index": "3",
                "generator_output": "array of shape 2",
                "x": "[array([[0.46474225, 0.2257669 , 0.20368437],\n       [0.45253438, 0.15823404, 0.14360165],\n       [0.51799903, 0.17312787, 0.43882016]]), array([[0.05531963, 0.68060994, 0.01310505],\n       [0.10893409, 0.98523306, 0.22737118],\n       [0.15346376, 0.86436529, 0.4235327 ]])]",
                "y": "[array([[0.73731019, 0.95126603, 0.10059117, 0.49223324],\n       [0.81735597, 0.8579159 , 0.47260513, 0.18054984],\n       [0.00494596, 0.18983512, 0.1211448 , 0.37917588]]), array([[0.51785107, 0.38095627, 0.36735878],\n       [0.3317447 , 0.2448713 , 0.55752323],\n       [0.72129618, 0.23450102, 0.63977107]])]",
                "batch_logs": "{'batch': 2, 'size': 3, 'loss': 0.49836814, 'dense_1_loss': 0.13489376, 'dropout_loss': 0.7269488}",
                "batch_size": "3",
                "outs": "[0.49836814, 0.13489376, 0.7269488]",
                "l": "'dropout_loss'",
                "o": "0.19915116826693216",
                "val_outs": "[0.4053945144017537, 0.30581893026828766, 0.19915116826693216]"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.RandomSequence object at 0x12ab9c9d0>",
                "model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "generator": "<test_training.RandomSequence object at 0x12aba9610>",
                "use_multiprocessing": "False",
                "workers": "1",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12acad3d0>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x12a36ded0>]",
                "epochs": "5",
                "max_queue_size": "10",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "True",
                "is_sequence": "True",
                "steps_per_epoch": "12",
                "val_gen": "True",
                "validation_steps": "12",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12ad55550>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x12ad553d0>, <keras.callbacks.ProgbarLogger object at 0x12ad55410>, <keras.callbacks.LambdaCallback object at 0x12a36ded0>, <keras.callbacks.History object at 0x12ad55550>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x12ad55e10>",
                "callback_model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x12ad55e90>",
                "val_enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x12ad554d0>",
                "val_data": "<test_training.RandomSequence object at 0x12ab9c9d0>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.2799253563086192, 'val_dense_1_loss': 0.20351597977181277, 'val_dropout_loss': 0.15281875741978487, 'loss': 0.4896574417750041, 'dense_1_loss': 0.21972213312983513, 'dropout_loss': 0.539870614806811}",
                "steps_done": "12",
                "batch_index": "12",
                "generator_output": "array of shape 2",
                "x": "[array([[0.21472568, 0.00839212, 0.07626943],\n       [0.32249606, 0.93406866, 0.97178009],\n       [0.95364409, 0.88256046, 0.01954905]]), array([[0.97515994, 0.39822945, 0.67094905],\n       [0.73696519, 0.78745957, 0.95483275],\n       [0.55510007, 0.1518813 , 0.79887374]])]",
                "y": "[array([[0.98478996, 0.51313147, 0.01870424, 0.53229032],\n       [0.03214285, 0.37459196, 0.04231062, 0.56061339],\n       [0.90582445, 0.33432855, 0.94053337, 0.99966136]]), array([[0.48053615, 0.77947947, 0.26024631],\n       [0.11509418, 0.13439063, 0.18438608],\n       [0.26197757, 0.12124476, 0.22179787]])]",
                "batch_logs": "{'batch': 11, 'size': 3, 'loss': 0.76964176, 'dense_1_loss': 0.26708975, 'dropout_loss': 1.005104}",
                "batch_size": "3",
                "outs": "[0.76964176, 0.26708975, 1.005104]",
                "l": "'dropout_loss'",
                "o": "0.15281875741978487",
                "val_outs": "[0.2799253563086192, 0.20351597977181277, 0.15281875741978487]"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.RandomSequence object at 0x12ad3f110>",
                "model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "generator": "<test_training.RandomSequence object at 0x12ab9c9d0>",
                "use_multiprocessing": "False",
                "workers": "0",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12ad55550>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "callbacks": "[<keras.callbacks.LambdaCallback object at 0x12a36ded0>]",
                "epochs": "5",
                "max_queue_size": "10",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "5",
                "do_validation": "True",
                "is_sequence": "True",
                "steps_per_epoch": "12",
                "val_gen": "True",
                "validation_steps": "12",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12ab0ccd0>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x12ad116d0>, <keras.callbacks.ProgbarLogger object at 0x12ad11190>, <keras.callbacks.LambdaCallback object at 0x12a36ded0>, <keras.callbacks.History object at 0x12ab0ccd0>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x12ad11a90>",
                "callback_model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "val_data": "<test_training.RandomSequence object at 0x12ad3f110>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.24686423192421594, 'val_dense_1_loss': 0.1729911199460427, 'val_dropout_loss': 0.14774622209370136, 'loss': 0.40624505281448364, 'dense_1_loss': 0.17696035529176393, 'dropout_loss': 0.45856938635309535}",
                "steps_done": "12",
                "batch_index": "12",
                "generator_output": "array of shape 2",
                "x": "[array([[0.7452409 , 0.0782688 , 0.58545018],\n       [0.3419853 , 0.89563022, 0.6227658 ],\n       [0.53248813, 0.54065278, 0.01030876]]), array([[0.23543   , 0.60047844, 0.95861554],\n       [0.37630508, 0.20202085, 0.84479816],\n       [0.1201779 , 0.33555582, 0.67641484]])]",
                "y": "[array([[0.26514647, 0.82049385, 0.76436275, 0.33568366],\n       [0.47246681, 0.85912571, 0.00837789, 0.49210659],\n       [0.19715162, 0.77316699, 0.69149776, 0.93295591]]), array([[0.5757794 , 0.71298225, 0.13949308],\n       [0.86749835, 0.32170947, 0.99842255],\n       [0.23922145, 0.20578631, 0.26340424]])]",
                "batch_logs": "{'batch': 11, 'size': 3, 'loss': 0.5417553, 'dense_1_loss': 0.18896712, 'dropout_loss': 0.70557636}",
                "batch_size": "3",
                "outs": "[0.5417553, 0.18896712, 0.70557636]",
                "l": "'dropout_loss'",
                "o": "0.14774622209370136",
                "val_outs": "[0.24686423192421594, 0.1729911199460427, 0.14774622209370136]"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.threadsafe_iter object at 0x12aac9d50>",
                "model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "generator": "<test_training.threadsafe_iter object at 0x12ab23ed0>",
                "use_multiprocessing": "False",
                "workers": "2",
                "steps_per_epoch": "2",
                "validation_steps": "1",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12ab0ccd0>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "epochs": "3",
                "max_queue_size": "2",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "3",
                "do_validation": "True",
                "is_sequence": "False",
                "val_gen": "True",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12acad210>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x12acade90>, <keras.callbacks.ProgbarLogger object at 0x12acadf10>, <keras.callbacks.History object at 0x12acad210>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x12acadf90>",
                "callback_model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "enqueuer": "<keras.utils.data_utils.GeneratorEnqueuer object at 0x12ad4f050>",
                "val_enqueuer": "<keras.utils.data_utils.GeneratorEnqueuer object at 0x12acad510>",
                "val_data": "<test_training.threadsafe_iter object at 0x12aac9d50>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.12580634653568268, 'val_dense_1_loss': 0.08597846329212189, 'val_dropout_loss': 0.07965576648712158, 'loss': 0.29641292989254, 'dense_1_loss': 0.08830020949244499, 'dropout_loss': 0.41622544825077057}",
                "steps_done": "2",
                "batch_index": "2",
                "generator_output": "([array([[0.91795043, 0.85831207, 0.15983874]]), array([[0.02749992, 0.70309232, 0.0778474 ]])], [array([[0.55261849, 0.55769237, 0.74210381, 0.39440575]]), array([[0.73294164, 0.55384271, 0.9678148 ]])])",
                "x": "[array([[0.91795043, 0.85831207, 0.15983874]]), array([[0.02749992, 0.70309232, 0.0778474 ]])]",
                "y": "[array([[0.55261849, 0.55769237, 0.74210381, 0.39440575]]), array([[0.73294164, 0.55384271, 0.9678148 ]])]",
                "batch_logs": "{'batch': 1, 'size': 1, 'loss': 0.32639694, 'dense_1_loss': 0.07581625, 'dropout_loss': 0.5011614}",
                "batch_size": "1",
                "outs": "[0.32639694, 0.07581625, 0.5011614]",
                "l": "'dropout_loss'",
                "o": "0.07965576648712158",
                "val_outs": "[0.12580634653568268, 0.08597846329212189, 0.07965576648712158]"
            }
        ],
        [
            {
                "initial_epoch": "0",
                "validation_data": "<test_training.threadsafe_iter object at 0x12ad11590>",
                "model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "generator": "<test_training.RandomSequence object at 0x12ab0ccd0>",
                "use_multiprocessing": "False",
                "workers": "2",
                "validation_steps": "1",
                "model.metrics_names": "['loss', 'dense_1_loss', 'dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12acad210>",
                "model.stateful_metric_names": "[]",
                "verbose": "1",
                "epochs": "3",
                "max_queue_size": "2",
                "model.uses_learning_phase": "True",
                "shuffle": "True",
                "model.stateful_metric_functions": "[]"
            },
            {
                "wait_time": "0.01",
                "epoch": "3",
                "do_validation": "True",
                "is_sequence": "True",
                "steps_per_epoch": "12",
                "val_gen": "True",
                "out_labels": "['loss', 'dense_1_loss', 'dropout_loss']",
                "callback_metrics": "['loss', 'dense_1_loss', 'dropout_loss', 'val_loss', 'val_dense_1_loss', 'val_dropout_loss']",
                "model.history": "<keras.callbacks.History object at 0x12acadf90>",
                "_callbacks": "[<keras.callbacks.BaseLogger object at 0x12acad990>, <keras.callbacks.ProgbarLogger object at 0x12acada10>, <keras.callbacks.History object at 0x12acadf90>]",
                "callbacks": "<keras.callbacks.CallbackList object at 0x12acad790>",
                "callback_model": "<keras.engine.training.Model object at 0x12a3726d0>",
                "enqueuer": "<keras.utils.data_utils.OrderedEnqueuer object at 0x12ad6ae90>",
                "val_enqueuer": "<keras.utils.data_utils.GeneratorEnqueuer object at 0x12acadb50>",
                "val_data": "<test_training.threadsafe_iter object at 0x12ad11590>",
                "callback_model.stop_training": "False",
                "epoch_logs": "{'val_loss': 0.19589273631572723, 'val_dense_1_loss': 0.1560831069946289, 'val_dropout_loss': 0.07961925864219666, 'loss': 0.38265037288268405, 'dense_1_loss': 0.1734140682965517, 'dropout_loss': 0.4184726079305013}",
                "steps_done": "12",
                "batch_index": "12",
                "generator_output": "array of shape 2",
                "x": "[array([[0.93554648, 0.81620651, 0.25361077],\n       [0.44245478, 0.08270969, 0.05057173],\n       [0.96031556, 0.34060875, 0.38418515]]), array([[0.26843607, 0.44755745, 0.12073934],\n       [0.09710739, 0.50577937, 0.98217769],\n       [0.58336829, 0.03577427, 0.18415636]])]",
                "y": "[array([[0.56299691, 0.18517006, 0.93614543, 0.2481242 ],\n       [0.96793466, 0.09624035, 0.89107958, 0.12730346],\n       [0.25117148, 0.21787031, 0.31334181, 0.00480162]]), array([[0.07915022, 0.60914765, 0.34887576],\n       [0.8638142 , 0.29094374, 0.43950641],\n       [0.16975463, 0.23221623, 0.40382332]])]",
                "batch_logs": "{'batch': 11, 'size': 3, 'loss': 0.377576, 'dense_1_loss': 0.19204454, 'dropout_loss': 0.37106287}",
                "batch_size": "3",
                "outs": "[0.377576, 0.19204454, 0.37106287]",
                "l": "'dropout_loss'",
                "o": "0.07961925864219666",
                "val_outs": "[0.19589273631572723, 0.1560831069946289, 0.07961925864219666]"
            }
        ]
    ],
    "2.1.6": [
        [
            {
                "initial_epoch": "int",
                "model": "Model",
                "generator": "threadsafe_iter",
                "use_multiprocessing": "bool",
                "workers": "int",
                "steps_per_epoch": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "GeneratorEnqueuer",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float32"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "RandomSequence",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "steps_per_epoch": "int",
                "validation_steps": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "OrderedEnqueuer",
                "val_enqueuer": "OrderedEnqueuer",
                "val_data": "RandomSequence",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "RandomSequence",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "steps_per_epoch": "int",
                "val_gen": "bool",
                "validation_steps": "int",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "OrderedEnqueuer",
                "val_enqueuer": "OrderedEnqueuer",
                "val_data": "RandomSequence",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "RandomSequence",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "callbacks": "list",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "steps_per_epoch": "int",
                "val_gen": "bool",
                "validation_steps": "int",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "val_data": "RandomSequence",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "threadsafe_iter",
                "model": "Model",
                "generator": "threadsafe_iter",
                "use_multiprocessing": "bool",
                "workers": "int",
                "steps_per_epoch": "int",
                "validation_steps": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "GeneratorEnqueuer",
                "val_enqueuer": "GeneratorEnqueuer",
                "val_data": "threadsafe_iter",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ],
        [
            {
                "initial_epoch": "int",
                "validation_data": "threadsafe_iter",
                "model": "Model",
                "generator": "RandomSequence",
                "use_multiprocessing": "bool",
                "workers": "int",
                "validation_steps": "int",
                "model.metrics_names": "list",
                "model.history": "History",
                "model.stateful_metric_names": "list",
                "verbose": "int",
                "epochs": "int",
                "max_queue_size": "int",
                "model.uses_learning_phase": "bool",
                "shuffle": "bool",
                "model.stateful_metric_functions": "list"
            },
            {
                "wait_time": "float",
                "epoch": "int",
                "do_validation": "bool",
                "is_sequence": "bool",
                "steps_per_epoch": "int",
                "val_gen": "bool",
                "out_labels": "list",
                "callback_metrics": "list",
                "model.history": "History",
                "_callbacks": "list",
                "callbacks": "CallbackList",
                "callback_model": "Model",
                "enqueuer": "OrderedEnqueuer",
                "val_enqueuer": "GeneratorEnqueuer",
                "val_data": "threadsafe_iter",
                "callback_model.stop_training": "bool",
                "epoch_logs": "dict",
                "steps_done": "int",
                "batch_index": "int",
                "generator_output": "tuple",
                "x": "list",
                "y": "list",
                "batch_logs": "dict",
                "batch_size": "int",
                "outs": "list",
                "l": "str",
                "o": "float64",
                "val_outs": "list"
            }
        ]
    ],
    "3.1.1": [
        "fit_generator crashes though keras.utils.data_utils.Sequence was used\n"
    ],
    "3.1.2": [
        "When model.fit_generator is used with workers=0 and subclasses of keras.utils.data_utils.Sequence for both training and validation data, API of Sequence is not recognized inside evaluate_generator, it raises:\n\n  File \".../keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \".../keras/engine/training.py\", line 1415, in fit_generator\n    initial_epoch=initial_epoch)\n  File \".../keras/engine/training_generator.py\", line 230, in fit_generator\n    validation_steps,\n  File \".../keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \".../keras/engine/training.py\", line 1469, in evaluate_generator\n    verbose=verbose)\n  File \".../keras/engine/training_generator.py\", line 298, in evaluate_generator\n    else:\nValueError: `steps=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps` or use the `keras.utils.Sequence` class.\nExample code:\n\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.utils.data_utils import Sequence\nimport numpy as np\n\nclass Dataset(Sequence):\n    def __getitem__(self, index):\n        return np.random.uniform(size=(16, 8)), np.random.uniform(size=(16, 1))\n    def __len__(self):\n        return 128\n\nmodel = Sequential([Dense(4, activation='relu', input_shape=(8,)),\n                    Dense(1, activation='sigmoid')])\nmodel.compile(loss='mse', optimizer='adam')\nmodel.fit_generator(generator=Dataset(), validation_data=Dataset(),\n                    workers=0)\nIssue can be fixed here by replacing:\n\nif isinstance(val_data, Sequence):\n    val_enqueuer_gen = iter(val_data)\nwith\n\nif isinstance(val_data, Sequence):\n    val_enqueuer_gen = iter(val_data)\n    validation_steps = len(val_data)\n"
    ]
}