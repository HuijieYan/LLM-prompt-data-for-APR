{
    "1.1.1": "@interfaces.legacy_get_updates_support\ndef get_updates(self, loss, params):\n    grads = self.optimizer.compute_gradients(loss, params)\n    self.updates = [K.update_add(self.iterations, 1)]\n    opt_update = self.optimizer.apply_gradients(\n        grads, global_step=self.iterations)\n    self.updates.append(opt_update)\n    return self.updates\n",
    "1.1.2": null,
    "1.2.1": "class TFOptimizer(Optimizer)",
    "1.2.2": "Wrapper class for native TensorFlow optimizers.\n    ",
    "1.2.3": null,
    "1.2.4": null,
    "1.2.5": null,
    "1.3.1": "keras/optimizers.py",
    "1.3.2": null,
    "1.4.1": [
        "@pytest.mark.skipif((K.backend() != 'tensorflow'),\n                    reason='Requires TensorFlow backend')\ndef test_tfoptimizer_pass_correct_named_params_to_native_tensorflow_optimizer():\n    from keras import constraints\n    from tensorflow import train\n\n    class MyTfOptimizer(train.Optimizer):\n        wrapping_optimizer = train.AdamOptimizer()\n\n        def compute_gradients(self, loss, **kwargs):\n            return super(MyTfOptimizer, self).compute_gradients(loss, **kwargs)\n\n        def apply_gradients(self, grads_and_vars, **kwargs):\n            return self.wrapping_optimizer.apply_gradients(grads_and_vars,\n                                                           **kwargs)\n    my_tf_optimizer = MyTfOptimizer(use_locking=False, name='MyTfOptimizer')\n    optimizer = optimizers.TFOptimizer(my_tf_optimizer)\n    model = Sequential()\n    model.add(Dense(num_classes, input_shape=(3,),\n                    kernel_constraint=constraints.MaxNorm(1)))\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    model.fit(np.random.random((5, 3)), np.random.random((5, num_classes)),\n              epochs=1, batch_size=5, verbose=0)"
    ],
    "1.4.2": [
        "tests/keras/optimizers_test.py"
    ],
    "2.1.1": [
        [
            "E       TypeError: compute_gradients() takes 2 positional arguments but 3 were given"
        ]
    ],
    "2.1.2": [
        [
            "@pytest.mark.skipif((K.backend() != 'tensorflow'),\n                        reason='Requires TensorFlow backend')\n    def test_tfoptimizer_pass_correct_named_params_to_native_tensorflow_optimizer():\n        from keras import constraints\n        from tensorflow import train\n    \n        class MyTfOptimizer(train.Optimizer):\n            wrapping_optimizer = train.AdamOptimizer()\n    \n            def compute_gradients(self, loss, **kwargs):\n                return super(MyTfOptimizer, self).compute_gradients(loss, **kwargs)\n    \n            def apply_gradients(self, grads_and_vars, **kwargs):\n                return self.wrapping_optimizer.apply_gradients(grads_and_vars,\n                                                               **kwargs)\n        my_tf_optimizer = MyTfOptimizer(use_locking=False, name='MyTfOptimizer')\n        optimizer = optimizers.TFOptimizer(my_tf_optimizer)\n        model = Sequential()\n        model.add(Dense(num_classes, input_shape=(3,),\n                        kernel_constraint=constraints.MaxNorm(1)))\n        model.compile(loss='mean_squared_error', optimizer=optimizer)\n        model.fit(np.random.random((5, 3)), np.random.random((5, num_classes)),\n>                 epochs=1, batch_size=5, verbose=0)\n\ntests/keras/optimizers_test.py:173: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nkeras/engine/training.py:1026: in fit\n    self._make_train_function()\nkeras/engine/training.py:509: in _make_train_function\n    loss=self.total_loss)\nkeras/legacy/interfaces.py:91: in wrapper\n    return func(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <keras.optimizers.TFOptimizer object at 0x12b018990>\nloss = <tf.Tensor 'loss/mul:0' shape=() dtype=float32>\nparams = [<tf.Variable 'dense_1/kernel:0' shape=(3, 2) dtype=float32_ref>, <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32_ref>]\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n>       grads = self.optimizer.compute_gradients(loss, params)",
            "\nkeras/optimizers.py:706: TypeError"
        ]
    ],
    "2.1.3": null,
    "2.1.4": null,
    "2.1.5": [
        [
            {
                "loss": {
                    "value": "<tf.Tensor 'loss/mul:0' shape=() dtype=float32>",
                    "shape": "TensorShape([])",
                    "omitted": false
                },
                "params": {
                    "value": "[<tf.Variable 'dense_1/kernel:0' shape=(3, 2) dtype=float32_ref>, <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32_ref>]",
                    "shape": "2",
                    "omitted": false
                },
                "self.iterations": {
                    "value": "<tf.Variable 'TFOptimizer/iterations:0' shape=() dtype=int64_ref>",
                    "shape": "TensorShape([])",
                    "omitted": false
                }
            },
            {
                "grads": {
                    "value": "[(<tf.Tensor 'training/TFOptimizer/gradients/dense_1/MatMul_grad/tuple/control_dependency_1:0' shape=(3, 2) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(3, 2) dtype=float32_ref>), (<tf.Tensor 'training/TFOptimizer/gradients/dense_1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(2,) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32_ref>)]",
                    "shape": "2",
                    "omitted": false
                },
                "self.updates": {
                    "value": "[<tf.Tensor 'training/TFOptimizer/AssignAdd:0' shape=() dtype=int64_ref>, <tf.Operation 'training/TFOptimizer/Adam' type=AssignAdd>]",
                    "shape": "2",
                    "omitted": false
                },
                "opt_update": {
                    "value": "<tf.Operation 'training/TFOptimizer/Adam' type=AssignAdd>",
                    "shape": null,
                    "omitted": false
                }
            }
        ]
    ],
    "2.1.6": [
        [
            {
                "loss": "Tensor",
                "params": "list",
                "self.iterations": "RefVariable"
            },
            {
                "grads": "list",
                "self.updates": "list",
                "opt_update": "Operation"
            }
        ]
    ],
    "3.1.1": null,
    "3.1.2": null
}