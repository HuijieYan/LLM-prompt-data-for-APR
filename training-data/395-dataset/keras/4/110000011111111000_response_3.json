{
    "keras": [
        {
            "bugID": 4,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 1,
                "6": 0,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 1,
                "6": 0,
                "7": 0
            },
            "start_line": 704,
            "file_name": "keras/optimizers.py",
            "replace_code": "def get_updates(self, loss, params):\n\n    grads_and_vars = self.optimizer.compute_gradients(loss, var_list=params)\n    if isinstance(self.optimizer, tf.train.Optimizer):\n        self.updates = [K.update_add(self.iterations, 1).op]\n        for var, grad in grads_and_vars:\n            if grad is None:\n                continue\n            update_op = self.optimizer.apply_gradients([(grad, var)])\n            self.updates.append(update_op)\n    else:\n        # check out how to make updates for AdamOptimizer\n        raise ValueError('Do not know how to make update for Keras engine. '\n                         'Try other Optimizer in tensorflow.keras.optimizers')\n    return self.updates"
        }
    ]
}