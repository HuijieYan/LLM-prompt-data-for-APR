{
    "keras": [
        {
            "bugID": 4,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 1,
                "3": 0,
                "4": 0,
                "5": 0,
                "6": 0,
                "7": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 1,
                "3": 0,
                "4": 0,
                "5": 0,
                "6": 0,
                "7": 1
            },
            "start_line": 704,
            "file_name": "keras/optimizers.py",
            "replace_code": "def get_updates(self, loss, params):\n        if not hasattr(self.optimizer, 'compute_gradients') or not hasattr(self.optimizer, 'apply_gradients'):\n            raise AttributeError(\"Optimizer must have 'compute_gradients' and 'apply_gradients' methods\")\n        \n        grads = self.optimizer.compute_gradients(loss, params)\n        if grads is None:\n            grads = []\n\n        if self.updates is None:\n            self.updates = [K.update_add(self.iterations, 1)]\n        else:\n            self.updates.append(K.update_add(self.iterations, 1))\n\n        opt_update = self.optimizer.apply_gradients(grads, global_step=self.iterations)\n        if opt_update is not None:\n            self.updates.append(opt_update)\n\n        return self.updates",
            "import_list": []
        }
    ]
}