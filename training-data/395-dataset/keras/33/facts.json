{
    "1.1.1": "def text_to_word_sequence(text,\n                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                          lower=True, split=\" \"):\n    \n    if lower:\n        text = text.lower()\n\n    if sys.version_info < (3,) and isinstance(text, unicode):\n        translate_map = dict((ord(c), unicode(split)) for c in filters)\n    else:\n        translate_map = maketrans(filters, split * len(filters))\n\n    text = text.translate(translate_map)\n    seq = text.split(split)\n    return [i for i in seq if i]\n",
    "1.1.2": "Converts a text to a sequence of words (or tokens).\n\n# Arguments\n    text: Input text (string).\n    filters: Sequence of characters to filter out.\n    lower: Whether to convert the input to lowercase.\n    split: Sentence split marker (string).\n\n# Returns\n    A list of words (or tokens).",
    "1.2.1": null,
    "1.2.2": null,
    "1.2.3": null,
    "1.2.4": null,
    "1.2.5": null,
    "1.3.1": "keras/preprocessing/text.py",
    "1.3.2": null,
    "1.4.1": [
        "def test_text_to_word_sequence_multichar_split():\n    text = 'hello!stop?world!'\n    assert text_to_word_sequence(text, split='stop') == ['hello', 'world']",
        "def test_text_to_word_sequence_unicode_multichar_split():\n    text = u'ali!stopveli?stopk\u0131rkstopdokuzstopelli'\n    assert text_to_word_sequence(text, split='stop') == [u'ali', u'veli', u'k\u0131rk', u'dokuz', u'elli']"
    ],
    "1.4.2": [
        "tests/keras/preprocessing/text_test.py",
        "tests/keras/preprocessing/text_test.py"
    ],
    "2.1.1": [
        [
            "E           ValueError: the first two maketrans arguments must have equal length"
        ],
        [
            "E           ValueError: the first two maketrans arguments must have equal length"
        ]
    ],
    "2.1.2": [
        [
            "def test_text_to_word_sequence_multichar_split():\n        text = 'hello!stop?world!'\n>       assert text_to_word_sequence(text, split='stop') == ['hello', 'world']\n\ntests/keras/preprocessing/text_test.py:78: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ntext = 'hello!stop?world!', filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\nlower = True, split = 'stop'\n\n    def text_to_word_sequence(text,\n                              filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                              lower=True, split=\" \"):\n        \"\"\"Converts a text to a sequence of words (or tokens).\n    \n        # Arguments\n            text: Input text (string).\n            filters: Sequence of characters to filter out.\n            lower: Whether to convert the input to lowercase.\n            split: Sentence split marker (string).\n    \n        # Returns\n            A list of words (or tokens).\n        \"\"\"\n        if lower:\n            text = text.lower()\n    \n        if sys.version_info < (3,) and isinstance(text, unicode):\n            translate_map = dict((ord(c), unicode(split)) for c in filters)\n        else:\n>           translate_map = maketrans(filters, split * len(filters))",
            "\nkeras/preprocessing/text.py:44: ValueError"
        ],
        [
            "def test_text_to_word_sequence_unicode_multichar_split():\n        text = u'ali!stopveli?stopk\u0131rkstopdokuzstopelli'\n>       assert text_to_word_sequence(text, split='stop') == [u'ali', u'veli', u'k\u0131rk', u'dokuz', u'elli']\n\ntests/keras/preprocessing/text_test.py:88: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ntext = 'ali!stopveli?stopk\u0131rkstopdokuzstopelli'\nfilters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = 'stop'\n\n    def text_to_word_sequence(text,\n                              filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                              lower=True, split=\" \"):\n        \"\"\"Converts a text to a sequence of words (or tokens).\n    \n        # Arguments\n            text: Input text (string).\n            filters: Sequence of characters to filter out.\n            lower: Whether to convert the input to lowercase.\n            split: Sentence split marker (string).\n    \n        # Returns\n            A list of words (or tokens).\n        \"\"\"\n        if lower:\n            text = text.lower()\n    \n        if sys.version_info < (3,) and isinstance(text, unicode):\n            translate_map = dict((ord(c), unicode(split)) for c in filters)\n        else:\n>           translate_map = maketrans(filters, split * len(filters))",
            "\nkeras/preprocessing/text.py:44: ValueError"
        ]
    ],
    "2.1.3": null,
    "2.1.4": null,
    "2.1.5": [
        [
            {
                "lower": "True",
                "text": "'hello!stop?world!'",
                "split": "'stop'",
                "filters": "'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{"
            },
            {
                "text": "'hellostopstopstopworldstop'",
                "translate_map": "{33: 'stop', 34: 'stop', 35: 'stop', 36: 'stop', 37: 'stop', 38: 'stop', 40: 'stop', 41: 'stop', 42: 'stop', 43: 'stop', 44: 'stop', 45: 'stop', 46: 'stop', 47: 'stop', 58: 'stop', 59: 'stop', 60: 'stop', 61: 'stop', 62: 'stop', 63: 'stop', 64: 'stop', 91: 'stop', 92: 'stop', 93: 'stop', 94: 'stop', 95: 'stop', 96: 'stop', 123: 'stop', 124: 'stop', 125: 'stop', 126: 'stop', 9: 'stop', 10: 'stop'}",
                "translate_dict": "{'!': 'stop', '\"': 'stop', '#': 'stop', '$': 'stop', '%': 'stop', '&': 'stop', '(': 'stop', ')': 'stop', '*': 'stop', '+': 'stop', ',': 'stop', '-': 'stop', '.': 'stop', '/': 'stop', ':': 'stop', ';': 'stop', '<': 'stop', '=': 'stop', '>': 'stop', '?': 'stop', '@': 'stop', '[': 'stop', '\\\\': 'stop', ']': 'stop', '^': 'stop', '_': 'stop', '`': 'stop', '{': 'stop', '",
                "seq": "['hello', '', '', 'world', '']"
            }
        ],
        [
            {
                "lower": "True",
                "text": "'ali!stopveli?stopk\u0131rkstopdokuzstopelli'",
                "split": "'stop'",
                "filters": "'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{"
            },
            {
                "text": "'alistopstopvelistopstopk\u0131rkstopdokuzstopelli'",
                "translate_map": "{33: 'stop', 34: 'stop', 35: 'stop', 36: 'stop', 37: 'stop', 38: 'stop', 40: 'stop', 41: 'stop', 42: 'stop', 43: 'stop', 44: 'stop', 45: 'stop', 46: 'stop', 47: 'stop', 58: 'stop', 59: 'stop', 60: 'stop', 61: 'stop', 62: 'stop', 63: 'stop', 64: 'stop', 91: 'stop', 92: 'stop', 93: 'stop', 94: 'stop', 95: 'stop', 96: 'stop', 123: 'stop', 124: 'stop', 125: 'stop', 126: 'stop', 9: 'stop', 10: 'stop'}",
                "translate_dict": "{'!': 'stop', '\"': 'stop', '#': 'stop', '$': 'stop', '%': 'stop', '&': 'stop', '(': 'stop', ')': 'stop', '*': 'stop', '+': 'stop', ',': 'stop', '-': 'stop', '.': 'stop', '/': 'stop', ':': 'stop', ';': 'stop', '<': 'stop', '=': 'stop', '>': 'stop', '?': 'stop', '@': 'stop', '[': 'stop', '\\\\': 'stop', ']': 'stop', '^': 'stop', '_': 'stop', '`': 'stop', '{': 'stop', '",
                "seq": "['ali', '', 'veli', '', 'k\u0131rk', 'dokuz', 'elli']"
            }
        ]
    ],
    "2.1.6": [
        [
            {
                "lower": "bool",
                "text": "str",
                "split": "str",
                "filters": "str"
            },
            {
                "text": "str",
                "translate_map": "dict",
                "translate_dict": "dict",
                "seq": "list"
            }
        ],
        [
            {
                "lower": "bool",
                "text": "str",
                "split": "str",
                "filters": "str"
            },
            {
                "text": "str",
                "translate_map": "dict",
                "translate_dict": "dict",
                "seq": "list"
            }
        ]
    ],
    "3.1.1": [
        "Tokenization crashes when split string has more than one character\n"
    ],
    "3.1.2": [
        "`from keras.preprocessing.text import Tokenizer\n\ntexts = ['Just any text.']\nt = Tokenizer(split=\"any\")\nt.fit_on_texts(texts)\nprint(t.word_index)`\n\nthrows an exception:\nValueError: the first two maketrans arguments must have equal length\n"
    ]
}