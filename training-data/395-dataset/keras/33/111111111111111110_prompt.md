Please fix the function/method provided below and provide the corrected function/method as the output.


# Buggy function source code
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/keras_33/keras/preprocessing/text.py

# this is the buggy function you need to fix
def text_to_word_sequence(text,
                          filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
                          lower=True, split=" "):
    """Converts a text to a sequence of words (or tokens).

    # Arguments
        text: Input text (string).
        filters: Sequence of characters to filter out.
        lower: Whether to convert the input to lowercase.
        split: Sentence split marker (string).

    # Returns
        A list of words (or tokens).
    """
    if lower:
        text = text.lower()

    if sys.version_info < (3,) and isinstance(text, unicode):
        translate_map = dict((ord(c), unicode(split)) for c in filters)
    else:
        translate_map = maketrans(filters, split * len(filters))

    text = text.translate(translate_map)
    seq = text.split(split)
    return [i for i in seq if i]

```

# Variable runtime value and type inside buggy function
## Buggy case 1
### input parameter runtime value and type for buggy function
lower, value: `True`, type: `bool`

text, value: `'hello!stop?world!'`, type: `str`

split, value: `'stop'`, type: `str`

filters, value: `'!"#$%&()*+,-./:;<=>?@[\\]^_`{`, type: `str`

### variable runtime value and type before buggy function return
text, value: `'hellostopstopstopworldstop'`, type: `str`

translate_map, value: `{33: 'stop', 34: 'stop', 35: 'stop', 36: 'stop', 37: 'stop', 38: 'stop', 40: 'stop', 41: 'stop', 42: 'stop', 43: 'stop', 44: 'stop', 45: 'stop', 46: 'stop', 47: 'stop', 58: 'stop', 59: 'stop', 60: 'stop', 61: 'stop', 62: 'stop', 63: 'stop', 64: 'stop', 91: 'stop', 92: 'stop', 93: 'stop', 94: 'stop', 95: 'stop', 96: 'stop', 123: 'stop', 124: 'stop', 125: 'stop', 126: 'stop', 9: 'stop', 10: 'stop'}`, type: `dict`

translate_dict, value: `{'!': 'stop', '"': 'stop', '#': 'stop', '$': 'stop', '%': 'stop', '&': 'stop', '(': 'stop', ')': 'stop', '*': 'stop', '+': 'stop', ',': 'stop', '-': 'stop', '.': 'stop', '/': 'stop', ':': 'stop', ';': 'stop', '<': 'stop', '=': 'stop', '>': 'stop', '?': 'stop', '@': 'stop', '[': 'stop', '\\': 'stop', ']': 'stop', '^': 'stop', '_': 'stop', '`': 'stop', '{': 'stop', '`, type: `dict`

seq, value: `['hello', '', '', 'world', '']`, type: `list`

## Buggy case 2
### input parameter runtime value and type for buggy function
lower, value: `True`, type: `bool`

text, value: `'ali!stopveli?stopkırkstopdokuzstopelli'`, type: `str`

split, value: `'stop'`, type: `str`

filters, value: `'!"#$%&()*+,-./:;<=>?@[\\]^_`{`, type: `str`

### variable runtime value and type before buggy function return
text, value: `'alistopstopvelistopstopkırkstopdokuzstopelli'`, type: `str`

translate_map, value: `{33: 'stop', 34: 'stop', 35: 'stop', 36: 'stop', 37: 'stop', 38: 'stop', 40: 'stop', 41: 'stop', 42: 'stop', 43: 'stop', 44: 'stop', 45: 'stop', 46: 'stop', 47: 'stop', 58: 'stop', 59: 'stop', 60: 'stop', 61: 'stop', 62: 'stop', 63: 'stop', 64: 'stop', 91: 'stop', 92: 'stop', 93: 'stop', 94: 'stop', 95: 'stop', 96: 'stop', 123: 'stop', 124: 'stop', 125: 'stop', 126: 'stop', 9: 'stop', 10: 'stop'}`, type: `dict`

translate_dict, value: `{'!': 'stop', '"': 'stop', '#': 'stop', '$': 'stop', '%': 'stop', '&': 'stop', '(': 'stop', ')': 'stop', '*': 'stop', '+': 'stop', ',': 'stop', '-': 'stop', '.': 'stop', '/': 'stop', ':': 'stop', ';': 'stop', '<': 'stop', '=': 'stop', '>': 'stop', '?': 'stop', '@': 'stop', '[': 'stop', '\\': 'stop', ']': 'stop', '^': 'stop', '_': 'stop', '`': 'stop', '{': 'stop', '`, type: `dict`

seq, value: `['ali', '', 'veli', '', 'kırk', 'dokuz', 'elli']`, type: `list`



# A test function for the buggy function
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/keras_33/tests/keras/preprocessing/text_test.py

def test_text_to_word_sequence_multichar_split():
    text = 'hello!stop?world!'
    assert text_to_word_sequence(text, split='stop') == ['hello', 'world']
```

## Error message from test function
```text
def test_text_to_word_sequence_multichar_split():
        text = 'hello!stop?world!'
>       assert text_to_word_sequence(text, split='stop') == ['hello', 'world']

tests/keras/preprocessing/text_test.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

text = 'hello!stop?world!', filters = '!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'
lower = True, split = 'stop'

    def text_to_word_sequence(text,
                              filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
                              lower=True, split=" "):
        """Converts a text to a sequence of words (or tokens).
    
        # Arguments
            text: Input text (string).
            filters: Sequence of characters to filter out.
            lower: Whether to convert the input to lowercase.
            split: Sentence split marker (string).
    
        # Returns
            A list of words (or tokens).
        """
        if lower:
            text = text.lower()
    
        if sys.version_info < (3,) and isinstance(text, unicode):
            translate_map = dict((ord(c), unicode(split)) for c in filters)
        else:
>           translate_map = maketrans(filters, split * len(filters))
E           ValueError: the first two maketrans arguments must have equal length

keras/preprocessing/text.py:44: ValueError

```
# A test function for the buggy function
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/keras_33/tests/keras/preprocessing/text_test.py

def test_text_to_word_sequence_unicode_multichar_split():
    text = u'ali!stopveli?stopkırkstopdokuzstopelli'
    assert text_to_word_sequence(text, split='stop') == [u'ali', u'veli', u'kırk', u'dokuz', u'elli']
```

## Error message from test function
```text
def test_text_to_word_sequence_unicode_multichar_split():
        text = u'ali!stopveli?stopkırkstopdokuzstopelli'
>       assert text_to_word_sequence(text, split='stop') == [u'ali', u'veli', u'kırk', u'dokuz', u'elli']

tests/keras/preprocessing/text_test.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

text = 'ali!stopveli?stopkırkstopdokuzstopelli'
filters = '!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower = True, split = 'stop'

    def text_to_word_sequence(text,
                              filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
                              lower=True, split=" "):
        """Converts a text to a sequence of words (or tokens).
    
        # Arguments
            text: Input text (string).
            filters: Sequence of characters to filter out.
            lower: Whether to convert the input to lowercase.
            split: Sentence split marker (string).
    
        # Returns
            A list of words (or tokens).
        """
        if lower:
            text = text.lower()
    
        if sys.version_info < (3,) and isinstance(text, unicode):
            translate_map = dict((ord(c), unicode(split)) for c in filters)
        else:
>           translate_map = maketrans(filters, split * len(filters))
E           ValueError: the first two maketrans arguments must have equal length

keras/preprocessing/text.py:44: ValueError

```


# A GitHub issue title for this bug
```text
Tokenization crashes when split string has more than one character
```

## The associated detailed issue description
```text
`from keras.preprocessing.text import Tokenizer

texts = ['Just any text.']
t = Tokenizer(split="any")
t.fit_on_texts(texts)
print(t.word_index)`

throws an exception:
ValueError: the first two maketrans arguments must have equal length
```



