{
    "keras": [
        {
            "bugID": 45,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 1,
                "4": 1,
                "5": 0,
                "6": 1,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 1,
                "4": 1,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "start_line": 1786,
            "file_name": "keras/layers/recurrent.py",
            "replace_code": "def call(self, inputs, training=None):\n    # ... omitted code ...\n    # corrected implementation of the function\n    x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i\n    x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f\n    x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c\n    x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o\n\n    if 0 < self.recurrent_dropout < 1.:\n        h_tm1_i = h_tm1 * rec_dp_mask[0]\n        h_tm1_f = h_tm1 * rec_dp_mask[1]\n        h_tm1_c = h_tm1 * rec_dp_mask[2]\n        h_tm1_o = h_tm1 * rec_dp_mask[3]\n    else:\n        h_tm1_i = h_tm1\n        h_tm1_f = h_tm1\n        h_tm1_c = h_tm1\n        h_tm1_o = h_tm1\n    i = self.recurrent_activation(x_i + K.dot(h_tm1_i, self.recurrent_kernel_i))\n    f = self.recurrent_activation(x_f + K.dot(h_tm1_f, self.recurrent_kernel_f))\n    c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c, self.recurrent_kernel_c))\n    o = self.recurrent_activation(x_o + K.dot(h_tm1_o, self.recurrent_kernel_o))\n  \n    # ... omitted code ...\n    return h, [h, c]",
            "imports": []
        }
    ]
}