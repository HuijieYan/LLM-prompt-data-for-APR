{
    "keras": [
        {
            "bugID": 45,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 0,
                "6": 1,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "start_line": 1786,
            "file_name": "keras/layers/recurrent.py",
            "replace_code": "def call(self, inputs, states, training=None):\n    # dropout matrices for input units\n    dp_mask = self._dropout_mask\n    # dropout matrices for recurrent units\n    rec_dp_mask = self._recurrent_dropout_mask\n\n    h_tm1 = states[0]  # previous memory state\n    c_tm1 = states[1]  # previous carry state\n\n    if self.implementation == 1:\n        if 0 < self.dropout < 1.:\n            inputs_i = inputs * dp_mask[0]\n            inputs_f = inputs * dp_mask[1]\n            inputs_c = inputs * dp_mask[2]\n            inputs_o = inputs * dp_mask[3]\n        else:\n            inputs_i = inputs\n            inputs_f = inputs\n            inputs_c = inputs\n            inputs_o = inputs\n        x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i\n        x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f\n        x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c\n        x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o\n\n        if 0 < self.recurrent_dropout < 1.:\n            h_tm1_i = h_tm1 * rec_dp_mask[0]\n            h_tm1_f = h_tm1 * rec_dp_mask[1]\n            h_tm1_c = h_tm1 * rec_dp_mask[2]\n            h_tm1_o = h_tm1 * rec_dp_mask[3]\n        else:\n            h_tm1_i = h_tm1\n            h_tm1_f = h_tm1\n            h_tm1_c = h_tm1\n            h_tm1_o = h_tm1\n        i = self.recurrent_activation(x_i + K.dot(h_tm1_i,\n                                                  self.recurrent_kernel_i))\n        f = self.recurrent_activation(x_f + K.dot(h_tm1_f,\n                                                  self.recurrent_kernel_f))\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,\n                                                        self.recurrent_kernel_c))\n        o = self.recurrent_activation(x_o + K.dot(h_tm1_o,\n                                                  self.recurrent_kernel_o))\n    else:\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n        z = K.dot(inputs, self.kernel)\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1 *= rec_dp_mask[0]\n        z += K.dot(h_tm1, self.recurrent_kernel)\n        if self.use_bias:\n            z = K.bias_add(z, self.bias)\n\n        z0 = z[:, :self.units]\n        z1 = z[:, self.units: 2 * self.units]\n        z2 = z[:, 2 * self.units: 3 * self.units]\n        z3 = z[:, 3 * self.units:]\n\n        i = self.recurrent_activation(z0)\n        f = self.recurrent_activation(z1)\n        c = f * c_tm1 + i * self.activation(z2)\n        o = self.recurrent_activation(z3)\n\n    h = o * self.activation(c)\n    if (0 < self.dropout) and (0 < self.recurrent_dropout):\n        if training is None:\n            training = True\n        h._uses_learning_phase = True\n    return h, [h, c]",
            "imports": []
        }
    ]
}