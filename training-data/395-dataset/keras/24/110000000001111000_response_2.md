```python
# The corrected set_model function
def set_model(self, model):
    self.model = model
    if K.backend() == 'tensorflow':
        self.sess = K.get_session()
    if self.histogram_freq and self.merged is None:
        for layer in self.model.layers:
            for weight in layer.weights:
                mapped_weight_name = weight.name.replace(':', '_')
                if self.write_grads and model.optimizer:
                    grads = model.optimizer.get_gradients(model.total_loss, weight)
                    tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)
                if self.write_images:
                    w_img = tf.expand_dims(weight, 0)
                    shape = K.int_shape(w_img)
                    if len(shape) == 4 and shape[-1] in [1, 3, 4]:
                        tf.summary.image(mapped_weight_name, w_img)
            if hasattr(layer, 'output'):
                tf.summary.histogram('{}_out'.format(layer.name), layer.output)
    self.merged = tf.summary.merge_all()
    self.writer = tf.summary.FileWriter(self.log_dir, self.sess.graph) if self.write_graph else tf.summary.FileWriter(self.log_dir)
    if self.embeddings_freq:
        embeddings_layer_names = [layer.name for layer in self.model.layers if isinstance(layer, Embedding)]
        embeddings = {layer.name: layer.weights[0] for layer in self.model.layers if layer.name in embeddings_layer_names}
        self.saver = tf.train.Saver(list(embeddings.values()))
        embeddings_metadata = {layer_name: self.embeddings_metadata for layer_name in embeddings.keys()} if isinstance(self.embeddings_metadata, str) else self.embeddings_metadata
        config = projector.ProjectorConfig()
        self.embeddings_ckpt_path = os.path.join(self.log_dir, 'keras_embedding.ckpt')
        for layer_name, tensor in embeddings.items():
            embedding = config.embeddings.add()
            embedding.tensor_name = tensor.name
            embedding.metadata_path = embeddings_metadata.get(layer_name, '')
        projector.visualize_embeddings(self.writer, config)
```