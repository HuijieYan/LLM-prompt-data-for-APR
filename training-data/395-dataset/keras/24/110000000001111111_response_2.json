{
    "keras": [
        {
            "bugID": 24,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 0,
                "5": 1,
                "6": 1,
                "7": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 0,
                "5": 1,
                "6": 0,
                "7": 1
            },
            "start_line": 734,
            "file_name": "keras/callbacks.py",
            "replace_code": "def set_model(self, model):\n\n    self.model = model\n    if K.backend() == 'tensorflow':\n        self.sess = K.get_session()\n    if self.histogram_freq and self.merged is None:\n        for layer in self.model.layers:\n            for weight in layer.weights:\n                mapped_weight_name = weight.name.replace(':', '_')\n                tf.summary.histogram(mapped_weight_name, weight)\n                if self.write_grads:\n                    grads = K.gradients(model.total_loss, weight)[0]\n                    tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\n                if self.write_images:\n                    w_img = tf.expand_dims(weight, axis=-1)\n                    w_img = tf.squeeze(w_img)\n                    tf.summary.image(mapped_weight_name, w_img)\n    \n            if hasattr(layer, 'output'):\n                tf.summary.histogram('{}_out'.format(layer.name),\n                                     layer.output)\n    self.merged = tf.summary.merge_all()\n    \n    if self.write_graph:\n        self.writer = tf.summary.FileWriter(self.log_dir,\n                                            self.sess.graph)\n    else:\n        self.writer = tf.summary.FileWriter(self.log_dir)\n    \n    if self.embeddings_freq:\n        embeddings_layer_names = self.embeddings_layer_names\n    \n        if not embeddings_layer_names:\n            embeddings_layer_names = [layer.name for layer in self.model.layers\n                                      if type(layer).__name__ == 'Embedding']\n    \n        embeddings = {layer.name: layer.weights[0]\n                      for layer in self.model.layers\n                      if layer.name in embeddings_layer_names}\n    \n        self.saver = tf.train.Saver(list(embeddings.values()))\n    \n        embeddings_metadata = {}\n    \n        if not isinstance(self.embeddings_metadata, str):\n            embeddings_metadata = self.embeddings_metadata\n        else:\n            embeddings_metadata = {layer_name: self.embeddings_metadata\n                                   for layer_name in embeddings.keys()}\n    \n        config = projector.ProjectorConfig()\n        self.embeddings_ckpt_path = os.path.join(self.log_dir,\n                                                 'keras_embedding.ckpt')\n    \n        for layer_name, tensor in embeddings.items():\n            embedding = config.embeddings.add()\n            embedding.tensor_name = tensor.name\n    \n            if layer_name in embeddings_metadata:\n                embedding.metadata_path = embeddings_metadata[layer_name]\n    \n        projector.visualize_embeddings(self.writer, config)"
        }
    ]
}