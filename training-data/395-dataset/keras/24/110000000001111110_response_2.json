{
    "keras": [
        {
            "bugID": 24,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 0,
                "5": 1,
                "6": 1,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 0,
                "5": 1,
                "6": 0,
                "7": 0
            },
            "start_line": 734,
            "file_name": "keras/callbacks.py",
            "replace_code": "def set_model(self, model):\n\n    self.model = model\n    if K.backend() == 'tensorflow':\n        self.sess = K.get_session()\n    if self.histogram_freq and self.merged is None:\n        for layer in self.model.layers:\n            for weight in layer.weights:\n                mapped_weight_name = weight.name.replace(':', '_')\n                tf.summary.histogram(mapped_weight_name, weight)\n                if self.write_grads:\n                    grads = model.optimizer.get_gradients(model.total_loss,weight)\n    \n                    def is_indexed_slices(grad):\n                        return type(grad).__name__ == 'IndexedSlices'\n                    grads = [\n                        grad.values if is_indexed_slices(grad) else grad\n                        for grad in grads]\n                    tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\n                if self.write_images:\n                    w_img = tf.expand_dims(weight, axis=0)\n    \n                    if len(w_img.shape) == 3:\n                        w_img = tf.transpose(w_img, perm=[2, 0, 1])\n    \n                    tf.summary.image(mapped_weight_name, w_img)\n    \n            if hasattr(layer, 'output'):\n                tf.summary.histogram('{}_out'.format(layer.name), layer.output)\n    \n        self.merged = tf.summary.merge_all()\n    \n        if self.write_graph:\n            self.writer = tf.summary.FileWriter(self.log_dir, self.sess.graph)\n        else:\n            self.writer = tf.summary.FileWriter(self.log_dir)\n    \n        if self.embeddings_freq:\n            embeddings_layer_names = self.embeddings_layer_names\n    \n            if not embeddings_layer_names:\n                embeddings_layer_names = [layer.name for layer in self.model.layers if type(layer).__name__ == 'Embedding']\n    \n            embeddings = {layer.name: layer.weights[0] for layer in self.model.layers if layer.name in embeddings_layer_names}\n    \n            self.saver = tf.train.Saver(list(embeddings.values()))\n    \n            embeddings_metadata = {}\n    \n            if not isinstance(self.embeddings_metadata, str):\n                embeddings_metadata = self.embeddings_metadata\n            else:\n                embeddings_metadata = {layer_name: self.embeddings_metadata for layer_name in embeddings.keys()}\n    \n            config = projector.ProjectorConfig()\n            self.embeddings_ckpt_path = os.path.join(self.log_dir, 'keras_embedding.ckpt')\n    \n            for layer_name, tensor in embeddings.items():\n                embedding = config.embeddings.add()\n                embedding.tensor_name = tensor.name\n    \n                if layer_name in embeddings_metadata:\n                    embedding.metadata_path = embeddings_metadata[layer_name]\n    \n            projector.visualize_embeddings(self.writer, config)"
        }
    ]
}