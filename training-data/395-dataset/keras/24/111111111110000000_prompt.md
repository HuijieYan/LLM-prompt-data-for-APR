Please fix the function/method provided below and provide the corrected function/method as the output.


# Buggy function source code
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/keras_24/keras/callbacks.py

# relative function's signature in this file
def is_indexed_slices(grad):
    # ... omitted code ...
    pass

# class declaration containing the buggy function
class TensorBoard(Callback):
    """
    TensorBoard basic visualizations.
    
    [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)
    is a visualization tool provided with TensorFlow.
    
    This callback writes a log for TensorBoard, which allows
    you to visualize dynamic graphs of your training and test
    metrics, as well as activation histograms for the different
    layers in your model.
    
    If you have installed TensorFlow with pip, you should be able
    to launch TensorBoard from the command line:
    ```sh
    tensorboard --logdir=/full_path_to_your_logs
    ```
    
    When using a backend other than TensorFlow, TensorBoard will still work
    (if you have TensorFlow installed), but the only feature available will
    be the display of the losses and metrics plots.
    
    # Arguments
        log_dir: the path of the directory where to save the log
            files to be parsed by TensorBoard.
        histogram_freq: frequency (in epochs) at which to compute activation
            and weight histograms for the layers of the model. If set to 0,
            histograms won't be computed. Validation data (or split) must be
            specified for histogram visualizations.
        write_graph: whether to visualize the graph in TensorBoard.
            The log file can become quite large when
            write_graph is set to True.
        write_grads: whether to visualize gradient histograms in TensorBoard.
            `histogram_freq` must be greater than 0.
        batch_size: size of batch of inputs to feed to the network
            for histograms computation.
        write_images: whether to write model weights to visualize as
            image in TensorBoard.
        embeddings_freq: frequency (in epochs) at which selected embedding
            layers will be saved.
        embeddings_layer_names: a list of names of layers to keep eye on. If
            None or empty list all the embedding layer will be watched.
        embeddings_metadata: a dictionary which maps layer name to a file name
            in which metadata for this embedding layer is saved. See the
            [details](https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional)
            about metadata files format. In case if the same metadata file is
            used for all embedding layers, string can be passed.
    """

    # ... omitted code ...


    # signature of a relative function in this class
    def is_indexed_slices(grad):
        # ... omitted code ...
        pass



    # this is the buggy function you need to fix
    def set_model(self, model):
        self.model = model
        if K.backend() == 'tensorflow':
            self.sess = K.get_session()
        if self.histogram_freq and self.merged is None:
            for layer in self.model.layers:
    
                for weight in layer.weights:
                    mapped_weight_name = weight.name.replace(':', '_')
                    tf.summary.histogram(mapped_weight_name, weight)
                    if self.write_grads:
                        grads = model.optimizer.get_gradients(model.total_loss,
                                                              weight)
    
                        def is_indexed_slices(grad):
                            return type(grad).__name__ == 'IndexedSlices'
                        grads = [
                            grad.values if is_indexed_slices(grad) else grad
                            for grad in grads]
                        tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)
                    if self.write_images:
                        w_img = tf.squeeze(weight)
                        shape = K.int_shape(w_img)
                        if len(shape) == 2:  # dense layer kernel case
                            if shape[0] > shape[1]:
                                w_img = tf.transpose(w_img)
                                shape = K.int_shape(w_img)
                            w_img = tf.reshape(w_img, [1,
                                                       shape[0],
                                                       shape[1],
                                                       1])
                        elif len(shape) == 3:  # convnet case
                            if K.image_data_format() == 'channels_last':
                                # switch to channels_first to display
                                # every kernel as a separate image
                                w_img = tf.transpose(w_img, perm=[2, 0, 1])
                                shape = K.int_shape(w_img)
                            w_img = tf.reshape(w_img, [shape[0],
                                                       shape[1],
                                                       shape[2],
                                                       1])
                        elif len(shape) == 1:  # bias case
                            w_img = tf.reshape(w_img, [1,
                                                       shape[0],
                                                       1,
                                                       1])
                        else:
                            # not possible to handle 3D convnets etc.
                            continue
    
                        shape = K.int_shape(w_img)
                        assert len(shape) == 4 and shape[-1] in [1, 3, 4]
                        tf.summary.image(mapped_weight_name, w_img)
    
                if hasattr(layer, 'output'):
                    tf.summary.histogram('{}_out'.format(layer.name),
                                         layer.output)
        self.merged = tf.summary.merge_all()
    
        if self.write_graph:
            self.writer = tf.summary.FileWriter(self.log_dir,
                                                self.sess.graph)
        else:
            self.writer = tf.summary.FileWriter(self.log_dir)
    
        if self.embeddings_freq:
            embeddings_layer_names = self.embeddings_layer_names
    
            if not embeddings_layer_names:
                embeddings_layer_names = [layer.name for layer in self.model.layers
                                          if type(layer).__name__ == 'Embedding']
    
            embeddings = {layer.name: layer.weights[0]
                          for layer in self.model.layers
                          if layer.name in embeddings_layer_names}
    
            self.saver = tf.train.Saver(list(embeddings.values()))
    
            embeddings_metadata = {}
    
            if not isinstance(self.embeddings_metadata, str):
                embeddings_metadata = self.embeddings_metadata
            else:
                embeddings_metadata = {layer_name: self.embeddings_metadata
                                       for layer_name in embeddings.keys()}
    
            config = projector.ProjectorConfig()
            self.embeddings_ckpt_path = os.path.join(self.log_dir,
                                                     'keras_embedding.ckpt')
    
            for layer_name, tensor in embeddings.items():
                embedding = config.embeddings.add()
                embedding.tensor_name = tensor.name
    
                if layer_name in embeddings_metadata:
                    embedding.metadata_path = embeddings_metadata[layer_name]
    
            projector.visualize_embeddings(self.writer, config)
    
```

# A test function for the buggy function
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/keras_24/tests/keras/test_callbacks.py

@keras_test
def test_TensorBoard_multi_input_output(tmpdir):
    np.random.seed(np.random.randint(1, 1e7))
    filepath = str(tmpdir / 'logs')

    (X_train, y_train), (X_test, y_test) = get_test_data(
        num_train=train_samples,
        num_test=test_samples,
        input_shape=(input_dim, input_dim),
        classification=True,
        num_classes=num_classes)
    y_test = np_utils.to_categorical(y_test)
    y_train = np_utils.to_categorical(y_train)

    def data_generator(train):
        if train:
            max_batch_index = len(X_train) // batch_size
        else:
            max_batch_index = len(X_test) // batch_size
        i = 0
        while 1:
            if train:
                # simulate multi-input/output models
                yield ([X_train[i * batch_size: (i + 1) * batch_size]] * 2,
                       [y_train[i * batch_size: (i + 1) * batch_size]] * 2)
            else:
                yield ([X_test[i * batch_size: (i + 1) * batch_size]] * 2,
                       [y_test[i * batch_size: (i + 1) * batch_size]] * 2)
            i += 1
            i = i % max_batch_index

    inp1 = Input((input_dim, input_dim))
    inp2 = Input((input_dim, input_dim))
    inp_3d = add([inp1, inp2])
    inp_2d = GlobalAveragePooling1D()(inp_3d)
    inp_pair = Lambda(lambda x: x)([inp_3d, inp_2d])  # test a layer with a list of output tensors
    hidden = dot(inp_pair, axes=-1)
    hidden = Dense(num_hidden, activation='relu')(hidden)
    hidden = Dropout(0.1)(hidden)
    output1 = Dense(num_classes, activation='softmax')(hidden)
    output2 = Dense(num_classes, activation='softmax')(hidden)
    model = Model(inputs=[inp1, inp2], outputs=[output1, output2])
    model.compile(loss='categorical_crossentropy',
                  optimizer='sgd',
                  metrics=['accuracy'])

    # we must generate new callbacks for each test, as they aren't stateless
    def callbacks_factory(histogram_freq):
        return [callbacks.TensorBoard(log_dir=filepath,
                                      histogram_freq=histogram_freq,
                                      write_images=True, write_grads=True,
                                      embeddings_freq=1,
                                      embeddings_layer_names=['dense_1'],
                                      batch_size=5)]

    # fit without validation data
    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,
              callbacks=callbacks_factory(histogram_freq=0), epochs=3)

    # fit with validation data and accuracy
    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,
              validation_data=([X_test] * 2, [y_test] * 2),
              callbacks=callbacks_factory(histogram_freq=1), epochs=2)

    # fit generator without validation data
    model.fit_generator(data_generator(True), len(X_train), epochs=2,
                        callbacks=callbacks_factory(histogram_freq=0))

    # fit generator with validation data and accuracy
    model.fit_generator(data_generator(True), len(X_train), epochs=2,
                        validation_data=([X_test] * 2, [y_test] * 2),
                        callbacks=callbacks_factory(histogram_freq=1))

    assert os.path.isdir(filepath)
    shutil.rmtree(filepath)
    assert not tmpdir.listdir()
```

## Error message from test function
```text
graph = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
node_def = name: "lambda_1_out/values_1"
op: "Pack"
attr {
  key: "N"
  value {
    i: 2
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "axis"
  value {
    i: 0
  }
}

inputs = [[<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]]
control_inputs = []

    def _create_c_op(graph, node_def, inputs, control_inputs):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of
          `Tensor`s (corresponding to sequence inputs, e.g. "int64 * N",
          "list(int64)"). The length of the list should be equal to the number of
          inputs specified by this operation's op def.
        control_inputs: A list of `Operation`s to set as control dependencies.
    
      Returns:
        A wrapped TF_Operation*.
      """
      # pylint: disable=protected-access
      op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),
                                      compat.as_str(node_def.name))
      if node_def.device:
        c_api.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          c_api.TF_AddInputList(op_desc, [t._as_tf_output() for t in op_input])
        else:
          c_api.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        c_api.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        c_api.TF_SetAttrValueProto(op_desc, compat.as_str(name), serialized)
    
      try:
>       c_op = c_api.TF_FinishOperation(op_desc)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 3 and 2
E       	From merging shape 0 with other shapes. for 'lambda_1_out/values_1' (op: 'Pack') with input shapes: [?,2,2], [?,2].

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1864: InvalidArgumentError

During handling of the above exception, another exception occurred:

self = <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x107edac90>
op_type_name = 'HistogramSummary', name = 'lambda_1_out/', keywords = {}
op_info = <tensorflow.python.framework.op_def_library._OpInfo object at 0x107ee3210>
op_def = name: "HistogramSummary"
input_arg {
  name: "tag"
  type: DT_STRING
}
input_arg {
  name: "values"
  type_attr: "T"
}...   type: DT_BFLOAT16
      type: DT_UINT16
      type: DT_HALF
      type: DT_UINT32
      type: DT_UINT64
    }
  }
}

g = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
deprecation_version = 0, default_type_attr_map = {'T': tf.float32}

    def _apply_op_helper(self, op_type_name, name=None, **keywords):
      """Implementation of apply_op that returns output_structure, op."""
      op_info = self._ops.get(op_type_name, None)
      if op_info is None:
        raise RuntimeError("Unrecognized Op name " + op_type_name)
      op_def = op_info.op_def
    
      # Determine the graph context.
      try:
        # Need to flatten all the arguments into a list.
        # pylint: disable=protected-access
        g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
        # pylint: enable=protected-access
      except AssertionError as e:
        raise RuntimeError(
            "Cannot determine graph for Op '%s' due to: %s"
            % (op_type_name, e.message))
    
      # Default name if not specified.
      if name is None:
        name = op_type_name
    
      # Check for deprecation
      deprecation_version = op_def.deprecation.version
      if deprecation_version:
        producer = g.graph_def_versions.producer
        if producer >= deprecation_version:
          raise NotImplementedError(
              ("Op %s is not available in GraphDef version %d. "
               "It has been removed in version %d. %s.") %
              (op_type_name, producer, deprecation_version,
               op_def.deprecation.explanation))
    
      # Fill in the list of default types for all "type" attrs.  This
      # will be used to choose a preferred dtype to convert to in the
      # absence of input type information.
      #
      # TODO(b/31302892): Currently the defaults don't work in the right
      # way if you have two inputs, one of whose type resolution depends
      # on the other.  Handling this will require restructuring this code
      # significantly.
      default_type_attr_map = {}
      for attr_def in op_def.attr:
        if attr_def.type != "type":
          continue
        key = attr_def.name
        if attr_def.HasField("default_value"):
          default_type_attr_map[key] = dtypes.as_dtype(
              attr_def.default_value.type)
    
      # Requires that op_def has passed validation (using the C++
      # ValidateOpDef() from ../framework/op_def_util.h).
      attrs = {}
      inputs = []
      input_types = []
      with g.as_default(), ops.name_scope(name) as scope:
    
        # Perform input type inference
        inferred_from = {}
        for input_arg in op_def.input_arg:
          input_name = input_arg.name
          if input_name in keywords:
            values = keywords.pop(input_name)
          elif input_name + "_" in keywords:
            # Handle the case where the name is a keyword or built-in
            # for Python so we use the name + _ instead.
            input_name += "_"
            values = keywords.pop(input_name)
          else:
            raise TypeError("No argument for input " + input_name)
    
          # Goals:
          # * Convert values to Tensors if it contains constants.
          # * Verify that values is a list if that matches the input_arg's
          #   type.
          # * If the input_arg's type is determined by attrs, either set
          #   those attrs and validate those attr values are legal (if
          #   they have not yet been set) or validate the input matches
          #   the type indicated by the attrs (if they have already been
          #   inferred via an earlier input).
          # * If the input_arg has an explicit type, make sure the input
          #   conforms.
    
          if _IsListParameter(input_arg):
            if not _IsListValue(values):
              raise TypeError(
                  "Expected list for '%s' argument to '%s' Op, not %s." %
                  (input_name, op_type_name, values))
            # In cases where we expect all elements of the list to have the
            # same dtype, try to cast non-Tensor elements to that type.
            dtype = None
            default_dtype = None
            if input_arg.type != types_pb2.DT_INVALID:
              dtype = input_arg.type
            elif input_arg.number_attr:
              if input_arg.type_attr in attrs:
                dtype = attrs[input_arg.type_attr]
              else:
                for t in values:
                  if isinstance(t, ops.Tensor):
                    dtype = t.dtype
                    break
    
              # dtype still not found, prefer using the default dtype
              # from the attr.
              if dtype is None and input_arg.type_attr in default_type_attr_map:
                default_dtype = default_type_attr_map[input_arg.type_attr]
    
            try:
              if not input_arg.is_ref and dtype:
                dtype = dtypes.as_dtype(dtype).base_dtype
              values = ops.internal_convert_n_to_tensor(
                  values,
                  name=input_arg.name,
                  dtype=dtype if dtype else None,
                  preferred_dtype=default_dtype,
                  as_ref=input_arg.is_ref)
              if input_arg.number_attr and len(
                  set(v.dtype.base_dtype for v in values)) > 1:
                raise TypeError()  # All types should match.
            except (TypeError, ValueError):
              # What types does the conversion function think values have?
              observed_types = []
              for value in values:
                try:
                  converted_value = ops.internal_convert_to_tensor(
                      value, as_ref=input_arg.is_ref)
                  observed_types.append(converted_value.dtype.base_dtype.name)
                except (TypeError, ValueError):
                  observed_types.append("<NOT CONVERTIBLE TO TENSOR>")
              observed = ", ".join(observed_types)
    
              prefix = (
                  "Tensors in list passed to '%s' of '%s' Op have types [%s]" %
                  (input_name, op_type_name, observed))
              if input_arg.number_attr:
                if input_arg.type != types_pb2.DT_INVALID:
                  raise TypeError("%s that do not match expected type %s." %
                                  (prefix, dtype.name))
                elif input_arg.type_attr in attrs:
                  raise TypeError("%s that do not match type %s inferred from "
                                  "earlier arguments." %
                                  (prefix, dtype.name))
                else:
                  raise TypeError("%s that don't all match." % prefix)
              else:
                raise TypeError(
                    "%s that are invalid. Tensors: %s" % (prefix, values))
    
            types = [x.dtype for x in values]
            inputs.extend(values)
          else:
            # In cases where we have an expected type, try to convert non-Tensor
            # arguments to that type.
            dtype = None
            default_dtype = None
            if input_arg.type != types_pb2.DT_INVALID:
              dtype = input_arg.type
            elif input_arg.type_attr in attrs:
              dtype = attrs[input_arg.type_attr]
            elif input_arg.type_attr in default_type_attr_map:
              # The dtype could not be inferred solely from the inputs,
              # so we prefer the attr's default, so code that adds a new attr
              # with a default is backwards compatible.
              default_dtype = default_type_attr_map[input_arg.type_attr]
    
            try:
              values = ops.internal_convert_to_tensor(
                  values,
                  name=input_arg.name,
                  dtype=dtype,
                  as_ref=input_arg.is_ref,
>                 preferred_dtype=default_dtype)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:527: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

value = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
dtype = None, name = 'values', as_ref = False, preferred_dtype = tf.float32
ctx = <tensorflow.python.eager.context.Context object at 0x1201bc610>
accept_symbolic_tensors = True, accept_composite_tensors = False

    def internal_convert_to_tensor(value,
                                   dtype=None,
                                   name=None,
                                   as_ref=False,
                                   preferred_dtype=None,
                                   ctx=None,
                                   accept_symbolic_tensors=True,
                                   accept_composite_tensors=False):
      """Implementation of the public convert_to_tensor."""
      if ctx is None:
        ctx = context.context()
      if isinstance(value, EagerTensor):
        if ctx.executing_eagerly():
          if dtype is not None:
            dtype = dtypes.as_dtype(dtype)
            value = _TensorTensorConversionFunction(value, dtype=dtype)
          return value
        else:
          graph = get_default_graph()
          if not graph.building_function:
            raise RuntimeError("Attempting to capture an EagerTensor without "
                               "building a function.")
          return graph.capture(value, name=name)
      elif ((not accept_symbolic_tensors) and isinstance(value, Tensor) and
            ctx.executing_eagerly()):
        # Found a symbolic tensor in an eager context.
        # This happens when we use the Keras functional API (i.e. calling layers
        # on the output of `keras.Input()`, which is symbolic) while eager
        # execution is enabled.
        if _is_keras_symbolic_tensor(value):
          # If the graph of the tensor isn't the Keras graph, we should still
          # fail, for the time being. TODO(fchollet): consider allowing
          # all symbolic tensors to raise this exception in this case.
          raise core._SymbolicException(  # pylint: disable=protected-access
              "Using the symbolic output of a Keras layer during eager execution.")
    
      if dtype is not None:
        dtype = dtypes.as_dtype(dtype)
      unwrapped_type = type(value)
      conversion_func_list = _tensor_conversion_func_cache.get(unwrapped_type, None)
      if conversion_func_list is None:
        with _tensor_conversion_func_lock:
          conversion_func_list = []
          for _, funcs_at_priority in sorted(
              _tensor_conversion_func_registry.items()):
            for base_type, conversion_func in funcs_at_priority:
              if isinstance(value, base_type):
                conversion_func_list.append((base_type, conversion_func))
          _tensor_conversion_func_cache[unwrapped_type] = conversion_func_list
    
      for base_type, conversion_func in conversion_func_list:
        # If dtype is None but preferred_dtype is not None, we try to
        # cast to preferred_dtype first.
        ret = None
        if dtype is None and preferred_dtype is not None:
          try:
            ret = conversion_func(
                value, dtype=preferred_dtype, name=name, as_ref=as_ref)
          except (TypeError, ValueError, errors.UnimplementedError,
                  errors.InvalidArgumentError):
            # Could not coerce the conversion to use the preferred dtype.
            ret = None
    
          if ret is not None and ret is not NotImplemented:
            if (ret.dtype.base_dtype !=
                dtypes.as_dtype(preferred_dtype).base_dtype):
              raise TypeError("convert_to_tensor did not convert to "
                              "the preferred dtype: %s vs %s " %
                              (ret.dtype.base_dtype,
                               dtypes.as_dtype(preferred_dtype).base_dtype))
    
        if ret is None:
>         ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

v = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
dtype = tf.float32, name = 'values', as_ref = False

    def _autopacking_conversion_function(v, dtype=None, name=None, as_ref=False):
      """Tensor conversion function that automatically packs arguments."""
      if as_ref:
        return NotImplemented
      inferred_dtype = _get_dtype_from_nested_lists(v)
      if inferred_dtype is None:
        # We did not find any tensor-like objects in the nested lists, so defer to
        # other conversion functions.
        return NotImplemented
      if dtype is None:
        dtype = inferred_dtype
      elif dtype != inferred_dtype:
        v = nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)
>     return _autopacking_helper(v, dtype, name or "packed")

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1145: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

list_or_tuple = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
dtype = tf.float32, name = 'values'

    def _autopacking_helper(list_or_tuple, dtype, name):
      """Converts the given list or tuple to a tensor by packing.
    
      Args:
        list_or_tuple: A (possibly nested) list or tuple containing a tensor.
        dtype: The element type of the returned tensor.
        name: A name for the returned tensor.
    
      Returns:
        A `tf.Tensor` with value equivalent to `list_or_tuple`.
      """
      if context.executing_eagerly():
        # NOTE: Fast path when all the items are tensors, this doesn't do any type
        # checking.
        if all(ops.is_dense_tensor_like(elem) for elem in list_or_tuple):
          return gen_array_ops.pack(list_or_tuple, name=name)
      must_pack = False
      converted_elems = []
      with ops.name_scope(name) as scope:
        for i, elem in enumerate(list_or_tuple):
          if ops.is_dense_tensor_like(elem):
            if dtype is not None and elem.dtype.base_dtype != dtype:
              raise TypeError("Cannot convert a list containing a tensor of dtype "
                              "%s to %s (Tensor is: %r)" %
                              (elem.dtype, dtype, elem))
            converted_elems.append(elem)
            must_pack = True
          elif isinstance(elem, (list, tuple)):
            converted_elem = _autopacking_helper(elem, dtype, str(i))
            if ops.is_dense_tensor_like(converted_elem):
              must_pack = True
            converted_elems.append(converted_elem)
          else:
            converted_elems.append(elem)
        if must_pack:
          elems_as_tensors = []
          for i, elem in enumerate(converted_elems):
            if ops.is_dense_tensor_like(elem):
              elems_as_tensors.append(elem)
            else:
              # NOTE(mrry): This is inefficient, but it enables us to
              # handle the case where the list arguments are other
              # convertible-to-tensor types, such as numpy arrays.
              elems_as_tensors.append(
                  constant_op.constant(elem, dtype=dtype, name=str(i)))
>         return gen_array_ops.pack(elems_as_tensors, name=scope)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1095: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

values = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
axis = 0, name = 'lambda_1_out/values_1/'

    def pack(values, axis=0, name=None):
      r"""Packs a list of `N` rank-`R` tensors into one rank-`(R+1)` tensor.
    
      Packs the `N` tensors in `values` into a tensor with rank one higher than each
      tensor in `values`, by packing them along the `axis` dimension.
      Given a list of tensors of shape `(A, B, C)`;
    
      if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.
      if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.
      Etc.
    
      For example:
    
      ```
      # 'x' is [1, 4]
      # 'y' is [2, 5]
      # 'z' is [3, 6]
      pack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.
      pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]
      ```
    
      This is the opposite of `unpack`.
    
      Args:
        values: A list of at least 1 `Tensor` objects with the same type.
          Must be of same shape and type.
        axis: An optional `int`. Defaults to `0`.
          Dimension along which to pack.  Negative values wrap around, so the
          valid range is `[-(R+1), R+1)`.
        name: A name for the operation (optional).
    
      Returns:
        A `Tensor`. Has the same type as `values`.
      """
      _ctx = _context._context or _context.context()
      if _ctx is not None and _ctx._thread_local_data.is_eager:
        try:
          _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(
            _ctx._context_handle, _ctx._thread_local_data.device_name, "Pack",
            name, _ctx._post_execution_callbacks, values, "axis", axis)
          return _result
        except _core._FallbackException:
          try:
            return pack_eager_fallback(
                values, axis=axis, name=name, ctx=_ctx)
          except _core._SymbolicException:
            pass  # Add nodes to the TensorFlow graph.
        except _core._NotOkStatusException as e:
          if name is not None:
            message = e.message + " name: " + name
          else:
            message = e.message
          _six.raise_from(_core._status_to_exception(e.code, message), None)
      # Add nodes to the TensorFlow graph.
      if not isinstance(values, (list, tuple)):
        raise TypeError(
            "Expected list for 'values' argument to "
            "'pack' Op, not %r." % values)
      _attr_N = len(values)
      if axis is None:
        axis = 0
      axis = _execute.make_int(axis, "axis")
      _, _, _op = _op_def_lib._apply_op_helper(
>           "Pack", values=values, axis=axis, name=name)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:5897: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x107aa6090>
op_type_name = 'Pack', name = 'lambda_1_out/values_1/', keywords = {}
op_info = <tensorflow.python.framework.op_def_library._OpInfo object at 0x107aace50>
op_def = name: "Pack"
input_arg {
  name: "values"
  type_attr: "T"
  number_attr: "N"
}
output_arg {
  name: "output"
  type_a... minimum: 1
}
attr {
  name: "T"
  type: "type"
}
attr {
  name: "axis"
  type: "int"
  default_value {
    i: 0
  }
}

g = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
deprecation_version = 0, default_type_attr_map = {}

    def _apply_op_helper(self, op_type_name, name=None, **keywords):
      """Implementation of apply_op that returns output_structure, op."""
      op_info = self._ops.get(op_type_name, None)
      if op_info is None:
        raise RuntimeError("Unrecognized Op name " + op_type_name)
      op_def = op_info.op_def
    
      # Determine the graph context.
      try:
        # Need to flatten all the arguments into a list.
        # pylint: disable=protected-access
        g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
        # pylint: enable=protected-access
      except AssertionError as e:
        raise RuntimeError(
            "Cannot determine graph for Op '%s' due to: %s"
            % (op_type_name, e.message))
    
      # Default name if not specified.
      if name is None:
        name = op_type_name
    
      # Check for deprecation
      deprecation_version = op_def.deprecation.version
      if deprecation_version:
        producer = g.graph_def_versions.producer
        if producer >= deprecation_version:
          raise NotImplementedError(
              ("Op %s is not available in GraphDef version %d. "
               "It has been removed in version %d. %s.") %
              (op_type_name, producer, deprecation_version,
               op_def.deprecation.explanation))
    
      # Fill in the list of default types for all "type" attrs.  This
      # will be used to choose a preferred dtype to convert to in the
      # absence of input type information.
      #
      # TODO(b/31302892): Currently the defaults don't work in the right
      # way if you have two inputs, one of whose type resolution depends
      # on the other.  Handling this will require restructuring this code
      # significantly.
      default_type_attr_map = {}
      for attr_def in op_def.attr:
        if attr_def.type != "type":
          continue
        key = attr_def.name
        if attr_def.HasField("default_value"):
          default_type_attr_map[key] = dtypes.as_dtype(
              attr_def.default_value.type)
    
      # Requires that op_def has passed validation (using the C++
      # ValidateOpDef() from ../framework/op_def_util.h).
      attrs = {}
      inputs = []
      input_types = []
      with g.as_default(), ops.name_scope(name) as scope:
    
        # Perform input type inference
        inferred_from = {}
        for input_arg in op_def.input_arg:
          input_name = input_arg.name
          if input_name in keywords:
            values = keywords.pop(input_name)
          elif input_name + "_" in keywords:
            # Handle the case where the name is a keyword or built-in
            # for Python so we use the name + _ instead.
            input_name += "_"
            values = keywords.pop(input_name)
          else:
            raise TypeError("No argument for input " + input_name)
    
          # Goals:
          # * Convert values to Tensors if it contains constants.
          # * Verify that values is a list if that matches the input_arg's
          #   type.
          # * If the input_arg's type is determined by attrs, either set
          #   those attrs and validate those attr values are legal (if
          #   they have not yet been set) or validate the input matches
          #   the type indicated by the attrs (if they have already been
          #   inferred via an earlier input).
          # * If the input_arg has an explicit type, make sure the input
          #   conforms.
    
          if _IsListParameter(input_arg):
            if not _IsListValue(values):
              raise TypeError(
                  "Expected list for '%s' argument to '%s' Op, not %s." %
                  (input_name, op_type_name, values))
            # In cases where we expect all elements of the list to have the
            # same dtype, try to cast non-Tensor elements to that type.
            dtype = None
            default_dtype = None
            if input_arg.type != types_pb2.DT_INVALID:
              dtype = input_arg.type
            elif input_arg.number_attr:
              if input_arg.type_attr in attrs:
                dtype = attrs[input_arg.type_attr]
              else:
                for t in values:
                  if isinstance(t, ops.Tensor):
                    dtype = t.dtype
                    break
    
              # dtype still not found, prefer using the default dtype
              # from the attr.
              if dtype is None and input_arg.type_attr in default_type_attr_map:
                default_dtype = default_type_attr_map[input_arg.type_attr]
    
            try:
              if not input_arg.is_ref and dtype:
                dtype = dtypes.as_dtype(dtype).base_dtype
              values = ops.internal_convert_n_to_tensor(
                  values,
                  name=input_arg.name,
                  dtype=dtype if dtype else None,
                  preferred_dtype=default_dtype,
                  as_ref=input_arg.is_ref)
              if input_arg.number_attr and len(
                  set(v.dtype.base_dtype for v in values)) > 1:
                raise TypeError()  # All types should match.
            except (TypeError, ValueError):
              # What types does the conversion function think values have?
              observed_types = []
              for value in values:
                try:
                  converted_value = ops.internal_convert_to_tensor(
                      value, as_ref=input_arg.is_ref)
                  observed_types.append(converted_value.dtype.base_dtype.name)
                except (TypeError, ValueError):
                  observed_types.append("<NOT CONVERTIBLE TO TENSOR>")
              observed = ", ".join(observed_types)
    
              prefix = (
                  "Tensors in list passed to '%s' of '%s' Op have types [%s]" %
                  (input_name, op_type_name, observed))
              if input_arg.number_attr:
                if input_arg.type != types_pb2.DT_INVALID:
                  raise TypeError("%s that do not match expected type %s." %
                                  (prefix, dtype.name))
                elif input_arg.type_attr in attrs:
                  raise TypeError("%s that do not match type %s inferred from "
                                  "earlier arguments." %
                                  (prefix, dtype.name))
                else:
                  raise TypeError("%s that don't all match." % prefix)
              else:
                raise TypeError(
                    "%s that are invalid. Tensors: %s" % (prefix, values))
    
            types = [x.dtype for x in values]
            inputs.extend(values)
          else:
            # In cases where we have an expected type, try to convert non-Tensor
            # arguments to that type.
            dtype = None
            default_dtype = None
            if input_arg.type != types_pb2.DT_INVALID:
              dtype = input_arg.type
            elif input_arg.type_attr in attrs:
              dtype = attrs[input_arg.type_attr]
            elif input_arg.type_attr in default_type_attr_map:
              # The dtype could not be inferred solely from the inputs,
              # so we prefer the attr's default, so code that adds a new attr
              # with a default is backwards compatible.
              default_dtype = default_type_attr_map[input_arg.type_attr]
    
            try:
              values = ops.internal_convert_to_tensor(
                  values,
                  name=input_arg.name,
                  dtype=dtype,
                  as_ref=input_arg.is_ref,
                  preferred_dtype=default_dtype)
            except TypeError as err:
              if dtype is None:
                raise err
              else:
                raise TypeError(
                    "Expected %s passed to parameter '%s' of op '%s', got %s of "
                    "type '%s' instead. Error: %s" %
                    (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,
                     repr(values), type(values).__name__, err))
            except ValueError:
              # What type does convert_to_tensor think it has?
              try:
                observed = ops.internal_convert_to_tensor(
                    values, as_ref=input_arg.is_ref).dtype.name
              except ValueError as err:
                raise ValueError(
                    "Tried to convert '%s' to a tensor and failed. Error: %s" %
                    (input_name, err))
              prefix = ("Input '%s' of '%s' Op has type %s that does not match" %
                        (input_name, op_type_name, observed))
              if input_arg.type != types_pb2.DT_INVALID:
                raise TypeError("%s expected type of %s." %
                                (prefix, dtypes.as_dtype(input_arg.type).name))
              else:
                # Update the maps with the default, if needed.
                k = input_arg.type_attr
                if k in default_type_attr_map:
                  if k not in attrs:
                    attrs[k] = default_type_attr_map[k]
                    if k not in inferred_from:
                      inferred_from[k] = "Default in OpDef"
    
                raise TypeError(
                    "%s type %s of argument '%s'." %
                    (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
                     inferred_from[input_arg.type_attr]))
    
            types = [values.dtype]
            inputs.append(values)
          base_types = [x.base_dtype for x in types]
    
          if input_arg.number_attr:
            # <number-attr> * <type> or <number-attr> * <type-attr>
            if input_arg.number_attr in attrs:
              if len(values) != attrs[input_arg.number_attr]:
                raise ValueError(
                    "List argument '%s' to '%s' Op with length %d must match "
                    "length %d of argument '%s'." %
                    (input_name, op_type_name, len(values),
                     attrs[input_arg.number_attr],
                     inferred_from[input_arg.number_attr]))
            else:
              attrs[input_arg.number_attr] = len(values)
              inferred_from[input_arg.number_attr] = input_name
              num_attr = _Attr(op_def, input_arg.number_attr)
              if num_attr.has_minimum and len(values) < num_attr.minimum:
                raise ValueError(
                    "List argument '%s' to '%s' Op with length %d shorter "
                    "than minimum length %d." %
                    (input_name, op_type_name, len(values), num_attr.minimum))
            # All tensors must have the same base type.
            if any(bt != base_types[0] for bt in base_types):
              raise TypeError(
                  "All tensors passed to '%s' of '%s' Op "
                  "must have the same type." %
                  (input_name, op_type_name))
            if input_arg.type != types_pb2.DT_INVALID:
              # <number-attr> * <type> case
              if base_types and base_types[0] != input_arg.type:
                assert False, "Unreachable"
            elif input_arg.type_attr in attrs:
              # <number-attr> * <type-attr> case, where <type-attr> already
              # has an inferred value.
              if base_types and base_types[0] != attrs[input_arg.type_attr]:
                assert False, "Unreachable"
            else:
              # <number-attr> * <type-attr> case, where we are now setting
              # the <type-attr> based on this input
              if not base_types:
                raise TypeError(
                    "Don't know how to infer type variable from empty input "
                    "list passed to input '%s' of '%s' Op." %
                    (input_name, op_type_name))
              attrs[input_arg.type_attr] = base_types[0]
              inferred_from[input_arg.type_attr] = input_name
              type_attr = _Attr(op_def, input_arg.type_attr)
              _SatisfiesTypeConstraint(base_types[0], type_attr,
                                       param_name=input_name)
          elif input_arg.type_attr:
            # <type-attr>
            attr_value = base_types[0]
            if input_arg.type_attr in attrs:
              if attrs[input_arg.type_attr] != attr_value:
                assert False, "Unreachable"
            else:
              for base_type in base_types:
                _SatisfiesTypeConstraint(base_type,
                                         _Attr(op_def, input_arg.type_attr),
                                         param_name=input_name)
              attrs[input_arg.type_attr] = attr_value
              inferred_from[input_arg.type_attr] = input_name
          elif input_arg.type_list_attr:
            # <type-list-attr>
            attr_value = base_types
            if input_arg.type_list_attr in attrs:
              if attrs[input_arg.type_list_attr] != attr_value:
                raise TypeError(
                    "Input '%s' of '%s' Op has type list of %s that does not "
                    "match type list %s of argument '%s'." %
                    (input_name, op_type_name,
                     ", ".join(dtypes.as_dtype(x).name for x in attr_value),
                     ", ".join(dtypes.as_dtype(x).name
                               for x in attrs[input_arg.type_list_attr]),
                     inferred_from[input_arg.type_list_attr]))
            else:
              for base_type in base_types:
                _SatisfiesTypeConstraint(base_type,
                                         _Attr(op_def, input_arg.type_list_attr),
                                         param_name=input_name)
              attrs[input_arg.type_list_attr] = attr_value
              inferred_from[input_arg.type_list_attr] = input_name
          else:
            # single Tensor with specified type
            if base_types[0] != input_arg.type:
              assert False, "Unreachable"
    
          if input_arg.is_ref:
            if not all(x._is_ref_dtype for x in types):  # pylint: disable=protected-access
              raise TypeError(
                  ("'%s' Op requires that input '%s' be a mutable tensor "
                   "(e.g.: a tf.Variable)") % (op_type_name, input_name))
            input_types.extend(types)
          else:
            input_types.extend(base_types)
    
        # Process remaining attrs
        for attr in op_def.attr:
          # Skip attrs that have already had their values inferred
          if attr.name in attrs:
            if attr.name in keywords:
              raise TypeError(
                  "Should not specify value for inferred attr '%s'." % attr.name)
            continue
          if attr.name in keywords:
            attrs[attr.name] = keywords.pop(attr.name)
          elif attr.name + "_" in keywords:
            # Attrs whose names match Python keywords have an extra '_'
            # appended, so we must check for that as well.
            attrs[attr.name] = keywords.pop(attr.name + "_")
          else:
            raise TypeError("No argument for attr " + attr.name)
    
        # Convert attr values to AttrValue protos.
        attr_protos = {}
        for attr_def in op_def.attr:
          key = attr_def.name
          value = attrs[key]
          attr_value = attr_value_pb2.AttrValue()
          if attr_def.HasField("default_value") and value is None:
            attr_value.CopyFrom(attr_def.default_value)
            attr_protos[key] = attr_value
            continue
          if attr_def.type.startswith("list("):
            if not _IsListValue(value):
              raise TypeError("Expected list for attr " + key)
            if attr_def.has_minimum:
              if len(value) < attr_def.minimum:
                raise ValueError("Attr '%s' of '%s' Op passed list of length %d "
                                 "less than minimum %d." %
                                 (key, op_type_name, len(value),
                                  attr_def.minimum))
            attr_value.list.SetInParent()
          if attr_def.type == "string":
            attr_value.s = _MakeStr(value, key)
            if attr_def.HasField("allowed_values"):
              if attr_value.s not in attr_def.allowed_values.list.s:
                raise ValueError(
                    "Attr '%s' of '%s' Op passed string '%s' not in: \"%s\"." %
                    (key, op_type_name, compat.as_text(attr_value.s),
                     '", "'.join(map(compat.as_text,
                                     attr_def.allowed_values.list.s))))
          elif attr_def.type == "list(string)":
            attr_value.list.s.extend([_MakeStr(x, key) for x in value])
            if attr_def.HasField("allowed_values"):
              for x in attr_value.list.s:
                if x not in attr_def.allowed_values.list.s:
                  raise ValueError(
                      "Attr '%s' of '%s' Op passed string '%s' not in: \"%s\"." %
                      (key, op_type_name, compat.as_text(x),
                       '", "'.join(map(compat.as_text,
                                       attr_def.allowed_values.list.s))))
          elif attr_def.type == "int":
            attr_value.i = _MakeInt(value, key)
            if attr_def.has_minimum:
              if attr_value.i < attr_def.minimum:
                raise ValueError(
                    "Attr '%s' of '%s' Op passed %d less than minimum %d." %
                    (key, op_type_name, attr_value.i, attr_def.minimum))
          elif attr_def.type == "list(int)":
            attr_value.list.i.extend([_MakeInt(x, key) for x in value])
          elif attr_def.type == "float":
            attr_value.f = _MakeFloat(value, key)
          elif attr_def.type == "list(float)":
            attr_value.list.f.extend([_MakeFloat(x, key) for x in value])
          elif attr_def.type == "bool":
            attr_value.b = _MakeBool(value, key)
          elif attr_def.type == "list(bool)":
            attr_value.list.b.extend([_MakeBool(x, key) for x in value])
          elif attr_def.type == "type":
            attr_value.type = _MakeType(value, attr_def)
          elif attr_def.type == "list(type)":
            attr_value.list.type.extend(
                [_MakeType(x, attr_def) for x in value])
          elif attr_def.type == "shape":
            attr_value.shape.CopyFrom(_MakeShape(value, key))
          elif attr_def.type == "list(shape)":
            attr_value.list.shape.extend(
                [_MakeShape(x, key) for x in value])
          elif attr_def.type == "tensor":
            attr_value.tensor.CopyFrom(_MakeTensor(value, key))
          elif attr_def.type == "list(tensor)":
            attr_value.list.tensor.extend(
                [_MakeTensor(x, key) for x in value])
          elif attr_def.type == "func":
            attr_value.func.CopyFrom(_MakeFunc(value, key))
          elif attr_def.type == "list(func)":
            attr_value.list.func.extend([_MakeFunc(x, key) for x in value])
          else:
            raise TypeError("Unrecognized Attr type " + attr_def.type)
    
          attr_protos[key] = attr_value
        del attrs  # attrs is no longer authoritative, use attr_protos instead
    
        # Determine output types (possibly using attrs)
        output_structure = []
        for arg in op_def.output_arg:
          if arg.number_attr:
            n = _AttrValue(attr_protos, arg.number_attr).i
            output_structure.append(n)
          elif arg.type_attr:
            t = _AttrValue(attr_protos, arg.type_attr)
            output_structure.append(None)
          elif arg.type_list_attr:
            t = _AttrValue(attr_protos, arg.type_list_attr)
            output_structure.append(len(t.list.type))
          else:
            output_structure.append(None)
    
        if keywords:
          raise TypeError("apply_op() got unexpected keyword arguments: " +
                          ", ".join(sorted(keywords.keys())))
    
        # NOTE(mrry): We add an explicit colocation constraint between
        # the newly created op and any of its reference-typed inputs.
        must_colocate_inputs = [val for arg, val in zip(op_def.input_arg, inputs)
                                if arg.is_ref]
        with _MaybeColocateWith(must_colocate_inputs):
          # Add Op to graph
          op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,
                           input_types=input_types, attrs=attr_protos,
>                          op_def=op_def)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:788: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tensorflow.python.framework.ops.Graph object at 0x1201bc390>, 'Pack', [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>])
kwargs = {'attrs': {'N': i: 2
, 'T': type: DT_FLOAT
, 'axis': i: 0
}, 'dtypes': None, 'input_types': [tf.float32, tf.float32], 'name': 'lambda_1_out/values_1/', ...}
invalid_args = []
named_args = {'attrs': {'N': i: 2
, 'T': type: DT_FLOAT
, 'axis': i: 0
}, 'compute_device': True, 'compute_shapes': True, 'dtypes': None, ...}
arg_name = 'compute_shapes'
spec = DeprecatedArgSpec(position=8, has_ok_value=False, ok_value=None)

    @functools.wraps(func)
    def new_func(*args, **kwargs):
      """Deprecation wrapper."""
      # TODO(apassos) figure out a way to have reasonable performance with
      # deprecation warnings and eager mode.
      if is_in_graph_mode.IS_IN_GRAPH_MODE() and _PRINT_DEPRECATION_WARNINGS:
        invalid_args = []
        named_args = tf_inspect.getcallargs(func, *args, **kwargs)
        for arg_name, spec in iter(deprecated_positions.items()):
          if (spec.position < len(args) and
              not (spec.has_ok_value and
                   _same_value(named_args[arg_name], spec.ok_value))):
            invalid_args.append(arg_name)
        if is_varargs_deprecated and len(args) > len(arg_spec.args):
          invalid_args.append(arg_spec.varargs)
        if is_kwargs_deprecated and kwargs:
          invalid_args.append(arg_spec.varkw)
        for arg_name in deprecated_arg_names:
          if (arg_name in kwargs and
              not (deprecated_positions[arg_name].has_ok_value and
                   _same_value(named_args[arg_name],
                               deprecated_positions[arg_name].ok_value))):
            invalid_args.append(arg_name)
        for arg_name in invalid_args:
          if (func, arg_name) not in _PRINTED_WARNING:
            if warn_once:
              _PRINTED_WARNING[(func, arg_name)] = True
            logging.warning(
                'From %s: calling %s (from %s) with %s is deprecated and will '
                'be removed %s.\nInstructions for updating:\n%s',
                _call_location(), decorator_utils.get_qualified_name(func),
                func.__module__, arg_name,
                'in a future version' if date is None else ('after %s' % date),
                instructions)
>     return func(*args, **kwargs)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:507: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
op_type = 'Pack'
inputs = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
dtypes = None, input_types = [tf.float32, tf.float32]
name = 'lambda_1_out/values_1'
attrs = {'N': i: 2
, 'T': type: DT_FLOAT
, 'axis': i: 0
}
op_def = name: "Pack"
input_arg {
  name: "values"
  type_attr: "T"
  number_attr: "N"
}
output_arg {
  name: "output"
  type_a... minimum: 1
}
attr {
  name: "T"
  type: "type"
}
attr {
  name: "axis"
  type: "int"
  default_value {
    i: 0
  }
}

compute_device = True

    @deprecated_args(None,
                     "Shapes are always computed; don't use the compute_shapes "
                     "as it has no effect.", "compute_shapes")
    def create_op(
        self,
        op_type,
        inputs,
        dtypes=None,  # pylint: disable=redefined-outer-name
        input_types=None,
        name=None,
        attrs=None,
        op_def=None,
        compute_shapes=True,
        compute_device=True):
      """Creates an `Operation` in this graph.
    
      This is a low-level interface for creating an `Operation`. Most
      programs will not call this method directly, and instead use the
      Python op constructors, such as `tf.constant()`, which add ops to
      the default graph.
    
      Args:
        op_type: The `Operation` type to create. This corresponds to the
          `OpDef.name` field for the proto that defines the operation.
        inputs: A list of `Tensor` objects that will be inputs to the `Operation`.
        dtypes: (Optional) A list of `DType` objects that will be the types of the
          tensors that the operation produces.
        input_types: (Optional.) A list of `DType`s that will be the types of the
          tensors that the operation consumes. By default, uses the base `DType`
          of each input in `inputs`. Operations that expect reference-typed inputs
          must specify `input_types` explicitly.
        name: (Optional.) A string name for the operation. If not specified, a
          name is generated based on `op_type`.
        attrs: (Optional.) A dictionary where the key is the attribute name (a
          string) and the value is the respective `attr` attribute of the
          `NodeDef` proto that will represent the operation (an `AttrValue`
          proto).
        op_def: (Optional.) The `OpDef` proto that describes the `op_type` that
          the operation will have.
        compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always
          computed).
        compute_device: (Optional.) If True, device functions will be executed to
          compute the device property of the Operation.
    
      Raises:
        TypeError: if any of the inputs is not a `Tensor`.
        ValueError: if colocation conflicts with existing device assignment.
    
      Returns:
        An `Operation` object.
      """
      del compute_shapes
    
      self._check_not_finalized()
      for idx, a in enumerate(inputs):
        if not isinstance(a, Tensor):
          raise TypeError("Input #%d is not a tensor: %s" % (idx, a))
      if name is None:
        name = op_type
      # If a names ends with a '/' it is a "name scope" and we use it as-is,
      # after removing the trailing '/'.
      if name and name[-1] == "/":
        name = name_from_scope_name(name)
      else:
        name = self.unique_name(name)
    
      node_def = _NodeDef(op_type, name, device=None, attrs=attrs)
    
      input_ops = set([t.op for t in inputs])
      control_inputs = self._control_dependencies_for_inputs(input_ops)
      # _create_op_helper mutates the new Operation. `_mutation_lock` ensures a
      # Session.run call cannot occur between creating and mutating the op.
      with self._mutation_lock():
        ret = Operation(
            node_def,
            self,
            inputs=inputs,
            output_types=dtypes,
            control_inputs=control_inputs,
            input_types=input_types,
            original_op=self._default_original_op,
>           op_def=op_def)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3616: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'Operation' object has no attribute '_c_op'") raised in repr()] Operation object at 0x1346c3410>
node_def = name: "lambda_1_out/values_1"
op: "Pack"
attr {
  key: "N"
  value {
    i: 2
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "axis"
  value {
    i: 0
  }
}

g = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
inputs = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
output_types = None, control_inputs = [], input_types = [tf.float32, tf.float32]
original_op = None
op_def = name: "Pack"
input_arg {
  name: "values"
  type_attr: "T"
  number_attr: "N"
}
output_arg {
  name: "output"
  type_a... minimum: 1
}
attr {
  name: "T"
  type: "type"
}
attr {
  name: "axis"
  type: "int"
  default_value {
    i: 0
  }
}


    def __init__(self,
                 node_def,
                 g,
                 inputs=None,
                 output_types=None,
                 control_inputs=None,
                 input_types=None,
                 original_op=None,
                 op_def=None):
      r"""Creates an `Operation`.
    
      NOTE: This constructor validates the name of the `Operation` (passed
      as `node_def.name`). Valid `Operation` names match the following
      regular expression:
    
          [A-Za-z0-9.][A-Za-z0-9_.\\-/]*
    
      Args:
        node_def: `node_def_pb2.NodeDef`.  `NodeDef` for the `Operation`. Used for
          attributes of `node_def_pb2.NodeDef`, typically `name`, `op`, and
          `device`.  The `input` attribute is irrelevant here as it will be
          computed when generating the model.
        g: `Graph`. The parent graph.
        inputs: list of `Tensor` objects. The inputs to this `Operation`.
        output_types: list of `DType` objects.  List of the types of the `Tensors`
          computed by this operation.  The length of this list indicates the
          number of output endpoints of the `Operation`.
        control_inputs: list of operations or tensors from which to have a control
          dependency.
        input_types: List of `DType` objects representing the types of the tensors
          accepted by the `Operation`.  By default uses `[x.dtype.base_dtype for x
          in inputs]`.  Operations that expect reference-typed inputs must specify
          these explicitly.
        original_op: Optional. Used to associate the new `Operation` with an
          existing `Operation` (for example, a replica with the op that was
          replicated).
        op_def: Optional. The `op_def_pb2.OpDef` proto that describes the op type
          that this `Operation` represents.
    
      Raises:
        TypeError: if control inputs are not Operations or Tensors,
          or if `node_def` is not a `NodeDef`,
          or if `g` is not a `Graph`,
          or if `inputs` are not tensors,
          or if `inputs` and `input_types` are incompatible.
        ValueError: if the `node_def` name is not valid.
      """
      # For internal use only: `node_def` can be set to a TF_Operation to create
      # an Operation for that op. This is useful for creating Operations for ops
      # indirectly created by C API methods, e.g. the ops created by
      # TF_ImportGraphDef. When `node_def` is a TF_Operation, all optional fields
      # should be None.
    
      if isinstance(node_def, node_def_pb2.NodeDef):
        if node_def.ByteSize() >= (1 << 31) or node_def.ByteSize() < 0:
          raise ValueError(
              "Cannot create a tensor proto whose content is larger than 2GB.")
        if not _VALID_OP_NAME_REGEX.match(node_def.name):
          raise ValueError("'%s' is not a valid node name" % node_def.name)
        c_op = None
      elif type(node_def).__name__ == "SwigPyObject":
        assert inputs is None
        assert output_types is None
        assert control_inputs is None
        assert input_types is None
        assert original_op is None
        assert op_def is None
        c_op = node_def
      else:
        raise TypeError("node_def needs to be a NodeDef: %s" % node_def)
    
      if not isinstance(g, Graph):
        raise TypeError("g needs to be a Graph: %s" % g)
      self._graph = g
    
      if inputs is None:
        inputs = []
      elif not isinstance(inputs, list):
        raise TypeError("inputs needs to be a list of Tensors: %s" % inputs)
      for a in inputs:
        if not isinstance(a, Tensor):
          raise TypeError("input needs to be a Tensor: %s" % a)
      if input_types is None:
        input_types = [i.dtype.base_dtype for i in inputs]
      else:
        if not all(
            x.is_compatible_with(i.dtype) for i, x in zip(inputs, input_types)):
          raise TypeError("In op '%s', input types (%s) are not compatible "
                          "with expected types (%s)" %
                          (node_def.name, [i.dtype for i in inputs], input_types))
    
      # Build the list of control inputs.
      control_input_ops = []
      if control_inputs:
        for c in control_inputs:
          control_op = None
          if isinstance(c, Operation):
            control_op = c
          elif isinstance(c, (Tensor, IndexedSlices)):
            control_op = c.op
          else:
            raise TypeError("Control input must be an Operation, "
                            "a Tensor, or IndexedSlices: %s" % c)
          control_input_ops.append(control_op)
    
      # This will be set by self.inputs.
      self._inputs_val = None
    
      # pylint: disable=protected-access
      self._id_value = self._graph._next_id()
      self._original_op = original_op
      self._traceback = tf_stack.extract_stack()
    
      # List of _UserDevSpecs holding code location of device context manager
      # invocations and the users original argument to them.
      self._device_code_locations = None
      # Dict mapping op name to file and line information for op colocation
      # context managers.
      self._colocation_code_locations = None
      self._control_flow_context = self.graph._get_control_flow_context()
      # pylint: enable=protected-access
    
      # Initialize self._c_op.
      if c_op:
        self._c_op = c_op
      else:
        if op_def is None:
          op_def = self._graph._get_op_def(node_def.op)
        # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
        # Refactor so we don't have to do this here.
        grouped_inputs = self._reconstruct_sequence_inputs(
            op_def, inputs, node_def.attr)
        self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
>                                 control_input_ops)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2027: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
node_def = name: "lambda_1_out/values_1"
op: "Pack"
attr {
  key: "N"
  value {
    i: 2
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "axis"
  value {
    i: 0
  }
}

inputs = [[<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]]
control_inputs = []

    def _create_c_op(graph, node_def, inputs, control_inputs):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of
          `Tensor`s (corresponding to sequence inputs, e.g. "int64 * N",
          "list(int64)"). The length of the list should be equal to the number of
          inputs specified by this operation's op def.
        control_inputs: A list of `Operation`s to set as control dependencies.
    
      Returns:
        A wrapped TF_Operation*.
      """
      # pylint: disable=protected-access
      op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),
                                      compat.as_str(node_def.name))
      if node_def.device:
        c_api.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          c_api.TF_AddInputList(op_desc, [t._as_tf_output() for t in op_input])
        else:
          c_api.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        c_api.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        c_api.TF_SetAttrValueProto(op_desc, compat.as_str(name), serialized)
    
      try:
        c_op = c_api.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
>       raise ValueError(str(e))
E       ValueError: Shapes must be equal rank, but are 3 and 2
E       	From merging shape 0 with other shapes. for 'lambda_1_out/values_1' (op: 'Pack') with input shapes: [?,2,2], [?,2].

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1867: ValueError

During handling of the above exception, another exception occurred:

graph = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
node_def = name: "lambda_1_out/packed"
op: "Pack"
attr {
  key: "N"
  value {
    i: 2
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "axis"
  value {
    i: 0
  }
}

inputs = [[<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]]
control_inputs = []

    def _create_c_op(graph, node_def, inputs, control_inputs):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of
          `Tensor`s (corresponding to sequence inputs, e.g. "int64 * N",
          "list(int64)"). The length of the list should be equal to the number of
          inputs specified by this operation's op def.
        control_inputs: A list of `Operation`s to set as control dependencies.
    
      Returns:
        A wrapped TF_Operation*.
      """
      # pylint: disable=protected-access
      op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),
                                      compat.as_str(node_def.name))
      if node_def.device:
        c_api.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          c_api.TF_AddInputList(op_desc, [t._as_tf_output() for t in op_input])
        else:
          c_api.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        c_api.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        c_api.TF_SetAttrValueProto(op_desc, compat.as_str(name), serialized)
    
      try:
>       c_op = c_api.TF_FinishOperation(op_desc)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 3 and 2
E       	From merging shape 0 with other shapes. for 'lambda_1_out/packed' (op: 'Pack') with input shapes: [?,2,2], [?,2].

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1864: InvalidArgumentError

During handling of the above exception, another exception occurred:

self = <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x107edac90>
op_type_name = 'HistogramSummary', name = 'lambda_1_out/', keywords = {}
op_info = <tensorflow.python.framework.op_def_library._OpInfo object at 0x107ee3210>
op_def = name: "HistogramSummary"
input_arg {
  name: "tag"
  type: DT_STRING
}
input_arg {
  name: "values"
  type_attr: "T"
}...   type: DT_BFLOAT16
      type: DT_UINT16
      type: DT_HALF
      type: DT_UINT32
      type: DT_UINT64
    }
  }
}

g = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
deprecation_version = 0, default_type_attr_map = {'T': tf.float32}

    def _apply_op_helper(self, op_type_name, name=None, **keywords):
      """Implementation of apply_op that returns output_structure, op."""
      op_info = self._ops.get(op_type_name, None)
      if op_info is None:
        raise RuntimeError("Unrecognized Op name " + op_type_name)
      op_def = op_info.op_def
    
      # Determine the graph context.
      try:
        # Need to flatten all the arguments into a list.
        # pylint: disable=protected-access
        g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
        # pylint: enable=protected-access
      except AssertionError as e:
        raise RuntimeError(
            "Cannot determine graph for Op '%s' due to: %s"
            % (op_type_name, e.message))
    
      # Default name if not specified.
      if name is None:
        name = op_type_name
    
      # Check for deprecation
      deprecation_version = op_def.deprecation.version
      if deprecation_version:
        producer = g.graph_def_versions.producer
        if producer >= deprecation_version:
          raise NotImplementedError(
              ("Op %s is not available in GraphDef version %d. "
               "It has been removed in version %d. %s.") %
              (op_type_name, producer, deprecation_version,
               op_def.deprecation.explanation))
    
      # Fill in the list of default types for all "type" attrs.  This
      # will be used to choose a preferred dtype to convert to in the
      # absence of input type information.
      #
      # TODO(b/31302892): Currently the defaults don't work in the right
      # way if you have two inputs, one of whose type resolution depends
      # on the other.  Handling this will require restructuring this code
      # significantly.
      default_type_attr_map = {}
      for attr_def in op_def.attr:
        if attr_def.type != "type":
          continue
        key = attr_def.name
        if attr_def.HasField("default_value"):
          default_type_attr_map[key] = dtypes.as_dtype(
              attr_def.default_value.type)
    
      # Requires that op_def has passed validation (using the C++
      # ValidateOpDef() from ../framework/op_def_util.h).
      attrs = {}
      inputs = []
      input_types = []
      with g.as_default(), ops.name_scope(name) as scope:
    
        # Perform input type inference
        inferred_from = {}
        for input_arg in op_def.input_arg:
          input_name = input_arg.name
          if input_name in keywords:
            values = keywords.pop(input_name)
          elif input_name + "_" in keywords:
            # Handle the case where the name is a keyword or built-in
            # for Python so we use the name + _ instead.
            input_name += "_"
            values = keywords.pop(input_name)
          else:
            raise TypeError("No argument for input " + input_name)
    
          # Goals:
          # * Convert values to Tensors if it contains constants.
          # * Verify that values is a list if that matches the input_arg's
          #   type.
          # * If the input_arg's type is determined by attrs, either set
          #   those attrs and validate those attr values are legal (if
          #   they have not yet been set) or validate the input matches
          #   the type indicated by the attrs (if they have already been
          #   inferred via an earlier input).
          # * If the input_arg has an explicit type, make sure the input
          #   conforms.
    
          if _IsListParameter(input_arg):
            if not _IsListValue(values):
              raise TypeError(
                  "Expected list for '%s' argument to '%s' Op, not %s." %
                  (input_name, op_type_name, values))
            # In cases where we expect all elements of the list to have the
            # same dtype, try to cast non-Tensor elements to that type.
            dtype = None
            default_dtype = None
            if input_arg.type != types_pb2.DT_INVALID:
              dtype = input_arg.type
            elif input_arg.number_attr:
              if input_arg.type_attr in attrs:
                dtype = attrs[input_arg.type_attr]
              else:
                for t in values:
                  if isinstance(t, ops.Tensor):
                    dtype = t.dtype
                    break
    
              # dtype still not found, prefer using the default dtype
              # from the attr.
              if dtype is None and input_arg.type_attr in default_type_attr_map:
                default_dtype = default_type_attr_map[input_arg.type_attr]
    
            try:
              if not input_arg.is_ref and dtype:
                dtype = dtypes.as_dtype(dtype).base_dtype
              values = ops.internal_convert_n_to_tensor(
                  values,
                  name=input_arg.name,
                  dtype=dtype if dtype else None,
                  preferred_dtype=default_dtype,
                  as_ref=input_arg.is_ref)
              if input_arg.number_attr and len(
                  set(v.dtype.base_dtype for v in values)) > 1:
                raise TypeError()  # All types should match.
            except (TypeError, ValueError):
              # What types does the conversion function think values have?
              observed_types = []
              for value in values:
                try:
                  converted_value = ops.internal_convert_to_tensor(
                      value, as_ref=input_arg.is_ref)
                  observed_types.append(converted_value.dtype.base_dtype.name)
                except (TypeError, ValueError):
                  observed_types.append("<NOT CONVERTIBLE TO TENSOR>")
              observed = ", ".join(observed_types)
    
              prefix = (
                  "Tensors in list passed to '%s' of '%s' Op have types [%s]" %
                  (input_name, op_type_name, observed))
              if input_arg.number_attr:
                if input_arg.type != types_pb2.DT_INVALID:
                  raise TypeError("%s that do not match expected type %s." %
                                  (prefix, dtype.name))
                elif input_arg.type_attr in attrs:
                  raise TypeError("%s that do not match type %s inferred from "
                                  "earlier arguments." %
                                  (prefix, dtype.name))
                else:
                  raise TypeError("%s that don't all match." % prefix)
              else:
                raise TypeError(
                    "%s that are invalid. Tensors: %s" % (prefix, values))
    
            types = [x.dtype for x in values]
            inputs.extend(values)
          else:
            # In cases where we have an expected type, try to convert non-Tensor
            # arguments to that type.
            dtype = None
            default_dtype = None
            if input_arg.type != types_pb2.DT_INVALID:
              dtype = input_arg.type
            elif input_arg.type_attr in attrs:
              dtype = attrs[input_arg.type_attr]
            elif input_arg.type_attr in default_type_attr_map:
              # The dtype could not be inferred solely from the inputs,
              # so we prefer the attr's default, so code that adds a new attr
              # with a default is backwards compatible.
              default_dtype = default_type_attr_map[input_arg.type_attr]
    
            try:
              values = ops.internal_convert_to_tensor(
                  values,
                  name=input_arg.name,
                  dtype=dtype,
                  as_ref=input_arg.is_ref,
                  preferred_dtype=default_dtype)
            except TypeError as err:
              if dtype is None:
                raise err
              else:
                raise TypeError(
                    "Expected %s passed to parameter '%s' of op '%s', got %s of "
                    "type '%s' instead. Error: %s" %
                    (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,
                     repr(values), type(values).__name__, err))
            except ValueError:
              # What type does convert_to_tensor think it has?
              try:
                observed = ops.internal_convert_to_tensor(
>                   values, as_ref=input_arg.is_ref).dtype.name

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:541: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

value = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
dtype = None, name = None, as_ref = False, preferred_dtype = None
ctx = <tensorflow.python.eager.context.Context object at 0x1201bc610>
accept_symbolic_tensors = True, accept_composite_tensors = False

    def internal_convert_to_tensor(value,
                                   dtype=None,
                                   name=None,
                                   as_ref=False,
                                   preferred_dtype=None,
                                   ctx=None,
                                   accept_symbolic_tensors=True,
                                   accept_composite_tensors=False):
      """Implementation of the public convert_to_tensor."""
      if ctx is None:
        ctx = context.context()
      if isinstance(value, EagerTensor):
        if ctx.executing_eagerly():
          if dtype is not None:
            dtype = dtypes.as_dtype(dtype)
            value = _TensorTensorConversionFunction(value, dtype=dtype)
          return value
        else:
          graph = get_default_graph()
          if not graph.building_function:
            raise RuntimeError("Attempting to capture an EagerTensor without "
                               "building a function.")
          return graph.capture(value, name=name)
      elif ((not accept_symbolic_tensors) and isinstance(value, Tensor) and
            ctx.executing_eagerly()):
        # Found a symbolic tensor in an eager context.
        # This happens when we use the Keras functional API (i.e. calling layers
        # on the output of `keras.Input()`, which is symbolic) while eager
        # execution is enabled.
        if _is_keras_symbolic_tensor(value):
          # If the graph of the tensor isn't the Keras graph, we should still
          # fail, for the time being. TODO(fchollet): consider allowing
          # all symbolic tensors to raise this exception in this case.
          raise core._SymbolicException(  # pylint: disable=protected-access
              "Using the symbolic output of a Keras layer during eager execution.")
    
      if dtype is not None:
        dtype = dtypes.as_dtype(dtype)
      unwrapped_type = type(value)
      conversion_func_list = _tensor_conversion_func_cache.get(unwrapped_type, None)
      if conversion_func_list is None:
        with _tensor_conversion_func_lock:
          conversion_func_list = []
          for _, funcs_at_priority in sorted(
              _tensor_conversion_func_registry.items()):
            for base_type, conversion_func in funcs_at_priority:
              if isinstance(value, base_type):
                conversion_func_list.append((base_type, conversion_func))
          _tensor_conversion_func_cache[unwrapped_type] = conversion_func_list
    
      for base_type, conversion_func in conversion_func_list:
        # If dtype is None but preferred_dtype is not None, we try to
        # cast to preferred_dtype first.
        ret = None
        if dtype is None and preferred_dtype is not None:
          try:
            ret = conversion_func(
                value, dtype=preferred_dtype, name=name, as_ref=as_ref)
          except (TypeError, ValueError, errors.UnimplementedError,
                  errors.InvalidArgumentError):
            # Could not coerce the conversion to use the preferred dtype.
            ret = None
    
          if ret is not None and ret is not NotImplemented:
            if (ret.dtype.base_dtype !=
                dtypes.as_dtype(preferred_dtype).base_dtype):
              raise TypeError("convert_to_tensor did not convert to "
                              "the preferred dtype: %s vs %s " %
                              (ret.dtype.base_dtype,
                               dtypes.as_dtype(preferred_dtype).base_dtype))
    
        if ret is None:
>         ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

v = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
dtype = tf.float32, name = None, as_ref = False

    def _autopacking_conversion_function(v, dtype=None, name=None, as_ref=False):
      """Tensor conversion function that automatically packs arguments."""
      if as_ref:
        return NotImplemented
      inferred_dtype = _get_dtype_from_nested_lists(v)
      if inferred_dtype is None:
        # We did not find any tensor-like objects in the nested lists, so defer to
        # other conversion functions.
        return NotImplemented
      if dtype is None:
        dtype = inferred_dtype
      elif dtype != inferred_dtype:
        v = nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)
>     return _autopacking_helper(v, dtype, name or "packed")

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1145: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

list_or_tuple = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
dtype = tf.float32, name = 'packed'

    def _autopacking_helper(list_or_tuple, dtype, name):
      """Converts the given list or tuple to a tensor by packing.
    
      Args:
        list_or_tuple: A (possibly nested) list or tuple containing a tensor.
        dtype: The element type of the returned tensor.
        name: A name for the returned tensor.
    
      Returns:
        A `tf.Tensor` with value equivalent to `list_or_tuple`.
      """
      if context.executing_eagerly():
        # NOTE: Fast path when all the items are tensors, this doesn't do any type
        # checking.
        if all(ops.is_dense_tensor_like(elem) for elem in list_or_tuple):
          return gen_array_ops.pack(list_or_tuple, name=name)
      must_pack = False
      converted_elems = []
      with ops.name_scope(name) as scope:
        for i, elem in enumerate(list_or_tuple):
          if ops.is_dense_tensor_like(elem):
            if dtype is not None and elem.dtype.base_dtype != dtype:
              raise TypeError("Cannot convert a list containing a tensor of dtype "
                              "%s to %s (Tensor is: %r)" %
                              (elem.dtype, dtype, elem))
            converted_elems.append(elem)
            must_pack = True
          elif isinstance(elem, (list, tuple)):
            converted_elem = _autopacking_helper(elem, dtype, str(i))
            if ops.is_dense_tensor_like(converted_elem):
              must_pack = True
            converted_elems.append(converted_elem)
          else:
            converted_elems.append(elem)
        if must_pack:
          elems_as_tensors = []
          for i, elem in enumerate(converted_elems):
            if ops.is_dense_tensor_like(elem):
              elems_as_tensors.append(elem)
            else:
              # NOTE(mrry): This is inefficient, but it enables us to
              # handle the case where the list arguments are other
              # convertible-to-tensor types, such as numpy arrays.
              elems_as_tensors.append(
                  constant_op.constant(elem, dtype=dtype, name=str(i)))
>         return gen_array_ops.pack(elems_as_tensors, name=scope)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1095: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

values = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
axis = 0, name = 'lambda_1_out/packed/'

    def pack(values, axis=0, name=None):
      r"""Packs a list of `N` rank-`R` tensors into one rank-`(R+1)` tensor.
    
      Packs the `N` tensors in `values` into a tensor with rank one higher than each
      tensor in `values`, by packing them along the `axis` dimension.
      Given a list of tensors of shape `(A, B, C)`;
    
      if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.
      if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.
      Etc.
    
      For example:
    
      ```
      # 'x' is [1, 4]
      # 'y' is [2, 5]
      # 'z' is [3, 6]
      pack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.
      pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]
      ```
    
      This is the opposite of `unpack`.
    
      Args:
        values: A list of at least 1 `Tensor` objects with the same type.
          Must be of same shape and type.
        axis: An optional `int`. Defaults to `0`.
          Dimension along which to pack.  Negative values wrap around, so the
          valid range is `[-(R+1), R+1)`.
        name: A name for the operation (optional).
    
      Returns:
        A `Tensor`. Has the same type as `values`.
      """
      _ctx = _context._context or _context.context()
      if _ctx is not None and _ctx._thread_local_data.is_eager:
        try:
          _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(
            _ctx._context_handle, _ctx._thread_local_data.device_name, "Pack",
            name, _ctx._post_execution_callbacks, values, "axis", axis)
          return _result
        except _core._FallbackException:
          try:
            return pack_eager_fallback(
                values, axis=axis, name=name, ctx=_ctx)
          except _core._SymbolicException:
            pass  # Add nodes to the TensorFlow graph.
        except _core._NotOkStatusException as e:
          if name is not None:
            message = e.message + " name: " + name
          else:
            message = e.message
          _six.raise_from(_core._status_to_exception(e.code, message), None)
      # Add nodes to the TensorFlow graph.
      if not isinstance(values, (list, tuple)):
        raise TypeError(
            "Expected list for 'values' argument to "
            "'pack' Op, not %r." % values)
      _attr_N = len(values)
      if axis is None:
        axis = 0
      axis = _execute.make_int(axis, "axis")
      _, _, _op = _op_def_lib._apply_op_helper(
>           "Pack", values=values, axis=axis, name=name)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:5897: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x107aa6090>
op_type_name = 'Pack', name = 'lambda_1_out/packed/', keywords = {}
op_info = <tensorflow.python.framework.op_def_library._OpInfo object at 0x107aace50>
op_def = name: "Pack"
input_arg {
  name: "values"
  type_attr: "T"
  number_attr: "N"
}
output_arg {
  name: "output"
  type_a... minimum: 1
}
attr {
  name: "T"
  type: "type"
}
attr {
  name: "axis"
  type: "int"
  default_value {
    i: 0
  }
}

g = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
deprecation_version = 0, default_type_attr_map = {}

    def _apply_op_helper(self, op_type_name, name=None, **keywords):
      """Implementation of apply_op that returns output_structure, op."""
      op_info = self._ops.get(op_type_name, None)
      if op_info is None:
        raise RuntimeError("Unrecognized Op name " + op_type_name)
      op_def = op_info.op_def
    
      # Determine the graph context.
      try:
        # Need to flatten all the arguments into a list.
        # pylint: disable=protected-access
        g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
        # pylint: enable=protected-access
      except AssertionError as e:
        raise RuntimeError(
            "Cannot determine graph for Op '%s' due to: %s"
            % (op_type_name, e.message))
    
      # Default name if not specified.
      if name is None:
        name = op_type_name
    
      # Check for deprecation
      deprecation_version = op_def.deprecation.version
      if deprecation_version:
        producer = g.graph_def_versions.producer
        if producer >= deprecation_version:
          raise NotImplementedError(
              ("Op %s is not available in GraphDef version %d. "
               "It has been removed in version %d. %s.") %
              (op_type_name, producer, deprecation_version,
               op_def.deprecation.explanation))
    
      # Fill in the list of default types for all "type" attrs.  This
      # will be used to choose a preferred dtype to convert to in the
      # absence of input type information.
      #
      # TODO(b/31302892): Currently the defaults don't work in the right
      # way if you have two inputs, one of whose type resolution depends
      # on the other.  Handling this will require restructuring this code
      # significantly.
      default_type_attr_map = {}
      for attr_def in op_def.attr:
        if attr_def.type != "type":
          continue
        key = attr_def.name
        if attr_def.HasField("default_value"):
          default_type_attr_map[key] = dtypes.as_dtype(
              attr_def.default_value.type)
    
      # Requires that op_def has passed validation (using the C++
      # ValidateOpDef() from ../framework/op_def_util.h).
      attrs = {}
      inputs = []
      input_types = []
      with g.as_default(), ops.name_scope(name) as scope:
    
        # Perform input type inference
        inferred_from = {}
        for input_arg in op_def.input_arg:
          input_name = input_arg.name
          if input_name in keywords:
            values = keywords.pop(input_name)
          elif input_name + "_" in keywords:
            # Handle the case where the name is a keyword or built-in
            # for Python so we use the name + _ instead.
            input_name += "_"
            values = keywords.pop(input_name)
          else:
            raise TypeError("No argument for input " + input_name)
    
          # Goals:
          # * Convert values to Tensors if it contains constants.
          # * Verify that values is a list if that matches the input_arg's
          #   type.
          # * If the input_arg's type is determined by attrs, either set
          #   those attrs and validate those attr values are legal (if
          #   they have not yet been set) or validate the input matches
          #   the type indicated by the attrs (if they have already been
          #   inferred via an earlier input).
          # * If the input_arg has an explicit type, make sure the input
          #   conforms.
    
          if _IsListParameter(input_arg):
            if not _IsListValue(values):
              raise TypeError(
                  "Expected list for '%s' argument to '%s' Op, not %s." %
                  (input_name, op_type_name, values))
            # In cases where we expect all elements of the list to have the
            # same dtype, try to cast non-Tensor elements to that type.
            dtype = None
            default_dtype = None
            if input_arg.type != types_pb2.DT_INVALID:
              dtype = input_arg.type
            elif input_arg.number_attr:
              if input_arg.type_attr in attrs:
                dtype = attrs[input_arg.type_attr]
              else:
                for t in values:
                  if isinstance(t, ops.Tensor):
                    dtype = t.dtype
                    break
    
              # dtype still not found, prefer using the default dtype
              # from the attr.
              if dtype is None and input_arg.type_attr in default_type_attr_map:
                default_dtype = default_type_attr_map[input_arg.type_attr]
    
            try:
              if not input_arg.is_ref and dtype:
                dtype = dtypes.as_dtype(dtype).base_dtype
              values = ops.internal_convert_n_to_tensor(
                  values,
                  name=input_arg.name,
                  dtype=dtype if dtype else None,
                  preferred_dtype=default_dtype,
                  as_ref=input_arg.is_ref)
              if input_arg.number_attr and len(
                  set(v.dtype.base_dtype for v in values)) > 1:
                raise TypeError()  # All types should match.
            except (TypeError, ValueError):
              # What types does the conversion function think values have?
              observed_types = []
              for value in values:
                try:
                  converted_value = ops.internal_convert_to_tensor(
                      value, as_ref=input_arg.is_ref)
                  observed_types.append(converted_value.dtype.base_dtype.name)
                except (TypeError, ValueError):
                  observed_types.append("<NOT CONVERTIBLE TO TENSOR>")
              observed = ", ".join(observed_types)
    
              prefix = (
                  "Tensors in list passed to '%s' of '%s' Op have types [%s]" %
                  (input_name, op_type_name, observed))
              if input_arg.number_attr:
                if input_arg.type != types_pb2.DT_INVALID:
                  raise TypeError("%s that do not match expected type %s." %
                                  (prefix, dtype.name))
                elif input_arg.type_attr in attrs:
                  raise TypeError("%s that do not match type %s inferred from "
                                  "earlier arguments." %
                                  (prefix, dtype.name))
                else:
                  raise TypeError("%s that don't all match." % prefix)
              else:
                raise TypeError(
                    "%s that are invalid. Tensors: %s" % (prefix, values))
    
            types = [x.dtype for x in values]
            inputs.extend(values)
          else:
            # In cases where we have an expected type, try to convert non-Tensor
            # arguments to that type.
            dtype = None
            default_dtype = None
            if input_arg.type != types_pb2.DT_INVALID:
              dtype = input_arg.type
            elif input_arg.type_attr in attrs:
              dtype = attrs[input_arg.type_attr]
            elif input_arg.type_attr in default_type_attr_map:
              # The dtype could not be inferred solely from the inputs,
              # so we prefer the attr's default, so code that adds a new attr
              # with a default is backwards compatible.
              default_dtype = default_type_attr_map[input_arg.type_attr]
    
            try:
              values = ops.internal_convert_to_tensor(
                  values,
                  name=input_arg.name,
                  dtype=dtype,
                  as_ref=input_arg.is_ref,
                  preferred_dtype=default_dtype)
            except TypeError as err:
              if dtype is None:
                raise err
              else:
                raise TypeError(
                    "Expected %s passed to parameter '%s' of op '%s', got %s of "
                    "type '%s' instead. Error: %s" %
                    (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,
                     repr(values), type(values).__name__, err))
            except ValueError:
              # What type does convert_to_tensor think it has?
              try:
                observed = ops.internal_convert_to_tensor(
                    values, as_ref=input_arg.is_ref).dtype.name
              except ValueError as err:
                raise ValueError(
                    "Tried to convert '%s' to a tensor and failed. Error: %s" %
                    (input_name, err))
              prefix = ("Input '%s' of '%s' Op has type %s that does not match" %
                        (input_name, op_type_name, observed))
              if input_arg.type != types_pb2.DT_INVALID:
                raise TypeError("%s expected type of %s." %
                                (prefix, dtypes.as_dtype(input_arg.type).name))
              else:
                # Update the maps with the default, if needed.
                k = input_arg.type_attr
                if k in default_type_attr_map:
                  if k not in attrs:
                    attrs[k] = default_type_attr_map[k]
                    if k not in inferred_from:
                      inferred_from[k] = "Default in OpDef"
    
                raise TypeError(
                    "%s type %s of argument '%s'." %
                    (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
                     inferred_from[input_arg.type_attr]))
    
            types = [values.dtype]
            inputs.append(values)
          base_types = [x.base_dtype for x in types]
    
          if input_arg.number_attr:
            # <number-attr> * <type> or <number-attr> * <type-attr>
            if input_arg.number_attr in attrs:
              if len(values) != attrs[input_arg.number_attr]:
                raise ValueError(
                    "List argument '%s' to '%s' Op with length %d must match "
                    "length %d of argument '%s'." %
                    (input_name, op_type_name, len(values),
                     attrs[input_arg.number_attr],
                     inferred_from[input_arg.number_attr]))
            else:
              attrs[input_arg.number_attr] = len(values)
              inferred_from[input_arg.number_attr] = input_name
              num_attr = _Attr(op_def, input_arg.number_attr)
              if num_attr.has_minimum and len(values) < num_attr.minimum:
                raise ValueError(
                    "List argument '%s' to '%s' Op with length %d shorter "
                    "than minimum length %d." %
                    (input_name, op_type_name, len(values), num_attr.minimum))
            # All tensors must have the same base type.
            if any(bt != base_types[0] for bt in base_types):
              raise TypeError(
                  "All tensors passed to '%s' of '%s' Op "
                  "must have the same type." %
                  (input_name, op_type_name))
            if input_arg.type != types_pb2.DT_INVALID:
              # <number-attr> * <type> case
              if base_types and base_types[0] != input_arg.type:
                assert False, "Unreachable"
            elif input_arg.type_attr in attrs:
              # <number-attr> * <type-attr> case, where <type-attr> already
              # has an inferred value.
              if base_types and base_types[0] != attrs[input_arg.type_attr]:
                assert False, "Unreachable"
            else:
              # <number-attr> * <type-attr> case, where we are now setting
              # the <type-attr> based on this input
              if not base_types:
                raise TypeError(
                    "Don't know how to infer type variable from empty input "
                    "list passed to input '%s' of '%s' Op." %
                    (input_name, op_type_name))
              attrs[input_arg.type_attr] = base_types[0]
              inferred_from[input_arg.type_attr] = input_name
              type_attr = _Attr(op_def, input_arg.type_attr)
              _SatisfiesTypeConstraint(base_types[0], type_attr,
                                       param_name=input_name)
          elif input_arg.type_attr:
            # <type-attr>
            attr_value = base_types[0]
            if input_arg.type_attr in attrs:
              if attrs[input_arg.type_attr] != attr_value:
                assert False, "Unreachable"
            else:
              for base_type in base_types:
                _SatisfiesTypeConstraint(base_type,
                                         _Attr(op_def, input_arg.type_attr),
                                         param_name=input_name)
              attrs[input_arg.type_attr] = attr_value
              inferred_from[input_arg.type_attr] = input_name
          elif input_arg.type_list_attr:
            # <type-list-attr>
            attr_value = base_types
            if input_arg.type_list_attr in attrs:
              if attrs[input_arg.type_list_attr] != attr_value:
                raise TypeError(
                    "Input '%s' of '%s' Op has type list of %s that does not "
                    "match type list %s of argument '%s'." %
                    (input_name, op_type_name,
                     ", ".join(dtypes.as_dtype(x).name for x in attr_value),
                     ", ".join(dtypes.as_dtype(x).name
                               for x in attrs[input_arg.type_list_attr]),
                     inferred_from[input_arg.type_list_attr]))
            else:
              for base_type in base_types:
                _SatisfiesTypeConstraint(base_type,
                                         _Attr(op_def, input_arg.type_list_attr),
                                         param_name=input_name)
              attrs[input_arg.type_list_attr] = attr_value
              inferred_from[input_arg.type_list_attr] = input_name
          else:
            # single Tensor with specified type
            if base_types[0] != input_arg.type:
              assert False, "Unreachable"
    
          if input_arg.is_ref:
            if not all(x._is_ref_dtype for x in types):  # pylint: disable=protected-access
              raise TypeError(
                  ("'%s' Op requires that input '%s' be a mutable tensor "
                   "(e.g.: a tf.Variable)") % (op_type_name, input_name))
            input_types.extend(types)
          else:
            input_types.extend(base_types)
    
        # Process remaining attrs
        for attr in op_def.attr:
          # Skip attrs that have already had their values inferred
          if attr.name in attrs:
            if attr.name in keywords:
              raise TypeError(
                  "Should not specify value for inferred attr '%s'." % attr.name)
            continue
          if attr.name in keywords:
            attrs[attr.name] = keywords.pop(attr.name)
          elif attr.name + "_" in keywords:
            # Attrs whose names match Python keywords have an extra '_'
            # appended, so we must check for that as well.
            attrs[attr.name] = keywords.pop(attr.name + "_")
          else:
            raise TypeError("No argument for attr " + attr.name)
    
        # Convert attr values to AttrValue protos.
        attr_protos = {}
        for attr_def in op_def.attr:
          key = attr_def.name
          value = attrs[key]
          attr_value = attr_value_pb2.AttrValue()
          if attr_def.HasField("default_value") and value is None:
            attr_value.CopyFrom(attr_def.default_value)
            attr_protos[key] = attr_value
            continue
          if attr_def.type.startswith("list("):
            if not _IsListValue(value):
              raise TypeError("Expected list for attr " + key)
            if attr_def.has_minimum:
              if len(value) < attr_def.minimum:
                raise ValueError("Attr '%s' of '%s' Op passed list of length %d "
                                 "less than minimum %d." %
                                 (key, op_type_name, len(value),
                                  attr_def.minimum))
            attr_value.list.SetInParent()
          if attr_def.type == "string":
            attr_value.s = _MakeStr(value, key)
            if attr_def.HasField("allowed_values"):
              if attr_value.s not in attr_def.allowed_values.list.s:
                raise ValueError(
                    "Attr '%s' of '%s' Op passed string '%s' not in: \"%s\"." %
                    (key, op_type_name, compat.as_text(attr_value.s),
                     '", "'.join(map(compat.as_text,
                                     attr_def.allowed_values.list.s))))
          elif attr_def.type == "list(string)":
            attr_value.list.s.extend([_MakeStr(x, key) for x in value])
            if attr_def.HasField("allowed_values"):
              for x in attr_value.list.s:
                if x not in attr_def.allowed_values.list.s:
                  raise ValueError(
                      "Attr '%s' of '%s' Op passed string '%s' not in: \"%s\"." %
                      (key, op_type_name, compat.as_text(x),
                       '", "'.join(map(compat.as_text,
                                       attr_def.allowed_values.list.s))))
          elif attr_def.type == "int":
            attr_value.i = _MakeInt(value, key)
            if attr_def.has_minimum:
              if attr_value.i < attr_def.minimum:
                raise ValueError(
                    "Attr '%s' of '%s' Op passed %d less than minimum %d." %
                    (key, op_type_name, attr_value.i, attr_def.minimum))
          elif attr_def.type == "list(int)":
            attr_value.list.i.extend([_MakeInt(x, key) for x in value])
          elif attr_def.type == "float":
            attr_value.f = _MakeFloat(value, key)
          elif attr_def.type == "list(float)":
            attr_value.list.f.extend([_MakeFloat(x, key) for x in value])
          elif attr_def.type == "bool":
            attr_value.b = _MakeBool(value, key)
          elif attr_def.type == "list(bool)":
            attr_value.list.b.extend([_MakeBool(x, key) for x in value])
          elif attr_def.type == "type":
            attr_value.type = _MakeType(value, attr_def)
          elif attr_def.type == "list(type)":
            attr_value.list.type.extend(
                [_MakeType(x, attr_def) for x in value])
          elif attr_def.type == "shape":
            attr_value.shape.CopyFrom(_MakeShape(value, key))
          elif attr_def.type == "list(shape)":
            attr_value.list.shape.extend(
                [_MakeShape(x, key) for x in value])
          elif attr_def.type == "tensor":
            attr_value.tensor.CopyFrom(_MakeTensor(value, key))
          elif attr_def.type == "list(tensor)":
            attr_value.list.tensor.extend(
                [_MakeTensor(x, key) for x in value])
          elif attr_def.type == "func":
            attr_value.func.CopyFrom(_MakeFunc(value, key))
          elif attr_def.type == "list(func)":
            attr_value.list.func.extend([_MakeFunc(x, key) for x in value])
          else:
            raise TypeError("Unrecognized Attr type " + attr_def.type)
    
          attr_protos[key] = attr_value
        del attrs  # attrs is no longer authoritative, use attr_protos instead
    
        # Determine output types (possibly using attrs)
        output_structure = []
        for arg in op_def.output_arg:
          if arg.number_attr:
            n = _AttrValue(attr_protos, arg.number_attr).i
            output_structure.append(n)
          elif arg.type_attr:
            t = _AttrValue(attr_protos, arg.type_attr)
            output_structure.append(None)
          elif arg.type_list_attr:
            t = _AttrValue(attr_protos, arg.type_list_attr)
            output_structure.append(len(t.list.type))
          else:
            output_structure.append(None)
    
        if keywords:
          raise TypeError("apply_op() got unexpected keyword arguments: " +
                          ", ".join(sorted(keywords.keys())))
    
        # NOTE(mrry): We add an explicit colocation constraint between
        # the newly created op and any of its reference-typed inputs.
        must_colocate_inputs = [val for arg, val in zip(op_def.input_arg, inputs)
                                if arg.is_ref]
        with _MaybeColocateWith(must_colocate_inputs):
          # Add Op to graph
          op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,
                           input_types=input_types, attrs=attr_protos,
>                          op_def=op_def)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:788: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tensorflow.python.framework.ops.Graph object at 0x1201bc390>, 'Pack', [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>])
kwargs = {'attrs': {'N': i: 2
, 'T': type: DT_FLOAT
, 'axis': i: 0
}, 'dtypes': None, 'input_types': [tf.float32, tf.float32], 'name': 'lambda_1_out/packed/', ...}
invalid_args = []
named_args = {'attrs': {'N': i: 2
, 'T': type: DT_FLOAT
, 'axis': i: 0
}, 'compute_device': True, 'compute_shapes': True, 'dtypes': None, ...}
arg_name = 'compute_shapes'
spec = DeprecatedArgSpec(position=8, has_ok_value=False, ok_value=None)

    @functools.wraps(func)
    def new_func(*args, **kwargs):
      """Deprecation wrapper."""
      # TODO(apassos) figure out a way to have reasonable performance with
      # deprecation warnings and eager mode.
      if is_in_graph_mode.IS_IN_GRAPH_MODE() and _PRINT_DEPRECATION_WARNINGS:
        invalid_args = []
        named_args = tf_inspect.getcallargs(func, *args, **kwargs)
        for arg_name, spec in iter(deprecated_positions.items()):
          if (spec.position < len(args) and
              not (spec.has_ok_value and
                   _same_value(named_args[arg_name], spec.ok_value))):
            invalid_args.append(arg_name)
        if is_varargs_deprecated and len(args) > len(arg_spec.args):
          invalid_args.append(arg_spec.varargs)
        if is_kwargs_deprecated and kwargs:
          invalid_args.append(arg_spec.varkw)
        for arg_name in deprecated_arg_names:
          if (arg_name in kwargs and
              not (deprecated_positions[arg_name].has_ok_value and
                   _same_value(named_args[arg_name],
                               deprecated_positions[arg_name].ok_value))):
            invalid_args.append(arg_name)
        for arg_name in invalid_args:
          if (func, arg_name) not in _PRINTED_WARNING:
            if warn_once:
              _PRINTED_WARNING[(func, arg_name)] = True
            logging.warning(
                'From %s: calling %s (from %s) with %s is deprecated and will '
                'be removed %s.\nInstructions for updating:\n%s',
                _call_location(), decorator_utils.get_qualified_name(func),
                func.__module__, arg_name,
                'in a future version' if date is None else ('after %s' % date),
                instructions)
>     return func(*args, **kwargs)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:507: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
op_type = 'Pack'
inputs = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
dtypes = None, input_types = [tf.float32, tf.float32]
name = 'lambda_1_out/packed'
attrs = {'N': i: 2
, 'T': type: DT_FLOAT
, 'axis': i: 0
}
op_def = name: "Pack"
input_arg {
  name: "values"
  type_attr: "T"
  number_attr: "N"
}
output_arg {
  name: "output"
  type_a... minimum: 1
}
attr {
  name: "T"
  type: "type"
}
attr {
  name: "axis"
  type: "int"
  default_value {
    i: 0
  }
}

compute_device = True

    @deprecated_args(None,
                     "Shapes are always computed; don't use the compute_shapes "
                     "as it has no effect.", "compute_shapes")
    def create_op(
        self,
        op_type,
        inputs,
        dtypes=None,  # pylint: disable=redefined-outer-name
        input_types=None,
        name=None,
        attrs=None,
        op_def=None,
        compute_shapes=True,
        compute_device=True):
      """Creates an `Operation` in this graph.
    
      This is a low-level interface for creating an `Operation`. Most
      programs will not call this method directly, and instead use the
      Python op constructors, such as `tf.constant()`, which add ops to
      the default graph.
    
      Args:
        op_type: The `Operation` type to create. This corresponds to the
          `OpDef.name` field for the proto that defines the operation.
        inputs: A list of `Tensor` objects that will be inputs to the `Operation`.
        dtypes: (Optional) A list of `DType` objects that will be the types of the
          tensors that the operation produces.
        input_types: (Optional.) A list of `DType`s that will be the types of the
          tensors that the operation consumes. By default, uses the base `DType`
          of each input in `inputs`. Operations that expect reference-typed inputs
          must specify `input_types` explicitly.
        name: (Optional.) A string name for the operation. If not specified, a
          name is generated based on `op_type`.
        attrs: (Optional.) A dictionary where the key is the attribute name (a
          string) and the value is the respective `attr` attribute of the
          `NodeDef` proto that will represent the operation (an `AttrValue`
          proto).
        op_def: (Optional.) The `OpDef` proto that describes the `op_type` that
          the operation will have.
        compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always
          computed).
        compute_device: (Optional.) If True, device functions will be executed to
          compute the device property of the Operation.
    
      Raises:
        TypeError: if any of the inputs is not a `Tensor`.
        ValueError: if colocation conflicts with existing device assignment.
    
      Returns:
        An `Operation` object.
      """
      del compute_shapes
    
      self._check_not_finalized()
      for idx, a in enumerate(inputs):
        if not isinstance(a, Tensor):
          raise TypeError("Input #%d is not a tensor: %s" % (idx, a))
      if name is None:
        name = op_type
      # If a names ends with a '/' it is a "name scope" and we use it as-is,
      # after removing the trailing '/'.
      if name and name[-1] == "/":
        name = name_from_scope_name(name)
      else:
        name = self.unique_name(name)
    
      node_def = _NodeDef(op_type, name, device=None, attrs=attrs)
    
      input_ops = set([t.op for t in inputs])
      control_inputs = self._control_dependencies_for_inputs(input_ops)
      # _create_op_helper mutates the new Operation. `_mutation_lock` ensures a
      # Session.run call cannot occur between creating and mutating the op.
      with self._mutation_lock():
        ret = Operation(
            node_def,
            self,
            inputs=inputs,
            output_types=dtypes,
            control_inputs=control_inputs,
            input_types=input_types,
            original_op=self._default_original_op,
>           op_def=op_def)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3616: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'Operation' object has no attribute '_c_op'") raised in repr()] Operation object at 0x1346c3650>
node_def = name: "lambda_1_out/packed"
op: "Pack"
attr {
  key: "N"
  value {
    i: 2
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "axis"
  value {
    i: 0
  }
}

g = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
inputs = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]
output_types = None, control_inputs = [], input_types = [tf.float32, tf.float32]
original_op = None
op_def = name: "Pack"
input_arg {
  name: "values"
  type_attr: "T"
  number_attr: "N"
}
output_arg {
  name: "output"
  type_a... minimum: 1
}
attr {
  name: "T"
  type: "type"
}
attr {
  name: "axis"
  type: "int"
  default_value {
    i: 0
  }
}


    def __init__(self,
                 node_def,
                 g,
                 inputs=None,
                 output_types=None,
                 control_inputs=None,
                 input_types=None,
                 original_op=None,
                 op_def=None):
      r"""Creates an `Operation`.
    
      NOTE: This constructor validates the name of the `Operation` (passed
      as `node_def.name`). Valid `Operation` names match the following
      regular expression:
    
          [A-Za-z0-9.][A-Za-z0-9_.\\-/]*
    
      Args:
        node_def: `node_def_pb2.NodeDef`.  `NodeDef` for the `Operation`. Used for
          attributes of `node_def_pb2.NodeDef`, typically `name`, `op`, and
          `device`.  The `input` attribute is irrelevant here as it will be
          computed when generating the model.
        g: `Graph`. The parent graph.
        inputs: list of `Tensor` objects. The inputs to this `Operation`.
        output_types: list of `DType` objects.  List of the types of the `Tensors`
          computed by this operation.  The length of this list indicates the
          number of output endpoints of the `Operation`.
        control_inputs: list of operations or tensors from which to have a control
          dependency.
        input_types: List of `DType` objects representing the types of the tensors
          accepted by the `Operation`.  By default uses `[x.dtype.base_dtype for x
          in inputs]`.  Operations that expect reference-typed inputs must specify
          these explicitly.
        original_op: Optional. Used to associate the new `Operation` with an
          existing `Operation` (for example, a replica with the op that was
          replicated).
        op_def: Optional. The `op_def_pb2.OpDef` proto that describes the op type
          that this `Operation` represents.
    
      Raises:
        TypeError: if control inputs are not Operations or Tensors,
          or if `node_def` is not a `NodeDef`,
          or if `g` is not a `Graph`,
          or if `inputs` are not tensors,
          or if `inputs` and `input_types` are incompatible.
        ValueError: if the `node_def` name is not valid.
      """
      # For internal use only: `node_def` can be set to a TF_Operation to create
      # an Operation for that op. This is useful for creating Operations for ops
      # indirectly created by C API methods, e.g. the ops created by
      # TF_ImportGraphDef. When `node_def` is a TF_Operation, all optional fields
      # should be None.
    
      if isinstance(node_def, node_def_pb2.NodeDef):
        if node_def.ByteSize() >= (1 << 31) or node_def.ByteSize() < 0:
          raise ValueError(
              "Cannot create a tensor proto whose content is larger than 2GB.")
        if not _VALID_OP_NAME_REGEX.match(node_def.name):
          raise ValueError("'%s' is not a valid node name" % node_def.name)
        c_op = None
      elif type(node_def).__name__ == "SwigPyObject":
        assert inputs is None
        assert output_types is None
        assert control_inputs is None
        assert input_types is None
        assert original_op is None
        assert op_def is None
        c_op = node_def
      else:
        raise TypeError("node_def needs to be a NodeDef: %s" % node_def)
    
      if not isinstance(g, Graph):
        raise TypeError("g needs to be a Graph: %s" % g)
      self._graph = g
    
      if inputs is None:
        inputs = []
      elif not isinstance(inputs, list):
        raise TypeError("inputs needs to be a list of Tensors: %s" % inputs)
      for a in inputs:
        if not isinstance(a, Tensor):
          raise TypeError("input needs to be a Tensor: %s" % a)
      if input_types is None:
        input_types = [i.dtype.base_dtype for i in inputs]
      else:
        if not all(
            x.is_compatible_with(i.dtype) for i, x in zip(inputs, input_types)):
          raise TypeError("In op '%s', input types (%s) are not compatible "
                          "with expected types (%s)" %
                          (node_def.name, [i.dtype for i in inputs], input_types))
    
      # Build the list of control inputs.
      control_input_ops = []
      if control_inputs:
        for c in control_inputs:
          control_op = None
          if isinstance(c, Operation):
            control_op = c
          elif isinstance(c, (Tensor, IndexedSlices)):
            control_op = c.op
          else:
            raise TypeError("Control input must be an Operation, "
                            "a Tensor, or IndexedSlices: %s" % c)
          control_input_ops.append(control_op)
    
      # This will be set by self.inputs.
      self._inputs_val = None
    
      # pylint: disable=protected-access
      self._id_value = self._graph._next_id()
      self._original_op = original_op
      self._traceback = tf_stack.extract_stack()
    
      # List of _UserDevSpecs holding code location of device context manager
      # invocations and the users original argument to them.
      self._device_code_locations = None
      # Dict mapping op name to file and line information for op colocation
      # context managers.
      self._colocation_code_locations = None
      self._control_flow_context = self.graph._get_control_flow_context()
      # pylint: enable=protected-access
    
      # Initialize self._c_op.
      if c_op:
        self._c_op = c_op
      else:
        if op_def is None:
          op_def = self._graph._get_op_def(node_def.op)
        # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
        # Refactor so we don't have to do this here.
        grouped_inputs = self._reconstruct_sequence_inputs(
            op_def, inputs, node_def.attr)
        self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
>                                 control_input_ops)

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2027: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
node_def = name: "lambda_1_out/packed"
op: "Pack"
attr {
  key: "N"
  value {
    i: 2
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "axis"
  value {
    i: 0
  }
}

inputs = [[<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]]
control_inputs = []

    def _create_c_op(graph, node_def, inputs, control_inputs):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of
          `Tensor`s (corresponding to sequence inputs, e.g. "int64 * N",
          "list(int64)"). The length of the list should be equal to the number of
          inputs specified by this operation's op def.
        control_inputs: A list of `Operation`s to set as control dependencies.
    
      Returns:
        A wrapped TF_Operation*.
      """
      # pylint: disable=protected-access
      op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),
                                      compat.as_str(node_def.name))
      if node_def.device:
        c_api.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          c_api.TF_AddInputList(op_desc, [t._as_tf_output() for t in op_input])
        else:
          c_api.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        c_api.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        c_api.TF_SetAttrValueProto(op_desc, compat.as_str(name), serialized)
    
      try:
        c_op = c_api.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
>       raise ValueError(str(e))
E       ValueError: Shapes must be equal rank, but are 3 and 2
E       	From merging shape 0 with other shapes. for 'lambda_1_out/packed' (op: 'Pack') with input shapes: [?,2,2], [?,2].

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1867: ValueError

During handling of the above exception, another exception occurred:

tmpdir = local('/private/var/folders/ng/72llsm517x12c2p18htksyjc0000gn/T/pytest-of-jerry/pytest-1292/popen-gw0/test_TensorBoard_multi_input_o0')

    @keras_test
    def test_TensorBoard_multi_input_output(tmpdir):
        np.random.seed(np.random.randint(1, 1e7))
        filepath = str(tmpdir / 'logs')
    
        (X_train, y_train), (X_test, y_test) = get_test_data(
            num_train=train_samples,
            num_test=test_samples,
            input_shape=(input_dim, input_dim),
            classification=True,
            num_classes=num_classes)
        y_test = np_utils.to_categorical(y_test)
        y_train = np_utils.to_categorical(y_train)
    
        def data_generator(train):
            if train:
                max_batch_index = len(X_train) // batch_size
            else:
                max_batch_index = len(X_test) // batch_size
            i = 0
            while 1:
                if train:
                    # simulate multi-input/output models
                    yield ([X_train[i * batch_size: (i + 1) * batch_size]] * 2,
                           [y_train[i * batch_size: (i + 1) * batch_size]] * 2)
                else:
                    yield ([X_test[i * batch_size: (i + 1) * batch_size]] * 2,
                           [y_test[i * batch_size: (i + 1) * batch_size]] * 2)
                i += 1
                i = i % max_batch_index
    
        inp1 = Input((input_dim, input_dim))
        inp2 = Input((input_dim, input_dim))
        inp_3d = add([inp1, inp2])
        inp_2d = GlobalAveragePooling1D()(inp_3d)
        inp_pair = Lambda(lambda x: x)([inp_3d, inp_2d])  # test a layer with a list of output tensors
        hidden = dot(inp_pair, axes=-1)
        hidden = Dense(num_hidden, activation='relu')(hidden)
        hidden = Dropout(0.1)(hidden)
        output1 = Dense(num_classes, activation='softmax')(hidden)
        output2 = Dense(num_classes, activation='softmax')(hidden)
        model = Model(inputs=[inp1, inp2], outputs=[output1, output2])
        model.compile(loss='categorical_crossentropy',
                      optimizer='sgd',
                      metrics=['accuracy'])
    
        # we must generate new callbacks for each test, as they aren't stateless
        def callbacks_factory(histogram_freq):
            return [callbacks.TensorBoard(log_dir=filepath,
                                          histogram_freq=histogram_freq,
                                          write_images=True, write_grads=True,
                                          embeddings_freq=1,
                                          embeddings_layer_names=['dense_1'],
                                          batch_size=5)]
    
        # fit without validation data
        model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,
                  callbacks=callbacks_factory(histogram_freq=0), epochs=3)
    
        # fit with validation data and accuracy
        model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,
                  validation_data=([X_test] * 2, [y_test] * 2),
>                 callbacks=callbacks_factory(histogram_freq=1), epochs=2)

tests/keras/test_callbacks.py:680: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
keras/engine/training.py:1037: in fit
    validation_steps=validation_steps)
keras/engine/training_arrays.py:115: in fit_loop
    callbacks.set_model(callback_model)
keras/callbacks.py:51: in set_model
    callback.set_model(model)
keras/callbacks.py:790: in set_model
    layer.output)
../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/summary/summary.py:179: in histogram
    tag=tag, values=values, name=scope)
../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py:329: in histogram_summary
    "HistogramSummary", tag=tag, values=values, name=name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x107edac90>
op_type_name = 'HistogramSummary', name = 'lambda_1_out/', keywords = {}
op_info = <tensorflow.python.framework.op_def_library._OpInfo object at 0x107ee3210>
op_def = name: "HistogramSummary"
input_arg {
  name: "tag"
  type: DT_STRING
}
input_arg {
  name: "values"
  type_attr: "T"
}...   type: DT_BFLOAT16
      type: DT_UINT16
      type: DT_HALF
      type: DT_UINT32
      type: DT_UINT64
    }
  }
}

g = <tensorflow.python.framework.ops.Graph object at 0x1201bc390>
deprecation_version = 0, default_type_attr_map = {'T': tf.float32}

    def _apply_op_helper(self, op_type_name, name=None, **keywords):
      """Implementation of apply_op that returns output_structure, op."""
      op_info = self._ops.get(op_type_name, None)
      if op_info is None:
        raise RuntimeError("Unrecognized Op name " + op_type_name)
      op_def = op_info.op_def
    
      # Determine the graph context.
      try:
        # Need to flatten all the arguments into a list.
        # pylint: disable=protected-access
        g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
        # pylint: enable=protected-access
      except AssertionError as e:
        raise RuntimeError(
            "Cannot determine graph for Op '%s' due to: %s"
            % (op_type_name, e.message))
    
      # Default name if not specified.
      if name is None:
        name = op_type_name
    
      # Check for deprecation
      deprecation_version = op_def.deprecation.version
      if deprecation_version:
        producer = g.graph_def_versions.producer
        if producer >= deprecation_version:
          raise NotImplementedError(
              ("Op %s is not available in GraphDef version %d. "
               "It has been removed in version %d. %s.") %
              (op_type_name, producer, deprecation_version,
               op_def.deprecation.explanation))
    
      # Fill in the list of default types for all "type" attrs.  This
      # will be used to choose a preferred dtype to convert to in the
      # absence of input type information.
      #
      # TODO(b/31302892): Currently the defaults don't work in the right
      # way if you have two inputs, one of whose type resolution depends
      # on the other.  Handling this will require restructuring this code
      # significantly.
      default_type_attr_map = {}
      for attr_def in op_def.attr:
        if attr_def.type != "type":
          continue
        key = attr_def.name
        if attr_def.HasField("default_value"):
          default_type_attr_map[key] = dtypes.as_dtype(
              attr_def.default_value.type)
    
      # Requires that op_def has passed validation (using the C++
      # ValidateOpDef() from ../framework/op_def_util.h).
      attrs = {}
      inputs = []
      input_types = []
      with g.as_default(), ops.name_scope(name) as scope:
    
        # Perform input type inference
        inferred_from = {}
        for input_arg in op_def.input_arg:
          input_name = input_arg.name
          if input_name in keywords:
            values = keywords.pop(input_name)
          elif input_name + "_" in keywords:
            # Handle the case where the name is a keyword or built-in
            # for Python so we use the name + _ instead.
            input_name += "_"
            values = keywords.pop(input_name)
          else:
            raise TypeError("No argument for input " + input_name)
    
          # Goals:
          # * Convert values to Tensors if it contains constants.
          # * Verify that values is a list if that matches the input_arg's
          #   type.
          # * If the input_arg's type is determined by attrs, either set
          #   those attrs and validate those attr values are legal (if
          #   they have not yet been set) or validate the input matches
          #   the type indicated by the attrs (if they have already been
          #   inferred via an earlier input).
          # * If the input_arg has an explicit type, make sure the input
          #   conforms.
    
          if _IsListParameter(input_arg):
            if not _IsListValue(values):
              raise TypeError(
                  "Expected list for '%s' argument to '%s' Op, not %s." %
                  (input_name, op_type_name, values))
            # In cases where we expect all elements of the list to have the
            # same dtype, try to cast non-Tensor elements to that type.
            dtype = None
            default_dtype = None
            if input_arg.type != types_pb2.DT_INVALID:
              dtype = input_arg.type
            elif input_arg.number_attr:
              if input_arg.type_attr in attrs:
                dtype = attrs[input_arg.type_attr]
              else:
                for t in values:
                  if isinstance(t, ops.Tensor):
                    dtype = t.dtype
                    break
    
              # dtype still not found, prefer using the default dtype
              # from the attr.
              if dtype is None and input_arg.type_attr in default_type_attr_map:
                default_dtype = default_type_attr_map[input_arg.type_attr]
    
            try:
              if not input_arg.is_ref and dtype:
                dtype = dtypes.as_dtype(dtype).base_dtype
              values = ops.internal_convert_n_to_tensor(
                  values,
                  name=input_arg.name,
                  dtype=dtype if dtype else None,
                  preferred_dtype=default_dtype,
                  as_ref=input_arg.is_ref)
              if input_arg.number_attr and len(
                  set(v.dtype.base_dtype for v in values)) > 1:
                raise TypeError()  # All types should match.
            except (TypeError, ValueError):
              # What types does the conversion function think values have?
              observed_types = []
              for value in values:
                try:
                  converted_value = ops.internal_convert_to_tensor(
                      value, as_ref=input_arg.is_ref)
                  observed_types.append(converted_value.dtype.base_dtype.name)
                except (TypeError, ValueError):
                  observed_types.append("<NOT CONVERTIBLE TO TENSOR>")
              observed = ", ".join(observed_types)
    
              prefix = (
                  "Tensors in list passed to '%s' of '%s' Op have types [%s]" %
                  (input_name, op_type_name, observed))
              if input_arg.number_attr:
                if input_arg.type != types_pb2.DT_INVALID:
                  raise TypeError("%s that do not match expected type %s." %
                                  (prefix, dtype.name))
                elif input_arg.type_attr in attrs:
                  raise TypeError("%s that do not match type %s inferred from "
                                  "earlier arguments." %
                                  (prefix, dtype.name))
                else:
                  raise TypeError("%s that don't all match." % prefix)
              else:
                raise TypeError(
                    "%s that are invalid. Tensors: %s" % (prefix, values))
    
            types = [x.dtype for x in values]
            inputs.extend(values)
          else:
            # In cases where we have an expected type, try to convert non-Tensor
            # arguments to that type.
            dtype = None
            default_dtype = None
            if input_arg.type != types_pb2.DT_INVALID:
              dtype = input_arg.type
            elif input_arg.type_attr in attrs:
              dtype = attrs[input_arg.type_attr]
            elif input_arg.type_attr in default_type_attr_map:
              # The dtype could not be inferred solely from the inputs,
              # so we prefer the attr's default, so code that adds a new attr
              # with a default is backwards compatible.
              default_dtype = default_type_attr_map[input_arg.type_attr]
    
            try:
              values = ops.internal_convert_to_tensor(
                  values,
                  name=input_arg.name,
                  dtype=dtype,
                  as_ref=input_arg.is_ref,
                  preferred_dtype=default_dtype)
            except TypeError as err:
              if dtype is None:
                raise err
              else:
                raise TypeError(
                    "Expected %s passed to parameter '%s' of op '%s', got %s of "
                    "type '%s' instead. Error: %s" %
                    (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,
                     repr(values), type(values).__name__, err))
            except ValueError:
              # What type does convert_to_tensor think it has?
              try:
                observed = ops.internal_convert_to_tensor(
                    values, as_ref=input_arg.is_ref).dtype.name
              except ValueError as err:
                raise ValueError(
                    "Tried to convert '%s' to a tensor and failed. Error: %s" %
>                   (input_name, err))
E               ValueError: Tried to convert 'values' to a tensor and failed. Error: Shapes must be equal rank, but are 3 and 2
E               	From merging shape 0 with other shapes. for 'lambda_1_out/packed' (op: 'Pack') with input shapes: [?,2,2], [?,2].

../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:545: ValueError

```


