Please fix the function/method provided below and provide the corrected function/method as the output.


# Buggy function source code
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/keras_8/keras/engine/network.py

# relative function's signature in this file
def add_unprocessed_node(layer, node_data):
    # ... omitted code ...
    pass

# relative function's signature in this file
def process_node(layer, node_data):
    # ... omitted code ...
    pass

# relative function's signature in this file
def process_layer(layer_data):
    # ... omitted code ...
    pass



    # this is the buggy function you need to fix
    @classmethod
    def from_config(cls, config, custom_objects=None):
        """Instantiates a Model from its config (output of `get_config()`).
    
        # Arguments
            config: Model config dictionary.
            custom_objects: Optional dictionary mapping names
                (strings) to custom classes or functions to be
                considered during deserialization.
    
        # Returns
            A model instance.
    
        # Raises
            ValueError: In case of improperly formatted config dict.
        """
        # Layer instances created during
        # the graph reconstruction process
        created_layers = {}
    
        # Dictionary mapping layer instances to
        # node data that specifies a layer call.
        # It acts as a queue that maintains any unprocessed
        # layer call until it becomes possible to process it
        # (i.e. until the input tensors to the call all exist).
        unprocessed_nodes = {}
    
        def add_unprocessed_node(layer, node_data):
            if layer not in unprocessed_nodes:
                unprocessed_nodes[layer] = [node_data]
            else:
                unprocessed_nodes[layer].append(node_data)
    
        def process_node(layer, node_data):
            input_tensors = []
            for input_data in node_data:
                inbound_layer_name = input_data[0]
                inbound_node_index = input_data[1]
                inbound_tensor_index = input_data[2]
                if len(input_data) == 3:
                    kwargs = {}
                elif len(input_data) == 4:
                    kwargs = input_data[3]
                else:
                    raise ValueError('Improperly formatted model config.')
                inbound_layer = created_layers[inbound_layer_name]
                if len(inbound_layer._inbound_nodes) <= inbound_node_index:
                    add_unprocessed_node(layer, node_data)
                    return
                inbound_node = inbound_layer._inbound_nodes[inbound_node_index]
                input_tensors.append(
                    inbound_node.output_tensors[inbound_tensor_index])
            # Call layer on its inputs, thus creating the node
            # and building the layer if needed.
            if input_tensors:
                layer(unpack_singleton(input_tensors), **kwargs)
    
        def process_layer(layer_data):
            """Deserializes a layer, then call it on appropriate inputs.
    
            # Arguments
                layer_data: layer config dict.
    
            # Raises
                ValueError: In case of improperly formatted `layer_data` dict.
            """
            layer_name = layer_data['name']
    
            # Instantiate layer.
            from ..layers import deserialize as deserialize_layer
    
            layer = deserialize_layer(layer_data,
                                      custom_objects=custom_objects)
            created_layers[layer_name] = layer
    
            # Gather layer inputs.
            inbound_nodes_data = layer_data['inbound_nodes']
            for node_data in inbound_nodes_data:
                # We don't process nodes (i.e. make layer calls)
                # on the fly because the inbound node may not yet exist,
                # in case of layer shared at different topological depths
                # (e.g. a model such as A(B(A(B(x)))))
                add_unprocessed_node(layer, node_data)
    
        # First, we create all layers and enqueue nodes to be processed
        for layer_data in config['layers']:
            process_layer(layer_data)
        # Then we process nodes in order of layer depth.
        # Nodes that cannot yet be processed (if the inbound node
        # does not yet exist) are re-enqueued, and the process
        # is repeated until all nodes are processed.
        while unprocessed_nodes:
            for layer_data in config['layers']:
                layer = created_layers[layer_data['name']]
                if layer in unprocessed_nodes:
                    for node_data in unprocessed_nodes.pop(layer):
                        process_node(layer, node_data)
    
        name = config.get('name')
        input_tensors = []
        output_tensors = []
        for layer_data in config['input_layers']:
            layer_name, node_index, tensor_index = layer_data
            assert layer_name in created_layers
            layer = created_layers[layer_name]
            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors
            input_tensors.append(layer_output_tensors[tensor_index])
        for layer_data in config['output_layers']:
            layer_name, node_index, tensor_index = layer_data
            assert layer_name in created_layers
            layer = created_layers[layer_name]
            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors
            output_tensors.append(layer_output_tensors[tensor_index])
        return cls(inputs=input_tensors, outputs=output_tensors, name=name)
    
```

# A test function for the buggy function
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/keras_8/tests/keras/engine/test_topology.py

def test_layer_sharing_at_heterogeneous_depth_order():
    # This tests for the bug in this issue
    # https://github.com/keras-team/keras/issues/11159
    # It occurs with layer sharing at heterogeneous depth when
    # the layers need to be applied in an order that differs from
    # the order that occurs in the config.

    input_shape = (1, 12)
    input_layer = Input(shape=input_shape)

    A = Dense(12, name='layer_a')
    r1 = layers.Reshape((12,))(input_layer)
    Aout1 = A(r1)

    r2 = layers.Reshape((12,))(A(input_layer))
    Aout2 = A(r2)

    # Note: if the order of the layers in the concat is
    # changed to ([Aout1, Aout2]) the bug doesn't trigger
    c1 = layers.concatenate([Aout2, Aout1])
    output = Dense(2, name='layer_b')(c1)

    M = Model(inputs=input_layer, outputs=output)

    x_val = np.random.random((10,) + input_shape)
    output_val = M.predict(x_val)

    config = M.get_config()
    weights = M.get_weights()

    M2 = Model.from_config(config)
    M2.set_weights(weights)

    output_val_2 = M2.predict(x_val)
    np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)
```

## Error message from test function
```text
def test_layer_sharing_at_heterogeneous_depth_order():
        # This tests for the bug in this issue
        # https://github.com/keras-team/keras/issues/11159
        # It occurs with layer sharing at heterogeneous depth when
        # the layers need to be applied in an order that differs from
        # the order that occurs in the config.
    
        input_shape = (1, 12)
        input_layer = Input(shape=input_shape)
    
        A = Dense(12, name='layer_a')
        r1 = layers.Reshape((12,))(input_layer)
        Aout1 = A(r1)
    
        r2 = layers.Reshape((12,))(A(input_layer))
        Aout2 = A(r2)
    
        # Note: if the order of the layers in the concat is
        # changed to ([Aout1, Aout2]) the bug doesn't trigger
        c1 = layers.concatenate([Aout2, Aout1])
        output = Dense(2, name='layer_b')(c1)
    
        M = Model(inputs=input_layer, outputs=output)
    
        x_val = np.random.random((10,) + input_shape)
        output_val = M.predict(x_val)
    
        config = M.get_config()
        weights = M.get_weights()
    
>       M2 = Model.from_config(config)

tests/keras/engine/test_topology.py:793: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
keras/engine/network.py:1029: in from_config
    process_node(layer, node_data)
keras/engine/network.py:988: in process_node
    layer(unpack_singleton(input_tensors), **kwargs)
keras/engine/base_layer.py:431: in __call__
    self.build(unpack_singleton(input_shapes))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <keras.layers.merge.Concatenate object at 0x128b70750>
input_shape = [(None, 12), (None, 1, 12)]

    def build(self, input_shape):
        # Used purely for shape validation.
        if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError('A `Concatenate` layer should be called '
                             'on a list of at least 2 inputs')
        if all([shape is None for shape in input_shape]):
            return
        reduced_inputs_shapes = [list(shape) for shape in input_shape]
        shape_set = set()
        for i in range(len(reduced_inputs_shapes)):
            del reduced_inputs_shapes[i][self.axis]
            shape_set.add(tuple(reduced_inputs_shapes[i]))
        if len(shape_set) > 1:
            raise ValueError('A `Concatenate` layer requires '
                             'inputs with matching shapes '
                             'except for the concat axis. '
>                            'Got inputs shapes: %s' % (input_shape))
E           ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12), (None, 1, 12)]

keras/layers/merge.py:362: ValueError

```


