{
    "keras": [
        {
            "bugID": 8,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 1,
                "3": 0,
                "4": 1,
                "5": 1,
                "6": 1,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 1,
                "3": 0,
                "4": 1,
                "5": 1,
                "6": 1,
                "7": 0
            },
            "start_line": 933,
            "file_name": "keras/engine/network.py",
            "replace_code": "def from_config(cls, config, custom_objects=None):\n    \"\"\"Instantiates a Model from its config (output of `get_config()`).\n\n    # Arguments\n        config: Model config dictionary.\n        custom_objects: Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n\n    # Returns\n        A model instance.\n\n    # Raises\n        ValueError: In case of improperly formatted config dict.\n    \"\"\"\n    # Layer instances created during\n    # the graph reconstruction process\n    created_layers = {}\n\n    # Dictionary mapping layer instances to\n    # node data that specifies a layer call.\n    # It acts as a queue that maintains any unprocessed\n    # layer call until it becomes possible to process it\n    # (i.e. until the input tensors to the call all exist).\n    unprocessed_nodes = {}\n\n    def add_unprocessed_node(layer_sharing, node_data):\n        if layer_sharing not in unprocessed_nodes:\n            unprocessed_nodes[layer_sharing] = [node_data]\n        else:\n            unprocessed_nodes[layer_sharing].append(node_data)\n\n    def process_node(layer_sharing, node_data):\n        layer_input_tensors = []\n        for input_shared_data in node_data:\n            inbound_layer_name_sharing = input_shared_data[0]\n            inbound_node_index_sharing = input_shared_data[1]\n            inbound_tensor_index_sharing = input_shared_data[2]\n            if len(input_shared_data) == 3:\n                shared_call_kwargs = {}\n            elif len(input_shared_data) == 4:\n                shared_call_kwargs = input_shared_data[3]\n            else:\n                raise ValueError('Improperly formatted model config.')\n            inbound_layer = created_layers[inbound_layer_name_sharing]\n            if len(inbound_layer._inbound_nodes) <= inbound_node_index_sharing:\n                add_unprocessed_node(layer_sharing, node_data)\n                return\n            inbound_node_sharing = inbound_layer._inbound_nodes[inbound_node_index_sharing]\n            layer_input_tensors.append(\n                inbound_node_sharing.output_tensors[inbound_tensor_index_sharing])\n        # Call layer on its inputs, thus creating the shared node\n        # and building the shared layer if needed.\n        if layer_input_tensors:\n            layer_sharing(unpack_singleton(layer_input_tensors), **shared_call_kwargs)\n\n    def process_layer(layer_shared_data):\n        \"\"\"Deserializes a layer, then call it on appropriate inputs.\n\n        # Arguments\n            layer_shared_data: layer config dict.\n\n        # Raises\n            ValueError: In case of improperly formatted `layer_shared_data` dict.\n        \"\"\"\n        layer_name_sharing = layer_shared_data['name']\n\n        # Instantiate layer.\n        from ..layers import deserialize as deserialize_layer\n\n        layer_sharing = deserialize_layer(layer_shared_data,\n                              custom_objects=custom_objects)\n        created_layers[layer_name_sharing] = layer_sharing\n\n        # Gather layer inputs.\n        inbound_nodes_shared_data = layer_shared_data['inbound_nodes']\n        for node_shared_data in inbound_nodes_shared_data:\n            # We don't process nodes (i.e. make layer calls)\n            # on the fly because the inbound shared node may not yet exist,\n            # in case of layer shared at different topological depths\n            # (e.g. a model such as A(B(A(B(x)))))\n            add_unprocessed_node(layer_sharing, node_shared_data)\n\n    # First, we create all layers and enqueue nodes to be processed\n    for layer_shared_data in config['layers']:\n        process_layer(layer_shared_data)\n    # Then we process nodes in order of layer depth.\n    # Nodes that cannot yet be processed (if the inbound shared node\n    # does not yet exist) are re-enqueued, and the process\n    # is repeated until all nodes are processed.\n    while unprocessed_nodes:\n        for layer_shared_data in config['layers']:\n            layer_to_receive = created_layers[layer_shared_data['name']]\n            if layer_to_receive in unprocessed_nodes:\n                for node_shared_data in unprocessed_nodes.pop(layer_to_receive):\n                    process_node(layer_to_receive, node_shared_data)\n\n    name_sharing = config.get('name')\n    tensor_inputs = []\n    tensor_outputs = []\n    for node_data_list in config['input_layers']:\n        layer_name, node_index, tensor_index = node_data_list\n        assert layer_name in created_layers\n        layer_shared = created_layers[layer_name]\n        layer_output_tensors_shared = layer_shared._inbound_nodes[node_index].output_tensors\n        tensor_inputs.append(layer_output_tensors_shared[tensor_index])\n    for node_data_list in config['output_layers']:\n        layer_name, node_index, tensor_index = node_data_list\n        assert layer_name in created_layers\n        layer_shared = created_layers[layer_name]\n        layer_output_tensors_shared = layer_shared._inbound_nodes[node_index].output_tensors\n        tensor_outputs.append(layer_output_tensors_shared[tensor_index])\n    return cls(inputs=tensor_inputs, outputs=tensor_outputs, name=name_sharing)",
            "imports": [
                "from ..layers import deserialize as deserialize_layer"
            ]
        }
    ]
}