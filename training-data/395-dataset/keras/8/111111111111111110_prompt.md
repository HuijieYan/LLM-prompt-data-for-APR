Please fix the function/method provided below and provide the corrected function/method as the output.


# Buggy function source code
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/keras_8/keras/engine/network.py

# relative function's signature in this file
def add_unprocessed_node(layer, node_data):
    # ... omitted code ...
    pass

# relative function's signature in this file
def process_node(layer, node_data):
    # ... omitted code ...
    pass

# relative function's signature in this file
def process_layer(layer_data):
    # ... omitted code ...
    pass

# class declaration containing the buggy function
class Network(Layer):
    """
    A Network is a directed acyclic graph of layers.
    
    It is the topological form of a "model". A Model
    is simply a Network with added training routines.
    
    # Properties
        name
        inputs
        outputs
        layers
        input_spec (list of class instances)
            each entry describes one required input:
                - ndim
                - dtype
        trainable (boolean)
        input_shape
        output_shape
        weights (list of variables)
        trainable_weights (list of variables)
        non_trainable_weights (list of variables)
        losses
        updates
        state_updates
        stateful
    
    # Methods
        __call__
        summary
        get_layer
        get_weights
        set_weights
        get_config
        compute_output_shape
        save
        add_loss
        add_update
        get_losses_for
        get_updates_for
        to_json
        to_yaml
        reset_states
    
    # Class Methods
        from_config
    
    # Raises
        TypeError: if input tensors are not Keras tensors
            (tensors returned by `Input`).
    """

    # ... omitted code ...


    # signature of a relative function in this class
    def add_unprocessed_node(layer, node_data):
        # ... omitted code ...
        pass

    # signature of a relative function in this class
    def process_node(layer, node_data):
        # ... omitted code ...
        pass

    # signature of a relative function in this class
    def process_layer(layer_data):
        # ... omitted code ...
        pass



    # this is the buggy function you need to fix
    @classmethod
    def from_config(cls, config, custom_objects=None):
        """Instantiates a Model from its config (output of `get_config()`).
    
        # Arguments
            config: Model config dictionary.
            custom_objects: Optional dictionary mapping names
                (strings) to custom classes or functions to be
                considered during deserialization.
    
        # Returns
            A model instance.
    
        # Raises
            ValueError: In case of improperly formatted config dict.
        """
        # Layer instances created during
        # the graph reconstruction process
        created_layers = {}
    
        # Dictionary mapping layer instances to
        # node data that specifies a layer call.
        # It acts as a queue that maintains any unprocessed
        # layer call until it becomes possible to process it
        # (i.e. until the input tensors to the call all exist).
        unprocessed_nodes = {}
    
        def add_unprocessed_node(layer, node_data):
            if layer not in unprocessed_nodes:
                unprocessed_nodes[layer] = [node_data]
            else:
                unprocessed_nodes[layer].append(node_data)
    
        def process_node(layer, node_data):
            input_tensors = []
            for input_data in node_data:
                inbound_layer_name = input_data[0]
                inbound_node_index = input_data[1]
                inbound_tensor_index = input_data[2]
                if len(input_data) == 3:
                    kwargs = {}
                elif len(input_data) == 4:
                    kwargs = input_data[3]
                else:
                    raise ValueError('Improperly formatted model config.')
                inbound_layer = created_layers[inbound_layer_name]
                if len(inbound_layer._inbound_nodes) <= inbound_node_index:
                    add_unprocessed_node(layer, node_data)
                    return
                inbound_node = inbound_layer._inbound_nodes[inbound_node_index]
                input_tensors.append(
                    inbound_node.output_tensors[inbound_tensor_index])
            # Call layer on its inputs, thus creating the node
            # and building the layer if needed.
            if input_tensors:
                layer(unpack_singleton(input_tensors), **kwargs)
    
        def process_layer(layer_data):
            """Deserializes a layer, then call it on appropriate inputs.
    
            # Arguments
                layer_data: layer config dict.
    
            # Raises
                ValueError: In case of improperly formatted `layer_data` dict.
            """
            layer_name = layer_data['name']
    
            # Instantiate layer.
            from ..layers import deserialize as deserialize_layer
    
            layer = deserialize_layer(layer_data,
                                      custom_objects=custom_objects)
            created_layers[layer_name] = layer
    
            # Gather layer inputs.
            inbound_nodes_data = layer_data['inbound_nodes']
            for node_data in inbound_nodes_data:
                # We don't process nodes (i.e. make layer calls)
                # on the fly because the inbound node may not yet exist,
                # in case of layer shared at different topological depths
                # (e.g. a model such as A(B(A(B(x)))))
                add_unprocessed_node(layer, node_data)
    
        # First, we create all layers and enqueue nodes to be processed
        for layer_data in config['layers']:
            process_layer(layer_data)
        # Then we process nodes in order of layer depth.
        # Nodes that cannot yet be processed (if the inbound node
        # does not yet exist) are re-enqueued, and the process
        # is repeated until all nodes are processed.
        while unprocessed_nodes:
            for layer_data in config['layers']:
                layer = created_layers[layer_data['name']]
                if layer in unprocessed_nodes:
                    for node_data in unprocessed_nodes.pop(layer):
                        process_node(layer, node_data)
    
        name = config.get('name')
        input_tensors = []
        output_tensors = []
        for layer_data in config['input_layers']:
            layer_name, node_index, tensor_index = layer_data
            assert layer_name in created_layers
            layer = created_layers[layer_name]
            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors
            input_tensors.append(layer_output_tensors[tensor_index])
        for layer_data in config['output_layers']:
            layer_name, node_index, tensor_index = layer_data
            assert layer_name in created_layers
            layer = created_layers[layer_name]
            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors
            output_tensors.append(layer_output_tensors[tensor_index])
        return cls(inputs=input_tensors, outputs=output_tensors, name=name)
    
```

# Variable runtime value and type inside buggy function
## Buggy case 1
### input parameter runtime value and type for buggy function
config, value: `array of shape 4`, type: `dict`

cls, value: `<class 'keras.engine.training.Model'>`, type: `type`

### variable runtime value and type before buggy function return
created_layers, value: `{'input_1': <keras.engine.input_layer.InputLayer object at 0x123484e10>, 'layer_a': <keras.layers.core.Dense object at 0x123484c90>, 'reshape_2': <keras.layers.core.Reshape object at 0x1234824d0>, 'reshape_1': <keras.layers.core.Reshape object at 0x123482290>, 'concatenate_1': <keras.layers.merge.Concatenate object at 0x123482910>, 'layer_b': <keras.layers.core.Dense object at 0x123482510>}`, type: `dict`

unprocessed_nodes, value: `{}`, type: `dict`

layer, value: `<keras.layers.core.Dense object at 0x123482510>`, type: `Dense`

node_data, value: `[['concatenate_1', 0, 0, {}]]`, type: `list`

input_tensors, value: `[<tf.Tensor 'input_1_1:0' shape=(?, 1, 12) dtype=float32>]`, type: `list`

layer_name, value: `'layer_b'`, type: `str`

layer_data, value: `['layer_b', 0, 0]`, type: `list`

add_unprocessed_node, value: `<function Network.from_config.<locals>.add_unprocessed_node at 0x1231b4c20>`, type: `function`

process_layer, value: `<function Network.from_config.<locals>.process_layer at 0x1231b4950>`, type: `function`

node_data_list, value: `[[['concatenate_1', 0, 0, {}]]]`, type: `list`

node_index, value: `0`, type: `int`

process_node, value: `<function Network.from_config.<locals>.process_node at 0x1231b4830>`, type: `function`

name, value: `'model_1'`, type: `str`

output_tensors, value: `[<tf.Tensor 'layer_b_1/BiasAdd:0' shape=(?, 2) dtype=float32>]`, type: `list`

tensor_index, value: `0`, type: `int`

layer_output_tensors, value: `[<tf.Tensor 'layer_b_1/BiasAdd:0' shape=(?, 2) dtype=float32>]`, type: `list`

layer._inbound_nodes, value: `[<keras.engine.base_layer.Node object at 0x1233d7710>]`, type: `list`



# Expected variable value and type in tests
## Expected case 1
### Input parameter value and type
config, value: `array of shape 4`, type: `dict`

cls, value: `<class 'keras.engine.training.Model'>`, type: `type`

### Expected variable value and type before function return
created_layers, expected value: `{'input_1': <keras.engine.input_layer.InputLayer object at 0x129b37510>, 'layer_a': <keras.layers.core.Dense object at 0x129b371d0>, 'reshape_2': <keras.layers.core.Reshape object at 0x129b2fb10>, 'reshape_1': <keras.layers.core.Reshape object at 0x129b2fc90>, 'concatenate_1': <keras.layers.merge.Concatenate object at 0x129b2f990>, 'layer_b': <keras.layers.core.Dense object at 0x129b37350>}`, type: `dict`

unprocessed_nodes, expected value: `{<keras.layers.core.Reshape object at 0x129b2fb10>: [[['layer_a', 1, 0, {}]]], <keras.layers.core.Reshape object at 0x129b2fc90>: [[['input_1', 0, 0, {}]]], <keras.layers.merge.Concatenate object at 0x129b2f990>: [[['layer_a', 2, 0, {}], ['layer_a', 0, 0, {}]]], <keras.layers.core.Dense object at 0x129b37350>: [[['concatenate_1', 0, 0, {}]]], <keras.layers.core.Dense object at 0x129b371d0>: [[['reshape_1', 0, 0, {}]]]}`, type: `dict`

layer, expected value: `<keras.layers.core.Dense object at 0x129b371d0>`, type: `Dense`

node_data, expected value: `[['reshape_1', 0, 0, {}]]`, type: `list`

input_tensors, expected value: `[]`, type: `list`

input_data, expected value: `['reshape_1', 0, 0, {}]`, type: `list`

inbound_layer_name, expected value: `'reshape_1'`, type: `str`

inbound_node_index, expected value: `0`, type: `int`

inbound_tensor_index, expected value: `0`, type: `int`

kwargs, expected value: `{}`, type: `dict`

inbound_layer, expected value: `<keras.layers.core.Reshape object at 0x129b2fc90>`, type: `Reshape`

inbound_layer._inbound_nodes, expected value: `[]`, type: `list`

add_unprocessed_node, expected value: `<function Network.from_config.<locals>.add_unprocessed_node at 0x129b60560>`, type: `function`

layer_data, expected value: `array of shape 4`, type: `dict`

process_layer, expected value: `<function Network.from_config.<locals>.process_layer at 0x129b608c0>`, type: `function`

process_node, expected value: `<function Network.from_config.<locals>.process_node at 0x129b604d0>`, type: `function`

layer._inbound_nodes, expected value: `[]`, type: `list`



# A test function for the buggy function
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/keras_8/tests/keras/engine/test_topology.py

def test_layer_sharing_at_heterogeneous_depth_order():
    # This tests for the bug in this issue
    # https://github.com/keras-team/keras/issues/11159
    # It occurs with layer sharing at heterogeneous depth when
    # the layers need to be applied in an order that differs from
    # the order that occurs in the config.

    input_shape = (1, 12)
    input_layer = Input(shape=input_shape)

    A = Dense(12, name='layer_a')
    r1 = layers.Reshape((12,))(input_layer)
    Aout1 = A(r1)

    r2 = layers.Reshape((12,))(A(input_layer))
    Aout2 = A(r2)

    # Note: if the order of the layers in the concat is
    # changed to ([Aout1, Aout2]) the bug doesn't trigger
    c1 = layers.concatenate([Aout2, Aout1])
    output = Dense(2, name='layer_b')(c1)

    M = Model(inputs=input_layer, outputs=output)

    x_val = np.random.random((10,) + input_shape)
    output_val = M.predict(x_val)

    config = M.get_config()
    weights = M.get_weights()

    M2 = Model.from_config(config)
    M2.set_weights(weights)

    output_val_2 = M2.predict(x_val)
    np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)
```

## Error message from test function
```text
def test_layer_sharing_at_heterogeneous_depth_order():
        # This tests for the bug in this issue
        # https://github.com/keras-team/keras/issues/11159
        # It occurs with layer sharing at heterogeneous depth when
        # the layers need to be applied in an order that differs from
        # the order that occurs in the config.
    
        input_shape = (1, 12)
        input_layer = Input(shape=input_shape)
    
        A = Dense(12, name='layer_a')
        r1 = layers.Reshape((12,))(input_layer)
        Aout1 = A(r1)
    
        r2 = layers.Reshape((12,))(A(input_layer))
        Aout2 = A(r2)
    
        # Note: if the order of the layers in the concat is
        # changed to ([Aout1, Aout2]) the bug doesn't trigger
        c1 = layers.concatenate([Aout2, Aout1])
        output = Dense(2, name='layer_b')(c1)
    
        M = Model(inputs=input_layer, outputs=output)
    
        x_val = np.random.random((10,) + input_shape)
        output_val = M.predict(x_val)
    
        config = M.get_config()
        weights = M.get_weights()
    
>       M2 = Model.from_config(config)

tests/keras/engine/test_topology.py:793: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
keras/engine/network.py:1029: in from_config
    process_node(layer, node_data)
keras/engine/network.py:988: in process_node
    layer(unpack_singleton(input_tensors), **kwargs)
keras/engine/base_layer.py:431: in __call__
    self.build(unpack_singleton(input_shapes))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <keras.layers.merge.Concatenate object at 0x128b70750>
input_shape = [(None, 12), (None, 1, 12)]

    def build(self, input_shape):
        # Used purely for shape validation.
        if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError('A `Concatenate` layer should be called '
                             'on a list of at least 2 inputs')
        if all([shape is None for shape in input_shape]):
            return
        reduced_inputs_shapes = [list(shape) for shape in input_shape]
        shape_set = set()
        for i in range(len(reduced_inputs_shapes)):
            del reduced_inputs_shapes[i][self.axis]
            shape_set.add(tuple(reduced_inputs_shapes[i]))
        if len(shape_set) > 1:
            raise ValueError('A `Concatenate` layer requires '
                             'inputs with matching shapes '
                             'except for the concat axis. '
>                            'Got inputs shapes: %s' % (input_shape))
E           ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12), (None, 1, 12)]

keras/layers/merge.py:362: ValueError

```


# A GitHub issue title for this bug
```text
Bug in loading model with shared layers accross multiple levels.
```

## The associated detailed issue description
```text
There is a bug in the from_config method of the Keras Network class. This bug occurs when loading a model from a config when the model uses a layer that is shared at multiple depths and the input tensors to the shared layer are not in the order of the layers in the model config file.

For example, the following model creates a single dense layer then applies it to the reshaped input x2. It is then applied to the non-reshaped input x1, and again at the reshaped output.

sl = Dense(12)

x2 = Input((1, 12))
r2 = Reshape((12,))(x2)
r21 = sl(r2)

x1 = Input((1, 12))
r1 = Reshape((12,))(sl(x1))

r11 = sl(r1)
c1 = Concatenate()([r11, r21])
o1 = Dense(2)(c1)
The layers of the model are as follows:

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_2 (InputLayer)            (None, 1, 12)        0
__________________________________________________________________________________________________
dense_1 (Dense)                 multiple             156         reshape_1[0][0]
                                                                 input_2[0][0]
                                                                 reshape_2[0][0]
__________________________________________________________________________________________________
input_1 (InputLayer)            (None, 1, 12)        0
__________________________________________________________________________________________________
reshape_2 (Reshape)             (None, 12)           0           dense_1[1][0]
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 12)           0           input_1[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 24)           0           dense_1[2][0]
                                                                 dense_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 2)            50          concatenate_1[0][0]
==================================================================================================
Note that the dense_2 layer has reshape_1 and reshape_2 as inputs but those layers come after dense_2 in the list of layers.

The code in keras/engine/network.py contains the from_config method that loads the model. Then loading, the layer order of above is followed when recreating the model. At each layer Keras attempts to deserialize the layer using the inputs. When trying to deserialize the dense_2 layer Keras tries to create the first output but cannot because the input layers reshape_1 aren't available, Keras next tries to create the second output using input_2 which works because these layers are available. Keras will re-queue the first node (and third node) and will creates it at the next attempt when the input layers are available, unfortunately in doing this it swaps the output order of the output nodes of the dense_2 layer. The model loading then fails at the concatenate_1 layer as it uses the output nodes [0] and [2] of dense_2 but the output node [0] is now from input_2 which has the incorrect shape.

Note that if we change the order that we apply the shared layer so that model layer order changes this bug can be avoided. The code to reproduce the bug including code to create the layers in an order that doesn't trigger the bug is on this gist:
https://gist.github.com/adocherty/5f5c9983310ef2cf28e3ccb63ad39740

The error triggered by this script is as follows:

  File "example_load_bug.py", line 57, in <module>
    models.load_model("test.h5")
  File ".../lib/python3.6/site-packages/keras/engine/saving.py", line 260, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ".../lib/python3.6/site-packages/keras/engine/saving.py", line 334, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ".../lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserialize
    printable_module_name='layer')
  File ".../lib/python3.6/site-packages/keras/utils/generic_utils.py", line 145, in deserialize_keras_object
    list(custom_objects.items())))
  File ".../lib/python3.6/site-packages/keras/engine/network.py", line 1027, in from_config
    process_node(layer, node_data)
  File ".../lib/python3.6/site-packages/keras/engine/network.py", line 986, in process_node
    layer(unpack_singleton(input_tensors), **kwargs)
  File ".../lib/python3.6/site-packages/keras/engine/base_layer.py", line 431, in __call__
    self.build(unpack_singleton(input_shapes))
  File ".../lib/python3.6/site-packages/keras/layers/merge.py", line 354, in build
    'Got inputs shapes: %s' % (input_shape))
ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12), (None, 1, 12)]
```



