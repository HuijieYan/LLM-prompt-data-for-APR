{
    "tornado": [
        {
            "bugID": 2,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 1,
                "4": 1,
                "5": 1,
                "6": 0,
                "7": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 1,
                "4": 1,
                "5": 1,
                "6": 0,
                "7": 1
            },
            "start_line": 376,
            "file_name": "tornado/http1connection.py",
            "replace_code": "def write_headers(\n    self,\n    start_line: Union[httputil.RequestStartLine, httputil.ResponseStartLine],\n    headers: httputil.HTTPHeaders,\n    chunk: bytes = None,\n) -> \"Future[None]\":\n    lines = []\n    if self.is_client:\n        if isinstance(start_line, httputil.RequestStartLine):\n            self._request_start_line = start_line\n            lines.append(utf8(\"%s %s HTTP/1.1\" % (start_line.method, start_line.path)))\n            self._chunking_output = (\n                start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n                and \"Content-Length\" not in headers\n                and \"Transfer-Encoding\" not in headers\n            )\n    else:\n        assert isinstance(start_line, httputil.ResponseStartLine)\n        assert self._request_start_line is not None\n        assert self._request_headers is not None\n        self._response_start_line = start_line\n        lines.append(utf8(\"HTTP/1.1 %d %s\" % (start_line.code, start_line.reason)))\n        self._chunking_output = (\n            self._request_start_line.version == \"HTTP/1.1\"\n            and start_line.code not in (204, 304)\n            and (start_line.code < 100 or start_line.code >= 200)\n            and \"Content-Length\" not in headers\n            and \"Transfer-Encoding\" not in headers\n        )\n        if (\n            self._request_start_line.version == \"HTTP/1.1\"\n            and self._disconnect_on_finish\n        ):\n            headers[\"Connection\"] = \"close\"\n        if (\n            self._request_start_line.version == \"HTTP/1.0\"\n            and self._request_headers.get(\"Connection\", \"\").lower() == \"keep-alive\"\n        ):\n            headers[\"Connection\"] = \"Keep-Alive\"\n    if self._chunking_output:\n        headers[\"Transfer-Encoding\"] = \"chunked\"\n\n    # Process header lines and prepare data for writing\n    # ...\n    \n    future = Future()  # Create a new Future\n    # Modify the future based on stream availability\n    if self.stream.closed():\n        future.set_exception(iostream.StreamClosedError())\n    else:\n        data = b\"\\r\\n\".join(lines) + b\"\\r\\n\\r\\n\"\n        if chunk:\n            data += self._format_chunk(chunk)\n        self._pending_write = self.stream.write(data)\n        future_add_done_callback(self._pending_write, self._on_write_complete)\n    return future",
            "import_list": []
        }
    ]
}