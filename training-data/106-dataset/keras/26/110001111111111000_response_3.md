```python
def rnn(step_function, inputs, initial_states,
        go_backwards=False, mask=None, constants=None,
        unroll=False, input_length=None):
    ndim_ = len(inputs.get_shape())
    if ndim_ < 3:
        raise ValueError('Input should be at least 3D.')

    if unroll:
        steps = input_length
    else:
        steps = tf.shape(inputs)[0]

    def _step(time, output_ta_t, *states):
        current_input = input_ta.read(time)
        mask_t = mask_ta.read(time)
        output, new_states = step_function(current_input, tuple(states) + tuple(constants))
        if getattr(output, '_uses_learning_phase', False):
            global uses_learning_phase
            uses_learning_phase = True
        for state, new_state in zip(states, new_states):
            new_state.set_shape(state.get_shape())
        tiled_mask_t = tf.tile(mask_t,
                               tf.concat([[1], tf.shape(output)[1:]], axis=0))
        output = tf.where(tiled_mask_t, output, states[0])
        new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]
        output_ta_t = output_ta_t.write(time, output)
        return (time + 1, output_ta_t) + tuple(new_states)

    if mask is not None:
        if not initial_states:
            raise ValueError('No initial states provided! '
                             'When using masking in an RNN, you should '
                             'provide initial states '
                             '(and your step function should return '
                             'as its first state at time `t` '
                             'the output at time `t-1`).')
        if go_backwards:
            mask = reverse(mask, 0)

        def _step(time, output_ta_t, *states):
            """RNN step function.

            Args:
                time: Current timestep value.
                output_ta_t: TensorArray.
                *states: List of states.
    
            Returns:
                Tuple: `(time + 1,output_ta_t) + tuple(new_states)`
            """
            current_input = input_ta.read(time)
            mask_t = mask_ta.read(time)
            output, new_states = step_function(current_input, tuple(states) + tuple(constants))
            if getattr(output, '_uses_learning_phase', False):
                global uses_learning_phase
                uses_learning_phase = True
            for state, new_state in zip(states, new_states):
                new_state.set_shape(state.get_shape())
            tiled_mask_t = tf.tile(mask_t,
                                   tf.concat([[1], tf.shape(output)[1:]], axis=0))
            output = tf.where(tiled_mask_t, output, states[0])
            new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]
            output_ta_t = output_ta_t.write(time, output)
            return (time + 1, output_ta_t) + tuple(new_states)
    else:
        def _step(time, output_ta_t, *states):
            """RNN step function.

                Args:
                    time: Current timestep value.
                    output_ta_t: TensorArray.
                    *states: List of states.

                Returns:
                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`
                """
            current_input = input_ta.read(time)
            output, new_states = step_function(current_input,
                                               tuple(states) +
                                               tuple(constants))
            if getattr(output, '_uses_learning_phase', False):
                global uses_learning_phase
                uses_learning_phase = True
            for state, new_state in zip(states, new_states):
                new_state.set_shape(state.get_shape())
            output_ta_t = output_ta_t.write(time, output)
            return (time + 1, output_ta_t) + tuple(new_states)

    final_outputs = control_flow_ops.while_loop(
        cond=lambda time, *_: time < steps,
        body=_step,
        loop_vars=(time, output_ta) + states,
        parallel_iterations=32,
        swap_memory=True)
    last_time = final_outputs[0]
    output_ta = final_outputs[1]
    new_states = final_outputs[2:]

    outputs = output_ta.stack()
    last_output = output_ta.read(last_time - 1)

    axes = [1, 0] + list(range(2, len(outputs.get_shape())))
    outputs = tf.transpose(outputs, perm=axes)
    last_output._uses_learning_phase = uses_learning_phase
    return last_output, outputs, new_states
```