{
    "keras": [
        {
            "bugID": 22,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 1,
                "3": 1,
                "4": 1,
                "5": 1,
                "6": 0,
                "7": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 1,
                "3": 1,
                "4": 1,
                "5": 1,
                "6": 0,
                "7": 1
            },
            "start_line": 33,
            "file_name": "keras/engine/input_layer.py",
            "replace_code": "def __init__(self, input_shape=None, batch_size=None,\n                 batch_input_shape=None,\n                 dtype=None, input_tensor=None, sparse=False, name=None):\n        if not name:\n            prefix = 'input'\n            name = prefix + '_' + str(K.get_uid(prefix))\n        super().__init__(dtype=dtype, name=name, trainable=False)\n        self.built = True\n        self.sparse = sparse\n\n        # Modify input_tensor creation logic\n        if input_tensor is None:\n            if not batch_input_shape:\n                if not input_shape:\n                    raise ValueError('An Input layer should be passed either '\n                                     'a `batch_input_shape` or an `input_shape`.')\n                else:\n                    batch_input_shape = (batch_size,) + tuple(input_shape)\n            batch_input_shape = tuple(batch_input_shape)\n\n            # Create input_tensor as a placeholder using batch_input_shape\n            input_tensor = K.placeholder(shape=batch_input_shape,\n                                         dtype=dtype,\n                                         sparse=self.sparse,\n                                         name=name)\n            self.is_placeholder = True\n        else:\n            self.is_placeholder = False\n\n        input_tensor._keras_shape = batch_input_shape\n        input_tensor._uses_learning_phase = False\n        input_tensor._keras_history = (self, 0, 0)\n        \n        # Create an input node to add to self.outbound_node\n        # and set output_tensors' _keras_history.\n        Node(self,\n             inbound_layers=[],\n             node_indices=[],\n             tensor_indices=[],\n             input_tensors=[input_tensor],\n             output_tensors=[input_tensor],\n             input_masks=[None],\n             output_masks=[None],\n             input_shapes=[batch_input_shape],\n             output_shapes=[batch_input_shape])",
            "imports": [
                "from keras.engine import InputLayer",
                "from keras.engine import Layer",
                "from keras.engine import Node",
                "from keras.engine import InputSpec",
                "from keras.utils import conv_utils",
                "from keras import backend as K",
                "from keras.legacy.interfaces import interfaces",
                "import numpy as np"
            ]
        }
    ]
}