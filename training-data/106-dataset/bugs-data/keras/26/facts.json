{
    "1.1.1": "def rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n    \n    ndim = len(inputs.get_shape())\n    if ndim < 3:\n        raise ValueError('Input should be at least 3D.')\n\n    # Transpose to time-major, i.e.\n    # from (batch, time, ...) to (time, batch, ...)\n    axes = [1, 0] + list(range(2, ndim))\n    inputs = tf.transpose(inputs, (axes))\n\n    if mask is not None:\n        if mask.dtype != tf.bool:\n            mask = tf.cast(mask, tf.bool)\n        if len(mask.get_shape()) == ndim - 1:\n            mask = expand_dims(mask)\n        mask = tf.transpose(mask, axes)\n\n    if constants is None:\n        constants = []\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if unroll:\n        if not inputs.get_shape()[0]:\n            raise ValueError('Unrolling requires a '\n                             'fixed number of timesteps.')\n        states = initial_states\n        successive_states = []\n        successive_outputs = []\n\n        input_list = tf.unstack(inputs)\n        if go_backwards:\n            input_list.reverse()\n\n        if mask is not None:\n            mask_list = tf.unstack(mask)\n            if go_backwards:\n                mask_list.reverse()\n\n            for inp, mask_t in zip(input_list, mask_list):\n                output, new_states = step_function(inp, states + constants)\n                if getattr(output, '_uses_learning_phase', False):\n                    uses_learning_phase = True\n\n                # tf.where needs its condition tensor\n                # to be the same shape as its two\n                # result tensors, but in our case\n                # the condition (mask) tensor is\n                # (nsamples, 1), and A and B are (nsamples, ndimensions).\n                # So we need to\n                # broadcast the mask to match the shape of A and B.\n                # That's what the tile call does,\n                # it just repeats the mask along its second dimension\n                # n times.\n                tiled_mask_t = tf.tile(mask_t,\n                                       tf.stack([1, tf.shape(output)[1]]))\n\n                if not successive_outputs:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = successive_outputs[-1]\n\n                output = tf.where(tiled_mask_t, output, prev_output)\n\n                return_states = []\n                for state, new_state in zip(states, new_states):\n                    # (see earlier comment for tile explanation)\n                    tiled_mask_t = tf.tile(mask_t,\n                                           tf.stack([1, tf.shape(new_state)[1]]))\n                    return_states.append(tf.where(tiled_mask_t,\n                                                  new_state,\n                                                  state))\n                states = return_states\n                successive_outputs.append(output)\n                successive_states.append(states)\n            last_output = successive_outputs[-1]\n            new_states = successive_states[-1]\n            outputs = tf.stack(successive_outputs)\n        else:\n            for inp in input_list:\n                output, states = step_function(inp, states + constants)\n                if getattr(output, '_uses_learning_phase', False):\n                    uses_learning_phase = True\n                successive_outputs.append(output)\n                successive_states.append(states)\n            last_output = successive_outputs[-1]\n            new_states = successive_states[-1]\n            outputs = tf.stack(successive_outputs)\n\n    else:\n        if go_backwards:\n            inputs = reverse(inputs, 0)\n\n        states = tuple(initial_states)\n\n        time_steps = tf.shape(inputs)[0]\n        outputs, _ = step_function(inputs[0], initial_states + constants)\n        output_ta = tensor_array_ops.TensorArray(\n            dtype=outputs.dtype,\n            size=time_steps,\n            tensor_array_name='output_ta')\n        input_ta = tensor_array_ops.TensorArray(\n            dtype=inputs.dtype,\n            size=time_steps,\n            tensor_array_name='input_ta')\n        input_ta = input_ta.unstack(inputs)\n        time = tf.constant(0, dtype='int32', name='time')\n\n        if mask is not None:\n            if not states:\n                raise ValueError('No initial states provided! '\n                                 'When using masking in an RNN, you should '\n                                 'provide initial states '\n                                 '(and your step function should return '\n                                 'as its first state at time `t` '\n                                 'the output at time `t-1`).')\n            if go_backwards:\n                mask = reverse(mask, 0)\n\n            mask_ta = tensor_array_ops.TensorArray(\n                dtype=tf.bool,\n                size=time_steps,\n                tensor_array_name='mask_ta')\n            mask_ta = mask_ta.unstack(mask)\n\n            def _step(time, output_ta_t, *states):\n                \n                current_input = input_ta.read(time)\n                mask_t = mask_ta.read(time)\n                output, new_states = step_function(current_input,\n                                                   tuple(states) +\n                                                   tuple(constants))\n                if getattr(output, '_uses_learning_phase', False):\n                    global uses_learning_phase\n                    uses_learning_phase = True\n                for state, new_state in zip(states, new_states):\n                    new_state.set_shape(state.get_shape())\n                tiled_mask_t = tf.tile(mask_t,\n                                       tf.stack([1, tf.shape(output)[1]]))\n                output = tf.where(tiled_mask_t, output, states[0])\n                new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\n                output_ta_t = output_ta_t.write(time, output)\n                return (time + 1, output_ta_t) + tuple(new_states)\n        else:\n            def _step(time, output_ta_t, *states):\n                \n                current_input = input_ta.read(time)\n                output, new_states = step_function(current_input,\n                                                   tuple(states) +\n                                                   tuple(constants))\n                if getattr(output, '_uses_learning_phase', False):\n                    global uses_learning_phase\n                    uses_learning_phase = True\n                for state, new_state in zip(states, new_states):\n                    new_state.set_shape(state.get_shape())\n                output_ta_t = output_ta_t.write(time, output)\n                return (time + 1, output_ta_t) + tuple(new_states)\n\n        final_outputs = control_flow_ops.while_loop(\n            cond=lambda time, *_: time < time_steps,\n            body=_step,\n            loop_vars=(time, output_ta) + states,\n            parallel_iterations=32,\n            swap_memory=True)\n        last_time = final_outputs[0]\n        output_ta = final_outputs[1]\n        new_states = final_outputs[2:]\n\n        outputs = output_ta.stack()\n        last_output = output_ta.read(last_time - 1)\n\n    axes = [1, 0] + list(range(2, len(outputs.get_shape())))\n    outputs = tf.transpose(outputs, axes)\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, outputs, new_states\n",
    "1.1.2": "Iterates over the time dimension of a tensor.\n\n# Arguments\n    step_function: RNN step function.\n        Parameters:\n            inputs: tensor with shape `(samples, ...)` (no time dimension),\n                representing input for the batch of samples at a certain\n                time step.\n            states: list of tensors.\n        Returns:\n            outputs: tensor with shape `(samples, output_dim)`\n                (no time dimension).\n            new_states: list of tensors, same length and shapes\n                as 'states'. The first state in the list must be the\n                output tensor at the previous timestep.\n    inputs: tensor of temporal data of shape `(samples, time, ...)`\n        (at least 3D).\n    initial_states: tensor with shape (samples, output_dim)\n        (no time dimension),\n        containing the initial values for the states used in\n        the step function.\n    go_backwards: boolean. If True, do the iteration over the time\n        dimension in reverse order and return the reversed sequence.\n    mask: binary tensor with shape `(samples, time, 1)`,\n        with a zero for every element that is masked.\n    constants: a list of constant values passed at each step.\n    unroll: whether to unroll the RNN or to use a symbolic loop (`while_loop` or `scan` depending on backend).\n    input_length: not relevant in the TensorFlow implementation.\n        Must be specified if using unrolling with Theano.\n\n# Returns\n    A tuple, `(last_output, outputs, new_states)`.\n\n        last_output: the latest output of the rnn, of shape `(samples, ...)`\n        outputs: tensor with shape `(samples, time, ...)` where each\n            entry `outputs[s, t]` is the output of the step function\n            at time `t` for sample `s`.\n        new_states: list of tensors, latest states returned by\n            the step function, of shape `(samples, ...)`.\n\n# Raises\n    ValueError: if input dimension is less than 3.\n    ValueError: if `unroll` is `True` but input timestep is not a fixed number.\n    ValueError: if `mask` is provided (not `None`) but states is not provided\n        (`len(states)` == 0).",
    "1.2.1": null,
    "1.2.2": null,
    "1.2.3": null,
    "1.3.1": "/Volumes/SSD2T/bgp_envs/repos/keras_26/keras/backend/tensorflow_backend.py",
    "1.3.2": [
        "constant(value, dtype=None, shape=None, name=None)",
        "shape(x)",
        "ndim(x)",
        "dtype(x)",
        "zeros_like(x, dtype=None, name=None)",
        "cast(x, dtype)",
        "transpose(x)",
        "tile(x, n)",
        "expand_dims(x, axis=-1)",
        "stack(x, axis=0)",
        "reverse(x, axes)",
        "_step(time, output_ta_t, *states)",
        "_step(time, output_ta_t, *states)"
    ],
    "1.4.1": [
        "    def test_rnn_additional_states(self):\n        # implement a simple RNN with an additional state\n        # whose shape is different from that of the output\n        num_samples = 4\n        input_dim = 5\n        output_dim = 3\n        timesteps = 6\n\n        _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n        _, h0 = parse_shape_or_val((num_samples, output_dim))\n        _, wi = parse_shape_or_val((input_dim, output_dim))\n        _, wh = parse_shape_or_val((output_dim, output_dim))\n        mask = np.random.randint(2, size=(num_samples, timesteps))\n\n        x_k = K.variable(x)\n        h0_k = [K.variable(h0), K.variable(np.concatenate([h0, h0], axis=-1))]\n        wi_k = K.variable(wi)\n        wh_k = K.variable(wh)\n        mask_k = K.variable(mask)\n\n        def rnn_fn(x_k, h_k):\n            assert len(h_k) == 2\n            y_k = K.dot(x_k, wi_k) + K.dot(h_k[0], wh_k)\n            return y_k, [y_k, K.concatenate([y_k, y_k], axis=-1)]\n\n        # test default setup\n        last_output_list = []\n        outputs_list = []\n        state_list = []\n\n        kwargs_list = [\n            {'go_backwards': False, 'mask': None},\n            {'go_backwards': False, 'mask': None, 'unroll': True, 'input_length': timesteps},\n            {'go_backwards': True, 'mask': None},\n            {'go_backwards': True, 'mask': None, 'unroll': True, 'input_length': timesteps},\n            {'go_backwards': False, 'mask': mask_k},\n            {'go_backwards': False, 'mask': mask_k, 'unroll': True, 'input_length': timesteps},\n        ]\n\n        for (i, kwargs) in enumerate(kwargs_list):\n            last_y1, y1, h1 = reference_operations.rnn(x, [wi, wh, None], h0, **kwargs)\n            last_y2, y2, h2 = K.rnn(rnn_fn, x_k, h0_k, **kwargs)\n\n            assert len(h2) == 2\n            last_y2 = K.eval(last_y2)\n            y2 = K.eval(y2)\n            h11 = h1[:, -1]\n            h12 = np.concatenate([h1[:, -1], h1[:, -1]], axis=-1)\n            h21 = K.eval(h2[0])\n            h22 = K.eval(h2[1])\n\n            if kwargs['mask'] is not None:\n                last_y1 = last_y1 * np.expand_dims(mask[:, -1], -1)\n                last_y2 = last_y2 * np.expand_dims(mask[:, -1], -1)\n                y1 = y1 * np.expand_dims(mask, -1)\n                y2 = y2 * np.expand_dims(mask, -1)\n                h11 = h11 * np.expand_dims(mask[:, -1], -1)\n                h21 = h21 * np.expand_dims(mask[:, -1], -1)\n                h12 = h12 * np.expand_dims(mask[:, -1], -1)\n                h22 = h22 * np.expand_dims(mask[:, -1], -1)\n\n            last_output_list.append(last_y2)\n            outputs_list.append(y2)\n            state_list.append((h21, h22))\n\n            if i % 2 == 0:\n                assert_allclose(last_y1, last_y2, atol=1e-05)\n                assert_allclose(y1, y2, atol=1e-05)\n                assert_allclose(h11, h21, atol=1e-05)\n                assert_allclose(h12, h22, atol=1e-05)\n            else:\n                assert_allclose(last_output_list[i - 1], last_output_list[i], atol=1e-05)\n                assert_allclose(outputs_list[i - 1], outputs_list[i], atol=1e-05)\n                assert_allclose(state_list[i - 1][0], state_list[i][0], atol=1e-05)\n                assert_allclose(state_list[i - 1][1], state_list[i][1], atol=1e-05)"
    ],
    "1.4.2": [
        "/Volumes/SSD2T/bgp_envs/repos/keras_26/tests/keras/backend/backend_test.py"
    ],
    "2.1.1": [
        [
            "E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 1 in both shapes must be equal, but are 6 and 3. Shapes are [4,6] and [4,3]. for 'while_2/Select_2' (op: 'Select') with input shapes: [4,3], [4,6], [4,6].",
            "E       ValueError: Dimension 1 in both shapes must be equal, but are 6 and 3. Shapes are [4,6] and [4,3]. for 'while_2/Select_2' (op: 'Select') with input shapes: [4,3], [4,6], [4,6]."
        ]
    ],
    "2.1.2": [
        [
            "graph = <tensorflow.python.framework.ops.Graph object at 0x13fb04f10>\nnode_def = name: \"while_2/Select_2\"\nop: \"Select\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\n\ninputs = [<tf.Tensor 'while_2/Tile:0' shape=(4, 3) dtype=bool>, <tf.Tensor 'while_2/concat:0' shape=(4, 6) dtype=float32>, <tf.Tensor 'while_2/Identity_3:0' shape=(4, 6) dtype=float32>]\ncontrol_inputs = []\n\n    def _create_c_op(graph, node_def, inputs, control_inputs):\n      \"\"\"Creates a TF_Operation.\n    \n      Args:\n        graph: a `Graph`.\n        node_def: `node_def_pb2.NodeDef` for the operation to create.\n        inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of\n          `Tensor`s (corresponding to sequence inputs, e.g. \"int64 * N\",\n          \"list(int64)\"). The length of the list should be equal to the number of\n          inputs specified by this operation's op def.\n        control_inputs: A list of `Operation`s to set as control dependencies.\n    \n      Returns:\n        A wrapped TF_Operation*.\n      \"\"\"\n      # pylint: disable=protected-access\n      op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),\n                                      compat.as_str(node_def.name))\n      if node_def.device:\n        c_api.TF_SetDevice(op_desc, compat.as_str(node_def.device))\n      # Add inputs\n      for op_input in inputs:\n        if isinstance(op_input, (list, tuple)):\n          c_api.TF_AddInputList(op_desc, [t._as_tf_output() for t in op_input])\n        else:\n          c_api.TF_AddInput(op_desc, op_input._as_tf_output())\n    \n      # Add control inputs\n      for control_input in control_inputs:\n        c_api.TF_AddControlInput(op_desc, control_input._c_op)\n      # pylint: enable=protected-access\n    \n      # Add attrs\n      for name, attr_value in node_def.attr.items():\n        serialized = attr_value.SerializeToString()\n        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.\n        # It might be worth creating a convenient way to re-use the same status.\n        c_api.TF_SetAttrValueProto(op_desc, compat.as_str(name), serialized)\n    \n      try:\n>       c_op = c_api.TF_FinishOperation(op_desc)",
            "\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1864: InvalidArgumentError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <backend_test.TestBackend object at 0x140018c10>\n\n    def test_rnn_additional_states(self):\n        # implement a simple RNN with an additional state\n        # whose shape is different from that of the output\n        num_samples = 4\n        input_dim = 5\n        output_dim = 3\n        timesteps = 6\n    \n        _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n        _, h0 = parse_shape_or_val((num_samples, output_dim))\n        _, wi = parse_shape_or_val((input_dim, output_dim))\n        _, wh = parse_shape_or_val((output_dim, output_dim))\n        mask = np.random.randint(2, size=(num_samples, timesteps))\n    \n        x_k = K.variable(x)\n        h0_k = [K.variable(h0), K.variable(np.concatenate([h0, h0], axis=-1))]\n        wi_k = K.variable(wi)\n        wh_k = K.variable(wh)\n        mask_k = K.variable(mask)\n    \n        def rnn_fn(x_k, h_k):\n            assert len(h_k) == 2\n            y_k = K.dot(x_k, wi_k) + K.dot(h_k[0], wh_k)\n            return y_k, [y_k, K.concatenate([y_k, y_k], axis=-1)]\n    \n        # test default setup\n        last_output_list = []\n        outputs_list = []\n        state_list = []\n    \n        kwargs_list = [\n            {'go_backwards': False, 'mask': None},\n            {'go_backwards': False, 'mask': None, 'unroll': True, 'input_length': timesteps},\n            {'go_backwards': True, 'mask': None},\n            {'go_backwards': True, 'mask': None, 'unroll': True, 'input_length': timesteps},\n            {'go_backwards': False, 'mask': mask_k},\n            {'go_backwards': False, 'mask': mask_k, 'unroll': True, 'input_length': timesteps},\n        ]\n    \n        for (i, kwargs) in enumerate(kwargs_list):\n            last_y1, y1, h1 = reference_operations.rnn(x, [wi, wh, None], h0, **kwargs)\n>           last_y2, y2, h2 = K.rnn(rnn_fn, x_k, h0_k, **kwargs)\n\ntests/keras/backend/backend_test.py:643: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nkeras/backend/tensorflow_backend.py:2906: in rnn\n    swap_memory=True)\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3501: in while_loop\n    return_same_structure)\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3012: in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:2937: in _BuildLoop\n    body_result = body(*packed_vars_for_body)\nkeras/backend/tensorflow_backend.py:2874: in _step\n    new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\nkeras/backend/tensorflow_backend.py:2874: in <listcomp>\n    new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:324: in new_func\n    return func(*args, **kwargs)\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:180: in wrapper\n    return target(*args, **kwargs)\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3270: in where\n    return gen_math_ops.select(condition=condition, x=x, y=y, name=name)\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py:9226: in select\n    \"Select\", condition=condition, t=x, e=y, name=name)\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:788: in _apply_op_helper\n    op_def=op_def)\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:507: in new_func\n    return func(*args, **kwargs)\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3616: in create_op\n    op_def=op_def)\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2027: in __init__\n    control_input_ops)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ngraph = <tensorflow.python.framework.ops.Graph object at 0x13fb04f10>\nnode_def = name: \"while_2/Select_2\"\nop: \"Select\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\n\ninputs = [<tf.Tensor 'while_2/Tile:0' shape=(4, 3) dtype=bool>, <tf.Tensor 'while_2/concat:0' shape=(4, 6) dtype=float32>, <tf.Tensor 'while_2/Identity_3:0' shape=(4, 6) dtype=float32>]\ncontrol_inputs = []\n\n    def _create_c_op(graph, node_def, inputs, control_inputs):\n      \"\"\"Creates a TF_Operation.\n    \n      Args:\n        graph: a `Graph`.\n        node_def: `node_def_pb2.NodeDef` for the operation to create.\n        inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of\n          `Tensor`s (corresponding to sequence inputs, e.g. \"int64 * N\",\n          \"list(int64)\"). The length of the list should be equal to the number of\n          inputs specified by this operation's op def.\n        control_inputs: A list of `Operation`s to set as control dependencies.\n    \n      Returns:\n        A wrapped TF_Operation*.\n      \"\"\"\n      # pylint: disable=protected-access\n      op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),\n                                      compat.as_str(node_def.name))\n      if node_def.device:\n        c_api.TF_SetDevice(op_desc, compat.as_str(node_def.device))\n      # Add inputs\n      for op_input in inputs:\n        if isinstance(op_input, (list, tuple)):\n          c_api.TF_AddInputList(op_desc, [t._as_tf_output() for t in op_input])\n        else:\n          c_api.TF_AddInput(op_desc, op_input._as_tf_output())\n    \n      # Add control inputs\n      for control_input in control_inputs:\n        c_api.TF_AddControlInput(op_desc, control_input._c_op)\n      # pylint: enable=protected-access\n    \n      # Add attrs\n      for name, attr_value in node_def.attr.items():\n        serialized = attr_value.SerializeToString()\n        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.\n        # It might be worth creating a convenient way to re-use the same status.\n        c_api.TF_SetAttrValueProto(op_desc, compat.as_str(name), serialized)\n    \n      try:\n        c_op = c_api.TF_FinishOperation(op_desc)\n      except errors.InvalidArgumentError as e:\n        # Convert to ValueError for backwards compatibility.\n>       raise ValueError(str(e))",
            "\n../../envs/keras_26/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1867: ValueError"
        ]
    ],
    "2.1.3": null,
    "2.1.4": null,
    "2.1.5": null,
    "2.1.6": null,
    "3.1.1": null,
    "3.1.2": null
}