{
    "keras": [
        {
            "bugID": 26,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 1,
                "3": 1,
                "4": 0,
                "5": 1,
                "6": 1,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 0,
                "1.4.2": 0,
                "2.1.1": 0,
                "2.1.2": 0,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 1,
                "4": 0,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "start_line": 2676,
            "file_name": "keras/backend/tensorflow_backend.py",
            "replace_code": "def rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n    import tensorflow as tf\n    import tensorflow.python.ops.control_flow_ops as control_flow_ops\n    import tensorflow.python.ops.tensor_array_ops as tensor_array_ops\n    \"\"\"Iterates over the time dimension of a tensor.\n    \n    # Arguments\n        step_function: RNN step function.\n        .\n        .\n        .\n        # Returns\n            A tuple, `(last_output, outputs, new_states)`.\n    \n                last_output: the latest output of the rnn, of shape `(samples, ...)`\n                outputs: tensor with shape `(samples, time, ...)` where each\n                    entry `outputs[s, t]` is the output of the step function\n                    at time `t` for sample `s`.\n                new_states: list of tensors, latest states returned by\n                    the step function, of shape `(samples, ...)`.\n    \n    # Raises\n        ValueError: if input dimension is less than 3.\n        ValueError: if `unroll` is `True` but input timestep is not a fixed number.\n        ValueError: if `mask` is provided (not `None`) but states is not provided\n            (`len(states)` == 0).\n    \"\"\"\n    import tensorflow as tf\n    import tensorflow.python.ops.control_flow_ops as control_flow_ops\n    import tensorflow.python.ops.tensor_array_ops as tensor_array_ops\n    \n    def reverse(x, axes):\n        return tf.reverse(x, axis=axes)\n    \n    def expand_dims(x, axis=-1):\n        return tf.expand_dims(x, axis=axis)\n    \n    def zeros_like(x):\n        return tf.zeros_like(x)\n    \n    def cast(x, dtype):\n        return tf.cast(x, dtype=dtype)\n    \n    def stack(x, axis=0):\n        return tf.stack(x, axis=axis)\n    \n    def _step(time, output_ta_t, *states):\n        # ... omitted code ...\n        pass\n    \n    def unstack(input_tensor):\n        return tf.unstack(input_tensor)\n    \n    def tile(x, n):\n        return tf.tile(x, n)\n    \n    def transpose(x):\n        return tf.transpose(x)\n    \n    def rnn_change_data(inputs):\n        shape = list(inputs.get_shape())\n        axes = [1, 0]\n        axes.extend(list(range(2, len(shape))))\n        return tf.transpose(inputs, perm=axes)\n    \n    def rnn_change_mask(mask, axes):\n        if mask.dtype != tf.bool:\n            mask = cast(mask, tf.bool)\n        if len(mask.get_shape()) == len(axes) - 1:\n            mask = expand_dims(mask)\n        return tf.transpose(mask, axes)\n    \n    def rnn_initial_constants(constants):\n        if constants is None:\n            return []\n        return constants\n    \n    def rnn_change_inputs_go_backwards(input_list, go_backwards):\n        if go_backwards:\n            input_list = reverse(input_list, axis=0)\n        return input_list\n    \n    def rnn_body_func_with_mask(inp, mask_ta, states):\n        current_input = inp\n        mask_t = mask_ta.read(time)\n        output, new_states = step_function(current_input, states + constants)\n        if getattr(output, '_uses_learning_phase', False):\n            global uses_learning_phase\n            uses_learning_phase = True\n        return create_result(output, states, new_states, mask_t, mask_t, mask_t, zeros_like(output), output_ta_t)\n    \n    def rnn_body_func_without_mask(inp, states):\n        current_input = inp\n        output, new_states = step_function(current_input, states + constants)\n        if getattr(output, '_uses_learning_phase', False):\n            global uses_learning_phase\n            uses_learning_phase = True\n        return create_result(output, states, new_states, None, states[0], None, None, output_ta_t)\n    \n    def rnn_loop(time, input_ta, output_ta, states, mask_ta=None):\n        if mask_ta is not None:\n            return rnn_body_func_with_mask(input_ta.read(time), mask_ta, states)\n        else:\n            return rnn_body_func_without_mask(input_ta.read(time), states)\n    \n    def create_result(output, return_states, new_states, tiled_mask_t, prev_output, output_ta_t, modified_states, modified_outputs):\n        output = tf.where(tiled_mask_t, output, prev_output)\n        return_states = [tf.where(tiled_mask_t, new_states[i], modified_states[i]) for i in range(len(return_states))]\n        modified_outputs = stack([output, output])\n        modified_states = return_states\n        result = control_flow_ops.while_loop(rnn_condition, rnn_loop, loop_vars, parallel_iterations=32, swap_memory=True)\n        return modified_outputs, modified_states\n    \n    def rnn_condition(time, *_):\n        return time < time_steps\n    \n    def tensor_array(stack, size, output_dtype, tensor_array_name):\n        return stack(dtype=output_dtype, size=size, tensor_array_name=tensor_array_name)\n    \n    def rnn_unroll_with_mask(input_list, mask_list):\n        successive_states = []\n        successive_outputs = []\n        if go_backwards:\n            input_list = reverse(input_list, axis=0)\n            mask_list = reverse(mask_list, axis=0)\n        for inp, mask_t in zip(input_list, mask_list):\n            output, new_states = step_function(inp, states + constants)\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n            tiled_mask_t = tile(mask_t, tf.stack([1, tf.shape(output)[1]]))\n            if not successive_outputs:\n                prev_output = zeros_like(output)\n            else:\n                prev_output = successive_outputs[-1]\n            output, successive_states = create_result(output, states, new_states, tiled_mask_t, prev_output, output, states, successive_states)\n        last_output = successive_outputs[-1]\n        new_states = successive_states[-1]\n        outputs = stack(successive_outputs)\n        return last_output, outputs, new_states\n    \n    def rnn_unroll_without_mask(input_list):\n        states = initial_states\n        successive_states = []\n        successive_outputs = []\n        for inp in input_list:\n            output, states = step_function(inp, states + constants)\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n            successive_outputs.append(output)\n            successive_states.append(states)\n        last_output = successive_outputs[-1]\n        new_states = successive_states[-1]\n        outputs = stack(successive_outputs)\n        return last_output, outputs, new_states\n    \n    def rnn_transpose(outputs, axes):\n        return tf.transpose(outputs, axes)\n    \n    def main_rn_function():\n        if go_backwards:\n            inputs = reverse(inputs, 0)\n        states = tuple(initial_states)\n        time_steps = tf.shape(inputs)[0]\n        outputs, _ = step_function(inputs[0], initial_states + constants)\n        output_ta = tensor_array(tensor_array_ops.TensorArray, time_steps, outputs.dtype, 'output_ta')\n        input_ta = tensor_array(tensor_array_ops.TensorArray, time_steps, inputs.dtype, 'input_ta')\n        input_ta = input_ta.unstack(inputs)\n        time = tf.constant(0, dtype='int32', name='time')\n    \n        if mask is not None:\n            if not states:\n                raise ValueError('No initial states provided! '\n                                 'When using masking in an RNN, you should '\n                                 'provide initial states '\n                                 '(and your step function should return '\n                                 'as its first state at time `t` '\n                                 'the output at time `t-1`).')\n            if go_backwards:\n                mask = reverse(mask, 0)\n            mask_ta = tensor_array(tensor_array_ops.TensorArray, time_steps, tf.bool, 'mask_ta')\n            mask_ta = mask_ta.unstack(mask)\n    \n            return rnn_unroll_with_mask(input_ta, mask_ta)\n        else:\n            return rnn_unroll_without_mask(input_ta)\n    \n    return main_rn_function()"
        }
    ]
}