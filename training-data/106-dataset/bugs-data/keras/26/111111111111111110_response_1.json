{
    "keras": [
        {
            "bugID": 26,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 0
            },
            "strata": {
                "1": 1,
                "2": 1,
                "3": 1,
                "4": 1,
                "5": 1,
                "6": 1,
                "7": 0
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 1,
                "1.3.2": 1,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 0
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 1,
                "4": 1,
                "5": 0,
                "6": 0,
                "7": 0
            },
            "start_line": 2676,
            "file_name": "backend/tensorflow_backend.py",
            "replace_code": "def rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n    import tensorflow as tf\n    import numpy as np\n    from keras.backend import reference_operations\n    \"\"\"Iterates over the time dimension of a tensor.\n    \n    # Arguments\n        step_function: RNN step function.\n            Parameters:\n                inputs: tensor with shape `(samples, ...)` (no time dimension),\n                    representing input for the batch of samples at a certain\n                    time step.\n                states: list of tensors.\n            Returns:\n                outputs: tensor with shape `(samples, output_dim)`\n                    (no time dimension).\n                new_states: list of tensors, same length and shapes\n                    as 'states'. The first state in the list must be the\n                    output tensor at the previous timestep.\n        inputs: tensor of temporal data of shape `(samples, time, ...)`\n            (at least 3D).\n        initial_states: tensor with shape (samples, output_dim)\n            (no time dimension),\n            containing the initial values for the states used in\n            the step function.\n        go_backwards: boolean. If True, do the iteration over the time\n            dimension in reverse order and return the reversed sequence.\n        mask: binary tensor with shape `(samples, time, 1)`,\n            with a zero for every element that is masked.\n        constants: a list of constant values passed at each step.\n        unroll: whether to unroll the RNN or to use a symbolic loop (`while_loop` or `scan` depending on backend).\n        input_length: not relevant in the TensorFlow implementation.\n            Must be specified if using unrolling with Theano.\n    \n    # Returns\n        A tuple, `(last_output, outputs, new_states)`.\n    \n            last_output: the latest output of the rnn, of shape `(samples, ...)`\n            outputs: tensor with shape `(samples, time, ...)` where each\n                entry `outputs[s, t]` is the output of the step function\n                at time `t` for sample `s`.\n            new_states: list of tensors, latest states returned by\n                the step function, of shape `(samples, ...)`.\n    \n    # Raises\n        ValueError: if input dimension is less than 3.\n        ValueError: if `unroll` is `True` but input timestep is not a fixed number.\n        ValueError: if `mask` is provided (not `None`) but states is not provided\n            (`len(states)` == 0).\n    \"\"\"\n    import tensorflow as tf\n    import numpy as np\n    from keras.backend import reference_operations\n    \n    def reverse(x, axes):\n        return tf.reverse(x, axes)\n    \n    def _step(time, output_ta_t, *states):\n        \"\"\"RNN step function.\n    \n        # Arguments\n            time: Current timestep value.\n            output_ta_t: TensorArray.\n            *states: List of states.\n    \n        # Returns\n            Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n        \"\"\"\n        current_input = input_ta.read(time)\n    \n        mask_t = mask_ta.read(time) if mask is not None else None\n        tiled_mask_t = tf.tile(mask_t, tf.stack([1, tf.shape(output)[1]])) if mask is not None else None\n    \n        output, new_states = step_function(current_input, [s.name for s in states] + constants)\n        if getattr(output, '_uses_learning_phase', False):\n            global uses_learning_phase\n            uses_learning_phase = True\n    \n        output = tf.where(tiled_mask_t, output, states[0]) if mask is not None else output\n        new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))] if mask is not None else new_states\n    \n        output_ta_t = output_ta_t.write(time, output)\n    \n        return (time + 1, output_ta_t) + new_states\n    \n    def rnn_fn(x_k, h_k):\n        assert len(h_k) == 2\n        y_k = K.dot(x_k, wi_k) + K.dot(h_k[0], wh_k)\n        return y_k, [y_k, K.concatenate([y_k, y_k], axis=-1)]\n    \n    last_output_list = []\n    outputs_list = []\n    state_list = []\n    \n    kwargs_list = [\n        {'go_backwards': False, 'mask': None},\n        {'go_backwards': False, 'mask': None, 'unroll': True, 'input_length': timesteps},\n        {'go_backwards': True, 'mask': None},\n        {'go_backwards': True, 'mask': None, 'unroll': True, 'input_length': timesteps},\n        {'go_backwards': False, 'mask': mask_k},\n        {'go_backwards': False, 'mask': mask_k, 'unroll': True, 'input_length': timesteps},\n    ]\n    \n    for (i, kwargs) in enumerate(kwargs_list):\n        last_y1, y1, h1 = reference_operations.rnn(x, [wi, wh, None], h0, **kwargs)\n        last_y2, y2, h2 = K.rnn(rnn_fn, x_k, h0_k, **kwargs)\n    \n        assert len(h2) == 2\n        last_y2 = K.eval(last_y2)\n        y2 = K.eval(y2)\n        h11 = h1[:, -1]\n        h12 = np.concatenate([h1[:, -1], h1[:, -1]], axis=-1)\n        h21 = K.eval(h2[0])\n        h22 = K.eval(h2[1])\n    \n        if kwargs['mask'] is not None:\n            last_y1 = last_y1 * np.expand_dims(mask[:, -1], -1)\n            last_y2 = last_y2 * np.expand_dims(mask[:, -1], -1)\n            y1 = y1 * np.expand_dims(mask, -1)\n            y2 = y2 * np.expand_dims(mask, -1)\n            h11 = h11 * np.expand_dims(mask[:, -1], -1)\n            h21 = h21 * np.expand_dims(mask[:, -1], -1)\n            h12 = h12 * np.expand_dims(mask[:, -1], -1)\n            h22 = h22 * np.expand_dims(mask[:, -1], -1)\n    \n        last_output_list.append(last_y2)\n        outputs_list.append(y2)\n        state_list.append((h21, h22))\n    \n        if i % 2 == 0:\n            assert_allclose(last_y1, last_y2, atol=1e-05)\n            assert_allclose(y1, y2, atol=1e-05)\n            assert_allclose(h11, h21, atol=1e-05)\n            assert_allclose(h12, h22, atol=1e-05)\n        else:\n            assert_allclose(last_output_list[i - 1], last_output_list[i], atol=1e-05)\n            assert_allclose(outputs_list[i - 1], outputs_list[i], atol=1e-05)\n            assert_allclose(state_list[i - 1][0], state_list[i][0], atol=1e-05)\n            assert_allclose(state_list[i - 1][1], state_list[i][1], atol=1e-05)"
        }
    ]
}