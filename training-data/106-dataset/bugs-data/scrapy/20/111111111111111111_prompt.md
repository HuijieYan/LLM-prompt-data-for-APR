Please fix the function/method provided below and provide the corrected function/method as the output.


# Buggy function source code
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/scrapy_20/scrapy/spiders/sitemap.py

# relative function's signature in this file
def iterloc(it, alt=False):
    # ... omitted code ...
    pass

# relative function's signature in this file
def _parse_sitemap(self, response):
    # ... omitted code ...
    pass

# relative function's signature in this file
def _get_sitemap_body(self, response):
    # ... omitted code ...
    pass

# class declaration containing the buggy function
class SitemapSpider(Spider):
    # ... omitted code ...


    # signature of a relative function in this class
    def _parse_sitemap(self, response):
        # ... omitted code ...
        pass

    # signature of a relative function in this class
    def _get_sitemap_body(self, response):
        # ... omitted code ...
        pass



    # this is the buggy function you need to fix
    def _parse_sitemap(self, response):
        if response.url.endswith('/robots.txt'):
            for url in sitemap_urls_from_robots(response.body):
                yield Request(url, callback=self._parse_sitemap)
        else:
            body = self._get_sitemap_body(response)
            if body is None:
                logger.warning("Ignoring invalid sitemap: %(response)s",
                               {'response': response}, extra={'spider': self})
                return
    
            s = Sitemap(body)
            if s.type == 'sitemapindex':
                for loc in iterloc(s, self.sitemap_alternate_links):
                    if any(x.search(loc) for x in self._follow):
                        yield Request(loc, callback=self._parse_sitemap)
            elif s.type == 'urlset':
                for loc in iterloc(s):
                    for r, c in self._cbs:
                        if r.search(loc):
                            yield Request(loc, callback=c)
                            break
    
```

# Variable runtime value and type inside buggy function
## Buggy case 1
### input parameter runtime value and type for buggy function
response.url, value: `'http://www.example.com/robots.txt'`, type: `str`

response, value: `<200 http://www.example.com/robots.txt>`, type: `TextResponse`

response.text, value: `'# Sitemap files\nSitemap: http://example.com/sitemap.xml\nSitemap: http://example.com/sitemap-product-index.xml\n'`, type: `str`

self.sitemap_alternate_links, value: `False`, type: `bool`

self._follow, value: `[re.compile('')]`, type: `list`

self._cbs, value: `[(re.compile(''), <bound method Spider.parse of <SitemapSpider 'example.com' at 0x10b65fc40>>)]`, type: `list`

### variable runtime value and type before buggy function return
url, 



# A test function for the buggy function
```python
# file name: /Volumes/SSD2T/bgp_envs/repos/scrapy_20/tests/test_spider.py

    def test_get_sitemap_urls_from_robotstxt(self):
        robots = b"""# Sitemap files
Sitemap: http://example.com/sitemap.xml
Sitemap: http://example.com/sitemap-product-index.xml
"""

        r = TextResponse(url="http://www.example.com/robots.txt", body=robots)
        spider = self.spider_class("example.com")
        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
                         ['http://example.com/sitemap.xml',
                          'http://example.com/sitemap-product-index.xml'])
```

## Error message from test function
```text
self = <tests.test_spider.SitemapSpiderTest testMethod=test_get_sitemap_urls_from_robotstxt>

        def test_get_sitemap_urls_from_robotstxt(self):
            robots = b"""# Sitemap files
    Sitemap: http://example.com/sitemap.xml
    Sitemap: http://example.com/sitemap-product-index.xml
    """
    
            r = TextResponse(url="http://www.example.com/robots.txt", body=robots)
            spider = self.spider_class("example.com")
>           self.assertEqual([req.url for req in spider._parse_sitemap(r)],
                             ['http://example.com/sitemap.xml',
                              'http://example.com/sitemap-product-index.xml'])

/Volumes/SSD2T/bgp_envs/repos/scrapy_20/tests/test_spider.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Volumes/SSD2T/bgp_envs/repos/scrapy_20/tests/test_spider.py:339: in <listcomp>
    self.assertEqual([req.url for req in spider._parse_sitemap(r)],
/Volumes/SSD2T/bgp_envs/repos/scrapy_20/scrapy/spiders/sitemap.py:35: in _parse_sitemap
    for url in sitemap_urls_from_robots(response.body):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

robots_text = b'# Sitemap files\nSitemap: http://example.com/sitemap.xml\nSitemap: http://example.com/sitemap-product-index.xml\n'

    def sitemap_urls_from_robots(robots_text):
        """Return an iterator over all sitemap urls contained in the given
        robots.txt file
        """
        for line in robots_text.splitlines():
>           if line.lstrip().startswith('Sitemap:'):
E           TypeError: startswith first arg must be bytes or a tuple of bytes, not str

/Volumes/SSD2T/bgp_envs/repos/scrapy_20/scrapy/utils/sitemap.py:42: TypeError

```


# A GitHub issue title for this bug
```text
PY3: SitemapSpider fail to extract sitemap URLs from robots.txt in Scrapy 1.1.0rc1
```

## The associated detailed issue description
```text
Environment
Mac OS X 10.10.5
Python 3.4.2
Scrapy 1.1.0rc1
Steps to Reproduce
Save the following spider as sitemap_spider.py.

from scrapy.spiders import SitemapSpider


class BlogSitemapSpider(SitemapSpider):
   name = "blog_sitemap"
   allowed_domains = ["blog.scrapinghub.com"]

   sitemap_urls = [
       'https://blog.scrapinghub.com/robots.txt',
   ]
   sitemap_rules = [
       (r'/2016/', 'parse'),
   ]

   def parse(self, response):
       pass
Run the following command.

$ scrapy runspider sitemap_spider.py
Expected Results
The spider crawl several pages according to the sitemaps without error.

Actual Results
The spider fail to extract sitemap URLs from robots.txt. No pages are crawled.

$ scrapy runspider sitemap_spider.py 2016-02-06 20:55:51 [scrapy] INFO: Scrapy 1.1.0rc1 started (bot: scrapybot)
2016-02-06 20:55:51 [scrapy] INFO: Overridden settings: {}
2016-02-06 20:55:52 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
2016-02-06 20:55:52 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-02-06 20:55:52 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-02-06 20:55:52 [scrapy] INFO: Enabled item pipelines:
[]
2016-02-06 20:55:52 [scrapy] INFO: Spider opened
2016-02-06 20:55:52 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-02-06 20:55:52 [scrapy] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/robots.txt> (referer: None)
2016-02-06 20:55:52 [scrapy] ERROR: Spider error processing <GET https://blog.scrapinghub.com/robots.txt> (referer: None)
Traceback (most recent call last):
  File "/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/spiders/sitemap.py", line 35, in _parse_sitemap
    for url in sitemap_urls_from_robots(response.body):
  File "/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/utils/sitemap.py", line 42, in sitemap_urls_from_robots
    if line.lstrip().startswith('Sitemap:'):
TypeError: startswith first arg must be bytes or a tuple of bytes, not str
2016-02-06 20:55:52 [scrapy] INFO: Closing spider (finished)
2016-02-06 20:55:52 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 231,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1009,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 2, 6, 11, 55, 52, 570098),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2016, 2, 6, 11, 55, 52, 97618)}
2016-02-06 20:55:52 [scrapy] INFO: Spider closed (finished)
```



# Instructions

1. Analyze the test case and its relationship with the error message, if applicable.
2. Identify the potential error location within the problematic function.
3. Explain the reasons behind the occurrence of the bug.
4. Suggest possible approaches for fixing the bug.
5. Present the corrected code for the problematic function.