The bug in the provided function is that it is not handling the case when the response is a robots.txt file correctly. It is trying to extract sitemap URLs from the robots.txt file but is not sending the correct type of request to parse those sitemap URLs.

In the given test case, when the response.url ends with '/robots.txt', the function is extracting sitemap URLs from the response body and then yielding Requests with the same callback, which is the same _parse_sitemap function. This will result in an infinite loop.

To fix this bug, we need to change the callback for the Requests when extracting sitemap URLs from robots.txt to the appropriate callback based on the type of sitemap (e.g., _parse_sitemap for sitemapindex and the appropriate callback for urlset).

Here's the corrected code for the problematic function:

```python
def _parse_sitemap(self, response):
    if response.url.endswith('/robots.txt'):
        for url in sitemap_urls_from_robots(response.text):
            yield Request(url, callback=self._parse_sitemap_robots)
    else:
        body = self._get_sitemap_body(response)
        if body is None:
            logger.warning("Ignoring invalid sitemap: %(response)s", {'response': response}, extra={'spider': self})
            return

        s = Sitemap(body)
        if s.type == 'sitemapindex':
            for loc in iterloc(s, self.sitemap_alternate_links):
                if any(x.search(loc) for x in self._follow):
                    yield Request(loc, callback=self._parse_sitemap)
        elif s.type == 'urlset':
            for loc in iterloc(s):
                for r, c in self._cbs:
                    if r.search(loc):
                        yield Request(loc, callback=c)
                        break

def _parse_sitemap_robots(self, response):
    body = self._get_sitemap_body(response)
    if body is None:
        logger.warning("Ignoring invalid sitemap: %(response)s", {'response': response}, extra={'spider': self})
        return

    s = Sitemap(body)
    if s.type == 'sitemapindex':
        for loc in iterloc(s, self.sitemap_alternate_links):
            if any(x.search(loc) for x in self._follow):
                yield Request(loc, callback=self._parse_sitemap_robots)
    elif s.type == 'urlset':
        for loc in iterloc(s):
            for r, c in self._cbs:
                if r.search(loc):
                    yield Request(loc, callback=c)
                    break
```

In the corrected code, a new function `_parse_sitemap_robots` is added to handle the callback when extracting sitemap URLs from robots.txt. This new function makes sure to handle the parsed sitemap URLs appropriately based on their type and the provided callbacks. This will prevent the infinite loop and properly handle sitemap URLs from robots.txt.