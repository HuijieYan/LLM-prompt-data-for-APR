The error is occurring in the `_parse_sitemap` function. There are two issues in the code:
1. On the line `for url in sitemap_urls_from_robots(response.body):`, the `response.body` should be replaced with `response.text` as the value of `response` is a `TextResponse` object, and we want to extract sitemap URLs from the text content.
2. The comparison on the line `if line.lstrip().startswith('Sitemap:')` is leading to a TypeError due to the input type mismatch. This is likely to be fixed by converting the string to bytes.

To fix this issue, we should make the following modifications to the `_parse_sitemap` function:

```python
def _parse_sitemap(self, response):
    if response.url.endswith('/robots.txt'):
        for url in sitemap_urls_from_robots(response.text.encode('utf-8')):
            yield Request(url, callback=self._parse_sitemap)
    else:
        body = self._get_sitemap_body(response)
        if body is None:
            logger.warning("Ignoring invalid sitemap: %(response)s",
                           {'response': response}, extra={'spider': self})
            return

        s = Sitemap(body)
        if s.type == 'sitemapindex':
            for loc in iterloc(s, self.sitemap_alternate_links):
                if any(x.search(loc) for x in self._follow):
                    yield Request(loc, callback=self._parse_sitemap)
        elif s.type == 'urlset':
            for loc in iterloc(s):
                for r, c in self._cbs:
                    if r.search(loc):
                        yield Request(loc, callback=c)
                        break
```

The changes made include using `response.text.encode('utf-8')` to convert the text to bytes when extracting sitemap URLs from the robots.txt file.

These modifications should address the issues and allow the spider to extract sitemap URLs from robots.txt as expected.