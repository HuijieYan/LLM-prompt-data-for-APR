The bug in the provided function is that it enters an infinite loop when the response.url ends with '/robots.txt'. This is because the function makes a recursive call to `_parse_sitemap` for each URL found in the robots.txt file.

To fix this bug, we need to modify the condition for handling the robots.txt file to prevent the infinite loop. We can accomplish this by checking whether the URL in the robots.txt file ends with '.xml' before making the recursive call.

Here's the corrected code for the problematic function:

```python
def _parse_sitemap(self, response):
    if response.url.endswith('/robots.txt'):
        for url in sitemap_urls_from_robots(response.text):  # Use response.text instead of response.body
            if url.endswith('.xml'):  # Check if the URL in robots.txt ends with '.xml' to prevent infinite loop
                yield Request(url, callback=self._parse_sitemap)
    else:
        body = self._get_sitemap_body(response)
        if body is None:
            logger.warning("Ignoring invalid sitemap: %(response)s", {'response': response}, extra={'spider': self})
            return

        s = Sitemap(body)
        if s.type == 'sitemapindex':
            for loc in iterloc(s, self.sitemap_alternate_links):
                if any(x.search(loc) for x in self._follow):
                    yield Request(loc, callback=self._parse_sitemap)
        elif s.type == 'urlset':
            for loc in iterloc(s):
                for r, c in self._cbs:
                    if r.search(loc):
                        yield Request(loc, callback=c)
                        break
```

With this modification, the function will no longer enter an infinite loop when processing the robots.txt file.