{
    "1.1.1": "@rpc_method()\ndef add_task(self, task_id=None, status=PENDING, runnable=True,\n             deps=None, new_deps=None, expl=None, resources=None,\n             priority=0, family='', module=None, params=None,\n             assistant=False, tracking_url=None, worker=None, batchable=None,\n             batch_id=None, retry_policy_dict={}, owners=None, **kwargs):\n    \n    assert worker is not None\n    worker_id = worker\n    worker = self._update_worker(worker_id)\n    retry_policy = self._generate_retry_policy(retry_policy_dict)\n\n    if worker.enabled:\n        _default_task = self._make_task(\n            task_id=task_id, status=PENDING, deps=deps, resources=resources,\n            priority=priority, family=family, module=module, params=params,\n        )\n    else:\n        _default_task = None\n\n    task = self._state.get_task(task_id, setdefault=_default_task)\n\n    if task is None or (task.status != RUNNING and not worker.enabled):\n        return\n\n    # for setting priority, we'll sometimes create tasks with unset family and params\n    if not task.family:\n        task.family = family\n    if not getattr(task, 'module', None):\n        task.module = module\n    if not task.params:\n        task.params = _get_default(params, {})\n\n    if batch_id is not None:\n        task.batch_id = batch_id\n    if status == RUNNING and not task.worker_running:\n        task.worker_running = worker_id\n        if batch_id:\n            task.resources_running = self._state.get_batch_running_tasks(batch_id)[0].resources_running\n        task.time_running = time.time()\n\n    if tracking_url is not None or task.status != RUNNING:\n        task.tracking_url = tracking_url\n        if task.batch_id is not None:\n            for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                batch_task.tracking_url = tracking_url\n\n    if batchable is not None:\n        task.batchable = batchable\n\n    if task.remove is not None:\n        task.remove = None  # unmark task for removal so it isn't removed after being added\n\n    if expl is not None:\n        task.expl = expl\n        if task.batch_id is not None:\n            for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                batch_task.expl = expl\n\n    if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:\n        # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n        if status == PENDING or status != task.status:\n            # Update the DB only if there was a acctual change, to prevent noise.\n            # We also check for status == PENDING b/c that's the default value\n            # (so checking for status != task.status woule lie)\n            self._update_task_history(task, status)\n        self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n\n    if status == FAILED and self._config.batch_emails:\n        batched_params, _ = self._state.get_batcher(worker_id, family)\n        if batched_params:\n            unbatched_params = {\n                param: value\n                for param, value in six.iteritems(task.params)\n                if param not in batched_params\n            }\n        else:\n            unbatched_params = task.params\n        try:\n            expl_raw = json.loads(expl)\n        except ValueError:\n            expl_raw = expl\n\n        self._email_batcher.add_failure(\n            task.pretty_id, task.family, unbatched_params, expl_raw, owners)\n        if task.status == DISABLED:\n            self._email_batcher.add_disable(\n                task.pretty_id, task.family, unbatched_params, owners)\n\n    if deps is not None:\n        task.deps = set(deps)\n\n    if new_deps is not None:\n        task.deps.update(new_deps)\n\n    if resources is not None:\n        task.resources = resources\n\n    if worker.enabled and not assistant:\n        task.stakeholders.add(worker_id)\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker_id)\n\n    self._update_priority(task, priority, worker_id)\n\n    # Because some tasks (non-dynamic dependencies) are `_make_task`ed\n    # before we know their retry_policy, we always set it here\n    task.retry_policy = retry_policy\n\n    if runnable and status != FAILED and worker.enabled:\n        task.workers.add(worker_id)\n        self._state.get_worker(worker_id).tasks.add(task)\n        task.runnable = runnable\n",
    "1.1.2": "* add task identified by task_id if it doesn't exist\n* if deps is not None, update dependency list\n* update status of task\n* add additional workers/stakeholders\n* update priority when needed",
    "1.2.1": "class Scheduler(object)",
    "1.2.2": "Async scheduler that can handle multiple workers, etc.\n\nCan be run locally or on a server (using RemoteScheduler + server.Server).",
    "1.2.3": [
        "_update_worker(self, worker_id, worker_reference=None, get_work=False)",
        "_update_priority(self, task, prio, worker)",
        "_generate_retry_policy(self, task_retry_policy_dict)",
        "resources(self)",
        "_update_task_history(self, task, status, host=None)"
    ],
    "1.3.1": "/Volumes/SSD2T/bgp_envs_non_pandas/repos/luigi_7/luigi/scheduler.py",
    "1.3.2": [
        "rpc_method(**request_args)",
        "_get_default(x, default)",
        "add_failure(self)",
        "add(self, key)",
        "add_failure(self)",
        "pretty_id(self)",
        "update(self, worker_reference, get_work=False)",
        "assistant(self)",
        "enabled(self)",
        "get_batch_running_tasks(self, batch_id)",
        "get_batcher(self, worker_id, family)",
        "get_task(self, task_id, default=None, setdefault=None)",
        "set_status(self, task, new_status, config=None)",
        "get_worker(self, worker_id)",
        "_update_worker(self, worker_id, worker_reference=None, get_work=False)",
        "_update_priority(self, task, prio, worker)",
        "_generate_retry_policy(self, task_retry_policy_dict)",
        "resources(self)",
        "_update_task_history(self, task, status, host=None)"
    ],
    "1.4.1": [
        "    def test_status_wont_override(self):\n        # Worker X is running A\n        # Worker Y wants to override the status to UNKNOWN (e.g. complete is throwing an exception)\n        self.sch.add_task(worker='X', task_id='A')\n        self.assertEqual(self.sch.get_work(worker='X')['task_id'], 'A')\n        self.sch.add_task(worker='Y', task_id='A', status=UNKNOWN)\n        self.assertEqual({'A'}, set(self.sch.task_list(RUNNING, '').keys()))"
    ],
    "1.4.2": [
        "/Volumes/SSD2T/bgp_envs_non_pandas/repos/luigi_7/test/scheduler_api_test.py"
    ],
    "2.1.1": [
        [
            "E       AssertionError: Items in the first set but not the second:\nE       'A'"
        ]
    ],
    "2.1.2": [
        [
            "self = <scheduler_api_test.SchedulerApiTest testMethod=test_status_wont_override>\n\n    def test_status_wont_override(self):\n        # Worker X is running A\n        # Worker Y wants to override the status to UNKNOWN (e.g. complete is throwing an exception)\n        self.sch.add_task(worker='X', task_id='A')\n        self.assertEqual(self.sch.get_work(worker='X')['task_id'], 'A')\n        self.sch.add_task(worker='Y', task_id='A', status=UNKNOWN)\n>       self.assertEqual({'A'}, set(self.sch.task_list(RUNNING, '').keys()))",
            "\ntest/scheduler_api_test.py:111: AssertionError"
        ]
    ],
    "2.1.3": null,
    "2.1.4": null,
    "2.1.5": null,
    "2.1.6": null,
    "3.1.1": [
        "What's the purpose of a worker telling the scheduler that a task has UNKNOWN status?\n"
    ],
    "3.1.2": [
        "The scheduler correctly marks a task as UNKNOWN when it first encounters that task as a dependency of another task being updated. It's assumed the worker will eventually update the state of such new task with either PENDING or DONE.\nBut a worker can (AT ANY TIME!) also update the status to UNKNOWN on three conditions:\n- when the scheduled tasks reach the task-limit (if the config is set)\n- when the .complete() of the task fails\n- when the .deps() of the task fails\n\nI can understand the intention of providing a visual feedback on the scheduler page in those conditions, but I'd argue that is wrong in all 3 cases to update the scheduler's status and the reasons is simply because those conditions may represent a flaky/local reality of things and it shouldn't be reflected in the central scheduler.\n\nI can give multiple examples of how things could go bad, but simply put because 1 worker can't run a complete(), it doesn't mean other workers can't. And if that's the case you'll have that \"bad\" worker continually overriding the actual scheduler's state with UNKNOWN, which could lead to a task instance running multiple times at once (it happened to us today).\n\nIf there's an actual coding issue where complete() fails systematically, I think it's ok for that task not to appear on the scheduler, after all that's the de facto unknown status of any task :)\n\nAm I missing something here? Should I file this as an issue?\n"
    ]
}