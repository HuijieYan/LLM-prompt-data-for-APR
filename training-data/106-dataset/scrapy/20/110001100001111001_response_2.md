The bug in the provided function seems to be in the nested if-else block where it checks if the response URL ends with '/robots.txt' or not, and then processes the sitemap accordingly. It seems that the bug may be related to the handling of the sitemap URLs and the creation of Request objects for them.

The reason behind the occurrence of the bug may be the mishandling of the sitemap URLs and the callback function for the Request objects in the if-else block.

One possible approach to fixing the bug could be to ensure that the Request objects are created with the correct callback function and the URL from the sitemap. Additionally, the handling of the sitemap URLs and the creation of Request objects in the nested if-else block needs to be revised.

Here's the corrected code for the problematic function:

```python
def _parse_sitemap(self, response):
    if response.url.endswith('/robots.txt'):
        for url in sitemap_urls_from_robots(response.text):
            yield Request(url, callback=self._parse_sitemap)
    else:
        body = self._get_sitemap_body(response)
        if body is None:
            logger.warning("Ignoring invalid sitemap: %(response)s",
                           {'response': response}, extra={'spider': self})
            return

        s = Sitemap(body)
        if s.type == 'sitemapindex':
            for loc in iterloc(s, self.sitemap_alternate_links):
                for pattern, callback in self._cbs:
                    if any(pattern.search(loc) for pattern in self._follow):
                        yield Request(loc, callback=self._parse_sitemap if callback is self._parse_sitemap else callback)
        elif s.type == 'urlset':
            for loc in iterloc(s):
                for pattern, callback in self._cbs:
                    if pattern.search(loc):
                        yield Request(loc, callback=self._parse_sitemap if callback is self._parse_sitemap else callback)
                        break
```

In the corrected code, the sitemap URLs are processed using the `sitemap_urls_from_robots` function for URLs ending with '/robots.txt'. Also, the Request objects are created with the correct callback function based on the sitemap type and the `self._cbs` list.