{
    "scrapy": [
        {
            "bugID": 20,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 1,
                "2.1.4": 1,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 1,
                "6": 1,
                "7": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 0,
                "1.2.2": 0,
                "1.2.3": 0,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 1,
                "2.1.6": 1,
                "3.1.1": 1,
                "3.1.2": 1,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 0,
                "3": 0,
                "4": 1,
                "5": 1,
                "6": 1,
                "7": 1
            },
            "start_line": 33,
            "file_name": "scrapy/spiders/sitemap.py",
            "replace_code": "def _parse_sitemap(self, response):\n    if response.url.endswith('/robots.txt'):\n        for url in sitemap_urls_from_robots(response.body):  # pass the bytes data to the function\n            yield Request(url.decode(), callback=self._parse_sitemap)  # decode the URL\n    else:\n        body = self._get_sitemap_body(response)\n        if body is None:\n            logger.warning(\"Ignoring invalid sitemap: %(response)s\",\n                           {'response': response}, extra={'spider': self})\n            return\n\n        s = Sitemap(body)\n        if s.type == 'sitemapindex':\n            for loc in iterloc(s, self.sitemap_alternate_links):\n                if any(x.search(loc) for x in self._follow):\n                    yield Request(loc.decode(), callback=self._parse_sitemap)  # decode the URL\n        elif s.type == 'urlset':\n            for loc in iterloc(s):\n                for r, c in self._cbs:\n                    if r.search(loc):\n                        yield Request(loc.decode(), callback=c)  # decode the URL\n                        break",
            "imports": []
        }
    ]
}