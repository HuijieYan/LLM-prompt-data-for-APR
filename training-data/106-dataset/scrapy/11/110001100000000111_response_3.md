The issue seems to be related to the response body being duplicated when using Scrapy to fetch a web page. The problem might be occurring in the `gunzip` function, which is used to gunzip the data received from the web server.

The bug in the `gunzip` function seems to be related to the handling of the gzip data. It looks like the function is trying to read the gzip data in chunks and concatenate it to the output, but it's not handling the end of the data properly. This is likely resulting in a duplicated response body.

One possible approach for fixing the bug is to refactor the `gunzip` function to properly handle the end of the gzip data, including any extrabuf data that may be present.

Here's the corrected `gunzip` function:

```python
import gzip
from io import BytesIO

def gunzip(data):
    """Gunzip the given data and return as much data as possible.

    This is resilient to CRC checksum errors.
    """
    with gzip.GzipFile(fileobj=BytesIO(data)) as f:
        return f.read()
```

In this corrected code, we directly use `gzip.GzipFile` to read the gunzipped data and return it as the output. This should handle the end of the gzip data properly and hopefully resolve the issue of the response body being duplicated.