{
    "luigi": [
        {
            "bugID": 31,
            "bitvector": {
                "1.1.1": 1,
                "1.1.2": 1,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "strata": {
                "1": 1,
                "2": 1,
                "3": 0,
                "4": 1,
                "5": 0,
                "6": 0,
                "7": 1
            },
            "available_bitvector": {
                "1.1.1": 1,
                "1.1.2": 0,
                "1.2.1": 1,
                "1.2.2": 1,
                "1.2.3": 1,
                "1.3.1": 0,
                "1.3.2": 0,
                "1.4.1": 1,
                "1.4.2": 1,
                "2.1.1": 1,
                "2.1.2": 1,
                "2.1.3": 0,
                "2.1.4": 0,
                "2.1.5": 0,
                "2.1.6": 0,
                "3.1.1": 0,
                "3.1.2": 0,
                "cot": 1
            },
            "available_strata": {
                "1": 1,
                "2": 1,
                "3": 0,
                "4": 1,
                "5": 0,
                "6": 0,
                "7": 1
            },
            "start_line": 624,
            "file_name": "luigi/scheduler.py",
            "replace_code": "def get_work(self, worker, host=None, assistant=False, **kwargs):\n    # TODO: remove any expired nodes\n\n    # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n    # resources.\n\n    # Resource checking looks both at currently available resources and at which resources would\n    # be available if all running tasks died and we rescheduled all workers greedily. We do both\n    # checks in order to prevent a worker with many low-priority tasks from starving other\n    # workers with higher priority tasks that share the same resources.\n\n    # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n    # nothing it can wait for\n\n    # Return remaining tasks that have no FAILED descendents\n    self.update(worker, {'host': host})\n    if assistant:\n        self.add_worker(worker, [('assistant', assistant)])\n    best_task = None\n    locally_pending_tasks = 0\n    running_tasks = []\n\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                          for worker in self._state.get_active_workers())\n\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n\n    # Add a check for available tasks\n    if not tasks:\n        return {'n_pending_tasks': 0, 'running_tasks': running_tasks, 'task_id': None, 'n_unique_pending': 0}\n\n    # Existing code for processing tasks\n    # ...\n\n    reply = {'n_pending_tasks': locally_pending_tasks,\n             'running_tasks': running_tasks,\n             'task_id': None,\n             'n_unique_pending': n_unique_pending}\n\n    if best_task:\n        self._state.set_status(best_task, RUNNING, self._config)\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n\n        reply['task_id'] = best_task.id\n        reply['task_family'] = best_task.family\n        reply['task_module'] = getattr(best_task, 'module', None)\n        reply['task_params'] = best_task.params\n\n    return reply",
            "imports": []
        }
    ]
}