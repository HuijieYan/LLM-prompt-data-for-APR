The bug in the `_clone_functional_model` function seems to be occurring due to the incorrect handling of input tensors and the computation of output tensors. The error message indicates that the output tensor `Tensor("swap_layer_1/Identity:0", shape=(?, 4), dtype=float32)` could not be computed.

The failing test involves creating a model with multiple inputs and outputs, including a custom `Lambda` layer and a custom `SwapLayer` with multiple inputs and outputs. This test is then trying to clone the model using the `clone_model` function, which in turn calls the `_clone_functional_model` function.

The error occurs when trying to compute output tensors, as indicated by the assertion failure. This suggests that the process of rebuilding the model using new input tensors and instantiated weights is not functioning as expected.

Github issue referred to a similar problem where the output_masks are always None due to an unsupported feature in the Lambda layer. This information can be used to identify the cause of the bug.

To fix the bug, the computation of output tensors and masks need to be handled more robustly, specifically for cases where a layer does not support masks.

Upon analyzing the issue, it seems that the bug may be caused by the handling of the Lambda layer in the `_clone_functional_model` function. Since the Lambda layer does not support masks, this leads to the output masks always being None, causing the assertion failure.

To fix this bug, we need to modify the `_clone_functional_model` function to handle layers that do not support masks, such as the Lambda layer, in a more flexible way. This can be done by checking the layer type and conditionally handling the computation of masks based on the layer type.

Here's the corrected version of the `_clone_functional_model` function:

```python
def _clone_functional_model(model, input_tensors=None):
    # ... (other code remains unchanged)

    for x, y in zip(model.inputs, input_tensors):
        tensor_map[x] = (y, None)  # tensor, mask
    
    # Iterate over every node in the reference model, in depth order
    depth_keys = list(model._nodes_by_depth.keys())
    depth_keys.sort(reverse=True)
    for depth in depth_keys:
        nodes = model._nodes_by_depth[depth]
        for node in nodes:
            # ... (other code remains unchanged)

            if not layer.supports_masking:
                computed_masks = [None] * len(computed_tensors)
            else:
                computed_masks = [mask for _, mask in computed_data]

            # Update tensor_map
            for x, y, mask in zip(reference_output_tensors, output_tensors, computed_masks):
                tensor_map[x] = (y, mask)

    # Check that we computed the model outputs, then instantiate a new model from inputs and outputs.
    output_tensors = []
    for x in model.outputs:
        assert x in tensor_map, 'Could not compute output ' + str(x)
        tensor, mask = tensor_map[x]
        output_tensors.append(tensor)

    return Model(input_tensors, output_tensors, name=model.name)
```

This corrected version of the function includes a conditional check for whether the layer supports masking. If it does not support masking, it sets the computed masks to None, ensuring that the function can handle layers like Lambda that do not support masks.

This correction should resolve the bug and ensure that the function can handle a wider range of layer types when computing new output tensors and masks.