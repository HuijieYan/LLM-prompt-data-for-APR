The bug within the `_clone_functional_model` function appears to be the failure to correctly create the input_layers list when input_tensors is not None. This results in an AssertionError when computing the model outputs due to missing entries in the tensor_map.

The failing test `test_clone_functional_model_with_multi_outputs` specifies a lambda layer with multiple outputs and a `SwapLayer` with multiple inputs and outputs. When the `clone_model` function is called on the model, it leads to the error message "Could not compute output Tensor('swap_layer_1/Identity:0', shape=(?, 4), dtype=float32)".

The GitHub issue highlights a similar issue where Lambda layers with multiple outputs are causing the clone_model function to fail.

To fix the bug:
1. When input_tensors is not None, the input_layers list should be correctly populated by iterating over the model's _input_layers and mapping each layer to its corresponding cloned input tensor.

2. The newly created input layers should also be properly cached in the layer_map.

3. Additionally, when checking if all previous input tensors are available in tensor_map, the loop should iterate over reference_input_tensors instead of model.inputs to ensure all inputs are accounted for.

Here's the corrected version of the `_clone_functional_model` function:

```python
def _clone_functional_model(model, input_tensors=None):
    # ... (previous function code)

    if input_tensors is None:
        # Create placeholders to build the model on top of.
        input_layers = []
        for layer in model._input_layers:
            input_tensor = Input(batch_shape=layer.batch_input_shape,
                                 dtype=layer.dtype,
                                 sparse=layer.sparse,
                                 name=layer.name)
            input_layers.append(input_tensor)
            # Cache newly created input layer.
            layer_map[layer] = input_tensor._keras_history[0]
    else:
        # Make sure that all input tensors come from a Keras layer.
        # If tensor comes from an input layer: cache the input layer.
        input_tensors = to_list(input_tensors)
        input_layers = []
        for i, x in enumerate(input_tensors):
            if not K.is_keras_tensor(x):
                name = model._input_layers[i].name
                input_tensor = Input(tensor=x,
                                     name='input_wrapper_for_' + name)
                input_layers.append(input_tensor)
                # Cache newly created input layer.
                original_input_layer = x._keras_history[0]
                newly_created_input_layer = input_tensor._keras_history[0]
                layer_map[original_input_layer] = newly_created_input_layer
            else:
                input_layers.append(x)

    # ... (remaining function code)
```

With these changes, the function should now correctly handle the creation of input layers and mapping them to their corresponding cloned input tensors, thereby resolving the issue and passing the failing test.