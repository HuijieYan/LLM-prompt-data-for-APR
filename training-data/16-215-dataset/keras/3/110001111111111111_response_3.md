The bug in the `_clone_functional_model` function seems to be related to the handling of input tensors and the computation of output tensors. The failing test is showing that the output tensor `Tensor("swap_layer_1/Identity:0", shape=(?, 4), dtype=float32)` could not be computed, leading to an assertion error.

The issue seems to be related to the handling of input and output tensors in the model cloning process. Specifically, it seems that there is a discrepancy in the creation and mapping of input tensors, as well as the computation of output tensors.

The GitHub issue further explains that the error is related to the handling of masks for layers that do not support masks, such as Lambda layers. This could lead to the output_masks always being None, instead of the expected [None, None].

To fix the bug, we need to address the handling of input_tensors, the creation and mapping of tensors, and the computation of output tensors in the `_clone_functional_model` function. Additionally, we need to improve the handling of layers without mask support.

Here's the corrected code for the `_clone_functional_model` function:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)
    if isinstance(model, Sequential):
        raise ValueError('Expected `model` argument to be a functional `Model` instance, got a `Sequential` instance instead:', model)

    layer_map = {}  # Cache for created layers
    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}

    if input_tensors is None:
        input_tensors = [Input(batch_shape=layer.batch_input_shape, dtype=layer.dtype, sparse=layer.sparse, name=layer.name)
                         for layer in model.inputs]
    else:
        input_tensors = to_list(input_tensors)
        for i, x in enumerate(input_tensors):
            if not K.is_keras_tensor(x):
                name = model.inputs[i].name
                input_tensor = Input(tensor=x, name='input_wrapper_for_' + name)
                input_tensors[i] = input_tensor

    for x, y in zip(model.inputs, input_tensors):
        tensor_map[x] = (y, None)  # tensor, mask

    nodes = model._nodes_by_depth
    for depth, nodes in nodes.items():
        for node in nodes:
            layer = node.outbound_layer

            if layer not in layer_map:
                new_layer = layer.__class__.from_config(layer.get_config())
                layer_map[layer] = new_layer
                layer = new_layer
            else:
                layer = layer_map[layer]
                if isinstance(layer, InputLayer):
                    continue

            reference_input_tensors = node.input_tensors
            reference_output_tensors = node.output_tensors

            computed_data = []
            for x in reference_input_tensors:
                if x in tensor_map:
                    computed_data.append(tensor_map[x])

            if len(computed_data) == len(reference_input_tensors):
                kwargs = node.arguments if node.arguments else {}
                computed_tensors = [x[0] for x in computed_data]
                output_tensors = to_list(layer(computed_tensors, **kwargs))
                for x, y in zip(reference_output_tensors, output_tensors):
                    tensor_map[x] = (y, None)

    output_tensors = [tensor_map[x][0] for x in model.outputs]
    return Model(input_tensors, output_tensors, name=model.name)
```

This corrected code addresses the mapping of input and output tensors, the handling of layer creation, and the computation of output tensors. It should now pass the failing test and resolve the issue reported in the GitHub case.