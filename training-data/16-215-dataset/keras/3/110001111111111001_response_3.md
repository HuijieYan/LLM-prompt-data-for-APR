The bug is likely located in the section of the function that deals with iterating over every node in the reference model. The error message suggests that the output tensor of the model is not being computed correctly.

The cause of the bug is likely due to the incorrect creation and mapping of input layers when input_tensors is None, as well as potential issues with the creation of new layers when using the from_config method.

One possible approach to fixing the bug is to ensure that input layers are created and mapped correctly when input_tensors is None, and to properly clone the layers using their configurations when necessary.

Here's the corrected code for the problematic function:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)
    if isinstance(model, Sequential):
        raise ValueError('Expected `model` argument to be a functional `Model` instance, got a `Sequential` instance instead:', model)

    layer_map = {}  # Cache for created layers
    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}
    input_layers = [layer for layer in model._input_layers]

    if input_tensors is None:
        input_tensors = [Input(batch_shape=layer.batch_input_shape, dtype=layer.dtype, sparse=layer.sparse, name=layer.name) for layer in input_layers]
    else:
        input_tensors = to_list(input_tensors)
        for i, x in enumerate(input_tensors):
            if not K.is_keras_tensor(x):
                name = input_layers[i].name
                input_tensor = Input(tensor=x, name='input_wrapper_for_' + name)
                input_tensors[i] = input_tensor

    for original, cloned in zip(model._input_layers, input_tensors):
        layer_map[original] = cloned

    # Iterate over every node in the reference model, in depth order
    depth_keys = list(model._nodes_by_depth.keys())
    depth_keys.sort(reverse=True)

    for depth in depth_keys:
        for node in model._nodes_by_depth[depth]:
            # Recover the corresponding layer
            layer = node.outbound_layer

            # Get or create layer
            if layer not in layer_map:
                # Clone layer
                new_layer = layer.__class__.from_config(layer.get_config())
                layer_map[layer] = new_layer
            else:
                # Reuse previously cloned layer
                layer = layer_map[layer]
    
            # Gather inputs to call the new layer
            reference_input_tensors = node.input_tensors
            reference_output_tensors = node.output_tensors

            # Check if all previous input tensors are available in tensor_map and call node.inbound_layer
            computed_data = [tensor_map[x] for x in reference_input_tensors if x in tensor_map]

            # Call layer
            if node.arguments:
                kwargs = node.arguments
            else:
                kwargs = {}

            output_tensors = to_list(layer(computed_data, **kwargs))

            # Update tensor_map
            for x, y in zip(reference_output_tensors, output_tensors):
                tensor_map[x] = y

    # Check that we did compute the model outputs, then instantiate a new model from inputs and outputs
    output_tensors = [tensor_map[x] for x in model.outputs]
    return Model(input_tensors, output_tensors, name=model.name)
```

In this corrected code, we ensure that input layers are created and mapped correctly when input_tensors is None, and we properly clone the layers using their configurations when necessary. The corrected code should pass the failing test and satisfy the expected input/output variable information.