{
    "1.1.1": "def write(\n    self, df, path, compression=\"snappy\", index=None, partition_cols=None, **kwargs\n):\n    self.validate_dataframe(df)\n    # thriftpy/protocol/compact.py:339:\n    # DeprecationWarning: tostring() is deprecated.\n    # Use tobytes() instead.\n\n    if \"partition_on\" in kwargs and partition_cols is not None:\n        raise ValueError(\n            \"Cannot use both partition_on and \"\n            \"partition_cols. Use partition_cols for \"\n            \"partitioning data\"\n        )\n    elif \"partition_on\" in kwargs:\n        partition_cols = kwargs.pop(\"partition_on\")\n\n    if partition_cols is not None:\n        kwargs[\"file_scheme\"] = \"hive\"\n\n    if is_s3_url(path):\n        # path is s3:// so we need to open the s3file in 'wb' mode.\n        # TODO: Support 'ab'\n\n        path, _, _, _ = get_filepath_or_buffer(path, mode=\"wb\")\n        # And pass the opened s3file to the fastparquet internal impl.\n        kwargs[\"open_with\"] = lambda path, _: path\n    else:\n        path, _, _, _ = get_filepath_or_buffer(path)\n\n    with catch_warnings(record=True):\n        self.api.write(\n            path,\n            df,\n            compression=compression,\n            write_index=index,\n            partition_on=partition_cols,\n            **kwargs\n        )\n",
    "1.1.2": null,
    "1.2.1": "class FastParquetImpl(BaseImpl)",
    "1.2.2": null,
    "1.2.3": [
        "write(self, df, path, compression='snappy', index=None, partition_cols=None, **kwargs)"
    ],
    "1.3.1": "pandas/io/parquet.py",
    "1.3.2": [
        "validate_dataframe(df)",
        "write(self, df, path, compression, **kwargs)",
        "write(self, df, path, compression='snappy', coerce_timestamps='ms', index=None, partition_cols=None, **kwargs)",
        "write(self, df, path, compression='snappy', index=None, partition_cols=None, **kwargs)"
    ],
    "1.4.1": [
        "@td.skip_if_no(\"fastparquet\")\n@td.skip_if_no(\"gcsfs\")\ndef test_to_parquet_gcs_new_file(monkeypatch, tmpdir):\n    \"\"\"Regression test for writing to a not-yet-existent GCS Parquet file.\"\"\"\n    df1 = DataFrame(\n        {\n            \"int\": [1, 3],\n            \"float\": [2.0, np.nan],\n            \"str\": [\"t\", \"s\"],\n            \"dt\": date_range(\"2018-06-18\", periods=2),\n        }\n    )\n\n    class MockGCSFileSystem:\n        def open(self, path, mode=\"r\", *args):\n            if \"w\" not in mode:\n                raise FileNotFoundError\n            return open(os.path.join(tmpdir, \"test.parquet\"), mode)\n\n    monkeypatch.setattr(\"gcsfs.GCSFileSystem\", MockGCSFileSystem)\n    df1.to_parquet(\n        \"gs://test/test.csv\", index=True, engine=\"fastparquet\", compression=None\n    )"
    ],
    "1.4.2": [
        "pandas/tests/io/test_gcs.py"
    ],
    "2.1.1": [
        [
            "E           FileNotFoundError"
        ]
    ],
    "2.1.2": [
        [
            "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f53608ebc70>\ntmpdir = local('/tmp/pytest-of-ubuntu/pytest-70822/test_to_parquet_gcs_new_file0')\n\n    @td.skip_if_no(\"fastparquet\")\n    @td.skip_if_no(\"gcsfs\")\n    def test_to_parquet_gcs_new_file(monkeypatch, tmpdir):\n        \"\"\"Regression test for writing to a not-yet-existent GCS Parquet file.\"\"\"\n        df1 = DataFrame(\n            {\n                \"int\": [1, 3],\n                \"float\": [2.0, np.nan],\n                \"str\": [\"t\", \"s\"],\n                \"dt\": date_range(\"2018-06-18\", periods=2),\n            }\n        )\n    \n        class MockGCSFileSystem:\n            def open(self, path, mode=\"r\", *args):\n                if \"w\" not in mode:\n                    raise FileNotFoundError\n                return open(os.path.join(tmpdir, \"test.parquet\"), mode)\n    \n        monkeypatch.setattr(\"gcsfs.GCSFileSystem\", MockGCSFileSystem)\n>       df1.to_parquet(\n            \"gs://test/test.csv\", index=True, engine=\"fastparquet\", compression=None\n        )\n\npandas/tests/io/test_gcs.py:84: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \npandas/core/frame.py:2155: in to_parquet\n    to_parquet(\npandas/io/parquet.py:249: in to_parquet\n    return impl.write(\npandas/io/parquet.py:170: in write\n    path, _, _, _ = get_filepath_or_buffer(path)\npandas/io/common.py:243: in get_filepath_or_buffer\n    return gcs.get_filepath_or_buffer(\npandas/io/gcs.py:17: in get_filepath_or_buffer\n    filepath_or_buffer = fs.open(filepath_or_buffer, mode)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <pandas.tests.io.test_gcs.test_to_parquet_gcs_new_file.<locals>.MockGCSFileSystem object at 0x7f53448da880>\npath = 'gs://test/test.csv', mode = 'rb', args = ()\n\n    def open(self, path, mode=\"r\", *args):\n        if \"w\" not in mode:\n>           raise FileNotFoundError",
            "\npandas/tests/io/test_gcs.py:80: FileNotFoundError"
        ]
    ],
    "2.1.3": null,
    "2.1.4": null,
    "2.1.5": [
        [
            {
                "df": {
                    "value": "   int  float str         dt\n0    1    2.0   t 2018-06-18\n1    3    NaN   s 2018-06-19",
                    "shape": "(2, 4)",
                    "omitted": false
                },
                "kwargs": {
                    "value": "{}",
                    "shape": "0",
                    "omitted": false
                },
                "path": {
                    "value": "'gs://test/test.csv'",
                    "shape": "18",
                    "omitted": false
                },
                "self.api": {
                    "value": "<module 'fastparquet' from '/home/ubuntu/Desktop/bgp_envs_local/envs/pandas_149/lib/python3.8/site-packages/fastparquet/__init__.py'>",
                    "shape": null,
                    "omitted": false
                },
                "index": {
                    "value": "True",
                    "shape": null,
                    "omitted": false
                }
            },
            {
                "kwargs": {
                    "value": "{'open_with': <function FastParquetImpl.write.<locals>.<lambda> at 0x7f31b171c4c0>}",
                    "shape": "1",
                    "omitted": false
                },
                "path": {
                    "value": "<_io.BufferedWriter name='/tmp/pytest-of-ubuntu/pytest-70824/test_to_parquet_gcs_new_file0/test.parquet'>",
                    "shape": null,
                    "omitted": false
                },
                "_": {
                    "value": "True",
                    "shape": null,
                    "omitted": false
                }
            }
        ]
    ],
    "2.1.6": [
        [
            {
                "df": "DataFrame",
                "kwargs": "dict",
                "path": "str",
                "self.api": "module",
                "index": "bool"
            },
            {
                "kwargs": "dict",
                "path": "BufferedWriter",
                "_": "bool"
            }
        ]
    ],
    "3.1.1": null,
    "3.1.2": null,
    "used_imports": "from warnings import catch_warnings\nfrom pandas.io.common import get_filepath_or_buffer, is_s3_url"
}