{
    "1.1.1": "def __init__(self, settings):\n    super(CrawlerProcess, self).__init__(settings)\n    install_shutdown_handlers(self._signal_shutdown)\n    configure_logging(settings)\n    log_scrapy_info(settings)\n",
    "1.1.2": null,
    "1.2.1": "scrapy/crawler.py",
    "1.2.2": "from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\nfrom scrapy.utils.log import LogCounterHandler, configure_logging, log_scrapy_info",
    "1.3.1": "class CrawlerProcess(CrawlerRunner)",
    "1.3.2": "A class to run multiple scrapy crawlers in a process simultaneously.\n\nThis class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\nfor starting a Twisted `reactor`_ and handling shutdown signals, like the\nkeyboard interrupt command Ctrl-C. It also configures top-level logging.\n\nThis utility should be a better fit than\n:class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\nTwisted `reactor`_ within your application.\n\nThe CrawlerProcess object must be instantiated with a\n:class:`~scrapy.settings.Settings` object.\n\nThis class shouldn't be needed (since Scrapy is responsible of using it\naccordingly) unless writing scripts that manually handle the crawling\nprocess. See :ref:`run-from-script` for an example.",
    "1.4.1": [
        "__init__(self, settings)",
        "_signal_shutdown(self, signum, _)"
    ],
    "1.4.2": null,
    "1.5.1": [
        "    def test_crawler_process_accepts_dict(self):\n        runner = CrawlerProcess({'foo': 'bar'})\n        self.assertEqual(runner.settings['foo'], 'bar')\n        self.assertEqual(\n            runner.settings['RETRY_ENABLED'],\n            default_settings.RETRY_ENABLED\n        )\n        self.assertIsInstance(runner.settings, Settings)"
    ],
    "1.5.2": [
        "tests/test_crawler.py"
    ],
    "2.1.1": [
        [
            "E       KeyError: 'BOT_NAME'"
        ]
    ],
    "2.1.2": [
        [
            "self = <tests.test_crawler.CrawlerProcessTest testMethod=test_crawler_process_accepts_dict>\n\n    def test_crawler_process_accepts_dict(self):\n>       runner = CrawlerProcess({'foo': 'bar'})\n\n/home/ubuntu/Desktop/bgp_envs_local/repos/scrapy_32/tests/test_crawler.py:110: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/home/ubuntu/Desktop/bgp_envs_local/repos/scrapy_32/scrapy/crawler.py:213: in __init__\n    log_scrapy_info(settings)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nsettings = {'foo': 'bar'}\n\n    def log_scrapy_info(settings):\n        logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n>                   {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})",
            "\n/home/ubuntu/Desktop/bgp_envs_local/repos/scrapy_32/scrapy/utils/log.py:108: KeyError"
        ]
    ],
    "2.2.1": null,
    "2.2.2": null,
    "2.3.1": [
        [
            {
                "settings": {
                    "value": "{'foo': 'bar'}",
                    "shape": "1",
                    "omitted": false
                }
            },
            {}
        ]
    ],
    "2.3.2": [
        [
            {
                "settings": "dict"
            },
            {}
        ]
    ],
    "3.1.1": null,
    "3.1.2": null
}