{
    "1.1.1": "def _robots_error(self, failure, netloc):\n    self._parsers.pop(netloc).callback(None)\n",
    "1.1.2": null,
    "1.2.1": "scrapy/downloadermiddlewares/robotstxt.py",
    "1.2.2": null,
    "1.3.1": "class RobotsTxtMiddleware(object)",
    "1.3.2": null,
    "1.4.1": null,
    "1.4.2": null,
    "1.5.1": [
        "    def test_robotstxt_immediate_error(self):\n        self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n        err = error.DNSLookupError('Robotstxt address not found')\n        def immediate_failure(request, spider):\n            deferred = Deferred()\n            deferred.errback(failure.Failure(err))\n            return deferred\n        self.crawler.engine.download.side_effect = immediate_failure\n\n        middleware = RobotsTxtMiddleware(self.crawler)\n        return self.assertNotIgnored(Request('http://site.local'), middleware)"
    ],
    "1.5.2": [
        "tests/test_downloadermiddleware_robotstxt.py"
    ],
    "2.1.1": [
        [
            "E       KeyError: 'site.local'"
        ]
    ],
    "2.1.2": [
        [
            "f = <bound method RobotsTxtMiddleware.robot_parser of <scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware object at 0x7f2248c0bf10>>\nargs = (<GET http://site.local>, None), kw = {}\n\n    def maybeDeferred(f, *args, **kw):\n        \"\"\"\n        Invoke a function that may or may not return a L{Deferred}.\n    \n        Call the given function with the given arguments.  If the returned\n        object is a L{Deferred}, return it.  If the returned object is a L{Failure},\n        wrap it with L{fail} and return it.  Otherwise, wrap it in L{succeed} and\n        return it.  If an exception is raised, convert it to a L{Failure}, wrap it\n        in L{fail}, and then return it.\n    \n        @type f: Any callable\n        @param f: The callable to invoke\n    \n        @param args: The arguments to pass to C{f}\n        @param kw: The keyword arguments to pass to C{f}\n    \n        @rtype: L{Deferred}\n        @return: The result of the function call, wrapped in a L{Deferred} if\n        necessary.\n        \"\"\"\n        try:\n>           result = f(*args, **kw)\n\n/home/ubuntu/Desktop/bgp_envs_local/envs/scrapy_21/lib/python3.8/site-packages/twisted/internet/defer.py:151: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware object at 0x7f2248c0bf10>\nrequest = <GET http://site.local>, spider = None\n\n    def robot_parser(self, request, spider):\n        url = urlparse_cached(request)\n        netloc = url.netloc\n    \n        if netloc not in self._parsers:\n            self._parsers[netloc] = Deferred()\n            robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={'dont_obey_robotstxt': True}\n            )\n            dfd = self.crawler.engine.download(robotsreq, spider)\n            dfd.addCallback(self._parse_robots, netloc)\n            dfd.addErrback(self._logerror, robotsreq, spider)\n            dfd.addErrback(self._robots_error, netloc)\n    \n>       if isinstance(self._parsers[netloc], Deferred):",
            "\n/home/ubuntu/Desktop/bgp_envs_local/repos/scrapy_21/scrapy/downloadermiddlewares/robotstxt.py:65: KeyError"
        ]
    ],
    "2.2.1": [
        [
            {
                "self._parsers": {
                    "value": "{'site.local': <Deferred at 0x7f8b25ece6a0>}",
                    "shape": "1",
                    "omitted": false
                },
                "netloc": {
                    "value": "'site.local'",
                    "shape": "10",
                    "omitted": false
                }
            },
            {
                "self._parsers": {
                    "value": "{}",
                    "shape": "0",
                    "omitted": false
                }
            }
        ]
    ],
    "2.2.2": [
        [
            {
                "self._parsers": "dict",
                "netloc": "str"
            },
            {
                "self._parsers": "dict"
            }
        ]
    ],
    "2.3.1": [
        [
            {
                "self._parsers": {
                    "value": "{'site.local': <Deferred at 0x7fcb264cae20>}",
                    "shape": "1",
                    "omitted": false
                },
                "netloc": {
                    "value": "'site.local'",
                    "shape": "10",
                    "omitted": false
                }
            },
            {
                "rp_dfd": {
                    "value": "<Deferred at 0x7fcb264cae20 current result: None>",
                    "shape": null,
                    "omitted": false
                },
                "self._parsers": {
                    "value": "{'site.local': None}",
                    "shape": "1",
                    "omitted": false
                }
            }
        ]
    ],
    "2.3.2": [
        [
            {
                "self._parsers": "dict",
                "netloc": "str"
            },
            {
                "rp_dfd": "Deferred",
                "self._parsers": "dict"
            }
        ]
    ],
    "3.1.1": [
        "KeyError in robotstxt middleware\n"
    ],
    "3.1.2": [
        "I'm getting these errors in robots.txt middleware:\n\n2016-01-27 16:18:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://yellowpages.co.th>\nTraceback (most recent call last):\n  File \"/Users/kmike/envs/scraping/lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/Users/kmike/svn/scrapy/scrapy/downloadermiddlewares/robotstxt.py\", line 65, in robot_parser\n    if isinstance(self._parsers[netloc], Deferred):\nKeyError: 'yellowpages.co.th'\nIt looks like #1473 caused it (I can't get this issue in Scrapy 1.0.4, but it present in Scrapy master). It happens when page failed to download and HTTP cache is enabled. I haven't debugged it further.\n"
    ]
}