{
    "scrapy:15": {
        "github_issue_title": [
            "Unicode Link Extractor\n"
        ],
        "github_issue_description": [
            "When using the following to extract all of the links from a response:\n\nself.link_extractor = LinkExtractor()\n...\nlinks = self.link_extractor.extract_links(response)\nOn rare occasions, the following error is thrown:\n\n2016-05-25 12:13:55,432 [root] [ERROR]  Error on http://detroit.curbed.com/2016/5/5/11605132/tiny-house-designer-show, traceback: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 1203, in mainLoop\n    self.runUntilCurrent()\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 825, in runUntilCurrent\n    call.func(*call.args, **call.kw)\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 393, in callback\n    self._startRunCallbacks(result)\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 501, in _startRunCallbacks\n    self._runCallbacks()\n--- <exception caught here> ---\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 588, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/var/www/html/DomainCrawler/DomainCrawler/spiders/hybrid_spider.py\", line 223, in parse\n    items.extend(self._extract_requests(response))\n  File \"/var/www/html/DomainCrawler/DomainCrawler/spiders/hybrid_spider.py\", line 477, in _extract_requests\n    links = self.link_extractor.extract_links(response)\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/linkextractors/lxmlhtml.py\", line 111, in extract_links\n    all_links.extend(self._process_links(links))\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/linkextractors/__init__.py\", line 103, in _process_links\n    link.url = canonicalize_url(urlparse(link.url))\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/utils/url.py\", line 85, in canonicalize_url\n    parse_url(url), encoding=encoding)\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/utils/url.py\", line 46, in _safe_ParseResult\n    to_native_str(parts.netloc.encode('idna')),\n  File \"/usr/local/lib/python2.7/encodings/idna.py\", line 164, in encode\n    result.append(ToASCII(label))\n  File \"/usr/local/lib/python2.7/encodings/idna.py\", line 73, in ToASCII\n    raise UnicodeError(\"label empty or too long\")\nexceptions.UnicodeError: label empty or too long\nI was able to find some information concerning the error from here.\nMy question is: What is the best way to handle this? Even if there is one bad link in the response, I'd want all of the other good links to be extracted.\n"
        ]
    }
}