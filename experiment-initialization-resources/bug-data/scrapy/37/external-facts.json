{
    "scrapy:37": {
        "github_issue_title": [
            "scrapy.Request no init error on invalid url\n"
        ],
        "github_issue_description": [
            "I stumbled on some weird issue, spider got some invalid url, but instead of crashing loudly when trying to create scrapy.Request() with invalid url it just silently ignored this error. Sample to reproduce\n\nfrom scrapy.spiders import Spider\nfrom scrapy import Request\n\n\nclass DmozSpider(Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n    ]\n\n    def parse(self, response):\n        invalid_url = \"/container.productlist.productslist.productthumbnail.articledetaillink.layerlink:open-layer/0/CLASSIC/-1/WEB$007cARBO$007c13263065/null$007cDisplay$0020Product$002f111499$002fAil$0020blanc$007c?t:ac=13263065\"\n        yield Request(invalid_url)\nthis generates following output:\n\n2017-02-09 12:21:04 [scrapy.core.engine] INFO: Spider opened\n2017-02-09 12:21:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-02-09 12:21:04 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\n2017-02-09 12:21:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2017-02-09 12:21:04 [scrapy.core.engine] INFO: Closing spider (finished)\nthere is no information about trying to generate this Request with invalid_url, no stacktrace, no error info from middleware. Why?\n"
        ]
    }
}