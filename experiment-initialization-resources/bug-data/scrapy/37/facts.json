{
    "1.1.1": "def _set_url(self, url):\n    if not isinstance(url, six.string_types):\n        raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n\n    s = safe_url_string(url, self.encoding)\n    self._url = escape_ajax(s)\n\n    if ':' not in self._url:\n        raise ValueError('Missing scheme in request url: %s' % self._url)\n",
    "1.1.2": null,
    "1.2.1": "class Request(object_ref)",
    "1.2.2": null,
    "1.2.3": [
        "encoding(self)"
    ],
    "1.2.4": null,
    "1.2.5": null,
    "1.3.1": "scrapy/http/request/__init__.py",
    "1.3.2": [
        "encoding(self)"
    ],
    "1.4.1": [
        "    def test_url_no_scheme(self):\n        self.assertRaises(ValueError, self.request_class, 'foo')\n        self.assertRaises(ValueError, self.request_class, '/foo/')\n        self.assertRaises(ValueError, self.request_class, '/foo:bar')"
    ],
    "1.4.2": [
        "tests/test_http_request.py"
    ],
    "2.1.1": [
        [
            "E       AssertionError: ValueError not raised by Request"
        ]
    ],
    "2.1.2": [
        [
            "self = <tests.test_http_request.RequestTest testMethod=test_url_no_scheme>\n\n    def test_url_no_scheme(self):\n        self.assertRaises(ValueError, self.request_class, 'foo')\n        self.assertRaises(ValueError, self.request_class, '/foo/')\n>       self.assertRaises(ValueError, self.request_class, '/foo:bar')",
            "\n/Volumes/SSD2T/bgp_envs/repos/scrapy_37/tests/test_http_request.py:56: AssertionError"
        ]
    ],
    "2.1.3": null,
    "2.1.4": null,
    "2.1.5": null,
    "2.1.6": null,
    "3.1.1": [
        "scrapy.Request no init error on invalid url\n"
    ],
    "3.1.2": [
        "I stumbled on some weird issue, spider got some invalid url, but instead of crashing loudly when trying to create scrapy.Request() with invalid url it just silently ignored this error. Sample to reproduce\n\nfrom scrapy.spiders import Spider\nfrom scrapy import Request\n\n\nclass DmozSpider(Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n    ]\n\n    def parse(self, response):\n        invalid_url = \"/container.productlist.productslist.productthumbnail.articledetaillink.layerlink:open-layer/0/CLASSIC/-1/WEB$007cARBO$007c13263065/null$007cDisplay$0020Product$002f111499$002fAil$0020blanc$007c?t:ac=13263065\"\n        yield Request(invalid_url)\nthis generates following output:\n\n2017-02-09 12:21:04 [scrapy.core.engine] INFO: Spider opened\n2017-02-09 12:21:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-02-09 12:21:04 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\n2017-02-09 12:21:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2017-02-09 12:21:04 [scrapy.core.engine] INFO: Closing spider (finished)\nthere is no information about trying to generate this Request with invalid_url, no stacktrace, no error info from middleware. Why?\n"
    ]
}