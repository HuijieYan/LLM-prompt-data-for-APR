{
    "1": "Assume that the following list of imports are available in the current environment, so you don't need to import them when generating a fix.\n```python\nfrom ..utils.generic_utils import unpack_singleton\nfrom ..layers import deserialize as deserialize_layer\n```\n\n# The source code of the buggy function\n```python\n# The relative path of the buggy file: keras/engine/network.py\n\n\n\n    # this is the buggy function you need to fix\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        \"\"\"Instantiates a Model from its config (output of `get_config()`).\n    \n        # Arguments\n            config: Model config dictionary.\n            custom_objects: Optional dictionary mapping names\n                (strings) to custom classes or functions to be\n                considered during deserialization.\n    \n        # Returns\n            A model instance.\n    \n        # Raises\n            ValueError: In case of improperly formatted config dict.\n        \"\"\"\n        # Layer instances created during\n        # the graph reconstruction process\n        created_layers = {}\n    \n        # Dictionary mapping layer instances to\n        # node data that specifies a layer call.\n        # It acts as a queue that maintains any unprocessed\n        # layer call until it becomes possible to process it\n        # (i.e. until the input tensors to the call all exist).\n        unprocessed_nodes = {}\n    \n        def add_unprocessed_node(layer, node_data):\n            if layer not in unprocessed_nodes:\n                unprocessed_nodes[layer] = [node_data]\n            else:\n                unprocessed_nodes[layer].append(node_data)\n    \n        def process_node(layer, node_data):\n            input_tensors = []\n            for input_data in node_data:\n                inbound_layer_name = input_data[0]\n                inbound_node_index = input_data[1]\n                inbound_tensor_index = input_data[2]\n                if len(input_data) == 3:\n                    kwargs = {}\n                elif len(input_data) == 4:\n                    kwargs = input_data[3]\n                else:\n                    raise ValueError('Improperly formatted model config.')\n                inbound_layer = created_layers[inbound_layer_name]\n                if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                input_tensors.append(\n                    inbound_node.output_tensors[inbound_tensor_index])\n            # Call layer on its inputs, thus creating the node\n            # and building the layer if needed.\n            if input_tensors:\n                layer(unpack_singleton(input_tensors), **kwargs)\n    \n        def process_layer(layer_data):\n            \"\"\"Deserializes a layer, then call it on appropriate inputs.\n    \n            # Arguments\n                layer_data: layer config dict.\n    \n            # Raises\n                ValueError: In case of improperly formatted `layer_data` dict.\n            \"\"\"\n            layer_name = layer_data['name']\n    \n            # Instantiate layer.\n            from ..layers import deserialize as deserialize_layer\n    \n            layer = deserialize_layer(layer_data,\n                                      custom_objects=custom_objects)\n            created_layers[layer_name] = layer\n    \n            # Gather layer inputs.\n            inbound_nodes_data = layer_data['inbound_nodes']\n            for node_data in inbound_nodes_data:\n                # We don't process nodes (i.e. make layer calls)\n                # on the fly because the inbound node may not yet exist,\n                # in case of layer shared at different topological depths\n                # (e.g. a model such as A(B(A(B(x)))))\n                add_unprocessed_node(layer, node_data)\n    \n        # First, we create all layers and enqueue nodes to be processed\n        for layer_data in config['layers']:\n            process_layer(layer_data)\n        # Then we process nodes in order of layer depth.\n        # Nodes that cannot yet be processed (if the inbound node\n        # does not yet exist) are re-enqueued, and the process\n        # is repeated until all nodes are processed.\n        while unprocessed_nodes:\n            for layer_data in config['layers']:\n                layer = created_layers[layer_data['name']]\n                if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n    \n        name = config.get('name')\n        input_tensors = []\n        output_tensors = []\n        for layer_data in config['input_layers']:\n            layer_name, node_index, tensor_index = layer_data\n            assert layer_name in created_layers\n            layer = created_layers[layer_name]\n            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n            input_tensors.append(layer_output_tensors[tensor_index])\n        for layer_data in config['output_layers']:\n            layer_name, node_index, tensor_index = layer_data\n            assert layer_name in created_layers\n            layer = created_layers[layer_name]\n            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n            output_tensors.append(layer_output_tensors[tensor_index])\n        return cls(inputs=input_tensors, outputs=output_tensors, name=name)\n    \n```",
    "2": "# The declaration of the class containing the buggy function\nclass Network(Layer):\n    \"\"\"\n    A Network is a directed acyclic graph of layers.\n    \n    It is the topological form of a \"model\". A Model\n    is simply a Network with added training routines.\n    \n    # Properties\n        name\n        inputs\n        outputs\n        layers\n        input_spec (list of class instances)\n            each entry describes one required input:\n                - ndim\n                - dtype\n        trainable (boolean)\n        input_shape\n        output_shape\n        weights (list of variables)\n        trainable_weights (list of variables)\n        non_trainable_weights (list of variables)\n        losses\n        updates\n        state_updates\n        stateful\n    \n    # Methods\n        __call__\n        summary\n        get_layer\n        get_weights\n        set_weights\n        get_config\n        compute_output_shape\n        save\n        add_loss\n        add_update\n        get_losses_for\n        get_updates_for\n        to_json\n        to_yaml\n        reset_states\n    \n    # Class Methods\n        from_config\n    \n    # Raises\n        TypeError: if input tensors are not Keras tensors\n            (tensors returned by `Input`).\n    \"\"\"\n\n\n",
    "3": "# This function from the same file, but not the same class, is called by the buggy function\ndef add_unprocessed_node(layer, node_data):\n    # Please ignore the body of this function\n\n# This function from the same file, but not the same class, is called by the buggy function\ndef process_node(layer, node_data):\n    # Please ignore the body of this function\n\n# This function from the same file, but not the same class, is called by the buggy function\ndef process_layer(layer_data):\n    # Please ignore the body of this function\n\n    # This function from the same class is called by the buggy function\n    def add_unprocessed_node(layer, node_data):\n        # Please ignore the body of this function\n\n    # This function from the same class is called by the buggy function\n    def process_node(layer, node_data):\n        # Please ignore the body of this function\n\n    # This function from the same class is called by the buggy function\n    def process_layer(layer_data):\n        # Please ignore the body of this function\n\n",
    "4": "# A failing test function for the buggy function\n```python\n# The relative path of the failing test file: tests/keras/engine/test_topology.py\n\ndef test_layer_sharing_at_heterogeneous_depth_order():\n    # This tests for the bug in this issue\n    # https://github.com/keras-team/keras/issues/11159\n    # It occurs with layer sharing at heterogeneous depth when\n    # the layers need to be applied in an order that differs from\n    # the order that occurs in the config.\n\n    input_shape = (1, 12)\n    input_layer = Input(shape=input_shape)\n\n    A = Dense(12, name='layer_a')\n    r1 = layers.Reshape((12,))(input_layer)\n    Aout1 = A(r1)\n\n    r2 = layers.Reshape((12,))(A(input_layer))\n    Aout2 = A(r2)\n\n    # Note: if the order of the layers in the concat is\n    # changed to ([Aout1, Aout2]) the bug doesn't trigger\n    c1 = layers.concatenate([Aout2, Aout1])\n    output = Dense(2, name='layer_b')(c1)\n\n    M = Model(inputs=input_layer, outputs=output)\n\n    x_val = np.random.random((10,) + input_shape)\n    output_val = M.predict(x_val)\n\n    config = M.get_config()\n    weights = M.get_weights()\n\n    M2 = Model.from_config(config)\n    M2.set_weights(weights)\n\n    output_val_2 = M2.predict(x_val)\n    np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)\n```\n\n\n",
    "5": "## The error message from the failing test\n```text\ndef test_layer_sharing_at_heterogeneous_depth_order():\n        # This tests for the bug in this issue\n        # https://github.com/keras-team/keras/issues/11159\n        # It occurs with layer sharing at heterogeneous depth when\n        # the layers need to be applied in an order that differs from\n        # the order that occurs in the config.\n    \n        input_shape = (1, 12)\n        input_layer = Input(shape=input_shape)\n    \n        A = Dense(12, name='layer_a')\n        r1 = layers.Reshape((12,))(input_layer)\n        Aout1 = A(r1)\n    \n        r2 = layers.Reshape((12,))(A(input_layer))\n        Aout2 = A(r2)\n    \n        # Note: if the order of the layers in the concat is\n        # changed to ([Aout1, Aout2]) the bug doesn't trigger\n        c1 = layers.concatenate([Aout2, Aout1])\n        output = Dense(2, name='layer_b')(c1)\n    \n        M = Model(inputs=input_layer, outputs=output)\n    \n        x_val = np.random.random((10,) + input_shape)\n        output_val = M.predict(x_val)\n    \n        config = M.get_config()\n        weights = M.get_weights()\n    \n>       M2 = Model.from_config(config)\n\ntests/keras/engine/test_topology.py:793: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nkeras/engine/network.py:1029: in from_config\n    process_node(layer, node_data)\nkeras/engine/network.py:988: in process_node\n    layer(unpack_singleton(input_tensors), **kwargs)\nkeras/engine/base_layer.py:431: in __call__\n    self.build(unpack_singleton(input_shapes))\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <keras.layers.merge.Concatenate object at 0x7f856793a690>\ninput_shape = [(None, 12), (None, 1, 12)]\n\n    def build(self, input_shape):\n        # Used purely for shape validation.\n        if not isinstance(input_shape, list) or len(input_shape) < 2:\n            raise ValueError('A `Concatenate` layer should be called '\n                             'on a list of at least 2 inputs')\n        if all([shape is None for shape in input_shape]):\n            return\n        reduced_inputs_shapes = [list(shape) for shape in input_shape]\n        shape_set = set()\n        for i in range(len(reduced_inputs_shapes)):\n            del reduced_inputs_shapes[i][self.axis]\n            shape_set.add(tuple(reduced_inputs_shapes[i]))\n        if len(shape_set) > 1:\n            raise ValueError('A `Concatenate` layer requires '\n                             'inputs with matching shapes '\n                             'except for the concat axis. '\n>                            'Got inputs shapes: %s' % (input_shape))\nE           ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12), (None, 1, 12)]\n\nkeras/layers/merge.py:362: ValueError\n\n```\n",
    "6": "# Runtime values and types of variables inside the buggy function\nEach case below includes input parameter values and types, and the values and types of relevant variables at the function's return, derived from executing failing tests. If an input parameter is not reflected in the output, it is assumed to remain unchanged. Note that some of these values at the function's return might be incorrect. Analyze these cases to identify why the tests are failing to effectively fix the bug.\n\n## Case 1\n### Runtime values and types of the input parameters of the buggy function\nconfig, value: `{'name': 'model_1', 'layers': [{'name': 'input_1', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 1, 12), 'dtype': 'float32', 'sparse': False, 'name': 'input_1'} ... {}]]]}], 'input_layers': [['input_1', 0, 0]], 'output_layers': [['layer_b', 0, 0]]}`, shape: `4`, type: `dict`\n\n### Runtime values and types of variables right before the buggy function's return\ncreated_layers, value: `{'input_1': <keras.engine.input_layer.InputLayer object at 0x7f56bb0ffe50>, 'layer_a': <keras.layers.core.Dense object at 0x7f56bb0ff650>, 'reshape_2': <keras.layers.core.Reshape object at 0x7f56bb0fff50>, 'reshape_1': <keras.layers.core.Reshape object at 0x7f56bb0ff910>, 'concatenate_1': <keras.layers.merge.Concatenate object at 0x7f56bb0ff7d0>, 'layer_b': <keras.layers.core.Dense object at 0x7f56bb0ffed0>}`, type: `dict`\n\nunprocessed_nodes, value: `{}`, type: `dict`\n\nnode_data, value: `[['concatenate_1', 0, 0, {}]]`, type: `list`\n\ninput_tensors, value: `[<tf.Tensor 'input_1_1:0' shape=(?, 1, 12) dtype=float32>]`, type: `list`\n\nlayer_name, value: `'layer_b'`, type: `str`\n\nlayer_data, value: `['layer_b', 0, 0]`, type: `list`\n\nnode_data_list, value: `[[['concatenate_1', 0, 0, {}]]]`, type: `list`\n\nnode_index, value: `0`, type: `int`\n\nname, value: `'model_1'`, type: `str`\n\noutput_tensors, value: `[<tf.Tensor 'layer_b_1/BiasAdd:0' shape=(?, 2) dtype=float32>]`, type: `list`\n\ntensor_index, value: `0`, type: `int`\n\nlayer_output_tensors, value: `[<tf.Tensor 'layer_b_1/BiasAdd:0' shape=(?, 2) dtype=float32>]`, type: `list`\n\nlayer._inbound_nodes, value: `[<keras.engine.base_layer.Node object at 0x7f56bb0e1190>]`, type: `list`\n\n",
    "7": "# Expected values and types of variables during the failing test execution\nEach case below includes input parameter values and types, and the expected values and types of relevant variables at the function's return. If an input parameter is not reflected in the output, it is assumed to remain unchanged. A corrected function must satisfy all these cases.\n\n## Expected case 1\n### Input parameter values and types\n### The values and types of buggy function's parameters\nconfig, value: `{'name': 'model_1', 'layers': [{'name': 'input_1', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 1, 12), 'dtype': 'float32', 'sparse': False, 'name': 'input_1'} ... {}]]]}], 'input_layers': [['input_1', 0, 0]], 'output_layers': [['layer_b', 0, 0]]}`, shape: `4`, type: `dict`\n\n### Expected values and types of variables right before the buggy function's return\ncreated_layers, expected value: `{'input_1': <keras.engine.input_layer.InputLayer object at 0x7f6af457aa90>, 'layer_a': <keras.layers.core.Dense object at 0x7f6af45ee090>, 'reshape_2': <keras.layers.core.Reshape object at 0x7f6af45ee150>, 'reshape_1': <keras.layers.core.Reshape object at 0x7f6af45eea90>, 'concatenate_1': <keras.layers.merge.Concatenate object at 0x7f6af45ee890>, 'layer_b': <keras.layers.core.Dense object at 0x7f6af45eea50>}`, type: `dict`\n\nunprocessed_nodes, expected value: `{<keras.layers.core.Reshape object at 0x7f6af45ee150>: [[['layer_a', 1, 0, {}]]], <keras.layers.core.Reshape object at 0x7f6af45eea90>: [[['input_1', 0, 0, {}]]], <keras.layers.merge.Concatenate object at 0x7f6af45ee890>: [[['layer_a', 2, 0, {}], ['layer_a', 0, 0, {}]]], <keras.layers.core.Dense object at 0x7f6af45eea50>: [[['concatenate_1', 0, 0, {}]]], <keras.layers.core.Dense object at 0x7f6af45ee090>: [[['reshape_1', 0, 0, {}]]]}`, type: `dict`\n\nnode_data, expected value: `[['reshape_1', 0, 0, {}]]`, type: `list`\n\ninput_tensors, expected value: `[]`, type: `list`\n\ninput_data, expected value: `['reshape_1', 0, 0, {}]`, type: `list`\n\ninbound_layer_name, expected value: `'reshape_1'`, type: `str`\n\ninbound_node_index, expected value: `0`, type: `int`\n\ninbound_tensor_index, expected value: `0`, type: `int`\n\nkwargs, expected value: `{}`, type: `dict`\n\ninbound_layer._inbound_nodes, expected value: `[]`, type: `list`\n\nlayer_data, expected value: `{'name': 'layer_a', 'class_name': 'Dense', 'config': {'name': 'layer_a', 'trainable': True, 'units': 12, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None} ... {}]], [['reshape_2', 0, 0, {}]]]}`, shape: `4`, type: `dict`\n\nlayer._inbound_nodes, expected value: `[]`, type: `list`\n\n",
    "8": "# A GitHub issue for this bug\n\nThe issue's title:\n```text\nBug in loading model with shared layers accross multiple levels.\n```\n\nThe issue's detailed description:\n```text\nThere is a bug in the from_config method of the Keras Network class. This bug occurs when loading a model from a config when the model uses a layer that is shared at multiple depths and the input tensors to the shared layer are not in the order of the layers in the model config file.\n\nFor example, the following model creates a single dense layer then applies it to the reshaped input x2. It is then applied to the non-reshaped input x1, and again at the reshaped output.\n\nsl = Dense(12)\n\nx2 = Input((1, 12))\nr2 = Reshape((12,))(x2)\nr21 = sl(r2)\n\nx1 = Input((1, 12))\nr1 = Reshape((12,))(sl(x1))\n\nr11 = sl(r1)\nc1 = Concatenate()([r11, r21])\no1 = Dense(2)(c1)\nThe layers of the model are as follows:\n\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to\n==================================================================================================\ninput_2 (InputLayer)            (None, 1, 12)        0\n__________________________________________________________________________________________________\ndense_1 (Dense)                 multiple             156         reshape_1[0][0]\n                                                                 input_2[0][0]\n                                                                 reshape_2[0][0]\n__________________________________________________________________________________________________\ninput_1 (InputLayer)            (None, 1, 12)        0\n__________________________________________________________________________________________________\nreshape_2 (Reshape)             (None, 12)           0           dense_1[1][0]\n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 12)           0           input_1[0][0]\n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 24)           0           dense_1[2][0]\n                                                                 dense_1[0][0]\n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 2)            50          concatenate_1[0][0]\n==================================================================================================\nNote that the dense_2 layer has reshape_1 and reshape_2 as inputs but those layers come after dense_2 in the list of layers.\n\nThe code in keras/engine/network.py contains the from_config method that loads the model. Then loading, the layer order of above is followed when recreating the model. At each layer Keras attempts to deserialize the layer using the inputs. When trying to deserialize the dense_2 layer Keras tries to create the first output but cannot because the input layers reshape_1 aren't available, Keras next tries to create the second output using input_2 which works because these layers are available. Keras will re-queue the first node (and third node) and will creates it at the next attempt when the input layers are available, unfortunately in doing this it swaps the output order of the output nodes of the dense_2 layer. The model loading then fails at the concatenate_1 layer as it uses the output nodes [0] and [2] of dense_2 but the output node [0] is now from input_2 which has the incorrect shape.\n\nNote that if we change the order that we apply the shared layer so that model layer order changes this bug can be avoided. The code to reproduce the bug including code to create the layers in an order that doesn't trigger the bug is on this gist:\nhttps://gist.github.com/adocherty/5f5c9983310ef2cf28e3ccb63ad39740\n\nThe error triggered by this script is as follows:\n\n  File \"example_load_bug.py\", line 57, in <module>\n    models.load_model(\"test.h5\")\n  File \".../lib/python3.6/site-packages/keras/engine/saving.py\", line 260, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \".../lib/python3.6/site-packages/keras/engine/saving.py\", line 334, in model_from_config\n    return deserialize(config, custom_objects=custom_objects)\n  File \".../lib/python3.6/site-packages/keras/layers/__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \".../lib/python3.6/site-packages/keras/utils/generic_utils.py\", line 145, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \".../lib/python3.6/site-packages/keras/engine/network.py\", line 1027, in from_config\n    process_node(layer, node_data)\n  File \".../lib/python3.6/site-packages/keras/engine/network.py\", line 986, in process_node\n    layer(unpack_singleton(input_tensors), **kwargs)\n  File \".../lib/python3.6/site-packages/keras/engine/base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \".../lib/python3.6/site-packages/keras/layers/merge.py\", line 354, in build\n    'Got inputs shapes: %s' % (input_shape))\nValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12), (None, 1, 12)]\n```\n\n",
    "9": "Your output should follow these steps:\n1. Analyze the buggy function and its relationship with the buggy class, related functions, test code, corresponding error message, the actual input/output variable information, the expected input/output variable information, the github issue.\n2. Identify a potential error location within the buggy function.\n3. Elucidate the bug's cause using:\n   (a) The buggy function, \n   (b) The buggy class docs, \n   (c) The related functions, \n   (d) The failing test, \n   (e) The corresponding error message, \n   (f) The actual input/output variable values, \n   (g) The expected input/output variable values, \n   (h) The GitHub Issue information\n\n4. Suggest approaches for fixing the bug.\n5. Present the corrected code for the buggy function such that it satisfied the following:\n   (a) the program passes the failing test, \n   (b) the function satisfies the expected input/output variable information provided, \n   (c) successfully resolves the issue posted in GitHub\n\n",
    "1.3.3": "Assume that the following list of imports are available in the current environment, so you don't need to import them when generating a fix.\n```python\nfrom ..utils.generic_utils import unpack_singleton\nfrom ..layers import deserialize as deserialize_layer\n```\n\n",
    "source_code_body": "# This function from the same file, but not the same class, is called by the buggy function\ndef add_unprocessed_node(layer, node_data):\n    # Please ignore the body of this function\n\n# This function from the same file, but not the same class, is called by the buggy function\ndef process_node(layer, node_data):\n    # Please ignore the body of this function\n\n# This function from the same file, but not the same class, is called by the buggy function\ndef process_layer(layer_data):\n    # Please ignore the body of this function\n\n# The declaration of the class containing the buggy function\nclass Network(Layer):\n    \"\"\"\n    A Network is a directed acyclic graph of layers.\n    \n    It is the topological form of a \"model\". A Model\n    is simply a Network with added training routines.\n    \n    # Properties\n        name\n        inputs\n        outputs\n        layers\n        input_spec (list of class instances)\n            each entry describes one required input:\n                - ndim\n                - dtype\n        trainable (boolean)\n        input_shape\n        output_shape\n        weights (list of variables)\n        trainable_weights (list of variables)\n        non_trainable_weights (list of variables)\n        losses\n        updates\n        state_updates\n        stateful\n    \n    # Methods\n        __call__\n        summary\n        get_layer\n        get_weights\n        set_weights\n        get_config\n        compute_output_shape\n        save\n        add_loss\n        add_update\n        get_losses_for\n        get_updates_for\n        to_json\n        to_yaml\n        reset_states\n    \n    # Class Methods\n        from_config\n    \n    # Raises\n        TypeError: if input tensors are not Keras tensors\n            (tensors returned by `Input`).\n    \"\"\"\n\n\n    # This function from the same class is called by the buggy function\n    def add_unprocessed_node(layer, node_data):\n        # Please ignore the body of this function\n\n    # This function from the same class is called by the buggy function\n    def process_node(layer, node_data):\n        # Please ignore the body of this function\n\n    # This function from the same class is called by the buggy function\n    def process_layer(layer_data):\n        # Please ignore the body of this function\n\n\n\n    # this is the buggy function you need to fix\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        \"\"\"Instantiates a Model from its config (output of `get_config()`).\n    \n        # Arguments\n            config: Model config dictionary.\n            custom_objects: Optional dictionary mapping names\n                (strings) to custom classes or functions to be\n                considered during deserialization.\n    \n        # Returns\n            A model instance.\n    \n        # Raises\n            ValueError: In case of improperly formatted config dict.\n        \"\"\"\n        # Layer instances created during\n        # the graph reconstruction process\n        created_layers = {}\n    \n        # Dictionary mapping layer instances to\n        # node data that specifies a layer call.\n        # It acts as a queue that maintains any unprocessed\n        # layer call until it becomes possible to process it\n        # (i.e. until the input tensors to the call all exist).\n        unprocessed_nodes = {}\n    \n        def add_unprocessed_node(layer, node_data):\n            if layer not in unprocessed_nodes:\n                unprocessed_nodes[layer] = [node_data]\n            else:\n                unprocessed_nodes[layer].append(node_data)\n    \n        def process_node(layer, node_data):\n            input_tensors = []\n            for input_data in node_data:\n                inbound_layer_name = input_data[0]\n                inbound_node_index = input_data[1]\n                inbound_tensor_index = input_data[2]\n                if len(input_data) == 3:\n                    kwargs = {}\n                elif len(input_data) == 4:\n                    kwargs = input_data[3]\n                else:\n                    raise ValueError('Improperly formatted model config.')\n                inbound_layer = created_layers[inbound_layer_name]\n                if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                input_tensors.append(\n                    inbound_node.output_tensors[inbound_tensor_index])\n            # Call layer on its inputs, thus creating the node\n            # and building the layer if needed.\n            if input_tensors:\n                layer(unpack_singleton(input_tensors), **kwargs)\n    \n        def process_layer(layer_data):\n            \"\"\"Deserializes a layer, then call it on appropriate inputs.\n    \n            # Arguments\n                layer_data: layer config dict.\n    \n            # Raises\n                ValueError: In case of improperly formatted `layer_data` dict.\n            \"\"\"\n            layer_name = layer_data['name']\n    \n            # Instantiate layer.\n            from ..layers import deserialize as deserialize_layer\n    \n            layer = deserialize_layer(layer_data,\n                                      custom_objects=custom_objects)\n            created_layers[layer_name] = layer\n    \n            # Gather layer inputs.\n            inbound_nodes_data = layer_data['inbound_nodes']\n            for node_data in inbound_nodes_data:\n                # We don't process nodes (i.e. make layer calls)\n                # on the fly because the inbound node may not yet exist,\n                # in case of layer shared at different topological depths\n                # (e.g. a model such as A(B(A(B(x)))))\n                add_unprocessed_node(layer, node_data)\n    \n        # First, we create all layers and enqueue nodes to be processed\n        for layer_data in config['layers']:\n            process_layer(layer_data)\n        # Then we process nodes in order of layer depth.\n        # Nodes that cannot yet be processed (if the inbound node\n        # does not yet exist) are re-enqueued, and the process\n        # is repeated until all nodes are processed.\n        while unprocessed_nodes:\n            for layer_data in config['layers']:\n                layer = created_layers[layer_data['name']]\n                if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n    \n        name = config.get('name')\n        input_tensors = []\n        output_tensors = []\n        for layer_data in config['input_layers']:\n            layer_name, node_index, tensor_index = layer_data\n            assert layer_name in created_layers\n            layer = created_layers[layer_name]\n            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n            input_tensors.append(layer_output_tensors[tensor_index])\n        for layer_data in config['output_layers']:\n            layer_name, node_index, tensor_index = layer_data\n            assert layer_name in created_layers\n            layer = created_layers[layer_name]\n            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n            output_tensors.append(layer_output_tensors[tensor_index])\n        return cls(inputs=input_tensors, outputs=output_tensors, name=name)\n    \n"
}