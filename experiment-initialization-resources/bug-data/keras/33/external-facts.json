{
    "keras:33": {
        "github_issue_title": [
            "Tokenization crashes when split string has more than one character\n"
        ],
        "github_issue_description": [
            "`from keras.preprocessing.text import Tokenizer\n\ntexts = ['Just any text.']\nt = Tokenizer(split=\"any\")\nt.fit_on_texts(texts)\nprint(t.word_index)`\n\nthrows an exception:\nValueError: the first two maketrans arguments must have equal length\n"
        ]
    }
}