{
    "keras:40": {
        "/home/ubuntu/Desktop/bgp_envs_local/repos/keras_40/keras/layers/recurrent.py": {
            "buggy_functions": [
                {
                    "function_name": "compute_output_shape",
                    "function_code": "def compute_output_shape(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n\n    if hasattr(self.cell.state_size, '__len__'):\n        output_dim = self.cell.state_size[0]\n    else:\n        output_dim = self.cell.state_size\n\n    if self.return_sequences:\n        output_shape = (input_shape[0], input_shape[1], output_dim)\n    else:\n        output_shape = (input_shape[0], output_dim)\n\n    if self.return_state:\n        state_shape = [(input_shape[0], output_dim) for _ in self.states]\n        return [output_shape] + state_shape\n    else:\n        return output_shape\n",
                    "decorators": [],
                    "docstring": null,
                    "start_line": 393,
                    "end_line": 411,
                    "variables": {
                        "isinstance": [
                            394
                        ],
                        "input_shape": [
                            394,
                            395,
                            403,
                            405,
                            408
                        ],
                        "list": [
                            394
                        ],
                        "hasattr": [
                            397
                        ],
                        "self.cell.state_size": [
                            400,
                            397,
                            398
                        ],
                        "self.cell": [
                            400,
                            397,
                            398
                        ],
                        "self": [
                            397,
                            398,
                            400,
                            402,
                            407,
                            408
                        ],
                        "output_dim": [
                            398,
                            400,
                            403,
                            405,
                            408
                        ],
                        "self.return_sequences": [
                            402
                        ],
                        "output_shape": [
                            409,
                            403,
                            411,
                            405
                        ],
                        "self.return_state": [
                            407
                        ],
                        "state_shape": [
                            408,
                            409
                        ],
                        "_": [
                            408
                        ],
                        "self.states": [
                            408
                        ]
                    },
                    "filtered_variables": {
                        "input_shape": [
                            394,
                            395,
                            403,
                            405,
                            408
                        ],
                        "self.cell.state_size": [
                            400,
                            397,
                            398
                        ],
                        "self.cell": [
                            400,
                            397,
                            398
                        ],
                        "self": [
                            397,
                            398,
                            400,
                            402,
                            407,
                            408
                        ],
                        "output_dim": [
                            398,
                            400,
                            403,
                            405,
                            408
                        ],
                        "self.return_sequences": [
                            402
                        ],
                        "output_shape": [
                            409,
                            403,
                            411,
                            405
                        ],
                        "self.return_state": [
                            407
                        ],
                        "state_shape": [
                            408,
                            409
                        ],
                        "_": [
                            408
                        ],
                        "self.states": [
                            408
                        ]
                    },
                    "diff_line_number": 398,
                    "class_data": {
                        "signature": "class RNN(Layer)",
                        "docstring": "Base class for recurrent layers.\n\n# Arguments\n    cell: A RNN cell instance. A RNN cell is a class that has:\n        - a `call(input_at_t, states_at_t)` method, returning\n            `(output_at_t, states_at_t_plus_1)`. The call method of the\n            cell can also take the optional argument `constants`, see\n            section \"Note on passing external constants\" below.\n        - a `state_size` attribute. This can be a single integer\n            (single state) in which case it is\n            the size of the recurrent state\n            (which should be the same as the size of the cell output).\n            This can also be a list/tuple of integers\n            (one size per state). In this case, the first entry\n            (`state_size[0]`) should be the same as\n            the size of the cell output.\n        It is also possible for `cell` to be a list of RNN cell instances,\n        in which cases the cells get stacked on after the other in the RNN,\n        implementing an efficient stacked RNN.\n    return_sequences: Boolean. Whether to return the last output.\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    unroll: Boolean (default False).\n        If True, the network will be unrolled,\n        else a symbolic loop will be used.\n        Unrolling can speed-up a RNN,\n        although it tends to be more memory-intensive.\n        Unrolling is only suitable for short sequences.\n    input_dim: dimensionality of the input (integer).\n        This argument (or alternatively,\n        the keyword argument `input_shape`)\n        is required when using this layer as the first layer in a model.\n    input_length: Length of input sequences, to be specified\n        when it is constant.\n        This argument is required if you are going to connect\n        `Flatten` then `Dense` layers upstream\n        (without it, the shape of the dense outputs cannot be computed).\n        Note that if the recurrent layer is not the first layer\n        in your model, you would need to specify the input length\n        at the level of the first layer\n        (e.g. via the `input_shape` argument)\n\n# Input shape\n    3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n# Output shape\n    - if `return_state`: a list of tensors. The first tensor is\n        the output. The remaining tensors are the last states,\n        each with shape `(batch_size, units)`.\n    - if `return_sequences`: 3D tensor with shape\n        `(batch_size, timesteps, units)`.\n    - else, 2D tensor with shape `(batch_size, units)`.\n\n# Masking\n    This layer supports masking for input data with a variable number\n    of timesteps. To introduce masks to your data,\n    use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n    set to `True`.\n\n# Note on using statefulness in RNNs\n    You can set RNN layers to be 'stateful', which means that the states\n    computed for the samples in one batch will be reused as initial states\n    for the samples in the next batch. This assumes a one-to-one mapping\n    between samples in different successive batches.\n\n    To enable statefulness:\n        - specify `stateful=True` in the layer constructor.\n        - specify a fixed batch size for your model, by passing\n            if sequential model:\n              `batch_input_shape=(...)` to the first layer in your model.\n            else for functional model with 1 or more Input layers:\n              `batch_shape=(...)` to all the first layers in your model.\n            This is the expected shape of your inputs\n            *including the batch size*.\n            It should be a tuple of integers, e.g. `(32, 10, 100)`.\n        - specify `shuffle=False` when calling fit().\n\n    To reset the states of your model, call `.reset_states()` on either\n    a specific layer, or on your entire model.\n\n# Note on specifying the initial state of RNNs\n    You can specify the initial state of RNN layers symbolically by\n    calling them with the keyword argument `initial_state`. The value of\n    `initial_state` should be a tensor or list of tensors representing\n    the initial state of the RNN layer.\n\n    You can specify the initial state of RNN layers numerically by\n    calling `reset_states` with the keyword argument `states`. The value of\n    `states` should be a numpy array or list of numpy arrays representing\n    the initial state of the RNN layer.\n\n# Note on passing external constants to RNNs\n    You can pass \"external\" constants to the cell using the `constants`\n    keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n    requires that the `cell.call` method accepts the same keyword argument\n    `constants`. Such constants can be used to condition the cell\n    transformation on additional static inputs (not changing over time),\n    a.k.a. an attention mechanism.\n\n# Examples\n\n```python\n    # First, let's define a RNN Cell, as a layer subclass.\n\n    class MinimalRNNCell(keras.layers.Layer):\n\n        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(MinimalRNNCell, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                          initializer='uniform',\n                                          name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.built = True\n\n        def call(self, inputs, states):\n            prev_output = states[0]\n            h = K.dot(inputs, self.kernel)\n            output = h + K.dot(prev_output, self.recurrent_kernel)\n            return output, [output]\n\n    # Let's use this cell in a RNN layer:\n\n    cell = MinimalRNNCell(32)\n    x = keras.Input((None, 5))\n    layer = RNN(cell)\n    y = layer(x)\n\n    # Here's how to use the cell to build a stacked RNN:\n\n    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n    x = keras.Input((None, 5))\n    layer = RNN(cells)\n    y = layer(x)\n```",
                        "constructor_docstring": null,
                        "functions": [
                            "def __init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs):\n    if isinstance(cell, (list, tuple)):\n        cell = StackedRNNCells(cell)\n    if not hasattr(cell, 'call'):\n        raise ValueError('`cell` should have a `call` method. The RNN was passed:', cell)\n    if not hasattr(cell, 'state_size'):\n        raise ValueError('The RNN cell should have an attribute `state_size` (tuple of integers, one integer per RNN state).')\n    super(RNN, self).__init__(**kwargs)\n    self.cell = cell\n    self.return_sequences = return_sequences\n    self.return_state = return_state\n    self.go_backwards = go_backwards\n    self.stateful = stateful\n    self.unroll = unroll\n    self.supports_masking = True\n    self.input_spec = [InputSpec(ndim=3)]\n    self.state_spec = None\n    self._states = None\n    self.constants_spec = None\n    self._num_constants = None",
                            "@property\ndef states(self):\n    if self._states is None:\n        if isinstance(self.cell.state_size, int):\n            num_states = 1\n        else:\n            num_states = len(self.cell.state_size)\n        return [None for _ in range(num_states)]\n    return self._states",
                            "@states.setter\ndef states(self, states):\n    self._states = states",
                            "def compute_output_shape(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    if hasattr(self.cell.state_size, '__len__'):\n        output_dim = self.cell.state_size[0]\n    else:\n        output_dim = self.cell.state_size\n    if self.return_sequences:\n        output_shape = (input_shape[0], input_shape[1], output_dim)\n    else:\n        output_shape = (input_shape[0], output_dim)\n    if self.return_state:\n        state_shape = [(input_shape[0], output_dim) for _ in self.states]\n        return [output_shape] + state_shape\n    else:\n        return output_shape",
                            "def compute_mask(self, inputs, mask):\n    if isinstance(mask, list):\n        mask = mask[0]\n    output_mask = mask if self.return_sequences else None\n    if self.return_state:\n        state_mask = [None for _ in self.states]\n        return [output_mask] + state_mask\n    else:\n        return output_mask",
                            "def build(self, input_shape):\n    if self._num_constants is not None:\n        constants_shape = input_shape[-self._num_constants:]\n    else:\n        constants_shape = None\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    batch_size = input_shape[0] if self.stateful else None\n    input_dim = input_shape[-1]\n    self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))\n    if isinstance(self.cell, Layer):\n        step_input_shape = (input_shape[0],) + input_shape[2:]\n        if constants_shape is not None:\n            self.cell.build([step_input_shape] + constants_shape)\n        else:\n            self.cell.build(step_input_shape)\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = list(self.cell.state_size)\n    else:\n        state_size = [self.cell.state_size]\n    if self.state_spec is not None:\n        if not [spec.shape[-1] for spec in self.state_spec] == state_size:\n            raise ValueError('An initial_state was passed that is not compatible with `cell.state_size`. Received `state_spec`={}; However `cell.state_size` is {}'.format(self.state_spec, self.cell.state_size))\n    else:\n        self.state_spec = [InputSpec(shape=(None, dim)) for dim in state_size]\n    if self.stateful:\n        self.reset_states()",
                            "def get_initial_state(self, inputs):\n    initial_state = K.zeros_like(inputs)\n    initial_state = K.sum(initial_state, axis=(1, 2))\n    initial_state = K.expand_dims(initial_state)\n    if hasattr(self.cell.state_size, '__len__'):\n        return [K.tile(initial_state, [1, dim]) for dim in self.cell.state_size]\n    else:\n        return [K.tile(initial_state, [1, self.cell.state_size])]",
                            "def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n    (inputs, initial_state, constants) = self._standardize_args(inputs, initial_state, constants)\n    if initial_state is None and constants is None:\n        return super(RNN, self).__call__(inputs, **kwargs)\n    additional_inputs = []\n    additional_specs = []\n    if initial_state is not None:\n        kwargs['initial_state'] = initial_state\n        additional_inputs += initial_state\n        self.state_spec = [InputSpec(shape=K.int_shape(state)) for state in initial_state]\n        additional_specs += self.state_spec\n    if constants is not None:\n        kwargs['constants'] = constants\n        additional_inputs += constants\n        self.constants_spec = [InputSpec(shape=K.int_shape(constant)) for constant in constants]\n        self._num_constants = len(constants)\n        additional_specs += self.constants_spec\n    is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')\n    for tensor in additional_inputs:\n        if hasattr(tensor, '_keras_history') != is_keras_tensor:\n            raise ValueError('The initial state or constants of an RNN layer cannot be specified with a mix of Keras tensors and non-Keras tensors')\n    if is_keras_tensor:\n        full_input = [inputs] + additional_inputs\n        full_input_spec = self.input_spec + additional_specs\n        original_input_spec = self.input_spec\n        self.input_spec = full_input_spec\n        output = super(RNN, self).__call__(full_input, **kwargs)\n        self.input_spec = original_input_spec\n        return output\n    else:\n        return super(RNN, self).__call__(inputs, **kwargs)",
                            "def call(self, inputs, mask=None, training=None, initial_state=None, constants=None):\n    if isinstance(inputs, list):\n        inputs = inputs[0]\n    if initial_state is not None:\n        pass\n    elif self.stateful:\n        initial_state = self.states\n    else:\n        initial_state = self.get_initial_state(inputs)\n    if isinstance(mask, list):\n        mask = mask[0]\n    if len(initial_state) != len(self.states):\n        raise ValueError('Layer has ' + str(len(self.states)) + ' states but was passed ' + str(len(initial_state)) + ' initial states.')\n    input_shape = K.int_shape(inputs)\n    timesteps = input_shape[1]\n    if self.unroll and timesteps in [None, 1]:\n        raise ValueError('Cannot unroll a RNN if the time dimension is undefined or equal to 1. \\n- If using a Sequential model, specify the time dimension by passing an `input_shape` or `batch_input_shape` argument to your first layer. If your first layer is an Embedding, you can also use the `input_length` argument.\\n- If using the functional API, specify the time dimension by passing a `shape` or `batch_shape` argument to your Input layer.')\n    kwargs = {}\n    if has_arg(self.cell.call, 'training'):\n        kwargs['training'] = training\n    if constants:\n        if not has_arg(self.cell.call, 'constants'):\n            raise ValueError('RNN cell does not support constants')\n\n        def step(inputs, states):\n            constants = states[-self._num_constants:]\n            states = states[:-self._num_constants]\n            return self.cell.call(inputs, states, constants=constants, **kwargs)\n    else:\n\n        def step(inputs, states):\n            return self.cell.call(inputs, states, **kwargs)\n    (last_output, outputs, states) = K.rnn(step, inputs, initial_state, constants=constants, go_backwards=self.go_backwards, mask=mask, unroll=self.unroll, input_length=timesteps)\n    if self.stateful:\n        updates = []\n        for i in range(len(states)):\n            updates.append((self.states[i], states[i]))\n        self.add_update(updates, inputs)\n    if self.return_sequences:\n        output = outputs\n    else:\n        output = last_output\n    if getattr(last_output, '_uses_learning_phase', False):\n        output._uses_learning_phase = True\n    if self.return_state:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        else:\n            states = list(states)\n        return [output] + states\n    else:\n        return output",
                            "def _standardize_args(self, inputs, initial_state, constants):\n    \"\"\"Brings the arguments of `__call__` that can contain input tensors to\n    standard format.\n\n    When running a model loaded from file, the input tensors\n    `initial_state` and `constants` can be passed to `RNN.__call__` as part\n    of `inputs` instead of by the dedicated keyword arguments. This method\n    makes sure the arguments are separated and that `initial_state` and\n    `constants` are lists of tensors (or None).\n\n    # Arguments\n        inputs: tensor or list/tuple of tensors\n        initial_state: tensor or list of tensors or None\n        constants: tensor or list of tensors or None\n\n    # Returns\n        inputs: tensor\n        initial_state: list of tensors or None\n        constants: list of tensors or None\n    \"\"\"\n    if isinstance(inputs, list):\n        assert initial_state is None and constants is None\n        if self._num_constants is not None:\n            constants = inputs[-self._num_constants:]\n            inputs = inputs[:-self._num_constants]\n        if len(inputs) > 1:\n            initial_state = inputs[1:]\n        inputs = inputs[0]\n\n    def to_list_or_none(x):\n        if x is None or isinstance(x, list):\n            return x\n        if isinstance(x, tuple):\n            return list(x)\n        return [x]\n    initial_state = to_list_or_none(initial_state)\n    constants = to_list_or_none(constants)\n    return (inputs, initial_state, constants)",
                            "def reset_states(self, states=None):\n    if not self.stateful:\n        raise AttributeError('Layer must be stateful.')\n    batch_size = self.input_spec[0].shape[0]\n    if not batch_size:\n        raise ValueError('If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \\n- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.\\n- If using the functional API, specify the time dimension by passing a `batch_shape` argument to your Input layer.')\n    if self.states[0] is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            self.states = [K.zeros((batch_size, dim)) for dim in self.cell.state_size]\n        else:\n            self.states = [K.zeros((batch_size, self.cell.state_size))]\n    elif states is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            for (state, dim) in zip(self.states, self.cell.state_size):\n                K.set_value(state, np.zeros((batch_size, dim)))\n        else:\n            K.set_value(self.states[0], np.zeros((batch_size, self.cell.state_size)))\n    else:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        if len(states) != len(self.states):\n            raise ValueError('Layer ' + self.name + ' expects ' + str(len(self.states)) + ' states, but it received ' + str(len(states)) + ' state values. Input received: ' + str(states))\n        for (index, (value, state)) in enumerate(zip(states, self.states)):\n            if hasattr(self.cell.state_size, '__len__'):\n                dim = self.cell.state_size[index]\n            else:\n                dim = self.cell.state_size\n            if value.shape != (batch_size, dim):\n                raise ValueError('State ' + str(index) + ' is incompatible with layer ' + self.name + ': expected shape=' + str((batch_size, dim)) + ', found shape=' + str(value.shape))\n            K.set_value(state, value)",
                            "def get_config(self):\n    config = {'return_sequences': self.return_sequences, 'return_state': self.return_state, 'go_backwards': self.go_backwards, 'stateful': self.stateful, 'unroll': self.unroll}\n    if self._num_constants is not None:\n        config['num_constants'] = self._num_constants\n    cell_config = self.cell.get_config()\n    config['cell'] = {'class_name': self.cell.__class__.__name__, 'config': cell_config}\n    base_config = super(RNN, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                            "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cell = deserialize_layer(config.pop('cell'), custom_objects=custom_objects)\n    num_constants = config.pop('num_constants', None)\n    layer = cls(cell, **config)\n    layer._num_constants = num_constants\n    return layer",
                            "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    if isinstance(self.cell, Layer):\n        return self.cell.trainable_weights\n    return []",
                            "@property\ndef non_trainable_weights(self):\n    if isinstance(self.cell, Layer):\n        if not self.trainable:\n            return self.cell.weights\n        return self.cell.non_trainable_weights\n    return []",
                            "@property\ndef losses(self):\n    if isinstance(self.cell, Layer):\n        return self.cell.losses\n    return []",
                            "def get_losses_for(self, inputs=None):\n    if isinstance(self.cell, Layer):\n        cell_losses = self.cell.get_losses_for(inputs)\n        return cell_losses + super(RNN, self).get_losses_for(inputs)\n    return super(RNN, self).get_losses_for(inputs)",
                            "def step(inputs, states):\n    constants = states[-self._num_constants:]\n    states = states[:-self._num_constants]\n    return self.cell.call(inputs, states, constants=constants, **kwargs)",
                            "def step(inputs, states):\n    return self.cell.call(inputs, states, **kwargs)"
                        ],
                        "constructor_variables": [
                            "self.cell = cell",
                            "self.return_sequences = return_sequences",
                            "self.return_state = return_state",
                            "self.go_backwards = go_backwards",
                            "self.stateful = stateful",
                            "self.unroll = unroll",
                            "self.supports_masking = True",
                            "self.input_spec = [InputSpec(ndim=3)]",
                            "self.state_spec = None",
                            "self._states = None",
                            "self.constants_spec = None",
                            "self._num_constants = None"
                        ],
                        "class_level_variables": [],
                        "class_decorators": [],
                        "function_signatures": [
                            "__init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
                            "states(self)",
                            "states(self, states)",
                            "compute_output_shape(self, input_shape)",
                            "compute_mask(self, inputs, mask)",
                            "build(self, input_shape)",
                            "get_initial_state(self, inputs)",
                            "__call__(self, inputs, initial_state=None, constants=None, **kwargs)",
                            "call(self, inputs, mask=None, training=None, initial_state=None, constants=None)",
                            "_standardize_args(self, inputs, initial_state, constants)",
                            "reset_states(self, states=None)",
                            "get_config(self)",
                            "from_config(cls, config, custom_objects=None)",
                            "trainable_weights(self)",
                            "non_trainable_weights(self)",
                            "losses(self)",
                            "get_losses_for(self, inputs=None)",
                            "step(inputs, states)",
                            "step(inputs, states)"
                        ],
                        "class_level_variable_names": [],
                        "constructor_variable_names": [
                            "constants_spec",
                            "supports_masking",
                            "_num_constants",
                            "input_spec",
                            "_states",
                            "return_sequences",
                            "stateful",
                            "unroll",
                            "go_backwards",
                            "state_spec",
                            "cell",
                            "return_state"
                        ]
                    },
                    "used_imports": [],
                    "variable_values": [
                        [
                            {
                                "input_shape": {
                                    "variable_value": "(None, 5, 4)",
                                    "variable_type": "tuple",
                                    "variable_shape": "3"
                                },
                                "self.cell.state_size": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.cell": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x7f7b4a9586d0>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.RNN object at 0x7f7b4a9bdfd0>",
                                    "variable_type": "RNN",
                                    "variable_shape": null
                                },
                                "output_dim": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "output_shape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "state_shape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "_": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.states": {
                                    "variable_value": "[None, None, None, None]",
                                    "variable_type": "list",
                                    "variable_shape": "4"
                                }
                            },
                            {
                                "input_shape": {
                                    "variable_value": "(None, 5, 4)",
                                    "variable_type": "tuple",
                                    "variable_shape": "3"
                                },
                                "self.cell.state_size": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.cell": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x7f7b4a9586d0>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.RNN object at 0x7f7b4a9bdfd0>",
                                    "variable_type": "RNN",
                                    "variable_shape": null
                                },
                                "output_dim": {
                                    "variable_value": "6",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "output_shape": {
                                    "variable_value": "(None, 5, 6)",
                                    "variable_type": "tuple",
                                    "variable_shape": "3"
                                },
                                "self.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "state_shape": {
                                    "variable_value": "[(None, 6), (None, 6), (None, 6), (None, 6)]",
                                    "variable_type": "list",
                                    "variable_shape": "4"
                                },
                                "_": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.states": {
                                    "variable_value": "[None, None, None, None]",
                                    "variable_type": "list",
                                    "variable_shape": "4"
                                }
                            }
                        ]
                    ],
                    "angelic_variable_values": [
                        [
                            {
                                "input_shape": {
                                    "variable_value": "(None, 5, 4)",
                                    "variable_type": "tuple",
                                    "variable_shape": "3"
                                },
                                "self.cell.state_size": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.cell": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x7fdc8bb7c890>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.RNN object at 0x7fdc8bb7c690>",
                                    "variable_type": "RNN",
                                    "variable_shape": null
                                },
                                "state_size": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "output_dim": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "output_shape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "self.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "state_shape": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                },
                                "dim": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            },
                            {
                                "input_shape": {
                                    "variable_value": "(None, 5, 4)",
                                    "variable_type": "tuple",
                                    "variable_shape": "3"
                                },
                                "self.cell.state_size": {
                                    "variable_value": "None",
                                    "variable_type": "NoneType",
                                    "variable_shape": null
                                },
                                "self.cell": {
                                    "variable_value": "<keras.layers.recurrent.StackedRNNCells object at 0x7fdc8bb7c890>",
                                    "variable_type": "StackedRNNCells",
                                    "variable_shape": null
                                },
                                "self": {
                                    "variable_value": "<keras.layers.recurrent.RNN object at 0x7fdc8bb7c690>",
                                    "variable_type": "RNN",
                                    "variable_shape": null
                                },
                                "state_size": {
                                    "variable_value": "(6, 6, 3, 3)",
                                    "variable_type": "tuple",
                                    "variable_shape": "4"
                                },
                                "output_dim": {
                                    "variable_value": "6",
                                    "variable_type": "int",
                                    "variable_shape": null
                                },
                                "self.return_sequences": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "output_shape": {
                                    "variable_value": "(None, 5, 6)",
                                    "variable_type": "tuple",
                                    "variable_shape": "3"
                                },
                                "self.return_state": {
                                    "variable_value": "True",
                                    "variable_type": "bool",
                                    "variable_shape": null
                                },
                                "state_shape": {
                                    "variable_value": "[(None, 6), (None, 6), (None, 3), (None, 3)]",
                                    "variable_type": "list",
                                    "variable_shape": "4"
                                },
                                "dim": {
                                    "variable_value": null,
                                    "variable_type": "None",
                                    "variable_shape": null
                                }
                            }
                        ]
                    ]
                }
            ],
            "inscope_functions": {
                "file_scope_functions": [
                    {
                        "code": "def _generate_dropout_ones(inputs, dims):\n    # Currently, CTNK can't instantiate `ones` with symbolic shapes.\n    # Will update workaround once CTNK supports it.\n    if K.backend() == 'cntk':\n        ones = K.ones_like(K.reshape(inputs[:, 0], (-1, 1)))\n        return K.tile(ones, (1, dims))\n    else:\n        return K.ones((K.shape(inputs)[0], dims))",
                        "signature": "_generate_dropout_ones(inputs, dims)"
                    },
                    {
                        "code": "def _generate_dropout_mask(ones, rate, training=None, count=1):\n    def dropped_inputs():\n        return K.dropout(ones, rate)\n\n    if count > 1:\n        return [K.in_train_phase(\n            dropped_inputs,\n            ones,\n            training=training) for _ in range(count)]\n    return K.in_train_phase(\n        dropped_inputs,\n        ones,\n        training=training)",
                        "signature": "_generate_dropout_mask(ones, rate, training=None, count=1)"
                    },
                    {
                        "code": "def step(inputs, states):\n    constants = states[-self._num_constants:]\n    states = states[:-self._num_constants]\n    return self.cell.call(inputs, states, constants=constants,\n                          **kwargs)",
                        "signature": "step(inputs, states)"
                    },
                    {
                        "code": "def step(inputs, states):\n    return self.cell.call(inputs, states, **kwargs)",
                        "signature": "step(inputs, states)"
                    },
                    {
                        "code": "def bias_initializer(shape, *args, **kwargs):\n    return K.concatenate([\n        self.bias_initializer((self.units,), *args, **kwargs),\n        initializers.Ones()((self.units,), *args, **kwargs),\n        self.bias_initializer((self.units * 2,), *args, **kwargs),\n    ])",
                        "signature": "bias_initializer(shape, *args, **kwargs)"
                    }
                ],
                "file_scope_classes": [
                    {
                        "class_declaration": "class StackedRNNCells:",
                        "functions": [
                            {
                                "code": "def __init__(self, cells, **kwargs):\n    for cell in cells:\n        if not hasattr(cell, 'call'):\n            raise ValueError('All cells must have a `call` method. '\n                             'received cells:', cells)\n        if not hasattr(cell, 'state_size'):\n            raise ValueError('All cells must have a '\n                             '`state_size` attribute. '\n                             'received cells:', cells)\n    self.cells = cells\n    super(StackedRNNCells, self).__init__(**kwargs)",
                                "signature": "__init__(self, cells, **kwargs)"
                            },
                            {
                                "code": "@property\ndef state_size(self):\n    # States are a flat list\n    # in reverse order of the cell stack.\n    # This allows to preserve the requirement\n    # `stack.state_size[0] == output_dim`.\n    # e.g. states of a 2-layer LSTM would be\n    # `[h2, c2, h1, c1]`\n    # (assuming one LSTM has states [h, c])\n    state_size = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            state_size += list(cell.state_size)\n        else:\n            state_size.append(cell.state_size)\n    return tuple(state_size)",
                                "signature": "state_size(self)"
                            },
                            {
                                "code": "def call(self, inputs, states, **kwargs):\n    # Recover per-cell states.\n    nested_states = []\n    for cell in self.cells[::-1]:\n        if hasattr(cell.state_size, '__len__'):\n            nested_states.append(states[:len(cell.state_size)])\n            states = states[len(cell.state_size):]\n        else:\n            nested_states.append([states[0]])\n            states = states[1:]\n    nested_states = nested_states[::-1]\n\n    # Call the cells in order and store the returned states.\n    new_nested_states = []\n    for cell, states in zip(self.cells, nested_states):\n        inputs, states = cell.call(inputs, states, **kwargs)\n        new_nested_states.append(states)\n\n    # Format the new states as a flat list\n    # in reverse cell order.\n    states = []\n    for cell_states in new_nested_states[::-1]:\n        states += cell_states\n    return inputs, states",
                                "signature": "call(self, inputs, states, **kwargs)"
                            },
                            {
                                "code": "def build(self, input_shape):\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell.build(input_shape)\n        if hasattr(cell.state_size, '__len__'):\n            output_dim = cell.state_size[0]\n        else:\n            output_dim = cell.state_size\n        input_shape = (input_shape[0], input_shape[1], output_dim)\n    self.built = True",
                                "signature": "build(self, input_shape)"
                            },
                            {
                                "code": "def get_config(self):\n    cells = []\n    for cell in self.cells:\n        cells.append({'class_name': cell.__class__.__name__,\n                      'config': cell.get_config()})\n    config = {'cells': cells}\n    base_config = super(StackedRNNCells, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                                "signature": "get_config(self)"
                            },
                            {
                                "code": "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cells = []\n    for cell_config in config.pop('cells'):\n        cells.append(deserialize_layer(cell_config,\n                                       custom_objects=custom_objects))\n    return cls(cells, **config)",
                                "signature": "from_config(cls, config, custom_objects=None)"
                            },
                            {
                                "code": "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.trainable_weights\n    return weights",
                                "signature": "trainable_weights(self)"
                            },
                            {
                                "code": "@property\ndef non_trainable_weights(self):\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.non_trainable_weights\n    if not self.trainable:\n        trainable_weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                trainable_weights += cell.trainable_weights\n        return trainable_weights + weights\n    return weights",
                                "signature": "non_trainable_weights(self)"
                            },
                            {
                                "code": "def get_weights(self):\n    \"\"\"Retrieves the weights of the model.\n\n    # Returns\n        A flat list of Numpy arrays.\n    \"\"\"\n    weights = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            weights += cell.weights\n    return K.batch_get_value(weights)",
                                "signature": "get_weights(self)"
                            },
                            {
                                "code": "def set_weights(self, weights):\n    \"\"\"Sets the weights of the model.\n\n    # Arguments\n        weights: A list of Numpy arrays with shapes and types matching\n            the output of `model.get_weights()`.\n    \"\"\"\n    tuples = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            num_param = len(cell.weights)\n            weights = weights[:num_param]\n            for sw, w in zip(cell.weights, weights):\n                tuples.append((sw, w))\n            weights = weights[num_param:]\n    K.batch_set_value(tuples)",
                                "signature": "set_weights(self, weights)"
                            },
                            {
                                "code": "@property\ndef losses(self):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.losses\n            losses += cell_losses\n    return losses",
                                "signature": "losses(self)"
                            },
                            {
                                "code": "def get_losses_for(self, inputs=None):\n    losses = []\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell_losses = cell.get_losses_for(inputs)\n            losses += cell_losses\n    return losses",
                                "signature": "get_losses_for(self, inputs=None)"
                            }
                        ]
                    },
                    {
                        "class_declaration": "class RNN:",
                        "functions": [
                            {
                                "code": "def __init__(self, cell,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if isinstance(cell, (list, tuple)):\n        cell = StackedRNNCells(cell)\n    if not hasattr(cell, 'call'):\n        raise ValueError('`cell` should have a `call` method. '\n                         'The RNN was passed:', cell)\n    if not hasattr(cell, 'state_size'):\n        raise ValueError('The RNN cell should have '\n                         'an attribute `state_size` '\n                         '(tuple of integers, '\n                         'one integer per RNN state).')\n    super(RNN, self).__init__(**kwargs)\n    self.cell = cell\n    self.return_sequences = return_sequences\n    self.return_state = return_state\n    self.go_backwards = go_backwards\n    self.stateful = stateful\n    self.unroll = unroll\n\n    self.supports_masking = True\n    self.input_spec = [InputSpec(ndim=3)]\n    self.state_spec = None\n    self._states = None\n    self.constants_spec = None\n    self._num_constants = None",
                                "signature": "__init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)"
                            },
                            {
                                "code": "@property\ndef states(self):\n    if self._states is None:\n        if isinstance(self.cell.state_size, int):\n            num_states = 1\n        else:\n            num_states = len(self.cell.state_size)\n        return [None for _ in range(num_states)]\n    return self._states",
                                "signature": "states(self)"
                            },
                            {
                                "code": "@states.setter\ndef states(self, states):\n    self._states = states",
                                "signature": "states(self, states)"
                            },
                            {
                                "code": "def compute_output_shape(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n\n    if hasattr(self.cell.state_size, '__len__'):\n        output_dim = self.cell.state_size[0]\n    else:\n        output_dim = self.cell.state_size\n\n    if self.return_sequences:\n        output_shape = (input_shape[0], input_shape[1], output_dim)\n    else:\n        output_shape = (input_shape[0], output_dim)\n\n    if self.return_state:\n        state_shape = [(input_shape[0], output_dim) for _ in self.states]\n        return [output_shape] + state_shape\n    else:\n        return output_shape",
                                "signature": "compute_output_shape(self, input_shape)"
                            },
                            {
                                "code": "def compute_mask(self, inputs, mask):\n    if isinstance(mask, list):\n        mask = mask[0]\n    output_mask = mask if self.return_sequences else None\n    if self.return_state:\n        state_mask = [None for _ in self.states]\n        return [output_mask] + state_mask\n    else:\n        return output_mask",
                                "signature": "compute_mask(self, inputs, mask)"
                            },
                            {
                                "code": "def build(self, input_shape):\n    # Note input_shape will be list of shapes of initial states and\n    # constants if these are passed in __call__.\n    if self._num_constants is not None:\n        constants_shape = input_shape[-self._num_constants:]\n    else:\n        constants_shape = None\n\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n\n    batch_size = input_shape[0] if self.stateful else None\n    input_dim = input_shape[-1]\n    self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))\n\n    # allow cell (if layer) to build before we set or validate state_spec\n    if isinstance(self.cell, Layer):\n        step_input_shape = (input_shape[0],) + input_shape[2:]\n        if constants_shape is not None:\n            self.cell.build([step_input_shape] + constants_shape)\n        else:\n            self.cell.build(step_input_shape)\n\n    # set or validate state_spec\n    if hasattr(self.cell.state_size, '__len__'):\n        state_size = list(self.cell.state_size)\n    else:\n        state_size = [self.cell.state_size]\n\n    if self.state_spec is not None:\n        # initial_state was passed in call, check compatibility\n        if not [spec.shape[-1] for spec in self.state_spec] == state_size:\n            raise ValueError(\n                'An initial_state was passed that is not compatible with '\n                '`cell.state_size`. Received `state_spec`={}; '\n                'However `cell.state_size` is '\n                '{}'.format(self.state_spec, self.cell.state_size))\n    else:\n        self.state_spec = [InputSpec(shape=(None, dim))\n                           for dim in state_size]\n    if self.stateful:\n        self.reset_states()",
                                "signature": "build(self, input_shape)"
                            },
                            {
                                "code": "def get_initial_state(self, inputs):\n    # build an all-zero tensor of shape (samples, output_dim)\n    initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n    initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n    initial_state = K.expand_dims(initial_state)  # (samples, 1)\n    if hasattr(self.cell.state_size, '__len__'):\n        return [K.tile(initial_state, [1, dim])\n                for dim in self.cell.state_size]\n    else:\n        return [K.tile(initial_state, [1, self.cell.state_size])]",
                                "signature": "get_initial_state(self, inputs)"
                            },
                            {
                                "code": "def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n    inputs, initial_state, constants = self._standardize_args(\n        inputs, initial_state, constants)\n\n    if initial_state is None and constants is None:\n        return super(RNN, self).__call__(inputs, **kwargs)\n\n    # If any of `initial_state` or `constants` are specified and are Keras\n    # tensors, then add them to the inputs and temporarily modify the\n    # input_spec to include them.\n\n    additional_inputs = []\n    additional_specs = []\n    if initial_state is not None:\n        kwargs['initial_state'] = initial_state\n        additional_inputs += initial_state\n        self.state_spec = [InputSpec(shape=K.int_shape(state))\n                           for state in initial_state]\n        additional_specs += self.state_spec\n    if constants is not None:\n        kwargs['constants'] = constants\n        additional_inputs += constants\n        self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n                               for constant in constants]\n        self._num_constants = len(constants)\n        additional_specs += self.constants_spec\n    # at this point additional_inputs cannot be empty\n    is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')\n    for tensor in additional_inputs:\n        if hasattr(tensor, '_keras_history') != is_keras_tensor:\n            raise ValueError('The initial state or constants of an RNN'\n                             ' layer cannot be specified with a mix of'\n                             ' Keras tensors and non-Keras tensors')\n\n    if is_keras_tensor:\n        # Compute the full input spec, including state and constants\n        full_input = [inputs] + additional_inputs\n        full_input_spec = self.input_spec + additional_specs\n        # Perform the call with temporarily replaced input_spec\n        original_input_spec = self.input_spec\n        self.input_spec = full_input_spec\n        output = super(RNN, self).__call__(full_input, **kwargs)\n        self.input_spec = original_input_spec\n        return output\n    else:\n        return super(RNN, self).__call__(inputs, **kwargs)",
                                "signature": "__call__(self, inputs, initial_state=None, constants=None, **kwargs)"
                            },
                            {
                                "code": "def call(self,\n         inputs,\n         mask=None,\n         training=None,\n         initial_state=None,\n         constants=None):\n    # input shape: `(samples, time (padded with zeros), input_dim)`\n    # note that the .build() method of subclasses MUST define\n    # self.input_spec and self.state_spec with complete input shapes.\n    if isinstance(inputs, list):\n        inputs = inputs[0]\n    if initial_state is not None:\n        pass\n    elif self.stateful:\n        initial_state = self.states\n    else:\n        initial_state = self.get_initial_state(inputs)\n\n    if isinstance(mask, list):\n        mask = mask[0]\n\n    if len(initial_state) != len(self.states):\n        raise ValueError('Layer has ' + str(len(self.states)) +\n                         ' states but was passed ' +\n                         str(len(initial_state)) +\n                         ' initial states.')\n    input_shape = K.int_shape(inputs)\n    timesteps = input_shape[1]\n    if self.unroll and timesteps in [None, 1]:\n        raise ValueError('Cannot unroll a RNN if the '\n                         'time dimension is undefined or equal to 1. \\n'\n                         '- If using a Sequential model, '\n                         'specify the time dimension by passing '\n                         'an `input_shape` or `batch_input_shape` '\n                         'argument to your first layer. If your '\n                         'first layer is an Embedding, you can '\n                         'also use the `input_length` argument.\\n'\n                         '- If using the functional API, specify '\n                         'the time dimension by passing a `shape` '\n                         'or `batch_shape` argument to your Input layer.')\n\n    kwargs = {}\n    if has_arg(self.cell.call, 'training'):\n        kwargs['training'] = training\n\n    if constants:\n        if not has_arg(self.cell.call, 'constants'):\n            raise ValueError('RNN cell does not support constants')\n\n        def step(inputs, states):\n            constants = states[-self._num_constants:]\n            states = states[:-self._num_constants]\n            return self.cell.call(inputs, states, constants=constants,\n                                  **kwargs)\n    else:\n        def step(inputs, states):\n            return self.cell.call(inputs, states, **kwargs)\n\n    last_output, outputs, states = K.rnn(step,\n                                         inputs,\n                                         initial_state,\n                                         constants=constants,\n                                         go_backwards=self.go_backwards,\n                                         mask=mask,\n                                         unroll=self.unroll,\n                                         input_length=timesteps)\n    if self.stateful:\n        updates = []\n        for i in range(len(states)):\n            updates.append((self.states[i], states[i]))\n        self.add_update(updates, inputs)\n\n    if self.return_sequences:\n        output = outputs\n    else:\n        output = last_output\n\n    # Properly set learning phase\n    if getattr(last_output, '_uses_learning_phase', False):\n        output._uses_learning_phase = True\n\n    if self.return_state:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        else:\n            states = list(states)\n        return [output] + states\n    else:\n        return output",
                                "signature": "call(self, inputs, mask=None, training=None, initial_state=None, constants=None)"
                            },
                            {
                                "code": "def _standardize_args(self, inputs, initial_state, constants):\n    \"\"\"Brings the arguments of `__call__` that can contain input tensors to\n    standard format.\n\n    When running a model loaded from file, the input tensors\n    `initial_state` and `constants` can be passed to `RNN.__call__` as part\n    of `inputs` instead of by the dedicated keyword arguments. This method\n    makes sure the arguments are separated and that `initial_state` and\n    `constants` are lists of tensors (or None).\n\n    # Arguments\n        inputs: tensor or list/tuple of tensors\n        initial_state: tensor or list of tensors or None\n        constants: tensor or list of tensors or None\n\n    # Returns\n        inputs: tensor\n        initial_state: list of tensors or None\n        constants: list of tensors or None\n    \"\"\"\n    if isinstance(inputs, list):\n        assert initial_state is None and constants is None\n        if self._num_constants is not None:\n            constants = inputs[-self._num_constants:]\n            inputs = inputs[:-self._num_constants]\n        if len(inputs) > 1:\n            initial_state = inputs[1:]\n        inputs = inputs[0]\n\n    def to_list_or_none(x):\n        if x is None or isinstance(x, list):\n            return x\n        if isinstance(x, tuple):\n            return list(x)\n        return [x]\n\n    initial_state = to_list_or_none(initial_state)\n    constants = to_list_or_none(constants)\n\n    return inputs, initial_state, constants",
                                "signature": "_standardize_args(self, inputs, initial_state, constants)"
                            },
                            {
                                "code": "def reset_states(self, states=None):\n    if not self.stateful:\n        raise AttributeError('Layer must be stateful.')\n    batch_size = self.input_spec[0].shape[0]\n    if not batch_size:\n        raise ValueError('If a RNN is stateful, it needs to know '\n                         'its batch size. Specify the batch size '\n                         'of your input tensors: \\n'\n                         '- If using a Sequential model, '\n                         'specify the batch size by passing '\n                         'a `batch_input_shape` '\n                         'argument to your first layer.\\n'\n                         '- If using the functional API, specify '\n                         'the time dimension by passing a '\n                         '`batch_shape` argument to your Input layer.')\n    # initialize state if None\n    if self.states[0] is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            self.states = [K.zeros((batch_size, dim))\n                           for dim in self.cell.state_size]\n        else:\n            self.states = [K.zeros((batch_size, self.cell.state_size))]\n    elif states is None:\n        if hasattr(self.cell.state_size, '__len__'):\n            for state, dim in zip(self.states, self.cell.state_size):\n                K.set_value(state, np.zeros((batch_size, dim)))\n        else:\n            K.set_value(self.states[0],\n                        np.zeros((batch_size, self.cell.state_size)))\n    else:\n        if not isinstance(states, (list, tuple)):\n            states = [states]\n        if len(states) != len(self.states):\n            raise ValueError('Layer ' + self.name + ' expects ' +\n                             str(len(self.states)) + ' states, '\n                             'but it received ' + str(len(states)) +\n                             ' state values. Input received: ' +\n                             str(states))\n        for index, (value, state) in enumerate(zip(states, self.states)):\n            if hasattr(self.cell.state_size, '__len__'):\n                dim = self.cell.state_size[index]\n            else:\n                dim = self.cell.state_size\n            if value.shape != (batch_size, dim):\n                raise ValueError('State ' + str(index) +\n                                 ' is incompatible with layer ' +\n                                 self.name + ': expected shape=' +\n                                 str((batch_size, dim)) +\n                                 ', found shape=' + str(value.shape))\n            # TODO: consider batch calls to `set_value`.\n            K.set_value(state, value)",
                                "signature": "reset_states(self, states=None)"
                            },
                            {
                                "code": "def get_config(self):\n    config = {'return_sequences': self.return_sequences,\n              'return_state': self.return_state,\n              'go_backwards': self.go_backwards,\n              'stateful': self.stateful,\n              'unroll': self.unroll}\n    if self._num_constants is not None:\n        config['num_constants'] = self._num_constants\n\n    cell_config = self.cell.get_config()\n    config['cell'] = {'class_name': self.cell.__class__.__name__,\n                      'config': cell_config}\n    base_config = super(RNN, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
                                "signature": "get_config(self)"
                            },
                            {
                                "code": "@classmethod\ndef from_config(cls, config, custom_objects=None):\n    from . import deserialize as deserialize_layer\n    cell = deserialize_layer(config.pop('cell'),\n                             custom_objects=custom_objects)\n    num_constants = config.pop('num_constants', None)\n    layer = cls(cell, **config)\n    layer._num_constants = num_constants\n    return layer",
                                "signature": "from_config(cls, config, custom_objects=None)"
                            },
                            {
                                "code": "@property\ndef trainable_weights(self):\n    if not self.trainable:\n        return []\n    if isinstance(self.cell, Layer):\n        return self.cell.trainable_weights\n    return []",
                                "signature": "trainable_weights(self)"
                            },
                            {
                                "code": "@property\ndef non_trainable_weights(self):\n    if isinstance(self.cell, Layer):\n        if not self.trainable:\n            return self.cell.weights\n        return self.cell.non_trainable_weights\n    return []",
                                "signature": "non_trainable_weights(self)"
                            },
                            {
                                "code": "@property\ndef losses(self):\n    if isinstance(self.cell, Layer):\n        return self.cell.losses\n    return []",
                                "signature": "losses(self)"
                            },
                            {
                                "code": "def get_losses_for(self, inputs=None):\n    if isinstance(self.cell, Layer):\n        cell_losses = self.cell.get_losses_for(inputs)\n        return cell_losses + super(RNN, self).get_losses_for(inputs)\n    return super(RNN, self).get_losses_for(inputs)",
                                "signature": "get_losses_for(self, inputs=None)"
                            }
                        ]
                    },
                    {
                        "class_declaration": "class SimpleRNNCell:",
                        "functions": [
                            {
                                "code": "def __init__(self, units,\n             activation='tanh',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             **kwargs):\n    super(SimpleRNNCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.state_size = self.units\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                                "signature": "__init__(self, units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, **kwargs)"
                            },
                            {
                                "code": "def build(self, input_shape):\n    self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                  name='kernel',\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units),\n        name='recurrent_kernel',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight(shape=(self.units,),\n                                    name='bias',\n                                    initializer=self.bias_initializer,\n                                    regularizer=self.bias_regularizer,\n                                    constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
                                "signature": "build(self, input_shape)"
                            },
                            {
                                "code": "def call(self, inputs, states, training=None):\n    prev_output = states[0]\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n            self.dropout,\n            training=training)\n    if (0 < self.recurrent_dropout < 1 and\n            self._recurrent_dropout_mask is None):\n        self._recurrent_dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, self.units),\n            self.recurrent_dropout,\n            training=training)\n\n    dp_mask = self._dropout_mask\n    rec_dp_mask = self._recurrent_dropout_mask\n\n    if dp_mask is not None:\n        h = K.dot(inputs * dp_mask, self.kernel)\n    else:\n        h = K.dot(inputs, self.kernel)\n    if self.bias is not None:\n        h = K.bias_add(h, self.bias)\n\n    if rec_dp_mask is not None:\n        prev_output *= rec_dp_mask\n    output = h + K.dot(prev_output, self.recurrent_kernel)\n    if self.activation is not None:\n        output = self.activation(output)\n\n    # Properly set learning phase on output tensor.\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            output._uses_learning_phase = True\n    return output, [output]",
                                "signature": "call(self, inputs, states, training=None)"
                            }
                        ]
                    },
                    {
                        "class_declaration": "class SimpleRNN:",
                        "functions": [
                            {
                                "code": "@interfaces.legacy_recurrent_support\ndef __init__(self, units,\n             activation='tanh',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             activity_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if 'implementation' in kwargs:\n        kwargs.pop('implementation')\n        warnings.warn('The `implementation` argument '\n                      'in `SimpleRNN` has been deprecated. '\n                      'Please remove it from your layer call.')\n    if K.backend() == 'theano':\n        warnings.warn(\n            'RNN dropout is no longer supported with the Theano backend '\n            'due to technical limitations. '\n            'You can either set `dropout` and `recurrent_dropout` to 0, '\n            'or use the TensorFlow backend.')\n        dropout = 0.\n        recurrent_dropout = 0.\n\n    cell = SimpleRNNCell(units,\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         recurrent_initializer=recurrent_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         recurrent_regularizer=recurrent_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         recurrent_constraint=recurrent_constraint,\n                         bias_constraint=bias_constraint,\n                         dropout=dropout,\n                         recurrent_dropout=recurrent_dropout)\n    super(SimpleRNN, self).__init__(cell,\n                                    return_sequences=return_sequences,\n                                    return_state=return_state,\n                                    go_backwards=go_backwards,\n                                    stateful=stateful,\n                                    unroll=unroll,\n                                    **kwargs)\n    self.activity_regularizer = regularizers.get(activity_regularizer)",
                                "signature": "__init__(self, units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)"
                            },
                            {
                                "code": "def call(self, inputs, mask=None, training=None, initial_state=None):\n    return super(SimpleRNN, self).call(inputs,\n                                       mask=mask,\n                                       training=training,\n                                       initial_state=initial_state)",
                                "signature": "call(self, inputs, mask=None, training=None, initial_state=None)"
                            },
                            {
                                "code": "@property\ndef units(self):\n    return self.cell.units",
                                "signature": "units(self)"
                            },
                            {
                                "code": "@property\ndef activation(self):\n    return self.cell.activation",
                                "signature": "activation(self)"
                            },
                            {
                                "code": "@property\ndef use_bias(self):\n    return self.cell.use_bias",
                                "signature": "use_bias(self)"
                            },
                            {
                                "code": "@property\ndef kernel_initializer(self):\n    return self.cell.kernel_initializer",
                                "signature": "kernel_initializer(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_initializer(self):\n    return self.cell.recurrent_initializer",
                                "signature": "recurrent_initializer(self)"
                            },
                            {
                                "code": "@property\ndef bias_initializer(self):\n    return self.cell.bias_initializer",
                                "signature": "bias_initializer(self)"
                            },
                            {
                                "code": "@property\ndef kernel_regularizer(self):\n    return self.cell.kernel_regularizer",
                                "signature": "kernel_regularizer(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_regularizer(self):\n    return self.cell.recurrent_regularizer",
                                "signature": "recurrent_regularizer(self)"
                            },
                            {
                                "code": "@property\ndef bias_regularizer(self):\n    return self.cell.bias_regularizer",
                                "signature": "bias_regularizer(self)"
                            },
                            {
                                "code": "@property\ndef kernel_constraint(self):\n    return self.cell.kernel_constraint",
                                "signature": "kernel_constraint(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_constraint(self):\n    return self.cell.recurrent_constraint",
                                "signature": "recurrent_constraint(self)"
                            },
                            {
                                "code": "@property\ndef bias_constraint(self):\n    return self.cell.bias_constraint",
                                "signature": "bias_constraint(self)"
                            },
                            {
                                "code": "@property\ndef dropout(self):\n    return self.cell.dropout",
                                "signature": "dropout(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_dropout(self):\n    return self.cell.recurrent_dropout",
                                "signature": "recurrent_dropout(self)"
                            },
                            {
                                "code": "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout}\n    base_config = super(SimpleRNN, self).get_config()\n    del base_config['cell']\n    return dict(list(base_config.items()) + list(config.items()))",
                                "signature": "get_config(self)"
                            },
                            {
                                "code": "@classmethod\ndef from_config(cls, config):\n    if 'implementation' in config:\n        config.pop('implementation')\n    return cls(**config)",
                                "signature": "from_config(cls, config)"
                            }
                        ]
                    },
                    {
                        "class_declaration": "class GRUCell:",
                        "functions": [
                            {
                                "code": "def __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             **kwargs):\n    super(GRUCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.recurrent_activation = activations.get(recurrent_activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.implementation = implementation\n    self.state_size = self.units\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                                "signature": "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, **kwargs)"
                            },
                            {
                                "code": "def build(self, input_shape):\n    input_dim = input_shape[-1]\n    self.kernel = self.add_weight(shape=(input_dim, self.units * 3),\n                                  name='kernel',\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units * 3),\n        name='recurrent_kernel',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n\n    if self.use_bias:\n        self.bias = self.add_weight(shape=(self.units * 3,),\n                                    name='bias',\n                                    initializer=self.bias_initializer,\n                                    regularizer=self.bias_regularizer,\n                                    constraint=self.bias_constraint)\n    else:\n        self.bias = None\n\n    self.kernel_z = self.kernel[:, :self.units]\n    self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n    self.kernel_r = self.kernel[:, self.units: self.units * 2]\n    self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                    self.units:\n                                                    self.units * 2]\n    self.kernel_h = self.kernel[:, self.units * 2:]\n    self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n    if self.use_bias:\n        self.bias_z = self.bias[:self.units]\n        self.bias_r = self.bias[self.units: self.units * 2]\n        self.bias_h = self.bias[self.units * 2:]\n    else:\n        self.bias_z = None\n        self.bias_r = None\n        self.bias_h = None\n    self.built = True",
                                "signature": "build(self, input_shape)"
                            },
                            {
                                "code": "def call(self, inputs, states, training=None):\n    h_tm1 = states[0]  # previous memory\n\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n            self.dropout,\n            training=training,\n            count=3)\n    if (0 < self.recurrent_dropout < 1 and\n            self._recurrent_dropout_mask is None):\n        self._recurrent_dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, self.units),\n            self.recurrent_dropout,\n            training=training,\n            count=3)\n\n    # dropout matrices for input units\n    dp_mask = self._dropout_mask\n    # dropout matrices for recurrent units\n    rec_dp_mask = self._recurrent_dropout_mask\n\n    if self.implementation == 1:\n        if 0. < self.dropout < 1.:\n            inputs_z = inputs * dp_mask[0]\n            inputs_r = inputs * dp_mask[1]\n            inputs_h = inputs * dp_mask[2]\n        else:\n            inputs_z = inputs\n            inputs_r = inputs\n            inputs_h = inputs\n        x_z = K.dot(inputs_z, self.kernel_z)\n        x_r = K.dot(inputs_r, self.kernel_r)\n        x_h = K.dot(inputs_h, self.kernel_h)\n        if self.use_bias:\n            x_z = K.bias_add(x_z, self.bias_z)\n            x_r = K.bias_add(x_r, self.bias_r)\n            x_h = K.bias_add(x_h, self.bias_h)\n\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1_z = h_tm1 * rec_dp_mask[0]\n            h_tm1_r = h_tm1 * rec_dp_mask[1]\n            h_tm1_h = h_tm1 * rec_dp_mask[2]\n        else:\n            h_tm1_z = h_tm1\n            h_tm1_r = h_tm1\n            h_tm1_h = h_tm1\n        z = self.recurrent_activation(x_z + K.dot(h_tm1_z,\n                                                  self.recurrent_kernel_z))\n        r = self.recurrent_activation(x_r + K.dot(h_tm1_r,\n                                                  self.recurrent_kernel_r))\n\n        hh = self.activation(x_h + K.dot(r * h_tm1_h,\n                                         self.recurrent_kernel_h))\n    else:\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n        matrix_x = K.dot(inputs, self.kernel)\n        if self.use_bias:\n            matrix_x = K.bias_add(matrix_x, self.bias)\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1 *= rec_dp_mask[0]\n        matrix_inner = K.dot(h_tm1,\n                             self.recurrent_kernel[:, :2 * self.units])\n\n        x_z = matrix_x[:, :self.units]\n        x_r = matrix_x[:, self.units: 2 * self.units]\n        recurrent_z = matrix_inner[:, :self.units]\n        recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n        z = self.recurrent_activation(x_z + recurrent_z)\n        r = self.recurrent_activation(x_r + recurrent_r)\n\n        x_h = matrix_x[:, 2 * self.units:]\n        recurrent_h = K.dot(r * h_tm1,\n                            self.recurrent_kernel[:, 2 * self.units:])\n        hh = self.activation(x_h + recurrent_h)\n    h = z * h_tm1 + (1 - z) * hh\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            h._uses_learning_phase = True\n    return h, [h]",
                                "signature": "call(self, inputs, states, training=None)"
                            }
                        ]
                    },
                    {
                        "class_declaration": "class GRU:",
                        "functions": [
                            {
                                "code": "@interfaces.legacy_recurrent_support\ndef __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             activity_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if implementation == 0:\n        warnings.warn('`implementation=0` has been deprecated, '\n                      'and now defaults to `implementation=1`.'\n                      'Please update your layer call.')\n    if K.backend() == 'theano':\n        warnings.warn(\n            'RNN dropout is no longer supported with the Theano backend '\n            'due to technical limitations. '\n            'You can either set `dropout` and `recurrent_dropout` to 0, '\n            'or use the TensorFlow backend.')\n        dropout = 0.\n        recurrent_dropout = 0.\n\n    cell = GRUCell(units,\n                   activation=activation,\n                   recurrent_activation=recurrent_activation,\n                   use_bias=use_bias,\n                   kernel_initializer=kernel_initializer,\n                   recurrent_initializer=recurrent_initializer,\n                   bias_initializer=bias_initializer,\n                   kernel_regularizer=kernel_regularizer,\n                   recurrent_regularizer=recurrent_regularizer,\n                   bias_regularizer=bias_regularizer,\n                   kernel_constraint=kernel_constraint,\n                   recurrent_constraint=recurrent_constraint,\n                   bias_constraint=bias_constraint,\n                   dropout=dropout,\n                   recurrent_dropout=recurrent_dropout,\n                   implementation=implementation)\n    super(GRU, self).__init__(cell,\n                              return_sequences=return_sequences,\n                              return_state=return_state,\n                              go_backwards=go_backwards,\n                              stateful=stateful,\n                              unroll=unroll,\n                              **kwargs)\n    self.activity_regularizer = regularizers.get(activity_regularizer)",
                                "signature": "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)"
                            },
                            {
                                "code": "def call(self, inputs, mask=None, training=None, initial_state=None):\n    return super(GRU, self).call(inputs,\n                                 mask=mask,\n                                 training=training,\n                                 initial_state=initial_state)",
                                "signature": "call(self, inputs, mask=None, training=None, initial_state=None)"
                            },
                            {
                                "code": "@property\ndef units(self):\n    return self.cell.units",
                                "signature": "units(self)"
                            },
                            {
                                "code": "@property\ndef activation(self):\n    return self.cell.activation",
                                "signature": "activation(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_activation(self):\n    return self.cell.recurrent_activation",
                                "signature": "recurrent_activation(self)"
                            },
                            {
                                "code": "@property\ndef use_bias(self):\n    return self.cell.use_bias",
                                "signature": "use_bias(self)"
                            },
                            {
                                "code": "@property\ndef kernel_initializer(self):\n    return self.cell.kernel_initializer",
                                "signature": "kernel_initializer(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_initializer(self):\n    return self.cell.recurrent_initializer",
                                "signature": "recurrent_initializer(self)"
                            },
                            {
                                "code": "@property\ndef bias_initializer(self):\n    return self.cell.bias_initializer",
                                "signature": "bias_initializer(self)"
                            },
                            {
                                "code": "@property\ndef kernel_regularizer(self):\n    return self.cell.kernel_regularizer",
                                "signature": "kernel_regularizer(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_regularizer(self):\n    return self.cell.recurrent_regularizer",
                                "signature": "recurrent_regularizer(self)"
                            },
                            {
                                "code": "@property\ndef bias_regularizer(self):\n    return self.cell.bias_regularizer",
                                "signature": "bias_regularizer(self)"
                            },
                            {
                                "code": "@property\ndef kernel_constraint(self):\n    return self.cell.kernel_constraint",
                                "signature": "kernel_constraint(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_constraint(self):\n    return self.cell.recurrent_constraint",
                                "signature": "recurrent_constraint(self)"
                            },
                            {
                                "code": "@property\ndef bias_constraint(self):\n    return self.cell.bias_constraint",
                                "signature": "bias_constraint(self)"
                            },
                            {
                                "code": "@property\ndef dropout(self):\n    return self.cell.dropout",
                                "signature": "dropout(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_dropout(self):\n    return self.cell.recurrent_dropout",
                                "signature": "recurrent_dropout(self)"
                            },
                            {
                                "code": "@property\ndef implementation(self):\n    return self.cell.implementation",
                                "signature": "implementation(self)"
                            },
                            {
                                "code": "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'recurrent_activation': activations.serialize(self.recurrent_activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout,\n              'implementation': self.implementation}\n    base_config = super(GRU, self).get_config()\n    del base_config['cell']\n    return dict(list(base_config.items()) + list(config.items()))",
                                "signature": "get_config(self)"
                            },
                            {
                                "code": "@classmethod\ndef from_config(cls, config):\n    if 'implementation' in config and config['implementation'] == 0:\n        config['implementation'] = 1\n    return cls(**config)",
                                "signature": "from_config(cls, config)"
                            }
                        ]
                    },
                    {
                        "class_declaration": "class LSTMCell:",
                        "functions": [
                            {
                                "code": "def __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             unit_forget_bias=True,\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             **kwargs):\n    super(LSTMCell, self).__init__(**kwargs)\n    self.units = units\n    self.activation = activations.get(activation)\n    self.recurrent_activation = activations.get(recurrent_activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.recurrent_initializer = initializers.get(recurrent_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.unit_forget_bias = unit_forget_bias\n\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.recurrent_constraint = constraints.get(recurrent_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n\n    self.dropout = min(1., max(0., dropout))\n    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n    self.implementation = implementation\n    self.state_size = (self.units, self.units)\n    self._dropout_mask = None\n    self._recurrent_dropout_mask = None",
                                "signature": "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, **kwargs)"
                            },
                            {
                                "code": "def build(self, input_shape):\n    input_dim = input_shape[-1]\n    self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n                                  name='kernel',\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units * 4),\n        name='recurrent_kernel',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n\n    if self.use_bias:\n        if self.unit_forget_bias:\n            def bias_initializer(shape, *args, **kwargs):\n                return K.concatenate([\n                    self.bias_initializer((self.units,), *args, **kwargs),\n                    initializers.Ones()((self.units,), *args, **kwargs),\n                    self.bias_initializer((self.units * 2,), *args, **kwargs),\n                ])\n        else:\n            bias_initializer = self.bias_initializer\n        self.bias = self.add_weight(shape=(self.units * 4,),\n                                    name='bias',\n                                    initializer=bias_initializer,\n                                    regularizer=self.bias_regularizer,\n                                    constraint=self.bias_constraint)\n    else:\n        self.bias = None\n\n    self.kernel_i = self.kernel[:, :self.units]\n    self.kernel_f = self.kernel[:, self.units: self.units * 2]\n    self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n    self.kernel_o = self.kernel[:, self.units * 3:]\n\n    self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n    self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n    self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n    self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n    if self.use_bias:\n        self.bias_i = self.bias[:self.units]\n        self.bias_f = self.bias[self.units: self.units * 2]\n        self.bias_c = self.bias[self.units * 2: self.units * 3]\n        self.bias_o = self.bias[self.units * 3:]\n    else:\n        self.bias_i = None\n        self.bias_f = None\n        self.bias_c = None\n        self.bias_o = None\n    self.built = True",
                                "signature": "build(self, input_shape)"
                            },
                            {
                                "code": "def call(self, inputs, states, training=None):\n    if 0 < self.dropout < 1 and self._dropout_mask is None:\n        self._dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n            self.dropout,\n            training=training,\n            count=4)\n    if (0 < self.recurrent_dropout < 1 and\n            self._recurrent_dropout_mask is None):\n        self._recurrent_dropout_mask = _generate_dropout_mask(\n            _generate_dropout_ones(inputs, self.units),\n            self.recurrent_dropout,\n            training=training,\n            count=4)\n\n    # dropout matrices for input units\n    dp_mask = self._dropout_mask\n    # dropout matrices for recurrent units\n    rec_dp_mask = self._recurrent_dropout_mask\n\n    h_tm1 = states[0]  # previous memory state\n    c_tm1 = states[1]  # previous carry state\n\n    if self.implementation == 1:\n        if 0 < self.dropout < 1.:\n            inputs_i = inputs * dp_mask[0]\n            inputs_f = inputs * dp_mask[1]\n            inputs_c = inputs * dp_mask[2]\n            inputs_o = inputs * dp_mask[3]\n        else:\n            inputs_i = inputs\n            inputs_f = inputs\n            inputs_c = inputs\n            inputs_o = inputs\n        x_i = K.dot(inputs_i, self.kernel_i)\n        x_f = K.dot(inputs_f, self.kernel_f)\n        x_c = K.dot(inputs_c, self.kernel_c)\n        x_o = K.dot(inputs_o, self.kernel_o)\n        if self.use_bias:\n            x_i = K.bias_add(x_i, self.bias_i)\n            x_f = K.bias_add(x_f, self.bias_f)\n            x_c = K.bias_add(x_c, self.bias_c)\n            x_o = K.bias_add(x_o, self.bias_o)\n\n        if 0 < self.recurrent_dropout < 1.:\n            h_tm1_i = h_tm1 * rec_dp_mask[0]\n            h_tm1_f = h_tm1 * rec_dp_mask[1]\n            h_tm1_c = h_tm1 * rec_dp_mask[2]\n            h_tm1_o = h_tm1 * rec_dp_mask[3]\n        else:\n            h_tm1_i = h_tm1\n            h_tm1_f = h_tm1\n            h_tm1_c = h_tm1\n            h_tm1_o = h_tm1\n        i = self.recurrent_activation(x_i + K.dot(h_tm1_i,\n                                                  self.recurrent_kernel_i))\n        f = self.recurrent_activation(x_f + K.dot(h_tm1_f,\n                                                  self.recurrent_kernel_f))\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,\n                                                        self.recurrent_kernel_c))\n        o = self.recurrent_activation(x_o + K.dot(h_tm1_o,\n                                                  self.recurrent_kernel_o))\n    else:\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n        z = K.dot(inputs, self.kernel)\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1 *= rec_dp_mask[0]\n        z += K.dot(h_tm1, self.recurrent_kernel)\n        if self.use_bias:\n            z = K.bias_add(z, self.bias)\n\n        z0 = z[:, :self.units]\n        z1 = z[:, self.units: 2 * self.units]\n        z2 = z[:, 2 * self.units: 3 * self.units]\n        z3 = z[:, 3 * self.units:]\n\n        i = self.recurrent_activation(z0)\n        f = self.recurrent_activation(z1)\n        c = f * c_tm1 + i * self.activation(z2)\n        o = self.recurrent_activation(z3)\n\n    h = o * self.activation(c)\n    if 0 < self.dropout + self.recurrent_dropout:\n        if training is None:\n            h._uses_learning_phase = True\n    return h, [h, c]",
                                "signature": "call(self, inputs, states, training=None)"
                            }
                        ]
                    },
                    {
                        "class_declaration": "class LSTM:",
                        "functions": [
                            {
                                "code": "@interfaces.legacy_recurrent_support\ndef __init__(self, units,\n             activation='tanh',\n             recurrent_activation='hard_sigmoid',\n             use_bias=True,\n             kernel_initializer='glorot_uniform',\n             recurrent_initializer='orthogonal',\n             bias_initializer='zeros',\n             unit_forget_bias=True,\n             kernel_regularizer=None,\n             recurrent_regularizer=None,\n             bias_regularizer=None,\n             activity_regularizer=None,\n             kernel_constraint=None,\n             recurrent_constraint=None,\n             bias_constraint=None,\n             dropout=0.,\n             recurrent_dropout=0.,\n             implementation=1,\n             return_sequences=False,\n             return_state=False,\n             go_backwards=False,\n             stateful=False,\n             unroll=False,\n             **kwargs):\n    if implementation == 0:\n        warnings.warn('`implementation=0` has been deprecated, '\n                      'and now defaults to `implementation=1`.'\n                      'Please update your layer call.')\n    if K.backend() == 'theano':\n        warnings.warn(\n            'RNN dropout is no longer supported with the Theano backend '\n            'due to technical limitations. '\n            'You can either set `dropout` and `recurrent_dropout` to 0, '\n            'or use the TensorFlow backend.')\n        dropout = 0.\n        recurrent_dropout = 0.\n\n    cell = LSTMCell(units,\n                    activation=activation,\n                    recurrent_activation=recurrent_activation,\n                    use_bias=use_bias,\n                    kernel_initializer=kernel_initializer,\n                    recurrent_initializer=recurrent_initializer,\n                    unit_forget_bias=unit_forget_bias,\n                    bias_initializer=bias_initializer,\n                    kernel_regularizer=kernel_regularizer,\n                    recurrent_regularizer=recurrent_regularizer,\n                    bias_regularizer=bias_regularizer,\n                    kernel_constraint=kernel_constraint,\n                    recurrent_constraint=recurrent_constraint,\n                    bias_constraint=bias_constraint,\n                    dropout=dropout,\n                    recurrent_dropout=recurrent_dropout,\n                    implementation=implementation)\n    super(LSTM, self).__init__(cell,\n                               return_sequences=return_sequences,\n                               return_state=return_state,\n                               go_backwards=go_backwards,\n                               stateful=stateful,\n                               unroll=unroll,\n                               **kwargs)\n    self.activity_regularizer = regularizers.get(activity_regularizer)",
                                "signature": "__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)"
                            },
                            {
                                "code": "def call(self, inputs, mask=None, training=None, initial_state=None):\n    return super(LSTM, self).call(inputs,\n                                  mask=mask,\n                                  training=training,\n                                  initial_state=initial_state)",
                                "signature": "call(self, inputs, mask=None, training=None, initial_state=None)"
                            },
                            {
                                "code": "@property\ndef units(self):\n    return self.cell.units",
                                "signature": "units(self)"
                            },
                            {
                                "code": "@property\ndef activation(self):\n    return self.cell.activation",
                                "signature": "activation(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_activation(self):\n    return self.cell.recurrent_activation",
                                "signature": "recurrent_activation(self)"
                            },
                            {
                                "code": "@property\ndef use_bias(self):\n    return self.cell.use_bias",
                                "signature": "use_bias(self)"
                            },
                            {
                                "code": "@property\ndef kernel_initializer(self):\n    return self.cell.kernel_initializer",
                                "signature": "kernel_initializer(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_initializer(self):\n    return self.cell.recurrent_initializer",
                                "signature": "recurrent_initializer(self)"
                            },
                            {
                                "code": "@property\ndef bias_initializer(self):\n    return self.cell.bias_initializer",
                                "signature": "bias_initializer(self)"
                            },
                            {
                                "code": "@property\ndef unit_forget_bias(self):\n    return self.cell.unit_forget_bias",
                                "signature": "unit_forget_bias(self)"
                            },
                            {
                                "code": "@property\ndef kernel_regularizer(self):\n    return self.cell.kernel_regularizer",
                                "signature": "kernel_regularizer(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_regularizer(self):\n    return self.cell.recurrent_regularizer",
                                "signature": "recurrent_regularizer(self)"
                            },
                            {
                                "code": "@property\ndef bias_regularizer(self):\n    return self.cell.bias_regularizer",
                                "signature": "bias_regularizer(self)"
                            },
                            {
                                "code": "@property\ndef kernel_constraint(self):\n    return self.cell.kernel_constraint",
                                "signature": "kernel_constraint(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_constraint(self):\n    return self.cell.recurrent_constraint",
                                "signature": "recurrent_constraint(self)"
                            },
                            {
                                "code": "@property\ndef bias_constraint(self):\n    return self.cell.bias_constraint",
                                "signature": "bias_constraint(self)"
                            },
                            {
                                "code": "@property\ndef dropout(self):\n    return self.cell.dropout",
                                "signature": "dropout(self)"
                            },
                            {
                                "code": "@property\ndef recurrent_dropout(self):\n    return self.cell.recurrent_dropout",
                                "signature": "recurrent_dropout(self)"
                            },
                            {
                                "code": "@property\ndef implementation(self):\n    return self.cell.implementation",
                                "signature": "implementation(self)"
                            },
                            {
                                "code": "def get_config(self):\n    config = {'units': self.units,\n              'activation': activations.serialize(self.activation),\n              'recurrent_activation': activations.serialize(self.recurrent_activation),\n              'use_bias': self.use_bias,\n              'kernel_initializer': initializers.serialize(self.kernel_initializer),\n              'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n              'bias_initializer': initializers.serialize(self.bias_initializer),\n              'unit_forget_bias': self.unit_forget_bias,\n              'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n              'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n              'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n              'kernel_constraint': constraints.serialize(self.kernel_constraint),\n              'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n              'bias_constraint': constraints.serialize(self.bias_constraint),\n              'dropout': self.dropout,\n              'recurrent_dropout': self.recurrent_dropout,\n              'implementation': self.implementation}\n    base_config = super(LSTM, self).get_config()\n    del base_config['cell']\n    return dict(list(base_config.items()) + list(config.items()))",
                                "signature": "get_config(self)"
                            },
                            {
                                "code": "@classmethod\ndef from_config(cls, config):\n    if 'implementation' in config and config['implementation'] == 0:\n        config['implementation'] = 1\n    return cls(**config)",
                                "signature": "from_config(cls, config)"
                            }
                        ]
                    }
                ]
            },
            "variables_in_file": {
                "Layer": [
                    736,
                    1568,
                    98,
                    131,
                    196,
                    742,
                    170,
                    139,
                    750,
                    144,
                    755,
                    20,
                    182,
                    439,
                    1111,
                    761,
                    157,
                    190
                ],
                "cell": [
                    130,
                    131,
                    132,
                    138,
                    139,
                    140,
                    1930,
                    143,
                    144,
                    145,
                    1947,
                    156,
                    157,
                    158,
                    1444,
                    169,
                    170,
                    43,
                    44,
                    171,
                    173,
                    47,
                    1460,
                    181,
                    182,
                    183,
                    189,
                    190,
                    191,
                    64,
                    65,
                    66,
                    68,
                    74,
                    75,
                    76,
                    77,
                    85,
                    86,
                    725,
                    728,
                    97,
                    98,
                    99,
                    100,
                    101,
                    354,
                    103,
                    355,
                    356,
                    358,
                    359,
                    999,
                    109,
                    110,
                    111,
                    365,
                    1013
                ],
                "cells": [
                    43,
                    108,
                    46,
                    110,
                    112,
                    50,
                    51,
                    119,
                    121,
                    123
                ],
                "hasattr": [
                    672,
                    65,
                    100,
                    356,
                    678,
                    359,
                    75,
                    44,
                    397,
                    47,
                    694,
                    471,
                    504,
                    506,
                    447
                ],
                "ValueError": [
                    546,
                    357,
                    455,
                    360,
                    553,
                    45,
                    48,
                    688,
                    660,
                    571,
                    507,
                    699
                ],
                "self.cells": [
                    64,
                    97,
                    130,
                    169,
                    74,
                    138,
                    109,
                    143,
                    51,
                    85,
                    181,
                    156,
                    189
                ],
                "self": [
                    2048,
                    2049,
                    51,
                    52,
                    64,
                    74,
                    85,
                    97,
                    105,
                    109,
                    113,
                    127,
                    130,
                    138,
                    141,
                    143,
                    156,
                    169,
                    181,
                    189,
                    364,
                    365,
                    366,
                    367,
                    368,
                    369,
                    370,
                    372,
                    373,
                    374,
                    375,
                    376,
                    377,
                    381,
                    382,
                    385,
                    387,
                    391,
                    397,
                    398,
                    400,
                    402,
                    407,
                    408,
                    416,
                    417,
                    418,
                    426,
                    427,
                    434,
                    436,
                    439,
                    442,
                    444,
                    447,
                    448,
                    450,
                    452,
                    454,
                    459,
                    461,
                    463,
                    464,
                    471,
                    473,
                    475,
                    478,
                    482,
                    493,
                    495,
                    499,
                    501,
                    502,
                    514,
                    516,
                    517,
                    518,
                    519,
                    522,
                    537,
                    538,
                    540,
                    545,
                    546,
                    552,
                    566,
                    570,
                    574,
                    575,
                    576,
                    580,
                    586,
                    588,
                    590,
                    593,
                    594,
                    596,
                    605,
                    636,
                    637,
                    638,
                    656,
                    658,
                    671,
                    672,
                    673,
                    674,
                    676,
                    678,
                    679,
                    682,
                    683,
                    687,
                    688,
                    689,
                    693,
                    694,
                    695,
                    697,
                    701,
                    708,
                    709,
                    710,
                    711,
                    712,
                    713,
                    714,
                    716,
                    717,
                    719,
                    734,
                    736,
                    737,
                    742,
                    743,
                    744,
                    745,
                    750,
                    751,
                    755,
                    756,
                    757,
                    758,
                    819,
                    820,
                    821,
                    822,
                    824,
                    825,
                    826,
                    828,
                    829,
                    830,
                    832,
                    833,
                    834,
                    836,
                    837,
                    838,
                    839,
                    840,
                    843,
                    845,
                    846,
                    847,
                    848,
                    849,
                    851,
                    852,
                    853,
                    854,
                    855,
                    857,
                    858,
                    859,
                    861,
                    862,
                    866,
                    867,
                    869,
                    871,
                    872,
                    873,
                    874,
                    875,
                    878,
                    879,
                    882,
                    884,
                    885,
                    886,
                    890,
                    891,
                    892,
                    895,
                    1013,
                    1020,
                    1023,
                    1030,
                    1034,
                    1038,
                    1042,
                    1046,
                    1050,
                    1054,
                    1058,
                    1062,
                    1066,
                    1070,
                    1074,
                    1078,
                    1082,
                    1085,
                    1086,
                    1087,
                    1088,
                    1089,
                    1090,
                    1091,
                    1092,
                    1093,
                    1094,
                    1095,
                    1096,
                    1097,
                    1098,
                    1099,
                    1100,
                    1180,
                    1181,
                    1182,
                    1183,
                    1184,
                    1186,
                    1187,
                    1188,
                    1190,
                    1191,
                    1192,
                    1194,
                    1195,
                    1196,
                    1198,
                    1199,
                    1200,
                    1201,
                    1202,
                    1203,
                    1207,
                    1209,
                    1210,
                    1211,
                    1212,
                    1213,
                    1215,
                    1216,
                    1217,
                    1219,
                    1220,
                    1222,
                    1223,
                    1224,
                    1226,
                    1228,
                    1229,
                    1230,
                    1231,
                    1232,
                    1233,
                    1234,
                    1235,
                    1237,
                    1238,
                    1239,
                    1240,
                    1242,
                    1243,
                    1244,
                    1245,
                    1250,
                    1251,
                    1253,
                    1256,
                    1257,
                    1258,
                    1259,
                    1260,
                    1265,
                    1267,
                    1269,
                    1270,
                    1278,
                    1279,
                    1280,
                    1281,
                    1282,
                    1283,
                    1284,
                    1286,
                    1294,
                    1295,
                    1296,
                    1297,
                    1299,
                    1300,
                    1302,
                    1304,
                    1305,
                    1306,
                    1307,
                    1310,
                    1312,
                    1313,
                    1314,
                    1315,
                    1317,
                    1318,
                    1320,
                    1322,
                    1323,
                    1325,
                    1460,
                    1467,
                    1470,
                    1477,
                    1481,
                    1485,
                    1489,
                    1493,
                    1497,
                    1501,
                    1505,
                    1509,
                    1513,
                    1517,
                    1521,
                    1525,
                    1529,
                    1533,
                    1537,
                    1540,
                    1541,
                    1542,
                    1543,
                    1544,
                    1545,
                    1546,
                    1547,
                    1548,
                    1549,
                    1550,
                    1551,
                    1552,
                    1553,
                    1554,
                    1555,
                    1556,
                    1557,
                    1642,
                    1643,
                    1644,
                    1645,
                    1646,
                    1648,
                    1649,
                    1650,
                    1651,
                    1653,
                    1654,
                    1655,
                    1657,
                    1658,
                    1659,
                    1661,
                    1662,
                    1663,
                    1664,
                    1665,
                    1666,
                    1670,
                    1672,
                    1673,
                    1674,
                    1675,
                    1676,
                    1678,
                    1679,
                    1680,
                    1682,
                    1683,
                    1686,
                    1687,
                    1688,
                    1691,
                    1692,
                    1695,
                    1696,
                    1698,
                    1700,
                    1701,
                    1702,
                    1703,
                    1705,
                    1706,
                    1707,
                    1708,
                    1710,
                    1711,
                    1712,
                    1713,
                    1714,
                    1716,
                    1717,
                    1718,
                    1719,
                    1720,
                    1723,
                    1724,
                    1726,
                    1729,
                    1730,
                    1731,
                    1732,
                    1733,
                    1738,
                    1740,
                    1745,
                    1746,
                    1756,
                    1757,
                    1758,
                    1759,
                    1760,
                    1761,
                    1762,
                    1763,
                    1764,
                    1766,
                    1776,
                    1777,
                    1778,
                    1779,
                    1780,
                    1781,
                    1782,
                    1783,
                    1785,
                    1787,
                    1788,
                    1790,
                    1791,
                    1792,
                    1794,
                    1795,
                    1796,
                    1797,
                    1799,
                    1800,
                    1801,
                    1802,
                    1804,
                    1805,
                    1947,
                    1954,
                    1957,
                    1964,
                    1968,
                    1972,
                    1976,
                    1980,
                    1984,
                    1988,
                    1992,
                    1996,
                    2000,
                    2004,
                    2008,
                    2012,
                    2016,
                    2020,
                    2024,
                    2028,
                    2031,
                    2032,
                    2033,
                    2034,
                    2035,
                    2036,
                    2037,
                    2038,
                    2039,
                    2040,
                    2041,
                    2042,
                    2043,
                    2044,
                    2045,
                    2046,
                    2047
                ],
                "__init__": [
                    1642,
                    364,
                    819,
                    52,
                    1013,
                    1460,
                    1947,
                    1180
                ],
                "super": [
                    2049,
                    518,
                    522,
                    1557,
                    1947,
                    1180,
                    1957,
                    819,
                    52,
                    1460,
                    1470,
                    1100,
                    719,
                    482,
                    1642,
                    364,
                    113,
                    757,
                    758,
                    1013,
                    1023
                ],
                "StackedRNNCells": [
                    113,
                    355,
                    52
                ],
                "kwargs": [
                    518,
                    522,
                    1686,
                    1687,
                    1688,
                    1180,
                    1953,
                    819,
                    52,
                    565,
                    567,
                    1466,
                    577,
                    580,
                    86,
                    985,
                    986,
                    482,
                    1642,
                    491,
                    364,
                    497,
                    1019
                ],
                "state_size": [
                    448,
                    66,
                    450,
                    68,
                    69,
                    454,
                    462,
                    63
                ],
                "cell.state_size": [
                    65,
                    66,
                    68,
                    100,
                    101,
                    103,
                    75,
                    76,
                    77
                ],
                "list": [
                    2051,
                    644,
                    647,
                    394,
                    533,
                    1559,
                    542,
                    414,
                    685,
                    431,
                    448,
                    66,
                    1102,
                    720,
                    606,
                    609,
                    354,
                    114,
                    634
                ],
                "state_size.append": [
                    68
                ],
                "tuple": [
                    354,
                    69,
                    646,
                    685,
                    606
                ],
                "property": [
                    1028,
                    135,
                    1032,
                    1036,
                    1040,
                    1044,
                    1531,
                    1048,
                    1052,
                    1056,
                    1060,
                    1064,
                    1962,
                    1068,
                    1966,
                    1072,
                    178,
                    1970,
                    1076,
                    54,
                    1974,
                    1080,
                    1978,
                    1982,
                    1986,
                    1475,
                    1990,
                    1479,
                    1994,
                    1483,
                    1998,
                    1487,
                    2002,
                    1491,
                    2006,
                    1495,
                    2010,
                    1499,
                    732,
                    2014,
                    1503,
                    2018,
                    1507,
                    740,
                    2022,
                    1511,
                    2026,
                    1515,
                    748,
                    1519,
                    1523,
                    1527,
                    379,
                    125,
                    1535
                ],
                "nested_states": [
                    73,
                    76,
                    79,
                    81,
                    85
                ],
                "nested_states.append": [
                    76,
                    79
                ],
                "states": [
                    389,
                    391,
                    677,
                    685,
                    686,
                    687,
                    690,
                    692,
                    693,
                    574,
                    575,
                    576,
                    580,
                    582,
                    76,
                    77,
                    1742,
                    79,
                    80,
                    592,
                    593,
                    1743,
                    85,
                    86,
                    87,
                    91,
                    93,
                    94,
                    606,
                    607,
                    609,
                    610,
                    865,
                    1248
                ],
                "len": [
                    385,
                    545,
                    546,
                    548,
                    171,
                    76,
                    77,
                    687,
                    592,
                    689,
                    690,
                    501,
                    639
                ],
                "new_nested_states": [
                    84,
                    92,
                    87
                ],
                "zip": [
                    679,
                    693,
                    85,
                    173
                ],
                "inputs": [
                    640,
                    513,
                    641,
                    522,
                    653,
                    1275,
                    2064,
                    2067,
                    533,
                    534,
                    1303,
                    1304,
                    1277,
                    1786,
                    1023,
                    540,
                    1787,
                    1957,
                    550,
                    1725,
                    1470,
                    191,
                    576,
                    580,
                    1732,
                    583,
                    594,
                    1747,
                    468,
                    1748,
                    86,
                    1749,
                    1750,
                    1752,
                    1753,
                    1754,
                    1755,
                    478,
                    479,
                    94,
                    482,
                    868,
                    1252,
                    874,
                    1259,
                    882,
                    756,
                    757,
                    758,
                    884,
                    1272,
                    1273,
                    634,
                    1271,
                    1276,
                    637,
                    638,
                    639
                ],
                "cell.call": [
                    86
                ],
                "new_nested_states.append": [
                    87
                ],
                "cell_states": [
                    92,
                    93
                ],
                "isinstance": [
                    131,
                    644,
                    646,
                    394,
                    139,
                    144,
                    533,
                    157,
                    414,
                    542,
                    170,
                    685,
                    431,
                    182,
                    439,
                    190,
                    606,
                    736,
                    98,
                    354,
                    742,
                    750,
                    755,
                    634,
                    382
                ],
                "cell.build": [
                    99
                ],
                "input_shape": [
                    99,
                    1669,
                    440,
                    550,
                    104,
                    551,
                    394,
                    395,
                    427,
                    843,
                    431,
                    432,
                    434,
                    403,
                    435,
                    405,
                    1206,
                    408
                ],
                "output_dim": [
                    101,
                    103,
                    104,
                    398,
                    400,
                    403,
                    405,
                    408
                ],
                "self.built": [
                    1720,
                    105,
                    1245,
                    862
                ],
                "cells.append": [
                    121,
                    110
                ],
                "cell.__class__.__name__": [
                    110
                ],
                "cell.__class__": [
                    110
                ],
                "cell.get_config": [
                    111
                ],
                "config": [
                    2051,
                    1540,
                    2055,
                    2056,
                    2057,
                    1559,
                    1563,
                    1564,
                    1565,
                    1085,
                    708,
                    714,
                    717,
                    1102,
                    720,
                    1106,
                    1107,
                    1108,
                    725,
                    727,
                    728,
                    2031,
                    112,
                    114,
                    120,
                    123
                ],
                "base_config": [
                    2049,
                    2050,
                    2051,
                    1100,
                    1101,
                    1102,
                    719,
                    720,
                    113,
                    114,
                    1557,
                    1558,
                    1559
                ],
                "get_config": [
                    2049,
                    1100,
                    719,
                    113,
                    1557
                ],
                "dict": [
                    2051,
                    1102,
                    720,
                    114,
                    1559
                ],
                "base_config.items": [
                    2051,
                    1102,
                    720,
                    114,
                    1559
                ],
                "config.items": [
                    2051,
                    1102,
                    720,
                    114,
                    1559
                ],
                "cell_config": [
                    120,
                    121,
                    716,
                    718
                ],
                "config.pop": [
                    120,
                    1107,
                    725,
                    727
                ],
                "deserialize_layer": [
                    121,
                    725
                ],
                "custom_objects": [
                    122,
                    726
                ],
                "cls": [
                    2057,
                    1108,
                    728,
                    123,
                    1565
                ],
                "classmethod": [
                    2053,
                    1104,
                    722,
                    116,
                    1561
                ],
                "self.trainable": [
                    743,
                    141,
                    734,
                    127
                ],
                "weights": [
                    129,
                    132,
                    133,
                    137,
                    140,
                    172,
                    173,
                    175,
                    146,
                    147,
                    155,
                    158,
                    159
                ],
                "cell.trainable_weights": [
                    145,
                    132
                ],
                "cell.non_trainable_weights": [
                    140
                ],
                "trainable_weights": [
                    145,
                    146,
                    142
                ],
                "cell.weights": [
                    171,
                    173,
                    158
                ],
                "K.batch_get_value": [
                    159
                ],
                "K": [
                    1280,
                    1792,
                    1282,
                    1283,
                    1284,
                    1921,
                    1294,
                    2063,
                    1296,
                    2064,
                    2065,
                    1299,
                    2067,
                    1685,
                    1304,
                    2072,
                    1306,
                    1435,
                    2075,
                    1309,
                    159,
                    2079,
                    673,
                    676,
                    550,
                    680,
                    1321,
                    682,
                    1790,
                    176,
                    1725,
                    705,
                    582,
                    468,
                    469,
                    470,
                    472,
                    475,
                    1756,
                    1757,
                    990,
                    1758,
                    1759,
                    1761,
                    1762,
                    1763,
                    868,
                    1252,
                    1764,
                    493,
                    1776,
                    882,
                    499,
                    884,
                    1778,
                    886,
                    1780,
                    1782,
                    890,
                    1787,
                    1278,
                    1279
                ],
                "tuples": [
                    168,
                    174,
                    176
                ],
                "num_param": [
                    171,
                    172,
                    175
                ],
                "sw": [
                    173,
                    174
                ],
                "w": [
                    173,
                    174
                ],
                "tuples.append": [
                    174
                ],
                "K.batch_set_value": [
                    176
                ],
                "losses": [
                    192,
                    193,
                    180,
                    184,
                    185,
                    188
                ],
                "cell_losses": [
                    192,
                    756,
                    757,
                    183,
                    184,
                    191
                ],
                "cell.losses": [
                    183
                ],
                "cell.get_losses_for": [
                    191
                ],
                "RNN": [
                    482,
                    901,
                    518,
                    522,
                    364,
                    719,
                    1331,
                    1811,
                    757,
                    758
                ],
                "self.cell": [
                    1537,
                    1030,
                    1034,
                    1038,
                    1042,
                    1046,
                    1050,
                    1054,
                    1058,
                    1062,
                    1066,
                    1070,
                    1074,
                    566,
                    1078,
                    570,
                    1082,
                    576,
                    580,
                    672,
                    674,
                    676,
                    678,
                    679,
                    683,
                    694,
                    695,
                    697,
                    716,
                    717,
                    736,
                    737,
                    742,
                    744,
                    745,
                    750,
                    751,
                    755,
                    756,
                    365,
                    382,
                    385,
                    397,
                    398,
                    400,
                    1964,
                    1968,
                    1972,
                    439,
                    1976,
                    442,
                    444,
                    1980,
                    447,
                    448,
                    1984,
                    450,
                    1988,
                    1477,
                    1992,
                    1481,
                    459,
                    1996,
                    1485,
                    2000,
                    1489,
                    2004,
                    1493,
                    471,
                    2008,
                    473,
                    1497,
                    475,
                    2012,
                    1501,
                    2016,
                    1505,
                    2020,
                    1509,
                    2024,
                    1513,
                    2028,
                    1517,
                    1521,
                    1525,
                    1529,
                    1533
                ],
                "self.return_sequences": [
                    416,
                    708,
                    366,
                    402,
                    596
                ],
                "return_sequences": [
                    1948,
                    1461,
                    366,
                    1014
                ],
                "self.return_state": [
                    417,
                    709,
                    367,
                    407,
                    605
                ],
                "return_state": [
                    1015,
                    1949,
                    1462,
                    367
                ],
                "self.go_backwards": [
                    368,
                    586,
                    710
                ],
                "go_backwards": [
                    368,
                    1016,
                    1950,
                    1463
                ],
                "self.stateful": [
                    711,
                    590,
                    463,
                    656,
                    369,
                    434,
                    537
                ],
                "stateful": [
                    1464,
                    369,
                    1017,
                    1951
                ],
                "self.unroll": [
                    552,
                    370,
                    588,
                    712
                ],
                "unroll": [
                    1952,
                    1465,
                    370,
                    1018
                ],
                "self.supports_masking": [
                    372
                ],
                "self.input_spec": [
                    514,
                    516,
                    517,
                    519,
                    658,
                    436,
                    373
                ],
                "InputSpec": [
                    461,
                    493,
                    499,
                    436,
                    373
                ],
                "self.state_spec": [
                    452,
                    454,
                    459,
                    461,
                    493,
                    495,
                    374
                ],
                "self._states": [
                    391,
                    387,
                    381,
                    375
                ],
                "self.constants_spec": [
                    376,
                    499,
                    502
                ],
                "self._num_constants": [
                    713,
                    426,
                    427,
                    638,
                    714,
                    501,
                    377,
                    636,
                    637,
                    574,
                    575
                ],
                "self.cell.state_size": [
                    385,
                    397,
                    398,
                    400,
                    672,
                    674,
                    676,
                    678,
                    679,
                    683,
                    694,
                    695,
                    697,
                    447,
                    448,
                    450,
                    459,
                    471,
                    473,
                    475,
                    382
                ],
                "int": [
                    382
                ],
                "num_states": [
                    385,
                    386,
                    383
                ],
                "_": [
                    408,
                    418,
                    386,
                    2078
                ],
                "range": [
                    592,
                    386,
                    2078
                ],
                "states.setter": [
                    389
                ],
                "output_shape": [
                    409,
                    403,
                    411,
                    405
                ],
                "state_shape": [
                    408,
                    409
                ],
                "self.states": [
                    545,
                    418,
                    546,
                    673,
                    676,
                    679,
                    682,
                    687,
                    593,
                    689,
                    693,
                    408,
                    538,
                    671
                ],
                "mask": [
                    416,
                    1024,
                    414,
                    1958,
                    415,
                    1471,
                    587,
                    542,
                    543
                ],
                "output_mask": [
                    416,
                    419,
                    421
                ],
                "state_mask": [
                    418,
                    419
                ],
                "constants_shape": [
                    441,
                    442,
                    427,
                    429
                ],
                "batch_size": [
                    673,
                    676,
                    680,
                    683,
                    434,
                    659,
                    436,
                    658,
                    698,
                    702
                ],
                "input_dim": [
                    1669,
                    1670,
                    435,
                    436,
                    1206,
                    1207
                ],
                "step_input_shape": [
                    440,
                    442,
                    444
                ],
                "self.cell.build": [
                    442,
                    444
                ],
                "spec.shape": [
                    454
                ],
                "spec": [
                    454
                ],
                "format": [
                    456
                ],
                "dim": [
                    673,
                    674,
                    679,
                    680,
                    461,
                    462,
                    697,
                    695,
                    472,
                    473,
                    698,
                    702
                ],
                "self.reset_states": [
                    464
                ],
                "initial_state": [
                    640,
                    1026,
                    650,
                    653,
                    535,
                    538,
                    540,
                    545,
                    548,
                    1960,
                    1473,
                    584,
                    468,
                    469,
                    470,
                    472,
                    475,
                    478,
                    479,
                    481,
                    490,
                    491,
                    492,
                    494,
                    635
                ],
                "K.zeros_like": [
                    468
                ],
                "K.sum": [
                    469
                ],
                "K.expand_dims": [
                    470
                ],
                "K.tile": [
                    472,
                    2065,
                    475
                ],
                "constants": [
                    576,
                    481,
                    574,
                    585,
                    651,
                    653,
                    496,
                    497,
                    498,
                    500,
                    501,
                    569,
                    635,
                    637,
                    478,
                    479
                ],
                "self._standardize_args": [
                    478
                ],
                "__call__": [
                    482,
                    522,
                    518
                ],
                "additional_inputs": [
                    513,
                    488,
                    492,
                    498,
                    504,
                    505
                ],
                "additional_specs": [
                    489,
                    514,
                    502,
                    495
                ],
                "K.int_shape": [
                    499,
                    493,
                    550
                ],
                "state": [
                    705,
                    679,
                    680,
                    493,
                    494,
                    693
                ],
                "constant": [
                    499,
                    500
                ],
                "is_keras_tensor": [
                    504,
                    506,
                    511
                ],
                "tensor": [
                    505,
                    506
                ],
                "full_input": [
                    513,
                    518
                ],
                "full_input_spec": [
                    514,
                    517
                ],
                "original_input_spec": [
                    516,
                    519
                ],
                "output": [
                    897,
                    610,
                    898,
                    612,
                    518,
                    520,
                    597,
                    599,
                    890,
                    603,
                    892
                ],
                "self.get_initial_state": [
                    540
                ],
                "str": [
                    546,
                    548,
                    689,
                    690,
                    692,
                    699,
                    702,
                    703
                ],
                "timesteps": [
                    552,
                    589,
                    551
                ],
                "has_arg": [
                    570,
                    566
                ],
                "self.cell.call": [
                    576,
                    570,
                    580,
                    566
                ],
                "training": [
                    896,
                    1025,
                    1472,
                    2082,
                    870,
                    1254,
                    1734,
                    1959,
                    876,
                    1261,
                    1326,
                    1806,
                    567,
                    2078,
                    1727
                ],
                "last_output": [
                    602,
                    582,
                    599
                ],
                "outputs": [
                    597,
                    582
                ],
                "K.rnn": [
                    582
                ],
                "step": [
                    582
                ],
                "updates": [
                    593,
                    594,
                    591
                ],
                "i": [
                    1799,
                    1801,
                    592,
                    593,
                    1776,
                    1780
                ],
                "updates.append": [
                    593
                ],
                "self.add_update": [
                    594
                ],
                "getattr": [
                    602
                ],
                "output._uses_learning_phase": [
                    897,
                    603
                ],
                "x": [
                    644,
                    645,
                    646,
                    647,
                    648
                ],
                "to_list_or_none": [
                    650,
                    651
                ],
                "AttributeError": [
                    657
                ],
                "shape": [
                    658
                ],
                "K.zeros": [
                    673,
                    676
                ],
                "K.set_value": [
                    680,
                    705,
                    682
                ],
                "np.zeros": [
                    680,
                    683
                ],
                "np": [
                    680,
                    683
                ],
                "self.name": [
                    688,
                    701
                ],
                "index": [
                    699,
                    693,
                    695
                ],
                "value": [
                    705,
                    698,
                    693,
                    703
                ],
                "enumerate": [
                    693
                ],
                "value.shape": [
                    698,
                    703
                ],
                "self.cell.get_config": [
                    716
                ],
                "self.cell.__class__.__name__": [
                    717
                ],
                "self.cell.__class__": [
                    717
                ],
                "num_constants": [
                    729,
                    727
                ],
                "layer": [
                    728,
                    729,
                    730
                ],
                "layer._num_constants": [
                    729
                ],
                "self.cell.trainable_weights": [
                    737
                ],
                "self.cell.weights": [
                    744
                ],
                "self.cell.non_trainable_weights": [
                    745
                ],
                "self.cell.losses": [
                    751
                ],
                "self.cell.get_losses_for": [
                    756
                ],
                "get_losses_for": [
                    757,
                    758
                ],
                "SimpleRNNCell": [
                    819,
                    999
                ],
                "self.units": [
                    1664,
                    1794,
                    1795,
                    1540,
                    1796,
                    1670,
                    1797,
                    1676,
                    1686,
                    1687,
                    1688,
                    1692,
                    1181,
                    1310,
                    1312,
                    1313,
                    1314,
                    1315,
                    1700,
                    1701,
                    1702,
                    1703,
                    1320,
                    1705,
                    1322,
                    1706,
                    1707,
                    1708,
                    1711,
                    1712,
                    1201,
                    1713,
                    1714,
                    820,
                    1207,
                    1213,
                    1085,
                    1220,
                    1732,
                    838,
                    843,
                    1228,
                    1229,
                    1230,
                    1232,
                    849,
                    1233,
                    1234,
                    1235,
                    1238,
                    855,
                    1239,
                    1240,
                    874,
                    1259,
                    1643,
                    2031
                ],
                "units": [
                    1444,
                    999,
                    1930,
                    1643,
                    820,
                    1181
                ],
                "self.activation": [
                    1541,
                    1801,
                    1323,
                    1644,
                    1804,
                    2032,
                    1299,
                    1780,
                    821,
                    1086,
                    891,
                    892,
                    1182
                ],
                "activations.get": [
                    1644,
                    1645,
                    821,
                    1182,
                    1183
                ],
                "activations": [
                    1541,
                    1542,
                    1644,
                    1645,
                    2032,
                    2033,
                    821,
                    1086,
                    1182,
                    1183
                ],
                "activation": [
                    1445,
                    1000,
                    1931,
                    1644,
                    821,
                    1182
                ],
                "self.use_bias": [
                    1184,
                    1281,
                    1760,
                    1219,
                    1543,
                    1646,
                    1710,
                    1682,
                    1791,
                    2034,
                    1237,
                    854,
                    822,
                    1305,
                    1087
                ],
                "use_bias": [
                    1184,
                    1447,
                    1001,
                    1933,
                    1646,
                    822
                ],
                "self.kernel_initializer": [
                    1088,
                    1186,
                    1544,
                    1672,
                    845,
                    1648,
                    2035,
                    824,
                    1209
                ],
                "initializers.get": [
                    1186,
                    1187,
                    1188,
                    1648,
                    1649,
                    1650,
                    824,
                    825,
                    826
                ],
                "initializers": [
                    1544,
                    1545,
                    1546,
                    1687,
                    1186,
                    1187,
                    1188,
                    824,
                    825,
                    826,
                    1088,
                    1089,
                    1090,
                    1648,
                    1649,
                    1650,
                    2035,
                    2036,
                    2037
                ],
                "kernel_initializer": [
                    1186,
                    1448,
                    1002,
                    1934,
                    1648,
                    824
                ],
                "self.recurrent_initializer": [
                    1089,
                    1187,
                    1545,
                    1678,
                    1649,
                    851,
                    2036,
                    825,
                    1215
                ],
                "recurrent_initializer": [
                    1187,
                    1449,
                    1003,
                    1935,
                    1649,
                    825
                ],
                "self.bias_initializer": [
                    1090,
                    1188,
                    1222,
                    1546,
                    1650,
                    2037,
                    1686,
                    1688,
                    857,
                    826,
                    1691
                ],
                "bias_initializer": [
                    1188,
                    1450,
                    1004,
                    1937,
                    1650,
                    826,
                    1691,
                    1694
                ],
                "self.kernel_regularizer": [
                    1091,
                    1190,
                    1673,
                    1547,
                    846,
                    1653,
                    2039,
                    1210,
                    828
                ],
                "regularizers.get": [
                    1954,
                    828,
                    1190,
                    1191,
                    1192,
                    1653,
                    1654,
                    1655,
                    1467,
                    1020,
                    829,
                    830
                ],
                "regularizers": [
                    1547,
                    1548,
                    1549,
                    1550,
                    2039,
                    1954,
                    1190,
                    1191,
                    1192,
                    1467,
                    828,
                    829,
                    830,
                    1091,
                    1092,
                    1093,
                    1094,
                    1653,
                    1654,
                    1655,
                    2040,
                    2041,
                    2042,
                    1020
                ],
                "kernel_regularizer": [
                    1190,
                    1451,
                    1005,
                    1938,
                    1653,
                    828
                ],
                "self.recurrent_regularizer": [
                    1216,
                    1092,
                    1191,
                    1548,
                    1679,
                    852,
                    1654,
                    2040,
                    829
                ],
                "recurrent_regularizer": [
                    1191,
                    1452,
                    1006,
                    1939,
                    1654,
                    829
                ],
                "self.bias_regularizer": [
                    1093,
                    1223,
                    1192,
                    1549,
                    1655,
                    2041,
                    858,
                    830,
                    1695
                ],
                "bias_regularizer": [
                    1192,
                    1453,
                    1007,
                    1940,
                    1655,
                    830
                ],
                "self.kernel_constraint": [
                    832,
                    1095,
                    2043,
                    1194,
                    1674,
                    847,
                    1551,
                    1657,
                    1211
                ],
                "constraints.get": [
                    832,
                    833,
                    834,
                    1194,
                    1195,
                    1196,
                    1657,
                    1658,
                    1659
                ],
                "constraints": [
                    832,
                    833,
                    834,
                    1095,
                    1096,
                    1097,
                    1194,
                    1195,
                    1196,
                    2043,
                    1551,
                    1552,
                    1553,
                    1657,
                    1658,
                    1659,
                    2044,
                    2045
                ],
                "kernel_constraint": [
                    832,
                    1194,
                    1454,
                    1008,
                    1941,
                    1657
                ],
                "self.recurrent_constraint": [
                    833,
                    1217,
                    1096,
                    1195,
                    1552,
                    1680,
                    853,
                    1658,
                    2044
                ],
                "recurrent_constraint": [
                    833,
                    1195,
                    1455,
                    1009,
                    1942,
                    1658
                ],
                "self.bias_constraint": [
                    1696,
                    834,
                    1224,
                    1097,
                    1659,
                    1196,
                    1553,
                    859,
                    2045
                ],
                "bias_constraint": [
                    834,
                    1196,
                    1456,
                    1010,
                    1943,
                    1659
                ],
                "self.dropout": [
                    1805,
                    1554,
                    1302,
                    1325,
                    1198,
                    1723,
                    1726,
                    836,
                    1098,
                    1746,
                    866,
                    1250,
                    869,
                    1253,
                    1270,
                    1785,
                    1661,
                    2046,
                    895
                ],
                "min": [
                    836,
                    837,
                    1198,
                    1199,
                    1661,
                    1662
                ],
                "max": [
                    836,
                    837,
                    1198,
                    1199,
                    1661,
                    1662
                ],
                "dropout": [
                    1441,
                    836,
                    996,
                    1927,
                    1198,
                    1457,
                    1011,
                    1944,
                    1661
                ],
                "self.recurrent_dropout": [
                    1286,
                    1805,
                    1555,
                    895,
                    1307,
                    1325,
                    1199,
                    1729,
                    837,
                    1733,
                    1099,
                    1766,
                    871,
                    1256,
                    875,
                    1260,
                    1788,
                    1662,
                    2047
                ],
                "recurrent_dropout": [
                    1442,
                    997,
                    837,
                    1928,
                    1199,
                    1458,
                    1012,
                    1945,
                    1662
                ],
                "self.state_size": [
                    1664,
                    1201,
                    838
                ],
                "self._dropout_mask": [
                    1665,
                    866,
                    867,
                    1250,
                    1251,
                    839,
                    1738,
                    878,
                    1265,
                    1202,
                    1723,
                    1724
                ],
                "self._recurrent_dropout_mask": [
                    1666,
                    1730,
                    1731,
                    840,
                    873,
                    872,
                    1257,
                    1258,
                    1740,
                    879,
                    1203,
                    1267
                ],
                "self.kernel": [
                    1700,
                    1701,
                    1670,
                    1702,
                    1703,
                    843,
                    1228,
                    1230,
                    882,
                    1234,
                    884,
                    1207,
                    1304,
                    1787
                ],
                "self.add_weight": [
                    1692,
                    1220,
                    1670,
                    843,
                    1675,
                    855,
                    848,
                    1207,
                    1212
                ],
                "self.recurrent_kernel": [
                    1705,
                    1322,
                    1675,
                    1706,
                    1229,
                    1707,
                    1231,
                    848,
                    1708,
                    1790,
                    1235,
                    890,
                    1212,
                    1310
                ],
                "self.bias": [
                    1792,
                    1698,
                    1220,
                    1226,
                    1306,
                    1711,
                    1712,
                    1713,
                    1714,
                    885,
                    886,
                    855,
                    1238,
                    1239,
                    1692,
                    861,
                    1240
                ],
                "prev_output": [
                    865,
                    890,
                    889
                ],
                "_generate_dropout_mask": [
                    867,
                    1251,
                    1731,
                    873,
                    1258,
                    1724
                ],
                "_generate_dropout_ones": [
                    868,
                    1252,
                    1732,
                    874,
                    1259,
                    1725
                ],
                "K.shape": [
                    2067,
                    868,
                    1725,
                    1252
                ],
                "dp_mask": [
                    1786,
                    1738,
                    878,
                    881,
                    882,
                    1265,
                    1747,
                    1748,
                    1749,
                    1271,
                    1272,
                    1273,
                    1750,
                    1303
                ],
                "rec_dp_mask": [
                    1287,
                    1288,
                    1289,
                    1767,
                    1768,
                    1740,
                    1769,
                    1770,
                    879,
                    1267,
                    888,
                    889,
                    1308,
                    1789
                ],
                "h": [
                    1324,
                    1804,
                    1327,
                    1328,
                    1807,
                    882,
                    1808,
                    884,
                    886,
                    890
                ],
                "K.dot": [
                    1280,
                    1294,
                    1296,
                    1299,
                    1304,
                    1309,
                    1321,
                    1790,
                    1756,
                    1757,
                    1758,
                    1759,
                    1776,
                    882,
                    1778,
                    884,
                    1780,
                    1782,
                    890,
                    1787,
                    1278,
                    1279
                ],
                "K.bias_add": [
                    1792,
                    1761,
                    1282,
                    1283,
                    1284,
                    1762,
                    1763,
                    1764,
                    886,
                    1306
                ],
                "kwargs.pop": [
                    986
                ],
                "warnings.warn": [
                    1922,
                    1432,
                    987,
                    1436,
                    1918,
                    991
                ],
                "warnings": [
                    1922,
                    1432,
                    987,
                    1436,
                    1918,
                    991
                ],
                "K.backend": [
                    1921,
                    1435,
                    990,
                    2063
                ],
                "SimpleRNN": [
                    1100,
                    1013,
                    1023
                ],
                "self.activity_regularizer": [
                    1954,
                    1094,
                    1550,
                    2042,
                    1467,
                    1020
                ],
                "activity_regularizer": [
                    1954,
                    1467,
                    1020
                ],
                "interfaces.legacy_recurrent_support": [
                    963,
                    1892,
                    1407
                ],
                "interfaces": [
                    963,
                    1892,
                    1407
                ],
                "call": [
                    1957,
                    1470,
                    1023
                ],
                "self.cell.units": [
                    1964,
                    1477,
                    1030
                ],
                "self.cell.activation": [
                    1968,
                    1481,
                    1034
                ],
                "self.cell.use_bias": [
                    1976,
                    1489,
                    1038
                ],
                "self.cell.kernel_initializer": [
                    1042,
                    1980,
                    1493
                ],
                "self.cell.recurrent_initializer": [
                    1984,
                    1497,
                    1046
                ],
                "self.cell.bias_initializer": [
                    1050,
                    1988,
                    1501
                ],
                "self.cell.kernel_regularizer": [
                    1505,
                    1996,
                    1054
                ],
                "self.cell.recurrent_regularizer": [
                    2000,
                    1058,
                    1509
                ],
                "self.cell.bias_regularizer": [
                    1513,
                    2004,
                    1062
                ],
                "self.cell.kernel_constraint": [
                    2008,
                    1066,
                    1517
                ],
                "self.cell.recurrent_constraint": [
                    1521,
                    2012,
                    1070
                ],
                "self.cell.bias_constraint": [
                    2016,
                    1074,
                    1525
                ],
                "self.cell.dropout": [
                    1529,
                    2020,
                    1078
                ],
                "self.cell.recurrent_dropout": [
                    2024,
                    1082,
                    1533
                ],
                "activations.serialize": [
                    1541,
                    1542,
                    2032,
                    2033,
                    1086
                ],
                "initializers.serialize": [
                    1088,
                    1089,
                    1090,
                    1544,
                    1545,
                    1546,
                    2035,
                    2036,
                    2037
                ],
                "regularizers.serialize": [
                    1091,
                    1092,
                    1093,
                    1094,
                    1547,
                    1548,
                    1549,
                    1550,
                    2039,
                    2040,
                    2041,
                    2042
                ],
                "constraints.serialize": [
                    1095,
                    1096,
                    1097,
                    1551,
                    1552,
                    1553,
                    2043,
                    2044,
                    2045
                ],
                "GRUCell": [
                    1444,
                    1180
                ],
                "self.recurrent_activation": [
                    1317,
                    1318,
                    1542,
                    1799,
                    1800,
                    1802,
                    1645,
                    1294,
                    1296,
                    1776,
                    1778,
                    2033,
                    1782,
                    1183
                ],
                "recurrent_activation": [
                    1932,
                    1645,
                    1446,
                    1183
                ],
                "self.implementation": [
                    2048,
                    1200,
                    1745,
                    1556,
                    1269,
                    1663
                ],
                "implementation": [
                    1200,
                    1459,
                    1431,
                    1946,
                    1917,
                    1663
                ],
                "self.kernel_z": [
                    1228,
                    1278
                ],
                "self.recurrent_kernel_z": [
                    1229,
                    1295
                ],
                "self.kernel_r": [
                    1230,
                    1279
                ],
                "self.recurrent_kernel_r": [
                    1297,
                    1231
                ],
                "self.kernel_h": [
                    1280,
                    1234
                ],
                "self.recurrent_kernel_h": [
                    1235,
                    1300
                ],
                "self.bias_z": [
                    1242,
                    1282,
                    1238
                ],
                "self.bias_r": [
                    1283,
                    1243,
                    1239
                ],
                "self.bias_h": [
                    1240,
                    1244,
                    1284
                ],
                "h_tm1": [
                    1287,
                    1288,
                    1289,
                    1291,
                    1292,
                    1293,
                    1308,
                    1309,
                    1321,
                    1324,
                    1742,
                    1248,
                    1767,
                    1768,
                    1769,
                    1770,
                    1772,
                    1773,
                    1774,
                    1775,
                    1789,
                    1790
                ],
                "inputs_z": [
                    1275,
                    1278,
                    1271
                ],
                "inputs_r": [
                    1272,
                    1276,
                    1279
                ],
                "inputs_h": [
                    1280,
                    1273,
                    1277
                ],
                "x_z": [
                    1312,
                    1282,
                    1317,
                    1294,
                    1278
                ],
                "x_r": [
                    1313,
                    1283,
                    1318,
                    1296,
                    1279
                ],
                "x_h": [
                    1280,
                    1284,
                    1320,
                    1323,
                    1299
                ],
                "h_tm1_z": [
                    1291,
                    1294,
                    1287
                ],
                "h_tm1_r": [
                    1288,
                    1296,
                    1292
                ],
                "h_tm1_h": [
                    1289,
                    1299,
                    1293
                ],
                "z": [
                    1792,
                    1794,
                    1795,
                    1796,
                    1317,
                    1797,
                    1324,
                    1294,
                    1787,
                    1790
                ],
                "r": [
                    1296,
                    1321,
                    1299,
                    1318
                ],
                "hh": [
                    1323,
                    1299,
                    1324
                ],
                "matrix_x": [
                    1312,
                    1313,
                    1320,
                    1304,
                    1306
                ],
                "matrix_inner": [
                    1314,
                    1315,
                    1309
                ],
                "recurrent_z": [
                    1314,
                    1317
                ],
                "recurrent_r": [
                    1315,
                    1318
                ],
                "recurrent_h": [
                    1321,
                    1323
                ],
                "h._uses_learning_phase": [
                    1807,
                    1327
                ],
                "GRU": [
                    1460,
                    1557,
                    1470
                ],
                "self.cell.recurrent_activation": [
                    1972,
                    1485
                ],
                "self.cell.implementation": [
                    1537,
                    2028
                ],
                "LSTMCell": [
                    1642,
                    1930
                ],
                "self.unit_forget_bias": [
                    1651,
                    1683,
                    2038
                ],
                "unit_forget_bias": [
                    1936,
                    1651
                ],
                "K.concatenate": [
                    1685
                ],
                "args": [
                    1688,
                    1686,
                    1687
                ],
                "initializers.Ones": [
                    1687
                ],
                "self.kernel_i": [
                    1756,
                    1700
                ],
                "self.kernel_f": [
                    1757,
                    1701
                ],
                "self.kernel_c": [
                    1758,
                    1702
                ],
                "self.kernel_o": [
                    1759,
                    1703
                ],
                "self.recurrent_kernel_i": [
                    1705,
                    1777
                ],
                "self.recurrent_kernel_f": [
                    1706,
                    1779
                ],
                "self.recurrent_kernel_c": [
                    1707,
                    1781
                ],
                "self.recurrent_kernel_o": [
                    1708,
                    1783
                ],
                "self.bias_i": [
                    1761,
                    1716,
                    1711
                ],
                "self.bias_f": [
                    1712,
                    1762,
                    1717
                ],
                "self.bias_c": [
                    1713,
                    1763,
                    1718
                ],
                "self.bias_o": [
                    1714,
                    1764,
                    1719
                ],
                "c_tm1": [
                    1801,
                    1780,
                    1743
                ],
                "inputs_i": [
                    1752,
                    1747,
                    1756
                ],
                "inputs_f": [
                    1753,
                    1748,
                    1757
                ],
                "inputs_c": [
                    1754,
                    1749,
                    1758
                ],
                "inputs_o": [
                    1755,
                    1750,
                    1759
                ],
                "x_i": [
                    1776,
                    1761,
                    1756
                ],
                "x_f": [
                    1762,
                    1778,
                    1757
                ],
                "x_c": [
                    1763,
                    1780,
                    1758
                ],
                "x_o": [
                    1764,
                    1782,
                    1759
                ],
                "h_tm1_i": [
                    1776,
                    1772,
                    1767
                ],
                "h_tm1_f": [
                    1768,
                    1778,
                    1773
                ],
                "h_tm1_c": [
                    1769,
                    1780,
                    1774
                ],
                "h_tm1_o": [
                    1770,
                    1782,
                    1775
                ],
                "f": [
                    1800,
                    1801,
                    1778,
                    1780
                ],
                "c": [
                    1808,
                    1801,
                    1780,
                    1804
                ],
                "o": [
                    1802,
                    1804,
                    1782
                ],
                "z0": [
                    1794,
                    1799
                ],
                "z1": [
                    1800,
                    1795
                ],
                "z2": [
                    1801,
                    1796
                ],
                "z3": [
                    1802,
                    1797
                ],
                "LSTM": [
                    2049,
                    1947,
                    1957
                ],
                "self.cell.unit_forget_bias": [
                    1992
                ],
                "ones": [
                    2081,
                    2064,
                    2065,
                    2072,
                    2077
                ],
                "K.ones_like": [
                    2064
                ],
                "K.reshape": [
                    2064
                ],
                "dims": [
                    2065,
                    2067
                ],
                "K.ones": [
                    2067
                ],
                "K.dropout": [
                    2072
                ],
                "rate": [
                    2072
                ],
                "count": [
                    2074,
                    2078
                ],
                "K.in_train_phase": [
                    2075,
                    2079
                ],
                "dropped_inputs": [
                    2080,
                    2076
                ]
            },
            "filtered_variables_in_file": {
                "Layer": [
                    736,
                    1568,
                    98,
                    131,
                    196,
                    742,
                    170,
                    139,
                    750,
                    144,
                    755,
                    20,
                    182,
                    439,
                    1111,
                    761,
                    157,
                    190
                ],
                "cell": [
                    130,
                    131,
                    132,
                    138,
                    139,
                    140,
                    1930,
                    143,
                    144,
                    145,
                    1947,
                    156,
                    157,
                    158,
                    1444,
                    169,
                    170,
                    43,
                    44,
                    171,
                    173,
                    47,
                    1460,
                    181,
                    182,
                    183,
                    189,
                    190,
                    191,
                    64,
                    65,
                    66,
                    68,
                    74,
                    75,
                    76,
                    77,
                    85,
                    86,
                    725,
                    728,
                    97,
                    98,
                    99,
                    100,
                    101,
                    354,
                    103,
                    355,
                    356,
                    358,
                    359,
                    999,
                    109,
                    110,
                    111,
                    365,
                    1013
                ],
                "cells": [
                    43,
                    108,
                    46,
                    110,
                    112,
                    50,
                    51,
                    119,
                    121,
                    123
                ],
                "self.cells": [
                    64,
                    97,
                    130,
                    169,
                    74,
                    138,
                    109,
                    143,
                    51,
                    85,
                    181,
                    156,
                    189
                ],
                "self": [
                    2048,
                    2049,
                    51,
                    52,
                    64,
                    74,
                    85,
                    97,
                    105,
                    109,
                    113,
                    127,
                    130,
                    138,
                    141,
                    143,
                    156,
                    169,
                    181,
                    189,
                    364,
                    365,
                    366,
                    367,
                    368,
                    369,
                    370,
                    372,
                    373,
                    374,
                    375,
                    376,
                    377,
                    381,
                    382,
                    385,
                    387,
                    391,
                    397,
                    398,
                    400,
                    402,
                    407,
                    408,
                    416,
                    417,
                    418,
                    426,
                    427,
                    434,
                    436,
                    439,
                    442,
                    444,
                    447,
                    448,
                    450,
                    452,
                    454,
                    459,
                    461,
                    463,
                    464,
                    471,
                    473,
                    475,
                    478,
                    482,
                    493,
                    495,
                    499,
                    501,
                    502,
                    514,
                    516,
                    517,
                    518,
                    519,
                    522,
                    537,
                    538,
                    540,
                    545,
                    546,
                    552,
                    566,
                    570,
                    574,
                    575,
                    576,
                    580,
                    586,
                    588,
                    590,
                    593,
                    594,
                    596,
                    605,
                    636,
                    637,
                    638,
                    656,
                    658,
                    671,
                    672,
                    673,
                    674,
                    676,
                    678,
                    679,
                    682,
                    683,
                    687,
                    688,
                    689,
                    693,
                    694,
                    695,
                    697,
                    701,
                    708,
                    709,
                    710,
                    711,
                    712,
                    713,
                    714,
                    716,
                    717,
                    719,
                    734,
                    736,
                    737,
                    742,
                    743,
                    744,
                    745,
                    750,
                    751,
                    755,
                    756,
                    757,
                    758,
                    819,
                    820,
                    821,
                    822,
                    824,
                    825,
                    826,
                    828,
                    829,
                    830,
                    832,
                    833,
                    834,
                    836,
                    837,
                    838,
                    839,
                    840,
                    843,
                    845,
                    846,
                    847,
                    848,
                    849,
                    851,
                    852,
                    853,
                    854,
                    855,
                    857,
                    858,
                    859,
                    861,
                    862,
                    866,
                    867,
                    869,
                    871,
                    872,
                    873,
                    874,
                    875,
                    878,
                    879,
                    882,
                    884,
                    885,
                    886,
                    890,
                    891,
                    892,
                    895,
                    1013,
                    1020,
                    1023,
                    1030,
                    1034,
                    1038,
                    1042,
                    1046,
                    1050,
                    1054,
                    1058,
                    1062,
                    1066,
                    1070,
                    1074,
                    1078,
                    1082,
                    1085,
                    1086,
                    1087,
                    1088,
                    1089,
                    1090,
                    1091,
                    1092,
                    1093,
                    1094,
                    1095,
                    1096,
                    1097,
                    1098,
                    1099,
                    1100,
                    1180,
                    1181,
                    1182,
                    1183,
                    1184,
                    1186,
                    1187,
                    1188,
                    1190,
                    1191,
                    1192,
                    1194,
                    1195,
                    1196,
                    1198,
                    1199,
                    1200,
                    1201,
                    1202,
                    1203,
                    1207,
                    1209,
                    1210,
                    1211,
                    1212,
                    1213,
                    1215,
                    1216,
                    1217,
                    1219,
                    1220,
                    1222,
                    1223,
                    1224,
                    1226,
                    1228,
                    1229,
                    1230,
                    1231,
                    1232,
                    1233,
                    1234,
                    1235,
                    1237,
                    1238,
                    1239,
                    1240,
                    1242,
                    1243,
                    1244,
                    1245,
                    1250,
                    1251,
                    1253,
                    1256,
                    1257,
                    1258,
                    1259,
                    1260,
                    1265,
                    1267,
                    1269,
                    1270,
                    1278,
                    1279,
                    1280,
                    1281,
                    1282,
                    1283,
                    1284,
                    1286,
                    1294,
                    1295,
                    1296,
                    1297,
                    1299,
                    1300,
                    1302,
                    1304,
                    1305,
                    1306,
                    1307,
                    1310,
                    1312,
                    1313,
                    1314,
                    1315,
                    1317,
                    1318,
                    1320,
                    1322,
                    1323,
                    1325,
                    1460,
                    1467,
                    1470,
                    1477,
                    1481,
                    1485,
                    1489,
                    1493,
                    1497,
                    1501,
                    1505,
                    1509,
                    1513,
                    1517,
                    1521,
                    1525,
                    1529,
                    1533,
                    1537,
                    1540,
                    1541,
                    1542,
                    1543,
                    1544,
                    1545,
                    1546,
                    1547,
                    1548,
                    1549,
                    1550,
                    1551,
                    1552,
                    1553,
                    1554,
                    1555,
                    1556,
                    1557,
                    1642,
                    1643,
                    1644,
                    1645,
                    1646,
                    1648,
                    1649,
                    1650,
                    1651,
                    1653,
                    1654,
                    1655,
                    1657,
                    1658,
                    1659,
                    1661,
                    1662,
                    1663,
                    1664,
                    1665,
                    1666,
                    1670,
                    1672,
                    1673,
                    1674,
                    1675,
                    1676,
                    1678,
                    1679,
                    1680,
                    1682,
                    1683,
                    1686,
                    1687,
                    1688,
                    1691,
                    1692,
                    1695,
                    1696,
                    1698,
                    1700,
                    1701,
                    1702,
                    1703,
                    1705,
                    1706,
                    1707,
                    1708,
                    1710,
                    1711,
                    1712,
                    1713,
                    1714,
                    1716,
                    1717,
                    1718,
                    1719,
                    1720,
                    1723,
                    1724,
                    1726,
                    1729,
                    1730,
                    1731,
                    1732,
                    1733,
                    1738,
                    1740,
                    1745,
                    1746,
                    1756,
                    1757,
                    1758,
                    1759,
                    1760,
                    1761,
                    1762,
                    1763,
                    1764,
                    1766,
                    1776,
                    1777,
                    1778,
                    1779,
                    1780,
                    1781,
                    1782,
                    1783,
                    1785,
                    1787,
                    1788,
                    1790,
                    1791,
                    1792,
                    1794,
                    1795,
                    1796,
                    1797,
                    1799,
                    1800,
                    1801,
                    1802,
                    1804,
                    1805,
                    1947,
                    1954,
                    1957,
                    1964,
                    1968,
                    1972,
                    1976,
                    1980,
                    1984,
                    1988,
                    1992,
                    1996,
                    2000,
                    2004,
                    2008,
                    2012,
                    2016,
                    2020,
                    2024,
                    2028,
                    2031,
                    2032,
                    2033,
                    2034,
                    2035,
                    2036,
                    2037,
                    2038,
                    2039,
                    2040,
                    2041,
                    2042,
                    2043,
                    2044,
                    2045,
                    2046,
                    2047
                ],
                "__init__": [
                    1642,
                    364,
                    819,
                    52,
                    1013,
                    1460,
                    1947,
                    1180
                ],
                "StackedRNNCells": [
                    113,
                    355,
                    52
                ],
                "kwargs": [
                    518,
                    522,
                    1686,
                    1687,
                    1688,
                    1180,
                    1953,
                    819,
                    52,
                    565,
                    567,
                    1466,
                    577,
                    580,
                    86,
                    985,
                    986,
                    482,
                    1642,
                    491,
                    364,
                    497,
                    1019
                ],
                "state_size": [
                    448,
                    66,
                    450,
                    68,
                    69,
                    454,
                    462,
                    63
                ],
                "cell.state_size": [
                    65,
                    66,
                    68,
                    100,
                    101,
                    103,
                    75,
                    76,
                    77
                ],
                "state_size.append": [
                    68
                ],
                "nested_states": [
                    73,
                    76,
                    79,
                    81,
                    85
                ],
                "nested_states.append": [
                    76,
                    79
                ],
                "states": [
                    389,
                    391,
                    677,
                    685,
                    686,
                    687,
                    690,
                    692,
                    693,
                    574,
                    575,
                    576,
                    580,
                    582,
                    76,
                    77,
                    1742,
                    79,
                    80,
                    592,
                    593,
                    1743,
                    85,
                    86,
                    87,
                    91,
                    93,
                    94,
                    606,
                    607,
                    609,
                    610,
                    865,
                    1248
                ],
                "new_nested_states": [
                    84,
                    92,
                    87
                ],
                "inputs": [
                    640,
                    513,
                    641,
                    522,
                    653,
                    1275,
                    2064,
                    2067,
                    533,
                    534,
                    1303,
                    1304,
                    1277,
                    1786,
                    1023,
                    540,
                    1787,
                    1957,
                    550,
                    1725,
                    1470,
                    191,
                    576,
                    580,
                    1732,
                    583,
                    594,
                    1747,
                    468,
                    1748,
                    86,
                    1749,
                    1750,
                    1752,
                    1753,
                    1754,
                    1755,
                    478,
                    479,
                    94,
                    482,
                    868,
                    1252,
                    874,
                    1259,
                    882,
                    756,
                    757,
                    758,
                    884,
                    1272,
                    1273,
                    634,
                    1271,
                    1276,
                    637,
                    638,
                    639
                ],
                "cell.call": [
                    86
                ],
                "new_nested_states.append": [
                    87
                ],
                "cell_states": [
                    92,
                    93
                ],
                "cell.build": [
                    99
                ],
                "input_shape": [
                    99,
                    1669,
                    440,
                    550,
                    104,
                    551,
                    394,
                    395,
                    427,
                    843,
                    431,
                    432,
                    434,
                    403,
                    435,
                    405,
                    1206,
                    408
                ],
                "output_dim": [
                    101,
                    103,
                    104,
                    398,
                    400,
                    403,
                    405,
                    408
                ],
                "self.built": [
                    1720,
                    105,
                    1245,
                    862
                ],
                "cells.append": [
                    121,
                    110
                ],
                "cell.__class__.__name__": [
                    110
                ],
                "cell.__class__": [
                    110
                ],
                "cell.get_config": [
                    111
                ],
                "config": [
                    2051,
                    1540,
                    2055,
                    2056,
                    2057,
                    1559,
                    1563,
                    1564,
                    1565,
                    1085,
                    708,
                    714,
                    717,
                    1102,
                    720,
                    1106,
                    1107,
                    1108,
                    725,
                    727,
                    728,
                    2031,
                    112,
                    114,
                    120,
                    123
                ],
                "base_config": [
                    2049,
                    2050,
                    2051,
                    1100,
                    1101,
                    1102,
                    719,
                    720,
                    113,
                    114,
                    1557,
                    1558,
                    1559
                ],
                "get_config": [
                    2049,
                    1100,
                    719,
                    113,
                    1557
                ],
                "base_config.items": [
                    2051,
                    1102,
                    720,
                    114,
                    1559
                ],
                "config.items": [
                    2051,
                    1102,
                    720,
                    114,
                    1559
                ],
                "cell_config": [
                    120,
                    121,
                    716,
                    718
                ],
                "config.pop": [
                    120,
                    1107,
                    725,
                    727
                ],
                "deserialize_layer": [
                    121,
                    725
                ],
                "custom_objects": [
                    122,
                    726
                ],
                "cls": [
                    2057,
                    1108,
                    728,
                    123,
                    1565
                ],
                "self.trainable": [
                    743,
                    141,
                    734,
                    127
                ],
                "weights": [
                    129,
                    132,
                    133,
                    137,
                    140,
                    172,
                    173,
                    175,
                    146,
                    147,
                    155,
                    158,
                    159
                ],
                "cell.trainable_weights": [
                    145,
                    132
                ],
                "cell.non_trainable_weights": [
                    140
                ],
                "trainable_weights": [
                    145,
                    146,
                    142
                ],
                "cell.weights": [
                    171,
                    173,
                    158
                ],
                "K.batch_get_value": [
                    159
                ],
                "K": [
                    1280,
                    1792,
                    1282,
                    1283,
                    1284,
                    1921,
                    1294,
                    2063,
                    1296,
                    2064,
                    2065,
                    1299,
                    2067,
                    1685,
                    1304,
                    2072,
                    1306,
                    1435,
                    2075,
                    1309,
                    159,
                    2079,
                    673,
                    676,
                    550,
                    680,
                    1321,
                    682,
                    1790,
                    176,
                    1725,
                    705,
                    582,
                    468,
                    469,
                    470,
                    472,
                    475,
                    1756,
                    1757,
                    990,
                    1758,
                    1759,
                    1761,
                    1762,
                    1763,
                    868,
                    1252,
                    1764,
                    493,
                    1776,
                    882,
                    499,
                    884,
                    1778,
                    886,
                    1780,
                    1782,
                    890,
                    1787,
                    1278,
                    1279
                ],
                "tuples": [
                    168,
                    174,
                    176
                ],
                "num_param": [
                    171,
                    172,
                    175
                ],
                "sw": [
                    173,
                    174
                ],
                "w": [
                    173,
                    174
                ],
                "tuples.append": [
                    174
                ],
                "K.batch_set_value": [
                    176
                ],
                "losses": [
                    192,
                    193,
                    180,
                    184,
                    185,
                    188
                ],
                "cell_losses": [
                    192,
                    756,
                    757,
                    183,
                    184,
                    191
                ],
                "cell.losses": [
                    183
                ],
                "cell.get_losses_for": [
                    191
                ],
                "RNN": [
                    482,
                    901,
                    518,
                    522,
                    364,
                    719,
                    1331,
                    1811,
                    757,
                    758
                ],
                "self.cell": [
                    1537,
                    1030,
                    1034,
                    1038,
                    1042,
                    1046,
                    1050,
                    1054,
                    1058,
                    1062,
                    1066,
                    1070,
                    1074,
                    566,
                    1078,
                    570,
                    1082,
                    576,
                    580,
                    672,
                    674,
                    676,
                    678,
                    679,
                    683,
                    694,
                    695,
                    697,
                    716,
                    717,
                    736,
                    737,
                    742,
                    744,
                    745,
                    750,
                    751,
                    755,
                    756,
                    365,
                    382,
                    385,
                    397,
                    398,
                    400,
                    1964,
                    1968,
                    1972,
                    439,
                    1976,
                    442,
                    444,
                    1980,
                    447,
                    448,
                    1984,
                    450,
                    1988,
                    1477,
                    1992,
                    1481,
                    459,
                    1996,
                    1485,
                    2000,
                    1489,
                    2004,
                    1493,
                    471,
                    2008,
                    473,
                    1497,
                    475,
                    2012,
                    1501,
                    2016,
                    1505,
                    2020,
                    1509,
                    2024,
                    1513,
                    2028,
                    1517,
                    1521,
                    1525,
                    1529,
                    1533
                ],
                "self.return_sequences": [
                    416,
                    708,
                    366,
                    402,
                    596
                ],
                "return_sequences": [
                    1948,
                    1461,
                    366,
                    1014
                ],
                "self.return_state": [
                    417,
                    709,
                    367,
                    407,
                    605
                ],
                "return_state": [
                    1015,
                    1949,
                    1462,
                    367
                ],
                "self.go_backwards": [
                    368,
                    586,
                    710
                ],
                "go_backwards": [
                    368,
                    1016,
                    1950,
                    1463
                ],
                "self.stateful": [
                    711,
                    590,
                    463,
                    656,
                    369,
                    434,
                    537
                ],
                "stateful": [
                    1464,
                    369,
                    1017,
                    1951
                ],
                "self.unroll": [
                    552,
                    370,
                    588,
                    712
                ],
                "unroll": [
                    1952,
                    1465,
                    370,
                    1018
                ],
                "self.supports_masking": [
                    372
                ],
                "self.input_spec": [
                    514,
                    516,
                    517,
                    519,
                    658,
                    436,
                    373
                ],
                "InputSpec": [
                    461,
                    493,
                    499,
                    436,
                    373
                ],
                "self.state_spec": [
                    452,
                    454,
                    459,
                    461,
                    493,
                    495,
                    374
                ],
                "self._states": [
                    391,
                    387,
                    381,
                    375
                ],
                "self.constants_spec": [
                    376,
                    499,
                    502
                ],
                "self._num_constants": [
                    713,
                    426,
                    427,
                    638,
                    714,
                    501,
                    377,
                    636,
                    637,
                    574,
                    575
                ],
                "self.cell.state_size": [
                    385,
                    397,
                    398,
                    400,
                    672,
                    674,
                    676,
                    678,
                    679,
                    683,
                    694,
                    695,
                    697,
                    447,
                    448,
                    450,
                    459,
                    471,
                    473,
                    475,
                    382
                ],
                "num_states": [
                    385,
                    386,
                    383
                ],
                "_": [
                    408,
                    418,
                    386,
                    2078
                ],
                "states.setter": [
                    389
                ],
                "output_shape": [
                    409,
                    403,
                    411,
                    405
                ],
                "state_shape": [
                    408,
                    409
                ],
                "self.states": [
                    545,
                    418,
                    546,
                    673,
                    676,
                    679,
                    682,
                    687,
                    593,
                    689,
                    693,
                    408,
                    538,
                    671
                ],
                "mask": [
                    416,
                    1024,
                    414,
                    1958,
                    415,
                    1471,
                    587,
                    542,
                    543
                ],
                "output_mask": [
                    416,
                    419,
                    421
                ],
                "state_mask": [
                    418,
                    419
                ],
                "constants_shape": [
                    441,
                    442,
                    427,
                    429
                ],
                "batch_size": [
                    673,
                    676,
                    680,
                    683,
                    434,
                    659,
                    436,
                    658,
                    698,
                    702
                ],
                "input_dim": [
                    1669,
                    1670,
                    435,
                    436,
                    1206,
                    1207
                ],
                "step_input_shape": [
                    440,
                    442,
                    444
                ],
                "self.cell.build": [
                    442,
                    444
                ],
                "spec.shape": [
                    454
                ],
                "spec": [
                    454
                ],
                "dim": [
                    673,
                    674,
                    679,
                    680,
                    461,
                    462,
                    697,
                    695,
                    472,
                    473,
                    698,
                    702
                ],
                "self.reset_states": [
                    464
                ],
                "initial_state": [
                    640,
                    1026,
                    650,
                    653,
                    535,
                    538,
                    540,
                    545,
                    548,
                    1960,
                    1473,
                    584,
                    468,
                    469,
                    470,
                    472,
                    475,
                    478,
                    479,
                    481,
                    490,
                    491,
                    492,
                    494,
                    635
                ],
                "K.zeros_like": [
                    468
                ],
                "K.sum": [
                    469
                ],
                "K.expand_dims": [
                    470
                ],
                "K.tile": [
                    472,
                    2065,
                    475
                ],
                "constants": [
                    576,
                    481,
                    574,
                    585,
                    651,
                    653,
                    496,
                    497,
                    498,
                    500,
                    501,
                    569,
                    635,
                    637,
                    478,
                    479
                ],
                "self._standardize_args": [
                    478
                ],
                "__call__": [
                    482,
                    522,
                    518
                ],
                "additional_inputs": [
                    513,
                    488,
                    492,
                    498,
                    504,
                    505
                ],
                "additional_specs": [
                    489,
                    514,
                    502,
                    495
                ],
                "K.int_shape": [
                    499,
                    493,
                    550
                ],
                "state": [
                    705,
                    679,
                    680,
                    493,
                    494,
                    693
                ],
                "constant": [
                    499,
                    500
                ],
                "is_keras_tensor": [
                    504,
                    506,
                    511
                ],
                "tensor": [
                    505,
                    506
                ],
                "full_input": [
                    513,
                    518
                ],
                "full_input_spec": [
                    514,
                    517
                ],
                "original_input_spec": [
                    516,
                    519
                ],
                "output": [
                    897,
                    610,
                    898,
                    612,
                    518,
                    520,
                    597,
                    599,
                    890,
                    603,
                    892
                ],
                "self.get_initial_state": [
                    540
                ],
                "timesteps": [
                    552,
                    589,
                    551
                ],
                "has_arg": [
                    570,
                    566
                ],
                "self.cell.call": [
                    576,
                    570,
                    580,
                    566
                ],
                "training": [
                    896,
                    1025,
                    1472,
                    2082,
                    870,
                    1254,
                    1734,
                    1959,
                    876,
                    1261,
                    1326,
                    1806,
                    567,
                    2078,
                    1727
                ],
                "last_output": [
                    602,
                    582,
                    599
                ],
                "outputs": [
                    597,
                    582
                ],
                "K.rnn": [
                    582
                ],
                "step": [
                    582
                ],
                "updates": [
                    593,
                    594,
                    591
                ],
                "i": [
                    1799,
                    1801,
                    592,
                    593,
                    1776,
                    1780
                ],
                "updates.append": [
                    593
                ],
                "self.add_update": [
                    594
                ],
                "output._uses_learning_phase": [
                    897,
                    603
                ],
                "x": [
                    644,
                    645,
                    646,
                    647,
                    648
                ],
                "to_list_or_none": [
                    650,
                    651
                ],
                "shape": [
                    658
                ],
                "K.zeros": [
                    673,
                    676
                ],
                "K.set_value": [
                    680,
                    705,
                    682
                ],
                "np.zeros": [
                    680,
                    683
                ],
                "np": [
                    680,
                    683
                ],
                "self.name": [
                    688,
                    701
                ],
                "index": [
                    699,
                    693,
                    695
                ],
                "value": [
                    705,
                    698,
                    693,
                    703
                ],
                "value.shape": [
                    698,
                    703
                ],
                "self.cell.get_config": [
                    716
                ],
                "self.cell.__class__.__name__": [
                    717
                ],
                "self.cell.__class__": [
                    717
                ],
                "num_constants": [
                    729,
                    727
                ],
                "layer": [
                    728,
                    729,
                    730
                ],
                "layer._num_constants": [
                    729
                ],
                "self.cell.trainable_weights": [
                    737
                ],
                "self.cell.weights": [
                    744
                ],
                "self.cell.non_trainable_weights": [
                    745
                ],
                "self.cell.losses": [
                    751
                ],
                "self.cell.get_losses_for": [
                    756
                ],
                "get_losses_for": [
                    757,
                    758
                ],
                "SimpleRNNCell": [
                    819,
                    999
                ],
                "self.units": [
                    1664,
                    1794,
                    1795,
                    1540,
                    1796,
                    1670,
                    1797,
                    1676,
                    1686,
                    1687,
                    1688,
                    1692,
                    1181,
                    1310,
                    1312,
                    1313,
                    1314,
                    1315,
                    1700,
                    1701,
                    1702,
                    1703,
                    1320,
                    1705,
                    1322,
                    1706,
                    1707,
                    1708,
                    1711,
                    1712,
                    1201,
                    1713,
                    1714,
                    820,
                    1207,
                    1213,
                    1085,
                    1220,
                    1732,
                    838,
                    843,
                    1228,
                    1229,
                    1230,
                    1232,
                    849,
                    1233,
                    1234,
                    1235,
                    1238,
                    855,
                    1239,
                    1240,
                    874,
                    1259,
                    1643,
                    2031
                ],
                "units": [
                    1444,
                    999,
                    1930,
                    1643,
                    820,
                    1181
                ],
                "self.activation": [
                    1541,
                    1801,
                    1323,
                    1644,
                    1804,
                    2032,
                    1299,
                    1780,
                    821,
                    1086,
                    891,
                    892,
                    1182
                ],
                "activations.get": [
                    1644,
                    1645,
                    821,
                    1182,
                    1183
                ],
                "activations": [
                    1541,
                    1542,
                    1644,
                    1645,
                    2032,
                    2033,
                    821,
                    1086,
                    1182,
                    1183
                ],
                "activation": [
                    1445,
                    1000,
                    1931,
                    1644,
                    821,
                    1182
                ],
                "self.use_bias": [
                    1184,
                    1281,
                    1760,
                    1219,
                    1543,
                    1646,
                    1710,
                    1682,
                    1791,
                    2034,
                    1237,
                    854,
                    822,
                    1305,
                    1087
                ],
                "use_bias": [
                    1184,
                    1447,
                    1001,
                    1933,
                    1646,
                    822
                ],
                "self.kernel_initializer": [
                    1088,
                    1186,
                    1544,
                    1672,
                    845,
                    1648,
                    2035,
                    824,
                    1209
                ],
                "initializers.get": [
                    1186,
                    1187,
                    1188,
                    1648,
                    1649,
                    1650,
                    824,
                    825,
                    826
                ],
                "initializers": [
                    1544,
                    1545,
                    1546,
                    1687,
                    1186,
                    1187,
                    1188,
                    824,
                    825,
                    826,
                    1088,
                    1089,
                    1090,
                    1648,
                    1649,
                    1650,
                    2035,
                    2036,
                    2037
                ],
                "kernel_initializer": [
                    1186,
                    1448,
                    1002,
                    1934,
                    1648,
                    824
                ],
                "self.recurrent_initializer": [
                    1089,
                    1187,
                    1545,
                    1678,
                    1649,
                    851,
                    2036,
                    825,
                    1215
                ],
                "recurrent_initializer": [
                    1187,
                    1449,
                    1003,
                    1935,
                    1649,
                    825
                ],
                "self.bias_initializer": [
                    1090,
                    1188,
                    1222,
                    1546,
                    1650,
                    2037,
                    1686,
                    1688,
                    857,
                    826,
                    1691
                ],
                "bias_initializer": [
                    1188,
                    1450,
                    1004,
                    1937,
                    1650,
                    826,
                    1691,
                    1694
                ],
                "self.kernel_regularizer": [
                    1091,
                    1190,
                    1673,
                    1547,
                    846,
                    1653,
                    2039,
                    1210,
                    828
                ],
                "regularizers.get": [
                    1954,
                    828,
                    1190,
                    1191,
                    1192,
                    1653,
                    1654,
                    1655,
                    1467,
                    1020,
                    829,
                    830
                ],
                "regularizers": [
                    1547,
                    1548,
                    1549,
                    1550,
                    2039,
                    1954,
                    1190,
                    1191,
                    1192,
                    1467,
                    828,
                    829,
                    830,
                    1091,
                    1092,
                    1093,
                    1094,
                    1653,
                    1654,
                    1655,
                    2040,
                    2041,
                    2042,
                    1020
                ],
                "kernel_regularizer": [
                    1190,
                    1451,
                    1005,
                    1938,
                    1653,
                    828
                ],
                "self.recurrent_regularizer": [
                    1216,
                    1092,
                    1191,
                    1548,
                    1679,
                    852,
                    1654,
                    2040,
                    829
                ],
                "recurrent_regularizer": [
                    1191,
                    1452,
                    1006,
                    1939,
                    1654,
                    829
                ],
                "self.bias_regularizer": [
                    1093,
                    1223,
                    1192,
                    1549,
                    1655,
                    2041,
                    858,
                    830,
                    1695
                ],
                "bias_regularizer": [
                    1192,
                    1453,
                    1007,
                    1940,
                    1655,
                    830
                ],
                "self.kernel_constraint": [
                    832,
                    1095,
                    2043,
                    1194,
                    1674,
                    847,
                    1551,
                    1657,
                    1211
                ],
                "constraints.get": [
                    832,
                    833,
                    834,
                    1194,
                    1195,
                    1196,
                    1657,
                    1658,
                    1659
                ],
                "constraints": [
                    832,
                    833,
                    834,
                    1095,
                    1096,
                    1097,
                    1194,
                    1195,
                    1196,
                    2043,
                    1551,
                    1552,
                    1553,
                    1657,
                    1658,
                    1659,
                    2044,
                    2045
                ],
                "kernel_constraint": [
                    832,
                    1194,
                    1454,
                    1008,
                    1941,
                    1657
                ],
                "self.recurrent_constraint": [
                    833,
                    1217,
                    1096,
                    1195,
                    1552,
                    1680,
                    853,
                    1658,
                    2044
                ],
                "recurrent_constraint": [
                    833,
                    1195,
                    1455,
                    1009,
                    1942,
                    1658
                ],
                "self.bias_constraint": [
                    1696,
                    834,
                    1224,
                    1097,
                    1659,
                    1196,
                    1553,
                    859,
                    2045
                ],
                "bias_constraint": [
                    834,
                    1196,
                    1456,
                    1010,
                    1943,
                    1659
                ],
                "self.dropout": [
                    1805,
                    1554,
                    1302,
                    1325,
                    1198,
                    1723,
                    1726,
                    836,
                    1098,
                    1746,
                    866,
                    1250,
                    869,
                    1253,
                    1270,
                    1785,
                    1661,
                    2046,
                    895
                ],
                "dropout": [
                    1441,
                    836,
                    996,
                    1927,
                    1198,
                    1457,
                    1011,
                    1944,
                    1661
                ],
                "self.recurrent_dropout": [
                    1286,
                    1805,
                    1555,
                    895,
                    1307,
                    1325,
                    1199,
                    1729,
                    837,
                    1733,
                    1099,
                    1766,
                    871,
                    1256,
                    875,
                    1260,
                    1788,
                    1662,
                    2047
                ],
                "recurrent_dropout": [
                    1442,
                    997,
                    837,
                    1928,
                    1199,
                    1458,
                    1012,
                    1945,
                    1662
                ],
                "self.state_size": [
                    1664,
                    1201,
                    838
                ],
                "self._dropout_mask": [
                    1665,
                    866,
                    867,
                    1250,
                    1251,
                    839,
                    1738,
                    878,
                    1265,
                    1202,
                    1723,
                    1724
                ],
                "self._recurrent_dropout_mask": [
                    1666,
                    1730,
                    1731,
                    840,
                    873,
                    872,
                    1257,
                    1258,
                    1740,
                    879,
                    1203,
                    1267
                ],
                "self.kernel": [
                    1700,
                    1701,
                    1670,
                    1702,
                    1703,
                    843,
                    1228,
                    1230,
                    882,
                    1234,
                    884,
                    1207,
                    1304,
                    1787
                ],
                "self.add_weight": [
                    1692,
                    1220,
                    1670,
                    843,
                    1675,
                    855,
                    848,
                    1207,
                    1212
                ],
                "self.recurrent_kernel": [
                    1705,
                    1322,
                    1675,
                    1706,
                    1229,
                    1707,
                    1231,
                    848,
                    1708,
                    1790,
                    1235,
                    890,
                    1212,
                    1310
                ],
                "self.bias": [
                    1792,
                    1698,
                    1220,
                    1226,
                    1306,
                    1711,
                    1712,
                    1713,
                    1714,
                    885,
                    886,
                    855,
                    1238,
                    1239,
                    1692,
                    861,
                    1240
                ],
                "prev_output": [
                    865,
                    890,
                    889
                ],
                "_generate_dropout_mask": [
                    867,
                    1251,
                    1731,
                    873,
                    1258,
                    1724
                ],
                "_generate_dropout_ones": [
                    868,
                    1252,
                    1732,
                    874,
                    1259,
                    1725
                ],
                "K.shape": [
                    2067,
                    868,
                    1725,
                    1252
                ],
                "dp_mask": [
                    1786,
                    1738,
                    878,
                    881,
                    882,
                    1265,
                    1747,
                    1748,
                    1749,
                    1271,
                    1272,
                    1273,
                    1750,
                    1303
                ],
                "rec_dp_mask": [
                    1287,
                    1288,
                    1289,
                    1767,
                    1768,
                    1740,
                    1769,
                    1770,
                    879,
                    1267,
                    888,
                    889,
                    1308,
                    1789
                ],
                "h": [
                    1324,
                    1804,
                    1327,
                    1328,
                    1807,
                    882,
                    1808,
                    884,
                    886,
                    890
                ],
                "K.dot": [
                    1280,
                    1294,
                    1296,
                    1299,
                    1304,
                    1309,
                    1321,
                    1790,
                    1756,
                    1757,
                    1758,
                    1759,
                    1776,
                    882,
                    1778,
                    884,
                    1780,
                    1782,
                    890,
                    1787,
                    1278,
                    1279
                ],
                "K.bias_add": [
                    1792,
                    1761,
                    1282,
                    1283,
                    1284,
                    1762,
                    1763,
                    1764,
                    886,
                    1306
                ],
                "kwargs.pop": [
                    986
                ],
                "warnings.warn": [
                    1922,
                    1432,
                    987,
                    1436,
                    1918,
                    991
                ],
                "warnings": [
                    1922,
                    1432,
                    987,
                    1436,
                    1918,
                    991
                ],
                "K.backend": [
                    1921,
                    1435,
                    990,
                    2063
                ],
                "SimpleRNN": [
                    1100,
                    1013,
                    1023
                ],
                "self.activity_regularizer": [
                    1954,
                    1094,
                    1550,
                    2042,
                    1467,
                    1020
                ],
                "activity_regularizer": [
                    1954,
                    1467,
                    1020
                ],
                "interfaces.legacy_recurrent_support": [
                    963,
                    1892,
                    1407
                ],
                "interfaces": [
                    963,
                    1892,
                    1407
                ],
                "call": [
                    1957,
                    1470,
                    1023
                ],
                "self.cell.units": [
                    1964,
                    1477,
                    1030
                ],
                "self.cell.activation": [
                    1968,
                    1481,
                    1034
                ],
                "self.cell.use_bias": [
                    1976,
                    1489,
                    1038
                ],
                "self.cell.kernel_initializer": [
                    1042,
                    1980,
                    1493
                ],
                "self.cell.recurrent_initializer": [
                    1984,
                    1497,
                    1046
                ],
                "self.cell.bias_initializer": [
                    1050,
                    1988,
                    1501
                ],
                "self.cell.kernel_regularizer": [
                    1505,
                    1996,
                    1054
                ],
                "self.cell.recurrent_regularizer": [
                    2000,
                    1058,
                    1509
                ],
                "self.cell.bias_regularizer": [
                    1513,
                    2004,
                    1062
                ],
                "self.cell.kernel_constraint": [
                    2008,
                    1066,
                    1517
                ],
                "self.cell.recurrent_constraint": [
                    1521,
                    2012,
                    1070
                ],
                "self.cell.bias_constraint": [
                    2016,
                    1074,
                    1525
                ],
                "self.cell.dropout": [
                    1529,
                    2020,
                    1078
                ],
                "self.cell.recurrent_dropout": [
                    2024,
                    1082,
                    1533
                ],
                "activations.serialize": [
                    1541,
                    1542,
                    2032,
                    2033,
                    1086
                ],
                "initializers.serialize": [
                    1088,
                    1089,
                    1090,
                    1544,
                    1545,
                    1546,
                    2035,
                    2036,
                    2037
                ],
                "regularizers.serialize": [
                    1091,
                    1092,
                    1093,
                    1094,
                    1547,
                    1548,
                    1549,
                    1550,
                    2039,
                    2040,
                    2041,
                    2042
                ],
                "constraints.serialize": [
                    1095,
                    1096,
                    1097,
                    1551,
                    1552,
                    1553,
                    2043,
                    2044,
                    2045
                ],
                "GRUCell": [
                    1444,
                    1180
                ],
                "self.recurrent_activation": [
                    1317,
                    1318,
                    1542,
                    1799,
                    1800,
                    1802,
                    1645,
                    1294,
                    1296,
                    1776,
                    1778,
                    2033,
                    1782,
                    1183
                ],
                "recurrent_activation": [
                    1932,
                    1645,
                    1446,
                    1183
                ],
                "self.implementation": [
                    2048,
                    1200,
                    1745,
                    1556,
                    1269,
                    1663
                ],
                "implementation": [
                    1200,
                    1459,
                    1431,
                    1946,
                    1917,
                    1663
                ],
                "self.kernel_z": [
                    1228,
                    1278
                ],
                "self.recurrent_kernel_z": [
                    1229,
                    1295
                ],
                "self.kernel_r": [
                    1230,
                    1279
                ],
                "self.recurrent_kernel_r": [
                    1297,
                    1231
                ],
                "self.kernel_h": [
                    1280,
                    1234
                ],
                "self.recurrent_kernel_h": [
                    1235,
                    1300
                ],
                "self.bias_z": [
                    1242,
                    1282,
                    1238
                ],
                "self.bias_r": [
                    1283,
                    1243,
                    1239
                ],
                "self.bias_h": [
                    1240,
                    1244,
                    1284
                ],
                "h_tm1": [
                    1287,
                    1288,
                    1289,
                    1291,
                    1292,
                    1293,
                    1308,
                    1309,
                    1321,
                    1324,
                    1742,
                    1248,
                    1767,
                    1768,
                    1769,
                    1770,
                    1772,
                    1773,
                    1774,
                    1775,
                    1789,
                    1790
                ],
                "inputs_z": [
                    1275,
                    1278,
                    1271
                ],
                "inputs_r": [
                    1272,
                    1276,
                    1279
                ],
                "inputs_h": [
                    1280,
                    1273,
                    1277
                ],
                "x_z": [
                    1312,
                    1282,
                    1317,
                    1294,
                    1278
                ],
                "x_r": [
                    1313,
                    1283,
                    1318,
                    1296,
                    1279
                ],
                "x_h": [
                    1280,
                    1284,
                    1320,
                    1323,
                    1299
                ],
                "h_tm1_z": [
                    1291,
                    1294,
                    1287
                ],
                "h_tm1_r": [
                    1288,
                    1296,
                    1292
                ],
                "h_tm1_h": [
                    1289,
                    1299,
                    1293
                ],
                "z": [
                    1792,
                    1794,
                    1795,
                    1796,
                    1317,
                    1797,
                    1324,
                    1294,
                    1787,
                    1790
                ],
                "r": [
                    1296,
                    1321,
                    1299,
                    1318
                ],
                "hh": [
                    1323,
                    1299,
                    1324
                ],
                "matrix_x": [
                    1312,
                    1313,
                    1320,
                    1304,
                    1306
                ],
                "matrix_inner": [
                    1314,
                    1315,
                    1309
                ],
                "recurrent_z": [
                    1314,
                    1317
                ],
                "recurrent_r": [
                    1315,
                    1318
                ],
                "recurrent_h": [
                    1321,
                    1323
                ],
                "h._uses_learning_phase": [
                    1807,
                    1327
                ],
                "GRU": [
                    1460,
                    1557,
                    1470
                ],
                "self.cell.recurrent_activation": [
                    1972,
                    1485
                ],
                "self.cell.implementation": [
                    1537,
                    2028
                ],
                "LSTMCell": [
                    1642,
                    1930
                ],
                "self.unit_forget_bias": [
                    1651,
                    1683,
                    2038
                ],
                "unit_forget_bias": [
                    1936,
                    1651
                ],
                "K.concatenate": [
                    1685
                ],
                "args": [
                    1688,
                    1686,
                    1687
                ],
                "initializers.Ones": [
                    1687
                ],
                "self.kernel_i": [
                    1756,
                    1700
                ],
                "self.kernel_f": [
                    1757,
                    1701
                ],
                "self.kernel_c": [
                    1758,
                    1702
                ],
                "self.kernel_o": [
                    1759,
                    1703
                ],
                "self.recurrent_kernel_i": [
                    1705,
                    1777
                ],
                "self.recurrent_kernel_f": [
                    1706,
                    1779
                ],
                "self.recurrent_kernel_c": [
                    1707,
                    1781
                ],
                "self.recurrent_kernel_o": [
                    1708,
                    1783
                ],
                "self.bias_i": [
                    1761,
                    1716,
                    1711
                ],
                "self.bias_f": [
                    1712,
                    1762,
                    1717
                ],
                "self.bias_c": [
                    1713,
                    1763,
                    1718
                ],
                "self.bias_o": [
                    1714,
                    1764,
                    1719
                ],
                "c_tm1": [
                    1801,
                    1780,
                    1743
                ],
                "inputs_i": [
                    1752,
                    1747,
                    1756
                ],
                "inputs_f": [
                    1753,
                    1748,
                    1757
                ],
                "inputs_c": [
                    1754,
                    1749,
                    1758
                ],
                "inputs_o": [
                    1755,
                    1750,
                    1759
                ],
                "x_i": [
                    1776,
                    1761,
                    1756
                ],
                "x_f": [
                    1762,
                    1778,
                    1757
                ],
                "x_c": [
                    1763,
                    1780,
                    1758
                ],
                "x_o": [
                    1764,
                    1782,
                    1759
                ],
                "h_tm1_i": [
                    1776,
                    1772,
                    1767
                ],
                "h_tm1_f": [
                    1768,
                    1778,
                    1773
                ],
                "h_tm1_c": [
                    1769,
                    1780,
                    1774
                ],
                "h_tm1_o": [
                    1770,
                    1782,
                    1775
                ],
                "f": [
                    1800,
                    1801,
                    1778,
                    1780
                ],
                "c": [
                    1808,
                    1801,
                    1780,
                    1804
                ],
                "o": [
                    1802,
                    1804,
                    1782
                ],
                "z0": [
                    1794,
                    1799
                ],
                "z1": [
                    1800,
                    1795
                ],
                "z2": [
                    1801,
                    1796
                ],
                "z3": [
                    1802,
                    1797
                ],
                "LSTM": [
                    2049,
                    1947,
                    1957
                ],
                "self.cell.unit_forget_bias": [
                    1992
                ],
                "ones": [
                    2081,
                    2064,
                    2065,
                    2072,
                    2077
                ],
                "K.ones_like": [
                    2064
                ],
                "K.reshape": [
                    2064
                ],
                "dims": [
                    2065,
                    2067
                ],
                "K.ones": [
                    2067
                ],
                "K.dropout": [
                    2072
                ],
                "rate": [
                    2072
                ],
                "count": [
                    2074,
                    2078
                ],
                "K.in_train_phase": [
                    2075,
                    2079
                ],
                "dropped_inputs": [
                    2080,
                    2076
                ]
            }
        },
        "test_data": [
            {
                "test_path": "/home/ubuntu/Desktop/bgp_envs_local/repos/keras_40/tests/keras/layers/recurrent_test.py",
                "test_function": "test_stacked_rnn_compute_output_shape",
                "test_function_code": "@keras_test\ndef test_stacked_rnn_compute_output_shape():\n    cells = [recurrent.LSTMCell(3),\n             recurrent.LSTMCell(6)]\n    layer = recurrent.RNN(cells, return_state=True, return_sequences=True)\n    output_shape = layer.compute_output_shape((None, timesteps, embedding_dim))\n    expected_output_shape = [(None, timesteps, 6),\n                             (None, 6),\n                             (None, 6),\n                             (None, 3),\n                             (None, 3)]\n    assert output_shape == expected_output_shape",
                "test_error": "assert [(None, 5, 6), (None, 6), (None, 6), (None, 6), (None, 6)] == [(None, 5, 6), (None, 6), (None, 6), (None, 3), (None, 3)]   At index 3 diff: (None, 6) != (None, 3)   Full diff:   - [(None, 5, 6), (None, 6), (None, 6), (None, 3), (None, 3)]   ?                                             ^          ^   + [(None, 5, 6), (None, 6), (None, 6), (None, 6), (None, 6)]   ?                                             ^          ^",
                "full_test_error": "@keras_test\n    def test_stacked_rnn_compute_output_shape():\n        cells = [recurrent.LSTMCell(3),\n                 recurrent.LSTMCell(6)]\n        layer = recurrent.RNN(cells, return_state=True, return_sequences=True)\n        output_shape = layer.compute_output_shape((None, timesteps, embedding_dim))\n        expected_output_shape = [(None, timesteps, 6),\n                                 (None, 6),\n                                 (None, 6),\n                                 (None, 3),\n                                 (None, 3)]\n>       assert output_shape == expected_output_shape\nE       assert [(None, 5, 6), (None, 6), (None, 6), (None, 6), (None, 6)] == [(None, 5, 6), (None, 6), (None, 6), (None, 3), (None, 3)]\nE         At index 3 diff: (None, 6) != (None, 3)\nE         Full diff:\nE         - [(None, 5, 6), (None, 6), (None, 6), (None, 3), (None, 3)]\nE         ?                                             ^          ^\nE         + [(None, 5, 6), (None, 6), (None, 6), (None, 6), (None, 6)]\nE         ?                                             ^          ^\n\ntests/keras/layers/recurrent_test.py:610: AssertionError",
                "traceback": null,
                "test_error_location": null,
                "test_function_decorators": [
                    "keras_test"
                ]
            }
        ]
    }
}