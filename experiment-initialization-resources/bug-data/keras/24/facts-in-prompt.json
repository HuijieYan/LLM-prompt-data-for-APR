{
    "1": "Assume that the following list of imports are available in the current environment, so you don't need to import them when generating a fix.\n```python\nimport os\nfrom . import backend as K\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\n```\n\n# The source code of the buggy function\n```python\n# The relative path of the buggy file: keras/callbacks.py\n\n\n\n    # this is the buggy function you need to fix\n    def set_model(self, model):\n        self.model = model\n        if K.backend() == 'tensorflow':\n            self.sess = K.get_session()\n        if self.histogram_freq and self.merged is None:\n            for layer in self.model.layers:\n    \n                for weight in layer.weights:\n                    mapped_weight_name = weight.name.replace(':', '_')\n                    tf.summary.histogram(mapped_weight_name, weight)\n                    if self.write_grads:\n                        grads = model.optimizer.get_gradients(model.total_loss,\n                                                              weight)\n    \n                        def is_indexed_slices(grad):\n                            return type(grad).__name__ == 'IndexedSlices'\n                        grads = [\n                            grad.values if is_indexed_slices(grad) else grad\n                            for grad in grads]\n                        tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\n                    if self.write_images:\n                        w_img = tf.squeeze(weight)\n                        shape = K.int_shape(w_img)\n                        if len(shape) == 2:  # dense layer kernel case\n                            if shape[0] > shape[1]:\n                                w_img = tf.transpose(w_img)\n                                shape = K.int_shape(w_img)\n                            w_img = tf.reshape(w_img, [1,\n                                                       shape[0],\n                                                       shape[1],\n                                                       1])\n                        elif len(shape) == 3:  # convnet case\n                            if K.image_data_format() == 'channels_last':\n                                # switch to channels_first to display\n                                # every kernel as a separate image\n                                w_img = tf.transpose(w_img, perm=[2, 0, 1])\n                                shape = K.int_shape(w_img)\n                            w_img = tf.reshape(w_img, [shape[0],\n                                                       shape[1],\n                                                       shape[2],\n                                                       1])\n                        elif len(shape) == 1:  # bias case\n                            w_img = tf.reshape(w_img, [1,\n                                                       shape[0],\n                                                       1,\n                                                       1])\n                        else:\n                            # not possible to handle 3D convnets etc.\n                            continue\n    \n                        shape = K.int_shape(w_img)\n                        assert len(shape) == 4 and shape[-1] in [1, 3, 4]\n                        tf.summary.image(mapped_weight_name, w_img)\n    \n                if hasattr(layer, 'output'):\n                    tf.summary.histogram('{}_out'.format(layer.name),\n                                         layer.output)\n        self.merged = tf.summary.merge_all()\n    \n        if self.write_graph:\n            self.writer = tf.summary.FileWriter(self.log_dir,\n                                                self.sess.graph)\n        else:\n            self.writer = tf.summary.FileWriter(self.log_dir)\n    \n        if self.embeddings_freq:\n            embeddings_layer_names = self.embeddings_layer_names\n    \n            if not embeddings_layer_names:\n                embeddings_layer_names = [layer.name for layer in self.model.layers\n                                          if type(layer).__name__ == 'Embedding']\n    \n            embeddings = {layer.name: layer.weights[0]\n                          for layer in self.model.layers\n                          if layer.name in embeddings_layer_names}\n    \n            self.saver = tf.train.Saver(list(embeddings.values()))\n    \n            embeddings_metadata = {}\n    \n            if not isinstance(self.embeddings_metadata, str):\n                embeddings_metadata = self.embeddings_metadata\n            else:\n                embeddings_metadata = {layer_name: self.embeddings_metadata\n                                       for layer_name in embeddings.keys()}\n    \n            config = projector.ProjectorConfig()\n            self.embeddings_ckpt_path = os.path.join(self.log_dir,\n                                                     'keras_embedding.ckpt')\n    \n            for layer_name, tensor in embeddings.items():\n                embedding = config.embeddings.add()\n                embedding.tensor_name = tensor.name\n    \n                if layer_name in embeddings_metadata:\n                    embedding.metadata_path = embeddings_metadata[layer_name]\n    \n            projector.visualize_embeddings(self.writer, config)\n    \n```",
    "2": "# The declaration of the class containing the buggy function\nclass TensorBoard(Callback):\n    \"\"\"\n    TensorBoard basic visualizations.\n    \n    [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\n    is a visualization tool provided with TensorFlow.\n    \n    This callback writes a log for TensorBoard, which allows\n    you to visualize dynamic graphs of your training and test\n    metrics, as well as activation histograms for the different\n    layers in your model.\n    \n    If you have installed TensorFlow with pip, you should be able\n    to launch TensorBoard from the command line:\n    ```sh\n    tensorboard --logdir=/full_path_to_your_logs\n    ```\n    \n    When using a backend other than TensorFlow, TensorBoard will still work\n    (if you have TensorFlow installed), but the only feature available will\n    be the display of the losses and metrics plots.\n    \n    # Arguments\n        log_dir: the path of the directory where to save the log\n            files to be parsed by TensorBoard.\n        histogram_freq: frequency (in epochs) at which to compute activation\n            and weight histograms for the layers of the model. If set to 0,\n            histograms won't be computed. Validation data (or split) must be\n            specified for histogram visualizations.\n        write_graph: whether to visualize the graph in TensorBoard.\n            The log file can become quite large when\n            write_graph is set to True.\n        write_grads: whether to visualize gradient histograms in TensorBoard.\n            `histogram_freq` must be greater than 0.\n        batch_size: size of batch of inputs to feed to the network\n            for histograms computation.\n        write_images: whether to write model weights to visualize as\n            image in TensorBoard.\n        embeddings_freq: frequency (in epochs) at which selected embedding\n            layers will be saved.\n        embeddings_layer_names: a list of names of layers to keep eye on. If\n            None or empty list all the embedding layer will be watched.\n        embeddings_metadata: a dictionary which maps layer name to a file name\n            in which metadata for this embedding layer is saved. See the\n            [details](https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional)\n            about metadata files format. In case if the same metadata file is\n            used for all embedding layers, string can be passed.\n    \"\"\"\n\n\n",
    "3": "# This function from the same file, but not the same class, is called by the buggy function\ndef is_indexed_slices(grad):\n    # Please ignore the body of this function\n\n    # This function from the same class is called by the buggy function\n    def is_indexed_slices(grad):\n        # Please ignore the body of this function\n\n",
    "4": "# A failing test function for the buggy function\n```python\n# The relative path of the failing test file: tests/keras/test_callbacks.py\n\n@keras_test\ndef test_TensorBoard_multi_input_output(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_test_data(\n        num_train=train_samples,\n        num_test=test_samples,\n        input_shape=(input_dim, input_dim),\n        classification=True,\n        num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield ([X_train[i * batch_size: (i + 1) * batch_size]] * 2,\n                       [y_train[i * batch_size: (i + 1) * batch_size]] * 2)\n            else:\n                yield ([X_test[i * batch_size: (i + 1) * batch_size]] * 2,\n                       [y_test[i * batch_size: (i + 1) * batch_size]] * 2)\n            i += 1\n            i = i % max_batch_index\n\n    inp1 = Input((input_dim, input_dim))\n    inp2 = Input((input_dim, input_dim))\n    inp_3d = add([inp1, inp2])\n    inp_2d = GlobalAveragePooling1D()(inp_3d)\n    inp_pair = Lambda(lambda x: x)([inp_3d, inp_2d])  # test a layer with a list of output tensors\n    hidden = dot(inp_pair, axes=-1)\n    hidden = Dense(num_hidden, activation='relu')(hidden)\n    hidden = Dropout(0.1)(hidden)\n    output1 = Dense(num_classes, activation='softmax')(hidden)\n    output2 = Dense(num_classes, activation='softmax')(hidden)\n    model = Model(inputs=[inp1, inp2], outputs=[output1, output2])\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    # we must generate new callbacks for each test, as they aren't stateless\n    def callbacks_factory(histogram_freq):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=1,\n                                      embeddings_layer_names=['dense_1'],\n                                      batch_size=5)]\n\n    # fit without validation data\n    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n              callbacks=callbacks_factory(histogram_freq=0), epochs=3)\n\n    # fit with validation data and accuracy\n    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n              validation_data=([X_test] * 2, [y_test] * 2),\n              callbacks=callbacks_factory(histogram_freq=1), epochs=2)\n\n    # fit generator without validation data\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        callbacks=callbacks_factory(histogram_freq=0))\n\n    # fit generator with validation data and accuracy\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        validation_data=([X_test] * 2, [y_test] * 2),\n                        callbacks=callbacks_factory(histogram_freq=1))\n\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()\n```\n\n\n",
    "5": "## The error message from the failing test\n```text\ngraph = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\nnode_def = name: \"lambda_1_out/values_1\"\nop: \"Pack\"\nattr {\n  key: \"N\"\n  value {\n    i: 2\n  }\n}\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"axis\"\n  value {\n    i: 0\n  }\n}\n\ninputs = [[<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]]\ncontrol_inputs = []\n\n    def _create_c_op(graph, node_def, inputs, control_inputs):\n      \"\"\"Creates a TF_Operation.\n    \n      Args:\n        graph: a `Graph`.\n        node_def: `node_def_pb2.NodeDef` for the operation to create.\n        inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of\n          `Tensor`s (corresponding to sequence inputs, e.g. \"int64 * N\",\n          \"list(int64)\"). The length of the list should be equal to the number of\n          inputs specified by this operation's op def.\n        control_inputs: A list of `Operation`s to set as control dependencies.\n    \n      Returns:\n        A wrapped TF_Operation*.\n      \"\"\"\n      # pylint: disable=protected-access\n      op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),\n                                      compat.as_str(node_def.name))\n      if node_def.device:\n        c_api.TF_SetDevice(op_desc, compat.as_str(node_def.device))\n      # Add inputs\n      for op_input in inputs:\n        if isinstance(op_input, (list, tuple)):\n          c_api.TF_AddInputList(op_desc, [t._as_tf_output() for t in op_input])\n        else:\n          c_api.TF_AddInput(op_desc, op_input._as_tf_output())\n    \n      # Add control inputs\n      for control_input in control_inputs:\n        c_api.TF_AddControlInput(op_desc, control_input._c_op)\n      # pylint: enable=protected-access\n    \n      # Add attrs\n      for name, attr_value in node_def.attr.items():\n        serialized = attr_value.SerializeToString()\n        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.\n        # It might be worth creating a convenient way to re-use the same status.\n        c_api.TF_SetAttrValueProto(op_desc, compat.as_str(name), serialized)\n    \n      try:\n>       c_op = c_api.TF_FinishOperation(op_desc)\nE       tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 3 and 2\nE       \tFrom merging shape 0 with other shapes. for 'lambda_1_out/values_1' (op: 'Pack') with input shapes: [?,2,2], [?,2].\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1864: InvalidArgumentError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x7f84323bdf50>\nop_type_name = 'HistogramSummary', name = 'lambda_1_out/', keywords = {}\nop_info = <tensorflow.python.framework.op_def_library._OpInfo object at 0x7f84323474d0>\nop_def = name: \"HistogramSummary\"\ninput_arg {\n  name: \"tag\"\n  type: DT_STRING\n}\ninput_arg {\n  name: \"values\"\n  type_attr: \"T\"\n}...   type: DT_BFLOAT16\n      type: DT_UINT16\n      type: DT_HALF\n      type: DT_UINT32\n      type: DT_UINT64\n    }\n  }\n}\n\ng = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\ndeprecation_version = 0, default_type_attr_map = {'T': tf.float32}\n\n    def _apply_op_helper(self, op_type_name, name=None, **keywords):\n      \"\"\"Implementation of apply_op that returns output_structure, op.\"\"\"\n      op_info = self._ops.get(op_type_name, None)\n      if op_info is None:\n        raise RuntimeError(\"Unrecognized Op name \" + op_type_name)\n      op_def = op_info.op_def\n    \n      # Determine the graph context.\n      try:\n        # Need to flatten all the arguments into a list.\n        # pylint: disable=protected-access\n        g = ops._get_graph_from_inputs(_Flatten(keywords.values()))\n        # pylint: enable=protected-access\n      except AssertionError as e:\n        raise RuntimeError(\n            \"Cannot determine graph for Op '%s' due to: %s\"\n            % (op_type_name, e.message))\n    \n      # Default name if not specified.\n      if name is None:\n        name = op_type_name\n    \n      # Check for deprecation\n      deprecation_version = op_def.deprecation.version\n      if deprecation_version:\n        producer = g.graph_def_versions.producer\n        if producer >= deprecation_version:\n          raise NotImplementedError(\n              (\"Op %s is not available in GraphDef version %d. \"\n               \"It has been removed in version %d. %s.\") %\n              (op_type_name, producer, deprecation_version,\n               op_def.deprecation.explanation))\n    \n      # Fill in the list of default types for all \"type\" attrs.  This\n      # will be used to choose a preferred dtype to convert to in the\n      # absence of input type information.\n      #\n      # TODO(b/31302892): Currently the defaults don't work in the right\n      # way if you have two inputs, one of whose type resolution depends\n      # on the other.  Handling this will require restructuring this code\n      # significantly.\n      default_type_attr_map = {}\n      for attr_def in op_def.attr:\n        if attr_def.type != \"type\":\n          continue\n        key = attr_def.name\n        if attr_def.HasField(\"default_value\"):\n          default_type_attr_map[key] = dtypes.as_dtype(\n              attr_def.default_value.type)\n    \n      # Requires that op_def has passed validation (using the C++\n      # ValidateOpDef() from ../framework/op_def_util.h).\n      attrs = {}\n      inputs = []\n      input_types = []\n      with g.as_default(), ops.name_scope(name) as scope:\n    \n        # Perform input type inference\n        inferred_from = {}\n        for input_arg in op_def.input_arg:\n          input_name = input_arg.name\n          if input_name in keywords:\n            values = keywords.pop(input_name)\n          elif input_name + \"_\" in keywords:\n            # Handle the case where the name is a keyword or built-in\n            # for Python so we use the name + _ instead.\n            input_name += \"_\"\n            values = keywords.pop(input_name)\n          else:\n            raise TypeError(\"No argument for input \" + input_name)\n    \n          # Goals:\n          # * Convert values to Tensors if it contains constants.\n          # * Verify that values is a list if that matches the input_arg's\n          #   type.\n          # * If the input_arg's type is determined by attrs, either set\n          #   those attrs and validate those attr values are legal (if\n          #   they have not yet been set) or validate the input matches\n          #   the type indicated by the attrs (if they have already been\n          #   inferred via an earlier input).\n          # * If the input_arg has an explicit type, make sure the input\n          #   conforms.\n    \n          if _IsListParameter(input_arg):\n            if not _IsListValue(values):\n              raise TypeError(\n                  \"Expected list for '%s' argument to '%s' Op, not %s.\" %\n                  (input_name, op_type_name, values))\n            # In cases where we expect all elements of the list to have the\n            # same dtype, try to cast non-Tensor elements to that type.\n            dtype = None\n            default_dtype = None\n            if input_arg.type != types_pb2.DT_INVALID:\n              dtype = input_arg.type\n            elif input_arg.number_attr:\n              if input_arg.type_attr in attrs:\n                dtype = attrs[input_arg.type_attr]\n              else:\n                for t in values:\n                  if isinstance(t, ops.Tensor):\n                    dtype = t.dtype\n                    break\n    \n              # dtype still not found, prefer using the default dtype\n              # from the attr.\n              if dtype is None and input_arg.type_attr in default_type_attr_map:\n                default_dtype = default_type_attr_map[input_arg.type_attr]\n    \n            try:\n              if not input_arg.is_ref and dtype:\n                dtype = dtypes.as_dtype(dtype).base_dtype\n              values = ops.internal_convert_n_to_tensor(\n                  values,\n                  name=input_arg.name,\n                  dtype=dtype if dtype else None,\n                  preferred_dtype=default_dtype,\n                  as_ref=input_arg.is_ref)\n              if input_arg.number_attr and len(\n                  set(v.dtype.base_dtype for v in values)) > 1:\n                raise TypeError()  # All types should match.\n            except (TypeError, ValueError):\n              # What types does the conversion function think values have?\n              observed_types = []\n              for value in values:\n                try:\n                  converted_value = ops.internal_convert_to_tensor(\n                      value, as_ref=input_arg.is_ref)\n                  observed_types.append(converted_value.dtype.base_dtype.name)\n                except (TypeError, ValueError):\n                  observed_types.append(\"<NOT CONVERTIBLE TO TENSOR>\")\n              observed = \", \".join(observed_types)\n    \n              prefix = (\n                  \"Tensors in list passed to '%s' of '%s' Op have types [%s]\" %\n                  (input_name, op_type_name, observed))\n              if input_arg.number_attr:\n                if input_arg.type != types_pb2.DT_INVALID:\n                  raise TypeError(\"%s that do not match expected type %s.\" %\n                                  (prefix, dtype.name))\n                elif input_arg.type_attr in attrs:\n                  raise TypeError(\"%s that do not match type %s inferred from \"\n                                  \"earlier arguments.\" %\n                                  (prefix, dtype.name))\n                else:\n                  raise TypeError(\"%s that don't all match.\" % prefix)\n              else:\n                raise TypeError(\n                    \"%s that are invalid. Tensors: %s\" % (prefix, values))\n    \n            types = [x.dtype for x in values]\n            inputs.extend(values)\n          else:\n            # In cases where we have an expected type, try to convert non-Tensor\n            # arguments to that type.\n            dtype = None\n            default_dtype = None\n            if input_arg.type != types_pb2.DT_INVALID:\n              dtype = input_arg.type\n            elif input_arg.type_attr in attrs:\n              dtype = attrs[input_arg.type_attr]\n            elif input_arg.type_attr in default_type_attr_map:\n              # The dtype could not be inferred solely from the inputs,\n              # so we prefer the attr's default, so code that adds a new attr\n              # with a default is backwards compatible.\n              default_dtype = default_type_attr_map[input_arg.type_attr]\n    \n            try:\n              values = ops.internal_convert_to_tensor(\n                  values,\n                  name=input_arg.name,\n                  dtype=dtype,\n                  as_ref=input_arg.is_ref,\n>                 preferred_dtype=default_dtype)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:527: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nvalue = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\ndtype = None, name = 'values', as_ref = False, preferred_dtype = tf.float32\nctx = <tensorflow.python.eager.context.Context object at 0x7f842c2bbad0>\naccept_symbolic_tensors = True, accept_composite_tensors = False\n\n    def internal_convert_to_tensor(value,\n                                   dtype=None,\n                                   name=None,\n                                   as_ref=False,\n                                   preferred_dtype=None,\n                                   ctx=None,\n                                   accept_symbolic_tensors=True,\n                                   accept_composite_tensors=False):\n      \"\"\"Implementation of the public convert_to_tensor.\"\"\"\n      if ctx is None:\n        ctx = context.context()\n      if isinstance(value, EagerTensor):\n        if ctx.executing_eagerly():\n          if dtype is not None:\n            dtype = dtypes.as_dtype(dtype)\n            value = _TensorTensorConversionFunction(value, dtype=dtype)\n          return value\n        else:\n          graph = get_default_graph()\n          if not graph.building_function:\n            raise RuntimeError(\"Attempting to capture an EagerTensor without \"\n                               \"building a function.\")\n          return graph.capture(value, name=name)\n      elif ((not accept_symbolic_tensors) and isinstance(value, Tensor) and\n            ctx.executing_eagerly()):\n        # Found a symbolic tensor in an eager context.\n        # This happens when we use the Keras functional API (i.e. calling layers\n        # on the output of `keras.Input()`, which is symbolic) while eager\n        # execution is enabled.\n        if _is_keras_symbolic_tensor(value):\n          # If the graph of the tensor isn't the Keras graph, we should still\n          # fail, for the time being. TODO(fchollet): consider allowing\n          # all symbolic tensors to raise this exception in this case.\n          raise core._SymbolicException(  # pylint: disable=protected-access\n              \"Using the symbolic output of a Keras layer during eager execution.\")\n    \n      if dtype is not None:\n        dtype = dtypes.as_dtype(dtype)\n      unwrapped_type = type(value)\n      conversion_func_list = _tensor_conversion_func_cache.get(unwrapped_type, None)\n      if conversion_func_list is None:\n        with _tensor_conversion_func_lock:\n          conversion_func_list = []\n          for _, funcs_at_priority in sorted(\n              _tensor_conversion_func_registry.items()):\n            for base_type, conversion_func in funcs_at_priority:\n              if isinstance(value, base_type):\n                conversion_func_list.append((base_type, conversion_func))\n          _tensor_conversion_func_cache[unwrapped_type] = conversion_func_list\n    \n      for base_type, conversion_func in conversion_func_list:\n        # If dtype is None but preferred_dtype is not None, we try to\n        # cast to preferred_dtype first.\n        ret = None\n        if dtype is None and preferred_dtype is not None:\n          try:\n            ret = conversion_func(\n                value, dtype=preferred_dtype, name=name, as_ref=as_ref)\n          except (TypeError, ValueError, errors.UnimplementedError,\n                  errors.InvalidArgumentError):\n            # Could not coerce the conversion to use the preferred dtype.\n            ret = None\n    \n          if ret is not None and ret is not NotImplemented:\n            if (ret.dtype.base_dtype !=\n                dtypes.as_dtype(preferred_dtype).base_dtype):\n              raise TypeError(\"convert_to_tensor did not convert to \"\n                              \"the preferred dtype: %s vs %s \" %\n                              (ret.dtype.base_dtype,\n                               dtypes.as_dtype(preferred_dtype).base_dtype))\n    \n        if ret is None:\n>         ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1224: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nv = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\ndtype = tf.float32, name = 'values', as_ref = False\n\n    def _autopacking_conversion_function(v, dtype=None, name=None, as_ref=False):\n      \"\"\"Tensor conversion function that automatically packs arguments.\"\"\"\n      if as_ref:\n        return NotImplemented\n      inferred_dtype = _get_dtype_from_nested_lists(v)\n      if inferred_dtype is None:\n        # We did not find any tensor-like objects in the nested lists, so defer to\n        # other conversion functions.\n        return NotImplemented\n      if dtype is None:\n        dtype = inferred_dtype\n      elif dtype != inferred_dtype:\n        v = nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)\n>     return _autopacking_helper(v, dtype, name or \"packed\")\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1145: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nlist_or_tuple = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\ndtype = tf.float32, name = 'values'\n\n    def _autopacking_helper(list_or_tuple, dtype, name):\n      \"\"\"Converts the given list or tuple to a tensor by packing.\n    \n      Args:\n        list_or_tuple: A (possibly nested) list or tuple containing a tensor.\n        dtype: The element type of the returned tensor.\n        name: A name for the returned tensor.\n    \n      Returns:\n        A `tf.Tensor` with value equivalent to `list_or_tuple`.\n      \"\"\"\n      if context.executing_eagerly():\n        # NOTE: Fast path when all the items are tensors, this doesn't do any type\n        # checking.\n        if all(ops.is_dense_tensor_like(elem) for elem in list_or_tuple):\n          return gen_array_ops.pack(list_or_tuple, name=name)\n      must_pack = False\n      converted_elems = []\n      with ops.name_scope(name) as scope:\n        for i, elem in enumerate(list_or_tuple):\n          if ops.is_dense_tensor_like(elem):\n            if dtype is not None and elem.dtype.base_dtype != dtype:\n              raise TypeError(\"Cannot convert a list containing a tensor of dtype \"\n                              \"%s to %s (Tensor is: %r)\" %\n                              (elem.dtype, dtype, elem))\n            converted_elems.append(elem)\n            must_pack = True\n          elif isinstance(elem, (list, tuple)):\n            converted_elem = _autopacking_helper(elem, dtype, str(i))\n            if ops.is_dense_tensor_like(converted_elem):\n              must_pack = True\n            converted_elems.append(converted_elem)\n          else:\n            converted_elems.append(elem)\n        if must_pack:\n          elems_as_tensors = []\n          for i, elem in enumerate(converted_elems):\n            if ops.is_dense_tensor_like(elem):\n              elems_as_tensors.append(elem)\n            else:\n              # NOTE(mrry): This is inefficient, but it enables us to\n              # handle the case where the list arguments are other\n              # convertible-to-tensor types, such as numpy arrays.\n              elems_as_tensors.append(\n                  constant_op.constant(elem, dtype=dtype, name=str(i)))\n>         return gen_array_ops.pack(elems_as_tensors, name=scope)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1095: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nvalues = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\naxis = 0, name = 'lambda_1_out/values_1/'\n\n    def pack(values, axis=0, name=None):\n      r\"\"\"Packs a list of `N` rank-`R` tensors into one rank-`(R+1)` tensor.\n    \n      Packs the `N` tensors in `values` into a tensor with rank one higher than each\n      tensor in `values`, by packing them along the `axis` dimension.\n      Given a list of tensors of shape `(A, B, C)`;\n    \n      if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\n      if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\n      Etc.\n    \n      For example:\n    \n      ```\n      # 'x' is [1, 4]\n      # 'y' is [2, 5]\n      # 'z' is [3, 6]\n      pack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.\n      pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]\n      ```\n    \n      This is the opposite of `unpack`.\n    \n      Args:\n        values: A list of at least 1 `Tensor` objects with the same type.\n          Must be of same shape and type.\n        axis: An optional `int`. Defaults to `0`.\n          Dimension along which to pack.  Negative values wrap around, so the\n          valid range is `[-(R+1), R+1)`.\n        name: A name for the operation (optional).\n    \n      Returns:\n        A `Tensor`. Has the same type as `values`.\n      \"\"\"\n      _ctx = _context._context or _context.context()\n      if _ctx is not None and _ctx._thread_local_data.is_eager:\n        try:\n          _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n            _ctx._context_handle, _ctx._thread_local_data.device_name, \"Pack\",\n            name, _ctx._post_execution_callbacks, values, \"axis\", axis)\n          return _result\n        except _core._FallbackException:\n          try:\n            return pack_eager_fallback(\n                values, axis=axis, name=name, ctx=_ctx)\n          except _core._SymbolicException:\n            pass  # Add nodes to the TensorFlow graph.\n        except _core._NotOkStatusException as e:\n          if name is not None:\n            message = e.message + \" name: \" + name\n          else:\n            message = e.message\n          _six.raise_from(_core._status_to_exception(e.code, message), None)\n      # Add nodes to the TensorFlow graph.\n      if not isinstance(values, (list, tuple)):\n        raise TypeError(\n            \"Expected list for 'values' argument to \"\n            \"'pack' Op, not %r.\" % values)\n      _attr_N = len(values)\n      if axis is None:\n        axis = 0\n      axis = _execute.make_int(axis, \"axis\")\n      _, _, _op = _op_def_lib._apply_op_helper(\n>           \"Pack\", values=values, axis=axis, name=name)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:5897: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x7f8432ec7150>\nop_type_name = 'Pack', name = 'lambda_1_out/values_1/', keywords = {}\nop_info = <tensorflow.python.framework.op_def_library._OpInfo object at 0x7f8432ecefd0>\nop_def = name: \"Pack\"\ninput_arg {\n  name: \"values\"\n  type_attr: \"T\"\n  number_attr: \"N\"\n}\noutput_arg {\n  name: \"output\"\n  type_a... minimum: 1\n}\nattr {\n  name: \"T\"\n  type: \"type\"\n}\nattr {\n  name: \"axis\"\n  type: \"int\"\n  default_value {\n    i: 0\n  }\n}\n\ng = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\ndeprecation_version = 0, default_type_attr_map = {}\n\n    def _apply_op_helper(self, op_type_name, name=None, **keywords):\n      \"\"\"Implementation of apply_op that returns output_structure, op.\"\"\"\n      op_info = self._ops.get(op_type_name, None)\n      if op_info is None:\n        raise RuntimeError(\"Unrecognized Op name \" + op_type_name)\n      op_def = op_info.op_def\n    \n      # Determine the graph context.\n      try:\n        # Need to flatten all the arguments into a list.\n        # pylint: disable=protected-access\n        g = ops._get_graph_from_inputs(_Flatten(keywords.values()))\n        # pylint: enable=protected-access\n      except AssertionError as e:\n        raise RuntimeError(\n            \"Cannot determine graph for Op '%s' due to: %s\"\n            % (op_type_name, e.message))\n    \n      # Default name if not specified.\n      if name is None:\n        name = op_type_name\n    \n      # Check for deprecation\n      deprecation_version = op_def.deprecation.version\n      if deprecation_version:\n        producer = g.graph_def_versions.producer\n        if producer >= deprecation_version:\n          raise NotImplementedError(\n              (\"Op %s is not available in GraphDef version %d. \"\n               \"It has been removed in version %d. %s.\") %\n              (op_type_name, producer, deprecation_version,\n               op_def.deprecation.explanation))\n    \n      # Fill in the list of default types for all \"type\" attrs.  This\n      # will be used to choose a preferred dtype to convert to in the\n      # absence of input type information.\n      #\n      # TODO(b/31302892): Currently the defaults don't work in the right\n      # way if you have two inputs, one of whose type resolution depends\n      # on the other.  Handling this will require restructuring this code\n      # significantly.\n      default_type_attr_map = {}\n      for attr_def in op_def.attr:\n        if attr_def.type != \"type\":\n          continue\n        key = attr_def.name\n        if attr_def.HasField(\"default_value\"):\n          default_type_attr_map[key] = dtypes.as_dtype(\n              attr_def.default_value.type)\n    \n      # Requires that op_def has passed validation (using the C++\n      # ValidateOpDef() from ../framework/op_def_util.h).\n      attrs = {}\n      inputs = []\n      input_types = []\n      with g.as_default(), ops.name_scope(name) as scope:\n    \n        # Perform input type inference\n        inferred_from = {}\n        for input_arg in op_def.input_arg:\n          input_name = input_arg.name\n          if input_name in keywords:\n            values = keywords.pop(input_name)\n          elif input_name + \"_\" in keywords:\n            # Handle the case where the name is a keyword or built-in\n            # for Python so we use the name + _ instead.\n            input_name += \"_\"\n            values = keywords.pop(input_name)\n          else:\n            raise TypeError(\"No argument for input \" + input_name)\n    \n          # Goals:\n          # * Convert values to Tensors if it contains constants.\n          # * Verify that values is a list if that matches the input_arg's\n          #   type.\n          # * If the input_arg's type is determined by attrs, either set\n          #   those attrs and validate those attr values are legal (if\n          #   they have not yet been set) or validate the input matches\n          #   the type indicated by the attrs (if they have already been\n          #   inferred via an earlier input).\n          # * If the input_arg has an explicit type, make sure the input\n          #   conforms.\n    \n          if _IsListParameter(input_arg):\n            if not _IsListValue(values):\n              raise TypeError(\n                  \"Expected list for '%s' argument to '%s' Op, not %s.\" %\n                  (input_name, op_type_name, values))\n            # In cases where we expect all elements of the list to have the\n            # same dtype, try to cast non-Tensor elements to that type.\n            dtype = None\n            default_dtype = None\n            if input_arg.type != types_pb2.DT_INVALID:\n              dtype = input_arg.type\n            elif input_arg.number_attr:\n              if input_arg.type_attr in attrs:\n                dtype = attrs[input_arg.type_attr]\n              else:\n                for t in values:\n                  if isinstance(t, ops.Tensor):\n                    dtype = t.dtype\n                    break\n    \n              # dtype still not found, prefer using the default dtype\n              # from the attr.\n              if dtype is None and input_arg.type_attr in default_type_attr_map:\n                default_dtype = default_type_attr_map[input_arg.type_attr]\n    \n            try:\n              if not input_arg.is_ref and dtype:\n                dtype = dtypes.as_dtype(dtype).base_dtype\n              values = ops.internal_convert_n_to_tensor(\n                  values,\n                  name=input_arg.name,\n                  dtype=dtype if dtype else None,\n                  preferred_dtype=default_dtype,\n                  as_ref=input_arg.is_ref)\n              if input_arg.number_attr and len(\n                  set(v.dtype.base_dtype for v in values)) > 1:\n                raise TypeError()  # All types should match.\n            except (TypeError, ValueError):\n              # What types does the conversion function think values have?\n              observed_types = []\n              for value in values:\n                try:\n                  converted_value = ops.internal_convert_to_tensor(\n                      value, as_ref=input_arg.is_ref)\n                  observed_types.append(converted_value.dtype.base_dtype.name)\n                except (TypeError, ValueError):\n                  observed_types.append(\"<NOT CONVERTIBLE TO TENSOR>\")\n              observed = \", \".join(observed_types)\n    \n              prefix = (\n                  \"Tensors in list passed to '%s' of '%s' Op have types [%s]\" %\n                  (input_name, op_type_name, observed))\n              if input_arg.number_attr:\n                if input_arg.type != types_pb2.DT_INVALID:\n                  raise TypeError(\"%s that do not match expected type %s.\" %\n                                  (prefix, dtype.name))\n                elif input_arg.type_attr in attrs:\n                  raise TypeError(\"%s that do not match type %s inferred from \"\n                                  \"earlier arguments.\" %\n                                  (prefix, dtype.name))\n                else:\n                  raise TypeError(\"%s that don't all match.\" % prefix)\n              else:\n                raise TypeError(\n                    \"%s that are invalid. Tensors: %s\" % (prefix, values))\n    \n            types = [x.dtype for x in values]\n            inputs.extend(values)\n          else:\n            # In cases where we have an expected type, try to convert non-Tensor\n            # arguments to that type.\n            dtype = None\n            default_dtype = None\n            if input_arg.type != types_pb2.DT_INVALID:\n              dtype = input_arg.type\n            elif input_arg.type_attr in attrs:\n              dtype = attrs[input_arg.type_attr]\n            elif input_arg.type_attr in default_type_attr_map:\n              # The dtype could not be inferred solely from the inputs,\n              # so we prefer the attr's default, so code that adds a new attr\n              # with a default is backwards compatible.\n              default_dtype = default_type_attr_map[input_arg.type_attr]\n    \n            try:\n              values = ops.internal_convert_to_tensor(\n                  values,\n                  name=input_arg.name,\n                  dtype=dtype,\n                  as_ref=input_arg.is_ref,\n                  preferred_dtype=default_dtype)\n            except TypeError as err:\n              if dtype is None:\n                raise err\n              else:\n                raise TypeError(\n                    \"Expected %s passed to parameter '%s' of op '%s', got %s of \"\n                    \"type '%s' instead. Error: %s\" %\n                    (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,\n                     repr(values), type(values).__name__, err))\n            except ValueError:\n              # What type does convert_to_tensor think it has?\n              try:\n                observed = ops.internal_convert_to_tensor(\n                    values, as_ref=input_arg.is_ref).dtype.name\n              except ValueError as err:\n                raise ValueError(\n                    \"Tried to convert '%s' to a tensor and failed. Error: %s\" %\n                    (input_name, err))\n              prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n                        (input_name, op_type_name, observed))\n              if input_arg.type != types_pb2.DT_INVALID:\n                raise TypeError(\"%s expected type of %s.\" %\n                                (prefix, dtypes.as_dtype(input_arg.type).name))\n              else:\n                # Update the maps with the default, if needed.\n                k = input_arg.type_attr\n                if k in default_type_attr_map:\n                  if k not in attrs:\n                    attrs[k] = default_type_attr_map[k]\n                    if k not in inferred_from:\n                      inferred_from[k] = \"Default in OpDef\"\n    \n                raise TypeError(\n                    \"%s type %s of argument '%s'.\" %\n                    (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n                     inferred_from[input_arg.type_attr]))\n    \n            types = [values.dtype]\n            inputs.append(values)\n          base_types = [x.base_dtype for x in types]\n    \n          if input_arg.number_attr:\n            # <number-attr> * <type> or <number-attr> * <type-attr>\n            if input_arg.number_attr in attrs:\n              if len(values) != attrs[input_arg.number_attr]:\n                raise ValueError(\n                    \"List argument '%s' to '%s' Op with length %d must match \"\n                    \"length %d of argument '%s'.\" %\n                    (input_name, op_type_name, len(values),\n                     attrs[input_arg.number_attr],\n                     inferred_from[input_arg.number_attr]))\n            else:\n              attrs[input_arg.number_attr] = len(values)\n              inferred_from[input_arg.number_attr] = input_name\n              num_attr = _Attr(op_def, input_arg.number_attr)\n              if num_attr.has_minimum and len(values) < num_attr.minimum:\n                raise ValueError(\n                    \"List argument '%s' to '%s' Op with length %d shorter \"\n                    \"than minimum length %d.\" %\n                    (input_name, op_type_name, len(values), num_attr.minimum))\n            # All tensors must have the same base type.\n            if any(bt != base_types[0] for bt in base_types):\n              raise TypeError(\n                  \"All tensors passed to '%s' of '%s' Op \"\n                  \"must have the same type.\" %\n                  (input_name, op_type_name))\n            if input_arg.type != types_pb2.DT_INVALID:\n              # <number-attr> * <type> case\n              if base_types and base_types[0] != input_arg.type:\n                assert False, \"Unreachable\"\n            elif input_arg.type_attr in attrs:\n              # <number-attr> * <type-attr> case, where <type-attr> already\n              # has an inferred value.\n              if base_types and base_types[0] != attrs[input_arg.type_attr]:\n                assert False, \"Unreachable\"\n            else:\n              # <number-attr> * <type-attr> case, where we are now setting\n              # the <type-attr> based on this input\n              if not base_types:\n                raise TypeError(\n                    \"Don't know how to infer type variable from empty input \"\n                    \"list passed to input '%s' of '%s' Op.\" %\n                    (input_name, op_type_name))\n              attrs[input_arg.type_attr] = base_types[0]\n              inferred_from[input_arg.type_attr] = input_name\n              type_attr = _Attr(op_def, input_arg.type_attr)\n              _SatisfiesTypeConstraint(base_types[0], type_attr,\n                                       param_name=input_name)\n          elif input_arg.type_attr:\n            # <type-attr>\n            attr_value = base_types[0]\n            if input_arg.type_attr in attrs:\n              if attrs[input_arg.type_attr] != attr_value:\n                assert False, \"Unreachable\"\n            else:\n              for base_type in base_types:\n                _SatisfiesTypeConstraint(base_type,\n                                         _Attr(op_def, input_arg.type_attr),\n                                         param_name=input_name)\n              attrs[input_arg.type_attr] = attr_value\n              inferred_from[input_arg.type_attr] = input_name\n          elif input_arg.type_list_attr:\n            # <type-list-attr>\n            attr_value = base_types\n            if input_arg.type_list_attr in attrs:\n              if attrs[input_arg.type_list_attr] != attr_value:\n                raise TypeError(\n                    \"Input '%s' of '%s' Op has type list of %s that does not \"\n                    \"match type list %s of argument '%s'.\" %\n                    (input_name, op_type_name,\n                     \", \".join(dtypes.as_dtype(x).name for x in attr_value),\n                     \", \".join(dtypes.as_dtype(x).name\n                               for x in attrs[input_arg.type_list_attr]),\n                     inferred_from[input_arg.type_list_attr]))\n            else:\n              for base_type in base_types:\n                _SatisfiesTypeConstraint(base_type,\n                                         _Attr(op_def, input_arg.type_list_attr),\n                                         param_name=input_name)\n              attrs[input_arg.type_list_attr] = attr_value\n              inferred_from[input_arg.type_list_attr] = input_name\n          else:\n            # single Tensor with specified type\n            if base_types[0] != input_arg.type:\n              assert False, \"Unreachable\"\n    \n          if input_arg.is_ref:\n            if not all(x._is_ref_dtype for x in types):  # pylint: disable=protected-access\n              raise TypeError(\n                  (\"'%s' Op requires that input '%s' be a mutable tensor \"\n                   \"(e.g.: a tf.Variable)\") % (op_type_name, input_name))\n            input_types.extend(types)\n          else:\n            input_types.extend(base_types)\n    \n        # Process remaining attrs\n        for attr in op_def.attr:\n          # Skip attrs that have already had their values inferred\n          if attr.name in attrs:\n            if attr.name in keywords:\n              raise TypeError(\n                  \"Should not specify value for inferred attr '%s'.\" % attr.name)\n            continue\n          if attr.name in keywords:\n            attrs[attr.name] = keywords.pop(attr.name)\n          elif attr.name + \"_\" in keywords:\n            # Attrs whose names match Python keywords have an extra '_'\n            # appended, so we must check for that as well.\n            attrs[attr.name] = keywords.pop(attr.name + \"_\")\n          else:\n            raise TypeError(\"No argument for attr \" + attr.name)\n    \n        # Convert attr values to AttrValue protos.\n        attr_protos = {}\n        for attr_def in op_def.attr:\n          key = attr_def.name\n          value = attrs[key]\n          attr_value = attr_value_pb2.AttrValue()\n          if attr_def.HasField(\"default_value\") and value is None:\n            attr_value.CopyFrom(attr_def.default_value)\n            attr_protos[key] = attr_value\n            continue\n          if attr_def.type.startswith(\"list(\"):\n            if not _IsListValue(value):\n              raise TypeError(\"Expected list for attr \" + key)\n            if attr_def.has_minimum:\n              if len(value) < attr_def.minimum:\n                raise ValueError(\"Attr '%s' of '%s' Op passed list of length %d \"\n                                 \"less than minimum %d.\" %\n                                 (key, op_type_name, len(value),\n                                  attr_def.minimum))\n            attr_value.list.SetInParent()\n          if attr_def.type == \"string\":\n            attr_value.s = _MakeStr(value, key)\n            if attr_def.HasField(\"allowed_values\"):\n              if attr_value.s not in attr_def.allowed_values.list.s:\n                raise ValueError(\n                    \"Attr '%s' of '%s' Op passed string '%s' not in: \\\"%s\\\".\" %\n                    (key, op_type_name, compat.as_text(attr_value.s),\n                     '\", \"'.join(map(compat.as_text,\n                                     attr_def.allowed_values.list.s))))\n          elif attr_def.type == \"list(string)\":\n            attr_value.list.s.extend([_MakeStr(x, key) for x in value])\n            if attr_def.HasField(\"allowed_values\"):\n              for x in attr_value.list.s:\n                if x not in attr_def.allowed_values.list.s:\n                  raise ValueError(\n                      \"Attr '%s' of '%s' Op passed string '%s' not in: \\\"%s\\\".\" %\n                      (key, op_type_name, compat.as_text(x),\n                       '\", \"'.join(map(compat.as_text,\n                                       attr_def.allowed_values.list.s))))\n          elif attr_def.type == \"int\":\n            attr_value.i = _MakeInt(value, key)\n            if attr_def.has_minimum:\n              if attr_value.i < attr_def.minimum:\n                raise ValueError(\n                    \"Attr '%s' of '%s' Op passed %d less than minimum %d.\" %\n                    (key, op_type_name, attr_value.i, attr_def.minimum))\n          elif attr_def.type == \"list(int)\":\n            attr_value.list.i.extend([_MakeInt(x, key) for x in value])\n          elif attr_def.type == \"float\":\n            attr_value.f = _MakeFloat(value, key)\n          elif attr_def.type == \"list(float)\":\n            attr_value.list.f.extend([_MakeFloat(x, key) for x in value])\n          elif attr_def.type == \"bool\":\n            attr_value.b = _MakeBool(value, key)\n          elif attr_def.type == \"list(bool)\":\n            attr_value.list.b.extend([_MakeBool(x, key) for x in value])\n          elif attr_def.type == \"type\":\n            attr_value.type = _MakeType(value, attr_def)\n          elif attr_def.type == \"list(type)\":\n            attr_value.list.type.extend(\n                [_MakeType(x, attr_def) for x in value])\n          elif attr_def.type == \"shape\":\n            attr_value.shape.CopyFrom(_MakeShape(value, key))\n          elif attr_def.type == \"list(shape)\":\n            attr_value.list.shape.extend(\n                [_MakeShape(x, key) for x in value])\n          elif attr_def.type == \"tensor\":\n            attr_value.tensor.CopyFrom(_MakeTensor(value, key))\n          elif attr_def.type == \"list(tensor)\":\n            attr_value.list.tensor.extend(\n                [_MakeTensor(x, key) for x in value])\n          elif attr_def.type == \"func\":\n            attr_value.func.CopyFrom(_MakeFunc(value, key))\n          elif attr_def.type == \"list(func)\":\n            attr_value.list.func.extend([_MakeFunc(x, key) for x in value])\n          else:\n            raise TypeError(\"Unrecognized Attr type \" + attr_def.type)\n    \n          attr_protos[key] = attr_value\n        del attrs  # attrs is no longer authoritative, use attr_protos instead\n    \n        # Determine output types (possibly using attrs)\n        output_structure = []\n        for arg in op_def.output_arg:\n          if arg.number_attr:\n            n = _AttrValue(attr_protos, arg.number_attr).i\n            output_structure.append(n)\n          elif arg.type_attr:\n            t = _AttrValue(attr_protos, arg.type_attr)\n            output_structure.append(None)\n          elif arg.type_list_attr:\n            t = _AttrValue(attr_protos, arg.type_list_attr)\n            output_structure.append(len(t.list.type))\n          else:\n            output_structure.append(None)\n    \n        if keywords:\n          raise TypeError(\"apply_op() got unexpected keyword arguments: \" +\n                          \", \".join(sorted(keywords.keys())))\n    \n        # NOTE(mrry): We add an explicit colocation constraint between\n        # the newly created op and any of its reference-typed inputs.\n        must_colocate_inputs = [val for arg, val in zip(op_def.input_arg, inputs)\n                                if arg.is_ref]\n        with _MaybeColocateWith(must_colocate_inputs):\n          # Add Op to graph\n          op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n                           input_types=input_types, attrs=attr_protos,\n>                          op_def=op_def)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:788: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nargs = (<tensorflow.python.framework.ops.Graph object at 0x7f842c246310>, 'Pack', [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>])\nkwargs = {'attrs': {'N': i: 2\n, 'T': type: DT_FLOAT\n, 'axis': i: 0\n}, 'dtypes': None, 'input_types': [tf.float32, tf.float32], 'name': 'lambda_1_out/values_1/', ...}\ninvalid_args = []\nnamed_args = {'attrs': {'N': i: 2\n, 'T': type: DT_FLOAT\n, 'axis': i: 0\n}, 'compute_device': True, 'compute_shapes': True, 'dtypes': None, ...}\narg_name = 'compute_shapes'\nspec = DeprecatedArgSpec(position=8, has_ok_value=False, ok_value=None)\n\n    @functools.wraps(func)\n    def new_func(*args, **kwargs):\n      \"\"\"Deprecation wrapper.\"\"\"\n      # TODO(apassos) figure out a way to have reasonable performance with\n      # deprecation warnings and eager mode.\n      if is_in_graph_mode.IS_IN_GRAPH_MODE() and _PRINT_DEPRECATION_WARNINGS:\n        invalid_args = []\n        named_args = tf_inspect.getcallargs(func, *args, **kwargs)\n        for arg_name, spec in iter(deprecated_positions.items()):\n          if (spec.position < len(args) and\n              not (spec.has_ok_value and\n                   _same_value(named_args[arg_name], spec.ok_value))):\n            invalid_args.append(arg_name)\n        if is_varargs_deprecated and len(args) > len(arg_spec.args):\n          invalid_args.append(arg_spec.varargs)\n        if is_kwargs_deprecated and kwargs:\n          invalid_args.append(arg_spec.varkw)\n        for arg_name in deprecated_arg_names:\n          if (arg_name in kwargs and\n              not (deprecated_positions[arg_name].has_ok_value and\n                   _same_value(named_args[arg_name],\n                               deprecated_positions[arg_name].ok_value))):\n            invalid_args.append(arg_name)\n        for arg_name in invalid_args:\n          if (func, arg_name) not in _PRINTED_WARNING:\n            if warn_once:\n              _PRINTED_WARNING[(func, arg_name)] = True\n            logging.warning(\n                'From %s: calling %s (from %s) with %s is deprecated and will '\n                'be removed %s.\\nInstructions for updating:\\n%s',\n                _call_location(), decorator_utils.get_qualified_name(func),\n                func.__module__, arg_name,\n                'in a future version' if date is None else ('after %s' % date),\n                instructions)\n>     return func(*args, **kwargs)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:507: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\nop_type = 'Pack'\ninputs = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\ndtypes = None, input_types = [tf.float32, tf.float32]\nname = 'lambda_1_out/values_1'\nattrs = {'N': i: 2\n, 'T': type: DT_FLOAT\n, 'axis': i: 0\n}\nop_def = name: \"Pack\"\ninput_arg {\n  name: \"values\"\n  type_attr: \"T\"\n  number_attr: \"N\"\n}\noutput_arg {\n  name: \"output\"\n  type_a... minimum: 1\n}\nattr {\n  name: \"T\"\n  type: \"type\"\n}\nattr {\n  name: \"axis\"\n  type: \"int\"\n  default_value {\n    i: 0\n  }\n}\n\ncompute_device = True\n\n    @deprecated_args(None,\n                     \"Shapes are always computed; don't use the compute_shapes \"\n                     \"as it has no effect.\", \"compute_shapes\")\n    def create_op(\n        self,\n        op_type,\n        inputs,\n        dtypes=None,  # pylint: disable=redefined-outer-name\n        input_types=None,\n        name=None,\n        attrs=None,\n        op_def=None,\n        compute_shapes=True,\n        compute_device=True):\n      \"\"\"Creates an `Operation` in this graph.\n    \n      This is a low-level interface for creating an `Operation`. Most\n      programs will not call this method directly, and instead use the\n      Python op constructors, such as `tf.constant()`, which add ops to\n      the default graph.\n    \n      Args:\n        op_type: The `Operation` type to create. This corresponds to the\n          `OpDef.name` field for the proto that defines the operation.\n        inputs: A list of `Tensor` objects that will be inputs to the `Operation`.\n        dtypes: (Optional) A list of `DType` objects that will be the types of the\n          tensors that the operation produces.\n        input_types: (Optional.) A list of `DType`s that will be the types of the\n          tensors that the operation consumes. By default, uses the base `DType`\n          of each input in `inputs`. Operations that expect reference-typed inputs\n          must specify `input_types` explicitly.\n        name: (Optional.) A string name for the operation. If not specified, a\n          name is generated based on `op_type`.\n        attrs: (Optional.) A dictionary where the key is the attribute name (a\n          string) and the value is the respective `attr` attribute of the\n          `NodeDef` proto that will represent the operation (an `AttrValue`\n          proto).\n        op_def: (Optional.) The `OpDef` proto that describes the `op_type` that\n          the operation will have.\n        compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always\n          computed).\n        compute_device: (Optional.) If True, device functions will be executed to\n          compute the device property of the Operation.\n    \n      Raises:\n        TypeError: if any of the inputs is not a `Tensor`.\n        ValueError: if colocation conflicts with existing device assignment.\n    \n      Returns:\n        An `Operation` object.\n      \"\"\"\n      del compute_shapes\n    \n      self._check_not_finalized()\n      for idx, a in enumerate(inputs):\n        if not isinstance(a, Tensor):\n          raise TypeError(\"Input #%d is not a tensor: %s\" % (idx, a))\n      if name is None:\n        name = op_type\n      # If a names ends with a '/' it is a \"name scope\" and we use it as-is,\n      # after removing the trailing '/'.\n      if name and name[-1] == \"/\":\n        name = name_from_scope_name(name)\n      else:\n        name = self.unique_name(name)\n    \n      node_def = _NodeDef(op_type, name, device=None, attrs=attrs)\n    \n      input_ops = set([t.op for t in inputs])\n      control_inputs = self._control_dependencies_for_inputs(input_ops)\n      # _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\n      # Session.run call cannot occur between creating and mutating the op.\n      with self._mutation_lock():\n        ret = Operation(\n            node_def,\n            self,\n            inputs=inputs,\n            output_types=dtypes,\n            control_inputs=control_inputs,\n            input_types=input_types,\n            original_op=self._default_original_op,\n>           op_def=op_def)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3616: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <[AttributeError(\"'Operation' object has no attribute '_c_op'\") raised in repr()] Operation object at 0x7f841b853d10>\nnode_def = name: \"lambda_1_out/values_1\"\nop: \"Pack\"\nattr {\n  key: \"N\"\n  value {\n    i: 2\n  }\n}\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"axis\"\n  value {\n    i: 0\n  }\n}\n\ng = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\ninputs = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\noutput_types = None, control_inputs = [], input_types = [tf.float32, tf.float32]\noriginal_op = None\nop_def = name: \"Pack\"\ninput_arg {\n  name: \"values\"\n  type_attr: \"T\"\n  number_attr: \"N\"\n}\noutput_arg {\n  name: \"output\"\n  type_a... minimum: 1\n}\nattr {\n  name: \"T\"\n  type: \"type\"\n}\nattr {\n  name: \"axis\"\n  type: \"int\"\n  default_value {\n    i: 0\n  }\n}\n\n\n    def __init__(self,\n                 node_def,\n                 g,\n                 inputs=None,\n                 output_types=None,\n                 control_inputs=None,\n                 input_types=None,\n                 original_op=None,\n                 op_def=None):\n      r\"\"\"Creates an `Operation`.\n    \n      NOTE: This constructor validates the name of the `Operation` (passed\n      as `node_def.name`). Valid `Operation` names match the following\n      regular expression:\n    \n          [A-Za-z0-9.][A-Za-z0-9_.\\\\-/]*\n    \n      Args:\n        node_def: `node_def_pb2.NodeDef`.  `NodeDef` for the `Operation`. Used for\n          attributes of `node_def_pb2.NodeDef`, typically `name`, `op`, and\n          `device`.  The `input` attribute is irrelevant here as it will be\n          computed when generating the model.\n        g: `Graph`. The parent graph.\n        inputs: list of `Tensor` objects. The inputs to this `Operation`.\n        output_types: list of `DType` objects.  List of the types of the `Tensors`\n          computed by this operation.  The length of this list indicates the\n          number of output endpoints of the `Operation`.\n        control_inputs: list of operations or tensors from which to have a control\n          dependency.\n        input_types: List of `DType` objects representing the types of the tensors\n          accepted by the `Operation`.  By default uses `[x.dtype.base_dtype for x\n          in inputs]`.  Operations that expect reference-typed inputs must specify\n          these explicitly.\n        original_op: Optional. Used to associate the new `Operation` with an\n          existing `Operation` (for example, a replica with the op that was\n          replicated).\n        op_def: Optional. The `op_def_pb2.OpDef` proto that describes the op type\n          that this `Operation` represents.\n    \n      Raises:\n        TypeError: if control inputs are not Operations or Tensors,\n          or if `node_def` is not a `NodeDef`,\n          or if `g` is not a `Graph`,\n          or if `inputs` are not tensors,\n          or if `inputs` and `input_types` are incompatible.\n        ValueError: if the `node_def` name is not valid.\n      \"\"\"\n      # For internal use only: `node_def` can be set to a TF_Operation to create\n      # an Operation for that op. This is useful for creating Operations for ops\n      # indirectly created by C API methods, e.g. the ops created by\n      # TF_ImportGraphDef. When `node_def` is a TF_Operation, all optional fields\n      # should be None.\n    \n      if isinstance(node_def, node_def_pb2.NodeDef):\n        if node_def.ByteSize() >= (1 << 31) or node_def.ByteSize() < 0:\n          raise ValueError(\n              \"Cannot create a tensor proto whose content is larger than 2GB.\")\n        if not _VALID_OP_NAME_REGEX.match(node_def.name):\n          raise ValueError(\"'%s' is not a valid node name\" % node_def.name)\n        c_op = None\n      elif type(node_def).__name__ == \"SwigPyObject\":\n        assert inputs is None\n        assert output_types is None\n        assert control_inputs is None\n        assert input_types is None\n        assert original_op is None\n        assert op_def is None\n        c_op = node_def\n      else:\n        raise TypeError(\"node_def needs to be a NodeDef: %s\" % node_def)\n    \n      if not isinstance(g, Graph):\n        raise TypeError(\"g needs to be a Graph: %s\" % g)\n      self._graph = g\n    \n      if inputs is None:\n        inputs = []\n      elif not isinstance(inputs, list):\n        raise TypeError(\"inputs needs to be a list of Tensors: %s\" % inputs)\n      for a in inputs:\n        if not isinstance(a, Tensor):\n          raise TypeError(\"input needs to be a Tensor: %s\" % a)\n      if input_types is None:\n        input_types = [i.dtype.base_dtype for i in inputs]\n      else:\n        if not all(\n            x.is_compatible_with(i.dtype) for i, x in zip(inputs, input_types)):\n          raise TypeError(\"In op '%s', input types (%s) are not compatible \"\n                          \"with expected types (%s)\" %\n                          (node_def.name, [i.dtype for i in inputs], input_types))\n    \n      # Build the list of control inputs.\n      control_input_ops = []\n      if control_inputs:\n        for c in control_inputs:\n          control_op = None\n          if isinstance(c, Operation):\n            control_op = c\n          elif isinstance(c, (Tensor, IndexedSlices)):\n            control_op = c.op\n          else:\n            raise TypeError(\"Control input must be an Operation, \"\n                            \"a Tensor, or IndexedSlices: %s\" % c)\n          control_input_ops.append(control_op)\n    \n      # This will be set by self.inputs.\n      self._inputs_val = None\n    \n      # pylint: disable=protected-access\n      self._id_value = self._graph._next_id()\n      self._original_op = original_op\n      self._traceback = tf_stack.extract_stack()\n    \n      # List of _UserDevSpecs holding code location of device context manager\n      # invocations and the users original argument to them.\n      self._device_code_locations = None\n      # Dict mapping op name to file and line information for op colocation\n      # context managers.\n      self._colocation_code_locations = None\n      self._control_flow_context = self.graph._get_control_flow_context()\n      # pylint: enable=protected-access\n    \n      # Initialize self._c_op.\n      if c_op:\n        self._c_op = c_op\n      else:\n        if op_def is None:\n          op_def = self._graph._get_op_def(node_def.op)\n        # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.\n        # Refactor so we don't have to do this here.\n        grouped_inputs = self._reconstruct_sequence_inputs(\n            op_def, inputs, node_def.attr)\n        self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n>                                 control_input_ops)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2027: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ngraph = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\nnode_def = name: \"lambda_1_out/values_1\"\nop: \"Pack\"\nattr {\n  key: \"N\"\n  value {\n    i: 2\n  }\n}\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"axis\"\n  value {\n    i: 0\n  }\n}\n\ninputs = [[<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]]\ncontrol_inputs = []\n\n    def _create_c_op(graph, node_def, inputs, control_inputs):\n      \"\"\"Creates a TF_Operation.\n    \n      Args:\n        graph: a `Graph`.\n        node_def: `node_def_pb2.NodeDef` for the operation to create.\n        inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of\n          `Tensor`s (corresponding to sequence inputs, e.g. \"int64 * N\",\n          \"list(int64)\"). The length of the list should be equal to the number of\n          inputs specified by this operation's op def.\n        control_inputs: A list of `Operation`s to set as control dependencies.\n    \n      Returns:\n        A wrapped TF_Operation*.\n      \"\"\"\n      # pylint: disable=protected-access\n      op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),\n                                      compat.as_str(node_def.name))\n      if node_def.device:\n        c_api.TF_SetDevice(op_desc, compat.as_str(node_def.device))\n      # Add inputs\n      for op_input in inputs:\n        if isinstance(op_input, (list, tuple)):\n          c_api.TF_AddInputList(op_desc, [t._as_tf_output() for t in op_input])\n        else:\n          c_api.TF_AddInput(op_desc, op_input._as_tf_output())\n    \n      # Add control inputs\n      for control_input in control_inputs:\n        c_api.TF_AddControlInput(op_desc, control_input._c_op)\n      # pylint: enable=protected-access\n    \n      # Add attrs\n      for name, attr_value in node_def.attr.items():\n        serialized = attr_value.SerializeToString()\n        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.\n        # It might be worth creating a convenient way to re-use the same status.\n        c_api.TF_SetAttrValueProto(op_desc, compat.as_str(name), serialized)\n    \n      try:\n        c_op = c_api.TF_FinishOperation(op_desc)\n      except errors.InvalidArgumentError as e:\n        # Convert to ValueError for backwards compatibility.\n>       raise ValueError(str(e))\nE       ValueError: Shapes must be equal rank, but are 3 and 2\nE       \tFrom merging shape 0 with other shapes. for 'lambda_1_out/values_1' (op: 'Pack') with input shapes: [?,2,2], [?,2].\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1867: ValueError\n\nDuring handling of the above exception, another exception occurred:\n\ngraph = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\nnode_def = name: \"lambda_1_out/packed\"\nop: \"Pack\"\nattr {\n  key: \"N\"\n  value {\n    i: 2\n  }\n}\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"axis\"\n  value {\n    i: 0\n  }\n}\n\ninputs = [[<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]]\ncontrol_inputs = []\n\n    def _create_c_op(graph, node_def, inputs, control_inputs):\n      \"\"\"Creates a TF_Operation.\n    \n      Args:\n        graph: a `Graph`.\n        node_def: `node_def_pb2.NodeDef` for the operation to create.\n        inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of\n          `Tensor`s (corresponding to sequence inputs, e.g. \"int64 * N\",\n          \"list(int64)\"). The length of the list should be equal to the number of\n          inputs specified by this operation's op def.\n        control_inputs: A list of `Operation`s to set as control dependencies.\n    \n      Returns:\n        A wrapped TF_Operation*.\n      \"\"\"\n      # pylint: disable=protected-access\n      op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),\n                                      compat.as_str(node_def.name))\n      if node_def.device:\n        c_api.TF_SetDevice(op_desc, compat.as_str(node_def.device))\n      # Add inputs\n      for op_input in inputs:\n        if isinstance(op_input, (list, tuple)):\n          c_api.TF_AddInputList(op_desc, [t._as_tf_output() for t in op_input])\n        else:\n          c_api.TF_AddInput(op_desc, op_input._as_tf_output())\n    \n      # Add control inputs\n      for control_input in control_inputs:\n        c_api.TF_AddControlInput(op_desc, control_input._c_op)\n      # pylint: enable=protected-access\n    \n      # Add attrs\n      for name, attr_value in node_def.attr.items():\n        serialized = attr_value.SerializeToString()\n        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.\n        # It might be worth creating a convenient way to re-use the same status.\n        c_api.TF_SetAttrValueProto(op_desc, compat.as_str(name), serialized)\n    \n      try:\n>       c_op = c_api.TF_FinishOperation(op_desc)\nE       tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 3 and 2\nE       \tFrom merging shape 0 with other shapes. for 'lambda_1_out/packed' (op: 'Pack') with input shapes: [?,2,2], [?,2].\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1864: InvalidArgumentError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x7f84323bdf50>\nop_type_name = 'HistogramSummary', name = 'lambda_1_out/', keywords = {}\nop_info = <tensorflow.python.framework.op_def_library._OpInfo object at 0x7f84323474d0>\nop_def = name: \"HistogramSummary\"\ninput_arg {\n  name: \"tag\"\n  type: DT_STRING\n}\ninput_arg {\n  name: \"values\"\n  type_attr: \"T\"\n}...   type: DT_BFLOAT16\n      type: DT_UINT16\n      type: DT_HALF\n      type: DT_UINT32\n      type: DT_UINT64\n    }\n  }\n}\n\ng = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\ndeprecation_version = 0, default_type_attr_map = {'T': tf.float32}\n\n    def _apply_op_helper(self, op_type_name, name=None, **keywords):\n      \"\"\"Implementation of apply_op that returns output_structure, op.\"\"\"\n      op_info = self._ops.get(op_type_name, None)\n      if op_info is None:\n        raise RuntimeError(\"Unrecognized Op name \" + op_type_name)\n      op_def = op_info.op_def\n    \n      # Determine the graph context.\n      try:\n        # Need to flatten all the arguments into a list.\n        # pylint: disable=protected-access\n        g = ops._get_graph_from_inputs(_Flatten(keywords.values()))\n        # pylint: enable=protected-access\n      except AssertionError as e:\n        raise RuntimeError(\n            \"Cannot determine graph for Op '%s' due to: %s\"\n            % (op_type_name, e.message))\n    \n      # Default name if not specified.\n      if name is None:\n        name = op_type_name\n    \n      # Check for deprecation\n      deprecation_version = op_def.deprecation.version\n      if deprecation_version:\n        producer = g.graph_def_versions.producer\n        if producer >= deprecation_version:\n          raise NotImplementedError(\n              (\"Op %s is not available in GraphDef version %d. \"\n               \"It has been removed in version %d. %s.\") %\n              (op_type_name, producer, deprecation_version,\n               op_def.deprecation.explanation))\n    \n      # Fill in the list of default types for all \"type\" attrs.  This\n      # will be used to choose a preferred dtype to convert to in the\n      # absence of input type information.\n      #\n      # TODO(b/31302892): Currently the defaults don't work in the right\n      # way if you have two inputs, one of whose type resolution depends\n      # on the other.  Handling this will require restructuring this code\n      # significantly.\n      default_type_attr_map = {}\n      for attr_def in op_def.attr:\n        if attr_def.type != \"type\":\n          continue\n        key = attr_def.name\n        if attr_def.HasField(\"default_value\"):\n          default_type_attr_map[key] = dtypes.as_dtype(\n              attr_def.default_value.type)\n    \n      # Requires that op_def has passed validation (using the C++\n      # ValidateOpDef() from ../framework/op_def_util.h).\n      attrs = {}\n      inputs = []\n      input_types = []\n      with g.as_default(), ops.name_scope(name) as scope:\n    \n        # Perform input type inference\n        inferred_from = {}\n        for input_arg in op_def.input_arg:\n          input_name = input_arg.name\n          if input_name in keywords:\n            values = keywords.pop(input_name)\n          elif input_name + \"_\" in keywords:\n            # Handle the case where the name is a keyword or built-in\n            # for Python so we use the name + _ instead.\n            input_name += \"_\"\n            values = keywords.pop(input_name)\n          else:\n            raise TypeError(\"No argument for input \" + input_name)\n    \n          # Goals:\n          # * Convert values to Tensors if it contains constants.\n          # * Verify that values is a list if that matches the input_arg's\n          #   type.\n          # * If the input_arg's type is determined by attrs, either set\n          #   those attrs and validate those attr values are legal (if\n          #   they have not yet been set) or validate the input matches\n          #   the type indicated by the attrs (if they have already been\n          #   inferred via an earlier input).\n          # * If the input_arg has an explicit type, make sure the input\n          #   conforms.\n    \n          if _IsListParameter(input_arg):\n            if not _IsListValue(values):\n              raise TypeError(\n                  \"Expected list for '%s' argument to '%s' Op, not %s.\" %\n                  (input_name, op_type_name, values))\n            # In cases where we expect all elements of the list to have the\n            # same dtype, try to cast non-Tensor elements to that type.\n            dtype = None\n            default_dtype = None\n            if input_arg.type != types_pb2.DT_INVALID:\n              dtype = input_arg.type\n            elif input_arg.number_attr:\n              if input_arg.type_attr in attrs:\n                dtype = attrs[input_arg.type_attr]\n              else:\n                for t in values:\n                  if isinstance(t, ops.Tensor):\n                    dtype = t.dtype\n                    break\n    \n              # dtype still not found, prefer using the default dtype\n              # from the attr.\n              if dtype is None and input_arg.type_attr in default_type_attr_map:\n                default_dtype = default_type_attr_map[input_arg.type_attr]\n    \n            try:\n              if not input_arg.is_ref and dtype:\n                dtype = dtypes.as_dtype(dtype).base_dtype\n              values = ops.internal_convert_n_to_tensor(\n                  values,\n                  name=input_arg.name,\n                  dtype=dtype if dtype else None,\n                  preferred_dtype=default_dtype,\n                  as_ref=input_arg.is_ref)\n              if input_arg.number_attr and len(\n                  set(v.dtype.base_dtype for v in values)) > 1:\n                raise TypeError()  # All types should match.\n            except (TypeError, ValueError):\n              # What types does the conversion function think values have?\n              observed_types = []\n              for value in values:\n                try:\n                  converted_value = ops.internal_convert_to_tensor(\n                      value, as_ref=input_arg.is_ref)\n                  observed_types.append(converted_value.dtype.base_dtype.name)\n                except (TypeError, ValueError):\n                  observed_types.append(\"<NOT CONVERTIBLE TO TENSOR>\")\n              observed = \", \".join(observed_types)\n    \n              prefix = (\n                  \"Tensors in list passed to '%s' of '%s' Op have types [%s]\" %\n                  (input_name, op_type_name, observed))\n              if input_arg.number_attr:\n                if input_arg.type != types_pb2.DT_INVALID:\n                  raise TypeError(\"%s that do not match expected type %s.\" %\n                                  (prefix, dtype.name))\n                elif input_arg.type_attr in attrs:\n                  raise TypeError(\"%s that do not match type %s inferred from \"\n                                  \"earlier arguments.\" %\n                                  (prefix, dtype.name))\n                else:\n                  raise TypeError(\"%s that don't all match.\" % prefix)\n              else:\n                raise TypeError(\n                    \"%s that are invalid. Tensors: %s\" % (prefix, values))\n    \n            types = [x.dtype for x in values]\n            inputs.extend(values)\n          else:\n            # In cases where we have an expected type, try to convert non-Tensor\n            # arguments to that type.\n            dtype = None\n            default_dtype = None\n            if input_arg.type != types_pb2.DT_INVALID:\n              dtype = input_arg.type\n            elif input_arg.type_attr in attrs:\n              dtype = attrs[input_arg.type_attr]\n            elif input_arg.type_attr in default_type_attr_map:\n              # The dtype could not be inferred solely from the inputs,\n              # so we prefer the attr's default, so code that adds a new attr\n              # with a default is backwards compatible.\n              default_dtype = default_type_attr_map[input_arg.type_attr]\n    \n            try:\n              values = ops.internal_convert_to_tensor(\n                  values,\n                  name=input_arg.name,\n                  dtype=dtype,\n                  as_ref=input_arg.is_ref,\n                  preferred_dtype=default_dtype)\n            except TypeError as err:\n              if dtype is None:\n                raise err\n              else:\n                raise TypeError(\n                    \"Expected %s passed to parameter '%s' of op '%s', got %s of \"\n                    \"type '%s' instead. Error: %s\" %\n                    (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,\n                     repr(values), type(values).__name__, err))\n            except ValueError:\n              # What type does convert_to_tensor think it has?\n              try:\n                observed = ops.internal_convert_to_tensor(\n>                   values, as_ref=input_arg.is_ref).dtype.name\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:541: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nvalue = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\ndtype = None, name = None, as_ref = False, preferred_dtype = None\nctx = <tensorflow.python.eager.context.Context object at 0x7f842c2bbad0>\naccept_symbolic_tensors = True, accept_composite_tensors = False\n\n    def internal_convert_to_tensor(value,\n                                   dtype=None,\n                                   name=None,\n                                   as_ref=False,\n                                   preferred_dtype=None,\n                                   ctx=None,\n                                   accept_symbolic_tensors=True,\n                                   accept_composite_tensors=False):\n      \"\"\"Implementation of the public convert_to_tensor.\"\"\"\n      if ctx is None:\n        ctx = context.context()\n      if isinstance(value, EagerTensor):\n        if ctx.executing_eagerly():\n          if dtype is not None:\n            dtype = dtypes.as_dtype(dtype)\n            value = _TensorTensorConversionFunction(value, dtype=dtype)\n          return value\n        else:\n          graph = get_default_graph()\n          if not graph.building_function:\n            raise RuntimeError(\"Attempting to capture an EagerTensor without \"\n                               \"building a function.\")\n          return graph.capture(value, name=name)\n      elif ((not accept_symbolic_tensors) and isinstance(value, Tensor) and\n            ctx.executing_eagerly()):\n        # Found a symbolic tensor in an eager context.\n        # This happens when we use the Keras functional API (i.e. calling layers\n        # on the output of `keras.Input()`, which is symbolic) while eager\n        # execution is enabled.\n        if _is_keras_symbolic_tensor(value):\n          # If the graph of the tensor isn't the Keras graph, we should still\n          # fail, for the time being. TODO(fchollet): consider allowing\n          # all symbolic tensors to raise this exception in this case.\n          raise core._SymbolicException(  # pylint: disable=protected-access\n              \"Using the symbolic output of a Keras layer during eager execution.\")\n    \n      if dtype is not None:\n        dtype = dtypes.as_dtype(dtype)\n      unwrapped_type = type(value)\n      conversion_func_list = _tensor_conversion_func_cache.get(unwrapped_type, None)\n      if conversion_func_list is None:\n        with _tensor_conversion_func_lock:\n          conversion_func_list = []\n          for _, funcs_at_priority in sorted(\n              _tensor_conversion_func_registry.items()):\n            for base_type, conversion_func in funcs_at_priority:\n              if isinstance(value, base_type):\n                conversion_func_list.append((base_type, conversion_func))\n          _tensor_conversion_func_cache[unwrapped_type] = conversion_func_list\n    \n      for base_type, conversion_func in conversion_func_list:\n        # If dtype is None but preferred_dtype is not None, we try to\n        # cast to preferred_dtype first.\n        ret = None\n        if dtype is None and preferred_dtype is not None:\n          try:\n            ret = conversion_func(\n                value, dtype=preferred_dtype, name=name, as_ref=as_ref)\n          except (TypeError, ValueError, errors.UnimplementedError,\n                  errors.InvalidArgumentError):\n            # Could not coerce the conversion to use the preferred dtype.\n            ret = None\n    \n          if ret is not None and ret is not NotImplemented:\n            if (ret.dtype.base_dtype !=\n                dtypes.as_dtype(preferred_dtype).base_dtype):\n              raise TypeError(\"convert_to_tensor did not convert to \"\n                              \"the preferred dtype: %s vs %s \" %\n                              (ret.dtype.base_dtype,\n                               dtypes.as_dtype(preferred_dtype).base_dtype))\n    \n        if ret is None:\n>         ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1224: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nv = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\ndtype = tf.float32, name = None, as_ref = False\n\n    def _autopacking_conversion_function(v, dtype=None, name=None, as_ref=False):\n      \"\"\"Tensor conversion function that automatically packs arguments.\"\"\"\n      if as_ref:\n        return NotImplemented\n      inferred_dtype = _get_dtype_from_nested_lists(v)\n      if inferred_dtype is None:\n        # We did not find any tensor-like objects in the nested lists, so defer to\n        # other conversion functions.\n        return NotImplemented\n      if dtype is None:\n        dtype = inferred_dtype\n      elif dtype != inferred_dtype:\n        v = nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)\n>     return _autopacking_helper(v, dtype, name or \"packed\")\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1145: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nlist_or_tuple = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\ndtype = tf.float32, name = 'packed'\n\n    def _autopacking_helper(list_or_tuple, dtype, name):\n      \"\"\"Converts the given list or tuple to a tensor by packing.\n    \n      Args:\n        list_or_tuple: A (possibly nested) list or tuple containing a tensor.\n        dtype: The element type of the returned tensor.\n        name: A name for the returned tensor.\n    \n      Returns:\n        A `tf.Tensor` with value equivalent to `list_or_tuple`.\n      \"\"\"\n      if context.executing_eagerly():\n        # NOTE: Fast path when all the items are tensors, this doesn't do any type\n        # checking.\n        if all(ops.is_dense_tensor_like(elem) for elem in list_or_tuple):\n          return gen_array_ops.pack(list_or_tuple, name=name)\n      must_pack = False\n      converted_elems = []\n      with ops.name_scope(name) as scope:\n        for i, elem in enumerate(list_or_tuple):\n          if ops.is_dense_tensor_like(elem):\n            if dtype is not None and elem.dtype.base_dtype != dtype:\n              raise TypeError(\"Cannot convert a list containing a tensor of dtype \"\n                              \"%s to %s (Tensor is: %r)\" %\n                              (elem.dtype, dtype, elem))\n            converted_elems.append(elem)\n            must_pack = True\n          elif isinstance(elem, (list, tuple)):\n            converted_elem = _autopacking_helper(elem, dtype, str(i))\n            if ops.is_dense_tensor_like(converted_elem):\n              must_pack = True\n            converted_elems.append(converted_elem)\n          else:\n            converted_elems.append(elem)\n        if must_pack:\n          elems_as_tensors = []\n          for i, elem in enumerate(converted_elems):\n            if ops.is_dense_tensor_like(elem):\n              elems_as_tensors.append(elem)\n            else:\n              # NOTE(mrry): This is inefficient, but it enables us to\n              # handle the case where the list arguments are other\n              # convertible-to-tensor types, such as numpy arrays.\n              elems_as_tensors.append(\n                  constant_op.constant(elem, dtype=dtype, name=str(i)))\n>         return gen_array_ops.pack(elems_as_tensors, name=scope)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1095: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nvalues = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\naxis = 0, name = 'lambda_1_out/packed/'\n\n    def pack(values, axis=0, name=None):\n      r\"\"\"Packs a list of `N` rank-`R` tensors into one rank-`(R+1)` tensor.\n    \n      Packs the `N` tensors in `values` into a tensor with rank one higher than each\n      tensor in `values`, by packing them along the `axis` dimension.\n      Given a list of tensors of shape `(A, B, C)`;\n    \n      if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\n      if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\n      Etc.\n    \n      For example:\n    \n      ```\n      # 'x' is [1, 4]\n      # 'y' is [2, 5]\n      # 'z' is [3, 6]\n      pack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.\n      pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]\n      ```\n    \n      This is the opposite of `unpack`.\n    \n      Args:\n        values: A list of at least 1 `Tensor` objects with the same type.\n          Must be of same shape and type.\n        axis: An optional `int`. Defaults to `0`.\n          Dimension along which to pack.  Negative values wrap around, so the\n          valid range is `[-(R+1), R+1)`.\n        name: A name for the operation (optional).\n    \n      Returns:\n        A `Tensor`. Has the same type as `values`.\n      \"\"\"\n      _ctx = _context._context or _context.context()\n      if _ctx is not None and _ctx._thread_local_data.is_eager:\n        try:\n          _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n            _ctx._context_handle, _ctx._thread_local_data.device_name, \"Pack\",\n            name, _ctx._post_execution_callbacks, values, \"axis\", axis)\n          return _result\n        except _core._FallbackException:\n          try:\n            return pack_eager_fallback(\n                values, axis=axis, name=name, ctx=_ctx)\n          except _core._SymbolicException:\n            pass  # Add nodes to the TensorFlow graph.\n        except _core._NotOkStatusException as e:\n          if name is not None:\n            message = e.message + \" name: \" + name\n          else:\n            message = e.message\n          _six.raise_from(_core._status_to_exception(e.code, message), None)\n      # Add nodes to the TensorFlow graph.\n      if not isinstance(values, (list, tuple)):\n        raise TypeError(\n            \"Expected list for 'values' argument to \"\n            \"'pack' Op, not %r.\" % values)\n      _attr_N = len(values)\n      if axis is None:\n        axis = 0\n      axis = _execute.make_int(axis, \"axis\")\n      _, _, _op = _op_def_lib._apply_op_helper(\n>           \"Pack\", values=values, axis=axis, name=name)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:5897: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x7f8432ec7150>\nop_type_name = 'Pack', name = 'lambda_1_out/packed/', keywords = {}\nop_info = <tensorflow.python.framework.op_def_library._OpInfo object at 0x7f8432ecefd0>\nop_def = name: \"Pack\"\ninput_arg {\n  name: \"values\"\n  type_attr: \"T\"\n  number_attr: \"N\"\n}\noutput_arg {\n  name: \"output\"\n  type_a... minimum: 1\n}\nattr {\n  name: \"T\"\n  type: \"type\"\n}\nattr {\n  name: \"axis\"\n  type: \"int\"\n  default_value {\n    i: 0\n  }\n}\n\ng = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\ndeprecation_version = 0, default_type_attr_map = {}\n\n    def _apply_op_helper(self, op_type_name, name=None, **keywords):\n      \"\"\"Implementation of apply_op that returns output_structure, op.\"\"\"\n      op_info = self._ops.get(op_type_name, None)\n      if op_info is None:\n        raise RuntimeError(\"Unrecognized Op name \" + op_type_name)\n      op_def = op_info.op_def\n    \n      # Determine the graph context.\n      try:\n        # Need to flatten all the arguments into a list.\n        # pylint: disable=protected-access\n        g = ops._get_graph_from_inputs(_Flatten(keywords.values()))\n        # pylint: enable=protected-access\n      except AssertionError as e:\n        raise RuntimeError(\n            \"Cannot determine graph for Op '%s' due to: %s\"\n            % (op_type_name, e.message))\n    \n      # Default name if not specified.\n      if name is None:\n        name = op_type_name\n    \n      # Check for deprecation\n      deprecation_version = op_def.deprecation.version\n      if deprecation_version:\n        producer = g.graph_def_versions.producer\n        if producer >= deprecation_version:\n          raise NotImplementedError(\n              (\"Op %s is not available in GraphDef version %d. \"\n               \"It has been removed in version %d. %s.\") %\n              (op_type_name, producer, deprecation_version,\n               op_def.deprecation.explanation))\n    \n      # Fill in the list of default types for all \"type\" attrs.  This\n      # will be used to choose a preferred dtype to convert to in the\n      # absence of input type information.\n      #\n      # TODO(b/31302892): Currently the defaults don't work in the right\n      # way if you have two inputs, one of whose type resolution depends\n      # on the other.  Handling this will require restructuring this code\n      # significantly.\n      default_type_attr_map = {}\n      for attr_def in op_def.attr:\n        if attr_def.type != \"type\":\n          continue\n        key = attr_def.name\n        if attr_def.HasField(\"default_value\"):\n          default_type_attr_map[key] = dtypes.as_dtype(\n              attr_def.default_value.type)\n    \n      # Requires that op_def has passed validation (using the C++\n      # ValidateOpDef() from ../framework/op_def_util.h).\n      attrs = {}\n      inputs = []\n      input_types = []\n      with g.as_default(), ops.name_scope(name) as scope:\n    \n        # Perform input type inference\n        inferred_from = {}\n        for input_arg in op_def.input_arg:\n          input_name = input_arg.name\n          if input_name in keywords:\n            values = keywords.pop(input_name)\n          elif input_name + \"_\" in keywords:\n            # Handle the case where the name is a keyword or built-in\n            # for Python so we use the name + _ instead.\n            input_name += \"_\"\n            values = keywords.pop(input_name)\n          else:\n            raise TypeError(\"No argument for input \" + input_name)\n    \n          # Goals:\n          # * Convert values to Tensors if it contains constants.\n          # * Verify that values is a list if that matches the input_arg's\n          #   type.\n          # * If the input_arg's type is determined by attrs, either set\n          #   those attrs and validate those attr values are legal (if\n          #   they have not yet been set) or validate the input matches\n          #   the type indicated by the attrs (if they have already been\n          #   inferred via an earlier input).\n          # * If the input_arg has an explicit type, make sure the input\n          #   conforms.\n    \n          if _IsListParameter(input_arg):\n            if not _IsListValue(values):\n              raise TypeError(\n                  \"Expected list for '%s' argument to '%s' Op, not %s.\" %\n                  (input_name, op_type_name, values))\n            # In cases where we expect all elements of the list to have the\n            # same dtype, try to cast non-Tensor elements to that type.\n            dtype = None\n            default_dtype = None\n            if input_arg.type != types_pb2.DT_INVALID:\n              dtype = input_arg.type\n            elif input_arg.number_attr:\n              if input_arg.type_attr in attrs:\n                dtype = attrs[input_arg.type_attr]\n              else:\n                for t in values:\n                  if isinstance(t, ops.Tensor):\n                    dtype = t.dtype\n                    break\n    \n              # dtype still not found, prefer using the default dtype\n              # from the attr.\n              if dtype is None and input_arg.type_attr in default_type_attr_map:\n                default_dtype = default_type_attr_map[input_arg.type_attr]\n    \n            try:\n              if not input_arg.is_ref and dtype:\n                dtype = dtypes.as_dtype(dtype).base_dtype\n              values = ops.internal_convert_n_to_tensor(\n                  values,\n                  name=input_arg.name,\n                  dtype=dtype if dtype else None,\n                  preferred_dtype=default_dtype,\n                  as_ref=input_arg.is_ref)\n              if input_arg.number_attr and len(\n                  set(v.dtype.base_dtype for v in values)) > 1:\n                raise TypeError()  # All types should match.\n            except (TypeError, ValueError):\n              # What types does the conversion function think values have?\n              observed_types = []\n              for value in values:\n                try:\n                  converted_value = ops.internal_convert_to_tensor(\n                      value, as_ref=input_arg.is_ref)\n                  observed_types.append(converted_value.dtype.base_dtype.name)\n                except (TypeError, ValueError):\n                  observed_types.append(\"<NOT CONVERTIBLE TO TENSOR>\")\n              observed = \", \".join(observed_types)\n    \n              prefix = (\n                  \"Tensors in list passed to '%s' of '%s' Op have types [%s]\" %\n                  (input_name, op_type_name, observed))\n              if input_arg.number_attr:\n                if input_arg.type != types_pb2.DT_INVALID:\n                  raise TypeError(\"%s that do not match expected type %s.\" %\n                                  (prefix, dtype.name))\n                elif input_arg.type_attr in attrs:\n                  raise TypeError(\"%s that do not match type %s inferred from \"\n                                  \"earlier arguments.\" %\n                                  (prefix, dtype.name))\n                else:\n                  raise TypeError(\"%s that don't all match.\" % prefix)\n              else:\n                raise TypeError(\n                    \"%s that are invalid. Tensors: %s\" % (prefix, values))\n    \n            types = [x.dtype for x in values]\n            inputs.extend(values)\n          else:\n            # In cases where we have an expected type, try to convert non-Tensor\n            # arguments to that type.\n            dtype = None\n            default_dtype = None\n            if input_arg.type != types_pb2.DT_INVALID:\n              dtype = input_arg.type\n            elif input_arg.type_attr in attrs:\n              dtype = attrs[input_arg.type_attr]\n            elif input_arg.type_attr in default_type_attr_map:\n              # The dtype could not be inferred solely from the inputs,\n              # so we prefer the attr's default, so code that adds a new attr\n              # with a default is backwards compatible.\n              default_dtype = default_type_attr_map[input_arg.type_attr]\n    \n            try:\n              values = ops.internal_convert_to_tensor(\n                  values,\n                  name=input_arg.name,\n                  dtype=dtype,\n                  as_ref=input_arg.is_ref,\n                  preferred_dtype=default_dtype)\n            except TypeError as err:\n              if dtype is None:\n                raise err\n              else:\n                raise TypeError(\n                    \"Expected %s passed to parameter '%s' of op '%s', got %s of \"\n                    \"type '%s' instead. Error: %s\" %\n                    (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,\n                     repr(values), type(values).__name__, err))\n            except ValueError:\n              # What type does convert_to_tensor think it has?\n              try:\n                observed = ops.internal_convert_to_tensor(\n                    values, as_ref=input_arg.is_ref).dtype.name\n              except ValueError as err:\n                raise ValueError(\n                    \"Tried to convert '%s' to a tensor and failed. Error: %s\" %\n                    (input_name, err))\n              prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n                        (input_name, op_type_name, observed))\n              if input_arg.type != types_pb2.DT_INVALID:\n                raise TypeError(\"%s expected type of %s.\" %\n                                (prefix, dtypes.as_dtype(input_arg.type).name))\n              else:\n                # Update the maps with the default, if needed.\n                k = input_arg.type_attr\n                if k in default_type_attr_map:\n                  if k not in attrs:\n                    attrs[k] = default_type_attr_map[k]\n                    if k not in inferred_from:\n                      inferred_from[k] = \"Default in OpDef\"\n    \n                raise TypeError(\n                    \"%s type %s of argument '%s'.\" %\n                    (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n                     inferred_from[input_arg.type_attr]))\n    \n            types = [values.dtype]\n            inputs.append(values)\n          base_types = [x.base_dtype for x in types]\n    \n          if input_arg.number_attr:\n            # <number-attr> * <type> or <number-attr> * <type-attr>\n            if input_arg.number_attr in attrs:\n              if len(values) != attrs[input_arg.number_attr]:\n                raise ValueError(\n                    \"List argument '%s' to '%s' Op with length %d must match \"\n                    \"length %d of argument '%s'.\" %\n                    (input_name, op_type_name, len(values),\n                     attrs[input_arg.number_attr],\n                     inferred_from[input_arg.number_attr]))\n            else:\n              attrs[input_arg.number_attr] = len(values)\n              inferred_from[input_arg.number_attr] = input_name\n              num_attr = _Attr(op_def, input_arg.number_attr)\n              if num_attr.has_minimum and len(values) < num_attr.minimum:\n                raise ValueError(\n                    \"List argument '%s' to '%s' Op with length %d shorter \"\n                    \"than minimum length %d.\" %\n                    (input_name, op_type_name, len(values), num_attr.minimum))\n            # All tensors must have the same base type.\n            if any(bt != base_types[0] for bt in base_types):\n              raise TypeError(\n                  \"All tensors passed to '%s' of '%s' Op \"\n                  \"must have the same type.\" %\n                  (input_name, op_type_name))\n            if input_arg.type != types_pb2.DT_INVALID:\n              # <number-attr> * <type> case\n              if base_types and base_types[0] != input_arg.type:\n                assert False, \"Unreachable\"\n            elif input_arg.type_attr in attrs:\n              # <number-attr> * <type-attr> case, where <type-attr> already\n              # has an inferred value.\n              if base_types and base_types[0] != attrs[input_arg.type_attr]:\n                assert False, \"Unreachable\"\n            else:\n              # <number-attr> * <type-attr> case, where we are now setting\n              # the <type-attr> based on this input\n              if not base_types:\n                raise TypeError(\n                    \"Don't know how to infer type variable from empty input \"\n                    \"list passed to input '%s' of '%s' Op.\" %\n                    (input_name, op_type_name))\n              attrs[input_arg.type_attr] = base_types[0]\n              inferred_from[input_arg.type_attr] = input_name\n              type_attr = _Attr(op_def, input_arg.type_attr)\n              _SatisfiesTypeConstraint(base_types[0], type_attr,\n                                       param_name=input_name)\n          elif input_arg.type_attr:\n            # <type-attr>\n            attr_value = base_types[0]\n            if input_arg.type_attr in attrs:\n              if attrs[input_arg.type_attr] != attr_value:\n                assert False, \"Unreachable\"\n            else:\n              for base_type in base_types:\n                _SatisfiesTypeConstraint(base_type,\n                                         _Attr(op_def, input_arg.type_attr),\n                                         param_name=input_name)\n              attrs[input_arg.type_attr] = attr_value\n              inferred_from[input_arg.type_attr] = input_name\n          elif input_arg.type_list_attr:\n            # <type-list-attr>\n            attr_value = base_types\n            if input_arg.type_list_attr in attrs:\n              if attrs[input_arg.type_list_attr] != attr_value:\n                raise TypeError(\n                    \"Input '%s' of '%s' Op has type list of %s that does not \"\n                    \"match type list %s of argument '%s'.\" %\n                    (input_name, op_type_name,\n                     \", \".join(dtypes.as_dtype(x).name for x in attr_value),\n                     \", \".join(dtypes.as_dtype(x).name\n                               for x in attrs[input_arg.type_list_attr]),\n                     inferred_from[input_arg.type_list_attr]))\n            else:\n              for base_type in base_types:\n                _SatisfiesTypeConstraint(base_type,\n                                         _Attr(op_def, input_arg.type_list_attr),\n                                         param_name=input_name)\n              attrs[input_arg.type_list_attr] = attr_value\n              inferred_from[input_arg.type_list_attr] = input_name\n          else:\n            # single Tensor with specified type\n            if base_types[0] != input_arg.type:\n              assert False, \"Unreachable\"\n    \n          if input_arg.is_ref:\n            if not all(x._is_ref_dtype for x in types):  # pylint: disable=protected-access\n              raise TypeError(\n                  (\"'%s' Op requires that input '%s' be a mutable tensor \"\n                   \"(e.g.: a tf.Variable)\") % (op_type_name, input_name))\n            input_types.extend(types)\n          else:\n            input_types.extend(base_types)\n    \n        # Process remaining attrs\n        for attr in op_def.attr:\n          # Skip attrs that have already had their values inferred\n          if attr.name in attrs:\n            if attr.name in keywords:\n              raise TypeError(\n                  \"Should not specify value for inferred attr '%s'.\" % attr.name)\n            continue\n          if attr.name in keywords:\n            attrs[attr.name] = keywords.pop(attr.name)\n          elif attr.name + \"_\" in keywords:\n            # Attrs whose names match Python keywords have an extra '_'\n            # appended, so we must check for that as well.\n            attrs[attr.name] = keywords.pop(attr.name + \"_\")\n          else:\n            raise TypeError(\"No argument for attr \" + attr.name)\n    \n        # Convert attr values to AttrValue protos.\n        attr_protos = {}\n        for attr_def in op_def.attr:\n          key = attr_def.name\n          value = attrs[key]\n          attr_value = attr_value_pb2.AttrValue()\n          if attr_def.HasField(\"default_value\") and value is None:\n            attr_value.CopyFrom(attr_def.default_value)\n            attr_protos[key] = attr_value\n            continue\n          if attr_def.type.startswith(\"list(\"):\n            if not _IsListValue(value):\n              raise TypeError(\"Expected list for attr \" + key)\n            if attr_def.has_minimum:\n              if len(value) < attr_def.minimum:\n                raise ValueError(\"Attr '%s' of '%s' Op passed list of length %d \"\n                                 \"less than minimum %d.\" %\n                                 (key, op_type_name, len(value),\n                                  attr_def.minimum))\n            attr_value.list.SetInParent()\n          if attr_def.type == \"string\":\n            attr_value.s = _MakeStr(value, key)\n            if attr_def.HasField(\"allowed_values\"):\n              if attr_value.s not in attr_def.allowed_values.list.s:\n                raise ValueError(\n                    \"Attr '%s' of '%s' Op passed string '%s' not in: \\\"%s\\\".\" %\n                    (key, op_type_name, compat.as_text(attr_value.s),\n                     '\", \"'.join(map(compat.as_text,\n                                     attr_def.allowed_values.list.s))))\n          elif attr_def.type == \"list(string)\":\n            attr_value.list.s.extend([_MakeStr(x, key) for x in value])\n            if attr_def.HasField(\"allowed_values\"):\n              for x in attr_value.list.s:\n                if x not in attr_def.allowed_values.list.s:\n                  raise ValueError(\n                      \"Attr '%s' of '%s' Op passed string '%s' not in: \\\"%s\\\".\" %\n                      (key, op_type_name, compat.as_text(x),\n                       '\", \"'.join(map(compat.as_text,\n                                       attr_def.allowed_values.list.s))))\n          elif attr_def.type == \"int\":\n            attr_value.i = _MakeInt(value, key)\n            if attr_def.has_minimum:\n              if attr_value.i < attr_def.minimum:\n                raise ValueError(\n                    \"Attr '%s' of '%s' Op passed %d less than minimum %d.\" %\n                    (key, op_type_name, attr_value.i, attr_def.minimum))\n          elif attr_def.type == \"list(int)\":\n            attr_value.list.i.extend([_MakeInt(x, key) for x in value])\n          elif attr_def.type == \"float\":\n            attr_value.f = _MakeFloat(value, key)\n          elif attr_def.type == \"list(float)\":\n            attr_value.list.f.extend([_MakeFloat(x, key) for x in value])\n          elif attr_def.type == \"bool\":\n            attr_value.b = _MakeBool(value, key)\n          elif attr_def.type == \"list(bool)\":\n            attr_value.list.b.extend([_MakeBool(x, key) for x in value])\n          elif attr_def.type == \"type\":\n            attr_value.type = _MakeType(value, attr_def)\n          elif attr_def.type == \"list(type)\":\n            attr_value.list.type.extend(\n                [_MakeType(x, attr_def) for x in value])\n          elif attr_def.type == \"shape\":\n            attr_value.shape.CopyFrom(_MakeShape(value, key))\n          elif attr_def.type == \"list(shape)\":\n            attr_value.list.shape.extend(\n                [_MakeShape(x, key) for x in value])\n          elif attr_def.type == \"tensor\":\n            attr_value.tensor.CopyFrom(_MakeTensor(value, key))\n          elif attr_def.type == \"list(tensor)\":\n            attr_value.list.tensor.extend(\n                [_MakeTensor(x, key) for x in value])\n          elif attr_def.type == \"func\":\n            attr_value.func.CopyFrom(_MakeFunc(value, key))\n          elif attr_def.type == \"list(func)\":\n            attr_value.list.func.extend([_MakeFunc(x, key) for x in value])\n          else:\n            raise TypeError(\"Unrecognized Attr type \" + attr_def.type)\n    \n          attr_protos[key] = attr_value\n        del attrs  # attrs is no longer authoritative, use attr_protos instead\n    \n        # Determine output types (possibly using attrs)\n        output_structure = []\n        for arg in op_def.output_arg:\n          if arg.number_attr:\n            n = _AttrValue(attr_protos, arg.number_attr).i\n            output_structure.append(n)\n          elif arg.type_attr:\n            t = _AttrValue(attr_protos, arg.type_attr)\n            output_structure.append(None)\n          elif arg.type_list_attr:\n            t = _AttrValue(attr_protos, arg.type_list_attr)\n            output_structure.append(len(t.list.type))\n          else:\n            output_structure.append(None)\n    \n        if keywords:\n          raise TypeError(\"apply_op() got unexpected keyword arguments: \" +\n                          \", \".join(sorted(keywords.keys())))\n    \n        # NOTE(mrry): We add an explicit colocation constraint between\n        # the newly created op and any of its reference-typed inputs.\n        must_colocate_inputs = [val for arg, val in zip(op_def.input_arg, inputs)\n                                if arg.is_ref]\n        with _MaybeColocateWith(must_colocate_inputs):\n          # Add Op to graph\n          op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n                           input_types=input_types, attrs=attr_protos,\n>                          op_def=op_def)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:788: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nargs = (<tensorflow.python.framework.ops.Graph object at 0x7f842c246310>, 'Pack', [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>])\nkwargs = {'attrs': {'N': i: 2\n, 'T': type: DT_FLOAT\n, 'axis': i: 0\n}, 'dtypes': None, 'input_types': [tf.float32, tf.float32], 'name': 'lambda_1_out/packed/', ...}\ninvalid_args = []\nnamed_args = {'attrs': {'N': i: 2\n, 'T': type: DT_FLOAT\n, 'axis': i: 0\n}, 'compute_device': True, 'compute_shapes': True, 'dtypes': None, ...}\narg_name = 'compute_shapes'\nspec = DeprecatedArgSpec(position=8, has_ok_value=False, ok_value=None)\n\n    @functools.wraps(func)\n    def new_func(*args, **kwargs):\n      \"\"\"Deprecation wrapper.\"\"\"\n      # TODO(apassos) figure out a way to have reasonable performance with\n      # deprecation warnings and eager mode.\n      if is_in_graph_mode.IS_IN_GRAPH_MODE() and _PRINT_DEPRECATION_WARNINGS:\n        invalid_args = []\n        named_args = tf_inspect.getcallargs(func, *args, **kwargs)\n        for arg_name, spec in iter(deprecated_positions.items()):\n          if (spec.position < len(args) and\n              not (spec.has_ok_value and\n                   _same_value(named_args[arg_name], spec.ok_value))):\n            invalid_args.append(arg_name)\n        if is_varargs_deprecated and len(args) > len(arg_spec.args):\n          invalid_args.append(arg_spec.varargs)\n        if is_kwargs_deprecated and kwargs:\n          invalid_args.append(arg_spec.varkw)\n        for arg_name in deprecated_arg_names:\n          if (arg_name in kwargs and\n              not (deprecated_positions[arg_name].has_ok_value and\n                   _same_value(named_args[arg_name],\n                               deprecated_positions[arg_name].ok_value))):\n            invalid_args.append(arg_name)\n        for arg_name in invalid_args:\n          if (func, arg_name) not in _PRINTED_WARNING:\n            if warn_once:\n              _PRINTED_WARNING[(func, arg_name)] = True\n            logging.warning(\n                'From %s: calling %s (from %s) with %s is deprecated and will '\n                'be removed %s.\\nInstructions for updating:\\n%s',\n                _call_location(), decorator_utils.get_qualified_name(func),\n                func.__module__, arg_name,\n                'in a future version' if date is None else ('after %s' % date),\n                instructions)\n>     return func(*args, **kwargs)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:507: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\nop_type = 'Pack'\ninputs = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\ndtypes = None, input_types = [tf.float32, tf.float32]\nname = 'lambda_1_out/packed'\nattrs = {'N': i: 2\n, 'T': type: DT_FLOAT\n, 'axis': i: 0\n}\nop_def = name: \"Pack\"\ninput_arg {\n  name: \"values\"\n  type_attr: \"T\"\n  number_attr: \"N\"\n}\noutput_arg {\n  name: \"output\"\n  type_a... minimum: 1\n}\nattr {\n  name: \"T\"\n  type: \"type\"\n}\nattr {\n  name: \"axis\"\n  type: \"int\"\n  default_value {\n    i: 0\n  }\n}\n\ncompute_device = True\n\n    @deprecated_args(None,\n                     \"Shapes are always computed; don't use the compute_shapes \"\n                     \"as it has no effect.\", \"compute_shapes\")\n    def create_op(\n        self,\n        op_type,\n        inputs,\n        dtypes=None,  # pylint: disable=redefined-outer-name\n        input_types=None,\n        name=None,\n        attrs=None,\n        op_def=None,\n        compute_shapes=True,\n        compute_device=True):\n      \"\"\"Creates an `Operation` in this graph.\n    \n      This is a low-level interface for creating an `Operation`. Most\n      programs will not call this method directly, and instead use the\n      Python op constructors, such as `tf.constant()`, which add ops to\n      the default graph.\n    \n      Args:\n        op_type: The `Operation` type to create. This corresponds to the\n          `OpDef.name` field for the proto that defines the operation.\n        inputs: A list of `Tensor` objects that will be inputs to the `Operation`.\n        dtypes: (Optional) A list of `DType` objects that will be the types of the\n          tensors that the operation produces.\n        input_types: (Optional.) A list of `DType`s that will be the types of the\n          tensors that the operation consumes. By default, uses the base `DType`\n          of each input in `inputs`. Operations that expect reference-typed inputs\n          must specify `input_types` explicitly.\n        name: (Optional.) A string name for the operation. If not specified, a\n          name is generated based on `op_type`.\n        attrs: (Optional.) A dictionary where the key is the attribute name (a\n          string) and the value is the respective `attr` attribute of the\n          `NodeDef` proto that will represent the operation (an `AttrValue`\n          proto).\n        op_def: (Optional.) The `OpDef` proto that describes the `op_type` that\n          the operation will have.\n        compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always\n          computed).\n        compute_device: (Optional.) If True, device functions will be executed to\n          compute the device property of the Operation.\n    \n      Raises:\n        TypeError: if any of the inputs is not a `Tensor`.\n        ValueError: if colocation conflicts with existing device assignment.\n    \n      Returns:\n        An `Operation` object.\n      \"\"\"\n      del compute_shapes\n    \n      self._check_not_finalized()\n      for idx, a in enumerate(inputs):\n        if not isinstance(a, Tensor):\n          raise TypeError(\"Input #%d is not a tensor: %s\" % (idx, a))\n      if name is None:\n        name = op_type\n      # If a names ends with a '/' it is a \"name scope\" and we use it as-is,\n      # after removing the trailing '/'.\n      if name and name[-1] == \"/\":\n        name = name_from_scope_name(name)\n      else:\n        name = self.unique_name(name)\n    \n      node_def = _NodeDef(op_type, name, device=None, attrs=attrs)\n    \n      input_ops = set([t.op for t in inputs])\n      control_inputs = self._control_dependencies_for_inputs(input_ops)\n      # _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\n      # Session.run call cannot occur between creating and mutating the op.\n      with self._mutation_lock():\n        ret = Operation(\n            node_def,\n            self,\n            inputs=inputs,\n            output_types=dtypes,\n            control_inputs=control_inputs,\n            input_types=input_types,\n            original_op=self._default_original_op,\n>           op_def=op_def)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3616: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <[AttributeError(\"'Operation' object has no attribute '_c_op'\") raised in repr()] Operation object at 0x7f841b853f50>\nnode_def = name: \"lambda_1_out/packed\"\nop: \"Pack\"\nattr {\n  key: \"N\"\n  value {\n    i: 2\n  }\n}\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"axis\"\n  value {\n    i: 0\n  }\n}\n\ng = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\ninputs = [<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]\noutput_types = None, control_inputs = [], input_types = [tf.float32, tf.float32]\noriginal_op = None\nop_def = name: \"Pack\"\ninput_arg {\n  name: \"values\"\n  type_attr: \"T\"\n  number_attr: \"N\"\n}\noutput_arg {\n  name: \"output\"\n  type_a... minimum: 1\n}\nattr {\n  name: \"T\"\n  type: \"type\"\n}\nattr {\n  name: \"axis\"\n  type: \"int\"\n  default_value {\n    i: 0\n  }\n}\n\n\n    def __init__(self,\n                 node_def,\n                 g,\n                 inputs=None,\n                 output_types=None,\n                 control_inputs=None,\n                 input_types=None,\n                 original_op=None,\n                 op_def=None):\n      r\"\"\"Creates an `Operation`.\n    \n      NOTE: This constructor validates the name of the `Operation` (passed\n      as `node_def.name`). Valid `Operation` names match the following\n      regular expression:\n    \n          [A-Za-z0-9.][A-Za-z0-9_.\\\\-/]*\n    \n      Args:\n        node_def: `node_def_pb2.NodeDef`.  `NodeDef` for the `Operation`. Used for\n          attributes of `node_def_pb2.NodeDef`, typically `name`, `op`, and\n          `device`.  The `input` attribute is irrelevant here as it will be\n          computed when generating the model.\n        g: `Graph`. The parent graph.\n        inputs: list of `Tensor` objects. The inputs to this `Operation`.\n        output_types: list of `DType` objects.  List of the types of the `Tensors`\n          computed by this operation.  The length of this list indicates the\n          number of output endpoints of the `Operation`.\n        control_inputs: list of operations or tensors from which to have a control\n          dependency.\n        input_types: List of `DType` objects representing the types of the tensors\n          accepted by the `Operation`.  By default uses `[x.dtype.base_dtype for x\n          in inputs]`.  Operations that expect reference-typed inputs must specify\n          these explicitly.\n        original_op: Optional. Used to associate the new `Operation` with an\n          existing `Operation` (for example, a replica with the op that was\n          replicated).\n        op_def: Optional. The `op_def_pb2.OpDef` proto that describes the op type\n          that this `Operation` represents.\n    \n      Raises:\n        TypeError: if control inputs are not Operations or Tensors,\n          or if `node_def` is not a `NodeDef`,\n          or if `g` is not a `Graph`,\n          or if `inputs` are not tensors,\n          or if `inputs` and `input_types` are incompatible.\n        ValueError: if the `node_def` name is not valid.\n      \"\"\"\n      # For internal use only: `node_def` can be set to a TF_Operation to create\n      # an Operation for that op. This is useful for creating Operations for ops\n      # indirectly created by C API methods, e.g. the ops created by\n      # TF_ImportGraphDef. When `node_def` is a TF_Operation, all optional fields\n      # should be None.\n    \n      if isinstance(node_def, node_def_pb2.NodeDef):\n        if node_def.ByteSize() >= (1 << 31) or node_def.ByteSize() < 0:\n          raise ValueError(\n              \"Cannot create a tensor proto whose content is larger than 2GB.\")\n        if not _VALID_OP_NAME_REGEX.match(node_def.name):\n          raise ValueError(\"'%s' is not a valid node name\" % node_def.name)\n        c_op = None\n      elif type(node_def).__name__ == \"SwigPyObject\":\n        assert inputs is None\n        assert output_types is None\n        assert control_inputs is None\n        assert input_types is None\n        assert original_op is None\n        assert op_def is None\n        c_op = node_def\n      else:\n        raise TypeError(\"node_def needs to be a NodeDef: %s\" % node_def)\n    \n      if not isinstance(g, Graph):\n        raise TypeError(\"g needs to be a Graph: %s\" % g)\n      self._graph = g\n    \n      if inputs is None:\n        inputs = []\n      elif not isinstance(inputs, list):\n        raise TypeError(\"inputs needs to be a list of Tensors: %s\" % inputs)\n      for a in inputs:\n        if not isinstance(a, Tensor):\n          raise TypeError(\"input needs to be a Tensor: %s\" % a)\n      if input_types is None:\n        input_types = [i.dtype.base_dtype for i in inputs]\n      else:\n        if not all(\n            x.is_compatible_with(i.dtype) for i, x in zip(inputs, input_types)):\n          raise TypeError(\"In op '%s', input types (%s) are not compatible \"\n                          \"with expected types (%s)\" %\n                          (node_def.name, [i.dtype for i in inputs], input_types))\n    \n      # Build the list of control inputs.\n      control_input_ops = []\n      if control_inputs:\n        for c in control_inputs:\n          control_op = None\n          if isinstance(c, Operation):\n            control_op = c\n          elif isinstance(c, (Tensor, IndexedSlices)):\n            control_op = c.op\n          else:\n            raise TypeError(\"Control input must be an Operation, \"\n                            \"a Tensor, or IndexedSlices: %s\" % c)\n          control_input_ops.append(control_op)\n    \n      # This will be set by self.inputs.\n      self._inputs_val = None\n    \n      # pylint: disable=protected-access\n      self._id_value = self._graph._next_id()\n      self._original_op = original_op\n      self._traceback = tf_stack.extract_stack()\n    \n      # List of _UserDevSpecs holding code location of device context manager\n      # invocations and the users original argument to them.\n      self._device_code_locations = None\n      # Dict mapping op name to file and line information for op colocation\n      # context managers.\n      self._colocation_code_locations = None\n      self._control_flow_context = self.graph._get_control_flow_context()\n      # pylint: enable=protected-access\n    \n      # Initialize self._c_op.\n      if c_op:\n        self._c_op = c_op\n      else:\n        if op_def is None:\n          op_def = self._graph._get_op_def(node_def.op)\n        # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.\n        # Refactor so we don't have to do this here.\n        grouped_inputs = self._reconstruct_sequence_inputs(\n            op_def, inputs, node_def.attr)\n        self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n>                                 control_input_ops)\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2027: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ngraph = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\nnode_def = name: \"lambda_1_out/packed\"\nop: \"Pack\"\nattr {\n  key: \"N\"\n  value {\n    i: 2\n  }\n}\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"axis\"\n  value {\n    i: 0\n  }\n}\n\ninputs = [[<tf.Tensor 'lambda_1/Identity:0' shape=(?, 2, 2) dtype=float32>, <tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>]]\ncontrol_inputs = []\n\n    def _create_c_op(graph, node_def, inputs, control_inputs):\n      \"\"\"Creates a TF_Operation.\n    \n      Args:\n        graph: a `Graph`.\n        node_def: `node_def_pb2.NodeDef` for the operation to create.\n        inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of\n          `Tensor`s (corresponding to sequence inputs, e.g. \"int64 * N\",\n          \"list(int64)\"). The length of the list should be equal to the number of\n          inputs specified by this operation's op def.\n        control_inputs: A list of `Operation`s to set as control dependencies.\n    \n      Returns:\n        A wrapped TF_Operation*.\n      \"\"\"\n      # pylint: disable=protected-access\n      op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),\n                                      compat.as_str(node_def.name))\n      if node_def.device:\n        c_api.TF_SetDevice(op_desc, compat.as_str(node_def.device))\n      # Add inputs\n      for op_input in inputs:\n        if isinstance(op_input, (list, tuple)):\n          c_api.TF_AddInputList(op_desc, [t._as_tf_output() for t in op_input])\n        else:\n          c_api.TF_AddInput(op_desc, op_input._as_tf_output())\n    \n      # Add control inputs\n      for control_input in control_inputs:\n        c_api.TF_AddControlInput(op_desc, control_input._c_op)\n      # pylint: enable=protected-access\n    \n      # Add attrs\n      for name, attr_value in node_def.attr.items():\n        serialized = attr_value.SerializeToString()\n        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.\n        # It might be worth creating a convenient way to re-use the same status.\n        c_api.TF_SetAttrValueProto(op_desc, compat.as_str(name), serialized)\n    \n      try:\n        c_op = c_api.TF_FinishOperation(op_desc)\n      except errors.InvalidArgumentError as e:\n        # Convert to ValueError for backwards compatibility.\n>       raise ValueError(str(e))\nE       ValueError: Shapes must be equal rank, but are 3 and 2\nE       \tFrom merging shape 0 with other shapes. for 'lambda_1_out/packed' (op: 'Pack') with input shapes: [?,2,2], [?,2].\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1867: ValueError\n\nDuring handling of the above exception, another exception occurred:\n\ntmpdir = local('/tmp/pytest-of-ubuntu/pytest-70736/popen-gw0/test_TensorBoard_multi_input_o0')\n\n    @keras_test\n    def test_TensorBoard_multi_input_output(tmpdir):\n        np.random.seed(np.random.randint(1, 1e7))\n        filepath = str(tmpdir / 'logs')\n    \n        (X_train, y_train), (X_test, y_test) = get_test_data(\n            num_train=train_samples,\n            num_test=test_samples,\n            input_shape=(input_dim, input_dim),\n            classification=True,\n            num_classes=num_classes)\n        y_test = np_utils.to_categorical(y_test)\n        y_train = np_utils.to_categorical(y_train)\n    \n        def data_generator(train):\n            if train:\n                max_batch_index = len(X_train) // batch_size\n            else:\n                max_batch_index = len(X_test) // batch_size\n            i = 0\n            while 1:\n                if train:\n                    # simulate multi-input/output models\n                    yield ([X_train[i * batch_size: (i + 1) * batch_size]] * 2,\n                           [y_train[i * batch_size: (i + 1) * batch_size]] * 2)\n                else:\n                    yield ([X_test[i * batch_size: (i + 1) * batch_size]] * 2,\n                           [y_test[i * batch_size: (i + 1) * batch_size]] * 2)\n                i += 1\n                i = i % max_batch_index\n    \n        inp1 = Input((input_dim, input_dim))\n        inp2 = Input((input_dim, input_dim))\n        inp_3d = add([inp1, inp2])\n        inp_2d = GlobalAveragePooling1D()(inp_3d)\n        inp_pair = Lambda(lambda x: x)([inp_3d, inp_2d])  # test a layer with a list of output tensors\n        hidden = dot(inp_pair, axes=-1)\n        hidden = Dense(num_hidden, activation='relu')(hidden)\n        hidden = Dropout(0.1)(hidden)\n        output1 = Dense(num_classes, activation='softmax')(hidden)\n        output2 = Dense(num_classes, activation='softmax')(hidden)\n        model = Model(inputs=[inp1, inp2], outputs=[output1, output2])\n        model.compile(loss='categorical_crossentropy',\n                      optimizer='sgd',\n                      metrics=['accuracy'])\n    \n        # we must generate new callbacks for each test, as they aren't stateless\n        def callbacks_factory(histogram_freq):\n            return [callbacks.TensorBoard(log_dir=filepath,\n                                          histogram_freq=histogram_freq,\n                                          write_images=True, write_grads=True,\n                                          embeddings_freq=1,\n                                          embeddings_layer_names=['dense_1'],\n                                          batch_size=5)]\n    \n        # fit without validation data\n        model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n                  callbacks=callbacks_factory(histogram_freq=0), epochs=3)\n    \n        # fit with validation data and accuracy\n        model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n                  validation_data=([X_test] * 2, [y_test] * 2),\n>                 callbacks=callbacks_factory(histogram_freq=1), epochs=2)\n\ntests/keras/test_callbacks.py:680: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nkeras/engine/training.py:1037: in fit\n    validation_steps=validation_steps)\nkeras/engine/training_arrays.py:115: in fit_loop\n    callbacks.set_model(callback_model)\nkeras/callbacks.py:51: in set_model\n    callback.set_model(model)\nkeras/callbacks.py:790: in set_model\n    layer.output)\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/summary/summary.py:179: in histogram\n    tag=tag, values=values, name=scope)\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py:329: in histogram_summary\n    \"HistogramSummary\", tag=tag, values=values, name=name)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x7f84323bdf50>\nop_type_name = 'HistogramSummary', name = 'lambda_1_out/', keywords = {}\nop_info = <tensorflow.python.framework.op_def_library._OpInfo object at 0x7f84323474d0>\nop_def = name: \"HistogramSummary\"\ninput_arg {\n  name: \"tag\"\n  type: DT_STRING\n}\ninput_arg {\n  name: \"values\"\n  type_attr: \"T\"\n}...   type: DT_BFLOAT16\n      type: DT_UINT16\n      type: DT_HALF\n      type: DT_UINT32\n      type: DT_UINT64\n    }\n  }\n}\n\ng = <tensorflow.python.framework.ops.Graph object at 0x7f842c246310>\ndeprecation_version = 0, default_type_attr_map = {'T': tf.float32}\n\n    def _apply_op_helper(self, op_type_name, name=None, **keywords):\n      \"\"\"Implementation of apply_op that returns output_structure, op.\"\"\"\n      op_info = self._ops.get(op_type_name, None)\n      if op_info is None:\n        raise RuntimeError(\"Unrecognized Op name \" + op_type_name)\n      op_def = op_info.op_def\n    \n      # Determine the graph context.\n      try:\n        # Need to flatten all the arguments into a list.\n        # pylint: disable=protected-access\n        g = ops._get_graph_from_inputs(_Flatten(keywords.values()))\n        # pylint: enable=protected-access\n      except AssertionError as e:\n        raise RuntimeError(\n            \"Cannot determine graph for Op '%s' due to: %s\"\n            % (op_type_name, e.message))\n    \n      # Default name if not specified.\n      if name is None:\n        name = op_type_name\n    \n      # Check for deprecation\n      deprecation_version = op_def.deprecation.version\n      if deprecation_version:\n        producer = g.graph_def_versions.producer\n        if producer >= deprecation_version:\n          raise NotImplementedError(\n              (\"Op %s is not available in GraphDef version %d. \"\n               \"It has been removed in version %d. %s.\") %\n              (op_type_name, producer, deprecation_version,\n               op_def.deprecation.explanation))\n    \n      # Fill in the list of default types for all \"type\" attrs.  This\n      # will be used to choose a preferred dtype to convert to in the\n      # absence of input type information.\n      #\n      # TODO(b/31302892): Currently the defaults don't work in the right\n      # way if you have two inputs, one of whose type resolution depends\n      # on the other.  Handling this will require restructuring this code\n      # significantly.\n      default_type_attr_map = {}\n      for attr_def in op_def.attr:\n        if attr_def.type != \"type\":\n          continue\n        key = attr_def.name\n        if attr_def.HasField(\"default_value\"):\n          default_type_attr_map[key] = dtypes.as_dtype(\n              attr_def.default_value.type)\n    \n      # Requires that op_def has passed validation (using the C++\n      # ValidateOpDef() from ../framework/op_def_util.h).\n      attrs = {}\n      inputs = []\n      input_types = []\n      with g.as_default(), ops.name_scope(name) as scope:\n    \n        # Perform input type inference\n        inferred_from = {}\n        for input_arg in op_def.input_arg:\n          input_name = input_arg.name\n          if input_name in keywords:\n            values = keywords.pop(input_name)\n          elif input_name + \"_\" in keywords:\n            # Handle the case where the name is a keyword or built-in\n            # for Python so we use the name + _ instead.\n            input_name += \"_\"\n            values = keywords.pop(input_name)\n          else:\n            raise TypeError(\"No argument for input \" + input_name)\n    \n          # Goals:\n          # * Convert values to Tensors if it contains constants.\n          # * Verify that values is a list if that matches the input_arg's\n          #   type.\n          # * If the input_arg's type is determined by attrs, either set\n          #   those attrs and validate those attr values are legal (if\n          #   they have not yet been set) or validate the input matches\n          #   the type indicated by the attrs (if they have already been\n          #   inferred via an earlier input).\n          # * If the input_arg has an explicit type, make sure the input\n          #   conforms.\n    \n          if _IsListParameter(input_arg):\n            if not _IsListValue(values):\n              raise TypeError(\n                  \"Expected list for '%s' argument to '%s' Op, not %s.\" %\n                  (input_name, op_type_name, values))\n            # In cases where we expect all elements of the list to have the\n            # same dtype, try to cast non-Tensor elements to that type.\n            dtype = None\n            default_dtype = None\n            if input_arg.type != types_pb2.DT_INVALID:\n              dtype = input_arg.type\n            elif input_arg.number_attr:\n              if input_arg.type_attr in attrs:\n                dtype = attrs[input_arg.type_attr]\n              else:\n                for t in values:\n                  if isinstance(t, ops.Tensor):\n                    dtype = t.dtype\n                    break\n    \n              # dtype still not found, prefer using the default dtype\n              # from the attr.\n              if dtype is None and input_arg.type_attr in default_type_attr_map:\n                default_dtype = default_type_attr_map[input_arg.type_attr]\n    \n            try:\n              if not input_arg.is_ref and dtype:\n                dtype = dtypes.as_dtype(dtype).base_dtype\n              values = ops.internal_convert_n_to_tensor(\n                  values,\n                  name=input_arg.name,\n                  dtype=dtype if dtype else None,\n                  preferred_dtype=default_dtype,\n                  as_ref=input_arg.is_ref)\n              if input_arg.number_attr and len(\n                  set(v.dtype.base_dtype for v in values)) > 1:\n                raise TypeError()  # All types should match.\n            except (TypeError, ValueError):\n              # What types does the conversion function think values have?\n              observed_types = []\n              for value in values:\n                try:\n                  converted_value = ops.internal_convert_to_tensor(\n                      value, as_ref=input_arg.is_ref)\n                  observed_types.append(converted_value.dtype.base_dtype.name)\n                except (TypeError, ValueError):\n                  observed_types.append(\"<NOT CONVERTIBLE TO TENSOR>\")\n              observed = \", \".join(observed_types)\n    \n              prefix = (\n                  \"Tensors in list passed to '%s' of '%s' Op have types [%s]\" %\n                  (input_name, op_type_name, observed))\n              if input_arg.number_attr:\n                if input_arg.type != types_pb2.DT_INVALID:\n                  raise TypeError(\"%s that do not match expected type %s.\" %\n                                  (prefix, dtype.name))\n                elif input_arg.type_attr in attrs:\n                  raise TypeError(\"%s that do not match type %s inferred from \"\n                                  \"earlier arguments.\" %\n                                  (prefix, dtype.name))\n                else:\n                  raise TypeError(\"%s that don't all match.\" % prefix)\n              else:\n                raise TypeError(\n                    \"%s that are invalid. Tensors: %s\" % (prefix, values))\n    \n            types = [x.dtype for x in values]\n            inputs.extend(values)\n          else:\n            # In cases where we have an expected type, try to convert non-Tensor\n            # arguments to that type.\n            dtype = None\n            default_dtype = None\n            if input_arg.type != types_pb2.DT_INVALID:\n              dtype = input_arg.type\n            elif input_arg.type_attr in attrs:\n              dtype = attrs[input_arg.type_attr]\n            elif input_arg.type_attr in default_type_attr_map:\n              # The dtype could not be inferred solely from the inputs,\n              # so we prefer the attr's default, so code that adds a new attr\n              # with a default is backwards compatible.\n              default_dtype = default_type_attr_map[input_arg.type_attr]\n    \n            try:\n              values = ops.internal_convert_to_tensor(\n                  values,\n                  name=input_arg.name,\n                  dtype=dtype,\n                  as_ref=input_arg.is_ref,\n                  preferred_dtype=default_dtype)\n            except TypeError as err:\n              if dtype is None:\n                raise err\n              else:\n                raise TypeError(\n                    \"Expected %s passed to parameter '%s' of op '%s', got %s of \"\n                    \"type '%s' instead. Error: %s\" %\n                    (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,\n                     repr(values), type(values).__name__, err))\n            except ValueError:\n              # What type does convert_to_tensor think it has?\n              try:\n                observed = ops.internal_convert_to_tensor(\n                    values, as_ref=input_arg.is_ref).dtype.name\n              except ValueError as err:\n                raise ValueError(\n                    \"Tried to convert '%s' to a tensor and failed. Error: %s\" %\n>                   (input_name, err))\nE               ValueError: Tried to convert 'values' to a tensor and failed. Error: Shapes must be equal rank, but are 3 and 2\nE               \tFrom merging shape 0 with other shapes. for 'lambda_1_out/packed' (op: 'Pack') with input shapes: [?,2,2], [?,2].\n\n../../envs/keras_24/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:545: ValueError\n\n```\n",
    "6": "# Runtime values and types of variables inside the buggy function\nEach case below includes input parameter values and types, and the values and types of relevant variables at the function's return, derived from executing failing tests. If an input parameter is not reflected in the output, it is assumed to remain unchanged. Note that some of these values at the function's return might be incorrect. Analyze these cases to identify why the tests are failing to effectively fix the bug.\n\n## Case 1\n### Runtime values and types of the input parameters of the buggy function\nself.histogram_freq, value: `0`, type: `int`\n\nself.write_grads, value: `True`, type: `bool`\n\nmodel.total_loss, value: `<tf.Tensor 'loss/add:0' shape=() dtype=float32>`, type: `Tensor`\n\nself.write_images, value: `True`, type: `bool`\n\nself.write_graph, value: `True`, type: `bool`\n\nself.log_dir, value: `'/tmp/pytest-of-ubuntu/pytest-70738/popen-gw0/test_TensorBoard_multi_input_o0/logs'`, type: `str`\n\nself.embeddings_freq, value: `1`, type: `int`\n\nself.embeddings_layer_names, value: `['dense_1']`, type: `list`\n\nself.embeddings_metadata, value: `{}`, type: `dict`\n\n### Runtime values and types of variables right before the buggy function's return\nembeddings_layer_names, value: `['dense_1']`, type: `list`\n\nembeddings, value: `{'dense_1': <tf.Variable 'dense_1/kernel:0' shape=(2, 4) dtype=float32_ref>}`, type: `dict`\n\nembeddings_metadata, value: `{}`, type: `dict`\n\nlayer_name, value: `'dense_1'`, type: `str`\n\nconfig, value: `embeddings {\n  tensor_name: \"dense_1/kernel:0\"\n}\n`, type: `ProjectorConfig`\n\nself.embeddings_ckpt_path, value: `'/tmp/pytest-of-ubuntu/pytest-70738/popen-gw0/test_TensorBoard_multi_input_o0/logs/keras_embedding.ckpt'`, type: `str`\n\ntensor, value: `<tf.Variable 'dense_1/kernel:0' shape=(2, 4) dtype=float32_ref>`, type: `RefVariable`\n\nembedding, value: `tensor_name: \"dense_1/kernel:0\"\n`, type: `EmbeddingInfo`\n\nconfig.embeddings, value: `[tensor_name: \"dense_1/kernel:0\"\n]`, type: `RepeatedCompositeContainer`\n\nembedding.tensor_name, value: `'dense_1/kernel:0'`, type: `str`\n\ntensor.name, value: `'dense_1/kernel:0'`, type: `str`\n\nembedding.metadata_path, value: `''`, type: `str`\n\n## Case 2\n### Runtime values and types of the input parameters of the buggy function\nself.histogram_freq, value: `1`, type: `int`\n\nself.write_grads, value: `True`, type: `bool`\n\nmodel.total_loss, value: `<tf.Tensor 'loss/add:0' shape=() dtype=float32>`, type: `Tensor`\n\nself.write_images, value: `True`, type: `bool`\n\nself.write_graph, value: `True`, type: `bool`\n\nself.log_dir, value: `'/tmp/pytest-of-ubuntu/pytest-70738/popen-gw0/test_TensorBoard_multi_input_o0/logs'`, type: `str`\n\nself.embeddings_freq, value: `1`, type: `int`\n\nself.embeddings_layer_names, value: `['dense_1']`, type: `list`\n\nself.embeddings_metadata, value: `{}`, type: `dict`\n\n### Runtime values and types of variables right before the buggy function's return\nweight, value: `<tf.Variable 'dense_1/kernel:0' shape=(2, 4) dtype=float32_ref>`, type: `RefVariable`\n\nlayer.weights, value: `[<tf.Variable 'dense_1/kernel:0' shape=(2, 4) dtype=float32_ref>, <tf.Variable 'dense_1/bias:0' shape=(4,) dtype=float32_ref>]`, type: `list`\n\nmapped_weight_name, value: `'dense_1/kernel_0'`, type: `str`\n\nweight.name, value: `'dense_1/kernel:0'`, type: `str`\n\ngrads, value: `[<tf.Tensor 'gradients/dense_1/MatMul_grad/MatMul_1:0' shape=(2, 4) dtype=float32>]`, type: `list`\n\ngrad, value: `<tf.Tensor 'gradients/dense_1/MatMul_grad/MatMul_1:0' shape=(2, 4) dtype=float32>`, type: `Tensor`\n\nlayer.output, value: `<tf.Tensor 'dense_1/Relu:0' shape=(?, 4) dtype=float32>`, type: `Tensor`\n\ni, value: `1`, type: `int`\n\noutput, value: `<tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>`, type: `Tensor`\n\nlayer.name, value: `'dense_1'`, type: `str`\n\n## Case 3\n### Runtime values and types of the input parameters of the buggy function\nself.histogram_freq, value: `1`, type: `int`\n\nself.write_grads, value: `True`, type: `bool`\n\nmodel.total_loss, value: `<tf.Tensor 'loss/add:0' shape=() dtype=float32>`, type: `Tensor`\n\nself.write_images, value: `True`, type: `bool`\n\nself.write_graph, value: `True`, type: `bool`\n\nself.log_dir, value: `'/tmp/pytest-of-ubuntu/pytest-70738/popen-gw0/test_TensorBoard_multi_input_o0/logs'`, type: `str`\n\nself.embeddings_freq, value: `1`, type: `int`\n\nself.embeddings_layer_names, value: `['dense_1']`, type: `list`\n\nself.embeddings_metadata, value: `{}`, type: `dict`\n\n### Runtime values and types of variables right before the buggy function's return\nweight, value: `<tf.Variable 'dense_1/kernel:0' shape=(2, 4) dtype=float32_ref>`, type: `RefVariable`\n\nlayer.weights, value: `[<tf.Variable 'dense_1/kernel:0' shape=(2, 4) dtype=float32_ref>, <tf.Variable 'dense_1/bias:0' shape=(4,) dtype=float32_ref>]`, type: `list`\n\nmapped_weight_name, value: `'dense_1/kernel_0'`, type: `str`\n\nweight.name, value: `'dense_1/kernel:0'`, type: `str`\n\ngrads, value: `[<tf.Tensor 'gradients_6/dense_1/MatMul_grad/MatMul_1:0' shape=(2, 4) dtype=float32>]`, type: `list`\n\ngrad, value: `<tf.Tensor 'gradients_6/dense_1/MatMul_grad/MatMul_1:0' shape=(2, 4) dtype=float32>`, type: `Tensor`\n\nlayer.output, value: `<tf.Tensor 'dense_1/Relu:0' shape=(?, 4) dtype=float32>`, type: `Tensor`\n\ni, value: `1`, type: `int`\n\noutput, value: `<tf.Tensor 'lambda_1/Identity_1:0' shape=(?, 2) dtype=float32>`, type: `Tensor`\n\nlayer.name, value: `'dense_1'`, type: `str`\n\n",
    "7": "# Expected values and types of variables during the failing test execution\nEach case below includes input parameter values and types, and the expected values and types of relevant variables at the function's return. If an input parameter is not reflected in the output, it is assumed to remain unchanged. A corrected function must satisfy all these cases.\n\n## Expected case 1\n### Input parameter values and types\n### The values and types of buggy function's parameters\nself.histogram_freq, value: `0`, type: `int`\n\nself.write_grads, value: `True`, type: `bool`\n\nmodel.total_loss, value: `<tf.Tensor 'loss/add:0' shape=() dtype=float32>`, type: `Tensor`\n\nself.write_images, value: `True`, type: `bool`\n\nself.write_graph, value: `True`, type: `bool`\n\nself.log_dir, value: `'/tmp/pytest-of-ubuntu/pytest-70737/popen-gw0/test_TensorBoard_multi_input_o0/logs'`, type: `str`\n\nself.embeddings_freq, value: `1`, type: `int`\n\nself.embeddings_layer_names, value: `['dense_1']`, type: `list`\n\nself.embeddings_metadata, value: `{}`, type: `dict`\n\n### Expected values and types of variables right before the buggy function's return\nembeddings_layer_names, expected value: `['dense_1']`, type: `list`\n\nembeddings, expected value: `{'dense_1': <tf.Variable 'dense_1/kernel:0' shape=(2, 4) dtype=float32_ref>}`, type: `dict`\n\nembeddings_metadata, expected value: `{}`, type: `dict`\n\nlayer_name, expected value: `'dense_1'`, type: `str`\n\nconfig, expected value: `embeddings {\n  tensor_name: \"dense_1/kernel:0\"\n}\n`, type: `ProjectorConfig`\n\nself.embeddings_ckpt_path, expected value: `'/tmp/pytest-of-ubuntu/pytest-70737/popen-gw0/test_TensorBoard_multi_input_o0/logs/keras_embedding.ckpt'`, type: `str`\n\ntensor, expected value: `<tf.Variable 'dense_1/kernel:0' shape=(2, 4) dtype=float32_ref>`, type: `RefVariable`\n\nembedding, expected value: `tensor_name: \"dense_1/kernel:0\"\n`, type: `EmbeddingInfo`\n\nconfig.embeddings, expected value: `[tensor_name: \"dense_1/kernel:0\"\n]`, type: `RepeatedCompositeContainer`\n\nembedding.tensor_name, expected value: `'dense_1/kernel:0'`, type: `str`\n\ntensor.name, expected value: `'dense_1/kernel:0'`, type: `str`\n\nembedding.metadata_path, expected value: `''`, type: `str`\n\n",
    "8": "",
    "9": "Your output should follow these steps:\n1. Analyze the buggy function and its relationship with the buggy class, related functions, test code, corresponding error message, the actual input/output variable information, the expected input/output variable information.\n2. Identify a potential error location within the buggy function.\n3. Elucidate the bug's cause using:\n   (a) The buggy function, \n   (b) The buggy class docs, \n   (c) The related functions, \n   (d) The failing test, \n   (e) The corresponding error message, \n   (f) The actual input/output variable values, \n   (g) The expected input/output variable values\n\n4. Suggest approaches for fixing the bug.\n5. Present the corrected code for the buggy function such that it satisfied the following:\n   (a) the program passes the failing test, \n   (b) the function satisfies the expected input/output variable information provided\n\n",
    "1.3.3": "Assume that the following list of imports are available in the current environment, so you don't need to import them when generating a fix.\n```python\nimport os\nfrom . import backend as K\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\n```\n\n",
    "source_code_section": "# The source code of the buggy function\n```python\n# The relative path of the buggy file: keras/callbacks.py\n\n\n\n    # this is the buggy function you need to fix\n    def set_model(self, model):\n        self.model = model\n        if K.backend() == 'tensorflow':\n            self.sess = K.get_session()\n        if self.histogram_freq and self.merged is None:\n            for layer in self.model.layers:\n    \n                for weight in layer.weights:\n                    mapped_weight_name = weight.name.replace(':', '_')\n                    tf.summary.histogram(mapped_weight_name, weight)\n                    if self.write_grads:\n                        grads = model.optimizer.get_gradients(model.total_loss,\n                                                              weight)\n    \n                        def is_indexed_slices(grad):\n                            return type(grad).__name__ == 'IndexedSlices'\n                        grads = [\n                            grad.values if is_indexed_slices(grad) else grad\n                            for grad in grads]\n                        tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\n                    if self.write_images:\n                        w_img = tf.squeeze(weight)\n                        shape = K.int_shape(w_img)\n                        if len(shape) == 2:  # dense layer kernel case\n                            if shape[0] > shape[1]:\n                                w_img = tf.transpose(w_img)\n                                shape = K.int_shape(w_img)\n                            w_img = tf.reshape(w_img, [1,\n                                                       shape[0],\n                                                       shape[1],\n                                                       1])\n                        elif len(shape) == 3:  # convnet case\n                            if K.image_data_format() == 'channels_last':\n                                # switch to channels_first to display\n                                # every kernel as a separate image\n                                w_img = tf.transpose(w_img, perm=[2, 0, 1])\n                                shape = K.int_shape(w_img)\n                            w_img = tf.reshape(w_img, [shape[0],\n                                                       shape[1],\n                                                       shape[2],\n                                                       1])\n                        elif len(shape) == 1:  # bias case\n                            w_img = tf.reshape(w_img, [1,\n                                                       shape[0],\n                                                       1,\n                                                       1])\n                        else:\n                            # not possible to handle 3D convnets etc.\n                            continue\n    \n                        shape = K.int_shape(w_img)\n                        assert len(shape) == 4 and shape[-1] in [1, 3, 4]\n                        tf.summary.image(mapped_weight_name, w_img)\n    \n                if hasattr(layer, 'output'):\n                    tf.summary.histogram('{}_out'.format(layer.name),\n                                         layer.output)\n        self.merged = tf.summary.merge_all()\n    \n        if self.write_graph:\n            self.writer = tf.summary.FileWriter(self.log_dir,\n                                                self.sess.graph)\n        else:\n            self.writer = tf.summary.FileWriter(self.log_dir)\n    \n        if self.embeddings_freq:\n            embeddings_layer_names = self.embeddings_layer_names\n    \n            if not embeddings_layer_names:\n                embeddings_layer_names = [layer.name for layer in self.model.layers\n                                          if type(layer).__name__ == 'Embedding']\n    \n            embeddings = {layer.name: layer.weights[0]\n                          for layer in self.model.layers\n                          if layer.name in embeddings_layer_names}\n    \n            self.saver = tf.train.Saver(list(embeddings.values()))\n    \n            embeddings_metadata = {}\n    \n            if not isinstance(self.embeddings_metadata, str):\n                embeddings_metadata = self.embeddings_metadata\n            else:\n                embeddings_metadata = {layer_name: self.embeddings_metadata\n                                       for layer_name in embeddings.keys()}\n    \n            config = projector.ProjectorConfig()\n            self.embeddings_ckpt_path = os.path.join(self.log_dir,\n                                                     'keras_embedding.ckpt')\n    \n            for layer_name, tensor in embeddings.items():\n                embedding = config.embeddings.add()\n                embedding.tensor_name = tensor.name\n    \n                if layer_name in embeddings_metadata:\n                    embedding.metadata_path = embeddings_metadata[layer_name]\n    \n            projector.visualize_embeddings(self.writer, config)\n    \n```",
    "source_code_body": "# The relative path of the buggy file: keras/callbacks.py\n\n# This function from the same file, but not the same class, is called by the buggy function\ndef is_indexed_slices(grad):\n    # Please ignore the body of this function\n\n# The declaration of the class containing the buggy function\nclass TensorBoard(Callback):\n    \"\"\"\n    TensorBoard basic visualizations.\n    \n    [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\n    is a visualization tool provided with TensorFlow.\n    \n    This callback writes a log for TensorBoard, which allows\n    you to visualize dynamic graphs of your training and test\n    metrics, as well as activation histograms for the different\n    layers in your model.\n    \n    If you have installed TensorFlow with pip, you should be able\n    to launch TensorBoard from the command line:\n    "
}