The bug occurs in the `_clone_functional_model` function of the `keras/models.py` file. The error message suggests that the output tensors are not being computed correctly, as `assert x in tensor_map` is failing for the output tensor "swap_layer_1/Identity:0".

The issue likely arises from the logic related to the creation of input tensors and caching of layers. Specifically, in the part where the code is iterating over every node in the reference model, it seems the mapping of input and output tensors is not functioning as expected, leading to incorrect computation.

To fix this bug:
1. Ensure that the creation of input tensors and caching of layers is done correctly to reflect the model structure accurately.
2. Verify that the correct layers and corresponding tensors are being used during the iterative process.
3. Check for any issues related to the handling of multi-output layers as in the test case provided.

Here's the corrected version of the `_clone_functional_model` function:

```python
def _clone_functional_model(model, input_tensors=None):
    if not isinstance(model, Model):
        raise ValueError('Expected `model` argument to be a `Model` instance, got ', model)
    if isinstance(model, Sequential):
        raise ValueError('Expected `model` argument to be a functional `Model` instance, '
                         'got a `Sequential` instance instead:', model)

    layer_map = {}  # Cache for created layers
    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}

    # Initialize input layers and input tensors
    if input_tensors is None:
        input_layers = []
        input_tensors = []
        for layer in model._input_layers:
            input_tensor = Input(batch_shape=layer.batch_input_shape,
                                 dtype=layer.dtype,
                                 sparse=layer.sparse,
                                 name=layer.name)
            input_tensors.append(input_tensor)
            layer_map[layer] = input_tensor
        input_tensors = to_list(input_tensors)
    else:
        for i, x in enumerate(input_tensors):
            if not K.is_keras_tensor(x):
                name = model._input_layers[i].name
                input_tensor = Input(tensor=x, name='input_wrapper_for_' + name)
                input_tensors[i] = input_tensor
                layer_map[x._keras_history[0]] = input_tensor

    # Iterate over every node in the reference model, in depth order
    depth_keys = list(model._nodes_by_depth.keys())
    depth_keys.sort(reverse=True)
    for depth in depth_keys:
        nodes = model._nodes_by_depth[depth]
        for node in nodes:
            # Recover the corresponding layer
            layer = node.outbound_layer

            # Get or create layer
            if layer not in layer_map:
                # Clone layer
                new_layer = layer.__class__.from_config(layer.get_config())
                layer_map[layer] = new_layer
            else:
                # Reuse previously cloned layer
                layer = layer_map[layer]

            # Gather and map input and output tensors
            reference_input_tensors = node.input_tensors
            reference_output_tensors = node.output_tensors
            computed_data = []

            for x in reference_input_tensors:
                if x in tensor_map:
                    computed_data.append(tensor_map[x])

            if len(computed_data) == len(reference_input_tensors):
                kwargs = node.arguments if node.arguments else {}
                computed_tensors = [x[0] for x in computed_data]
                kwargs['mask'] = [x[1] for x in computed_data] if has_arg(layer.call, 'mask') else None
                output_tensors = to_list(layer(computed_tensors, **kwargs))

                for x, y in zip(reference_output_tensors, output_tensors):
                    tensor_map[x] = (y, None)

    # Instantiate a new model from inputs and outputs
    output_tensors = [tensor_map[x][0] for x in model.outputs]
    return Model(input_tensors, output_tensors, name=model.name)
```