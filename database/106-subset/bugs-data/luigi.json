{
  "project": "luigi",
  "bugs": [
    {
      "id": 4,
      "buggy_code_blocks": [
        {
          "filename": "luigi/contrib/redshift.py",
          "source_code": "    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        logger.info(\"Inserting file: %s\", f)\n        colnames = ''\n        if len(self.columns) > 0:\n            colnames = \",\".join([x[0] for x in self.columns])\n            colnames = '({})'.format(colnames)\n\n        cursor.execute(\"\"\"\n         COPY {table} {colnames} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table=self.table,\n            colnames=colnames,\n            source=f,\n            creds=self._credentials(),\n            options=self.copy_options)\n        )"
        }
      ],
      "features": {
        "class_definition": "class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n\n    * Subclass and override the required attributes:\n\n      * `host`,\n      * `database`,\n      * `user`,\n      * `password`,\n      * `table`,\n      * `columns`,\n      * `s3_load_path`.\n\n    * You can also override the attributes provided by the\n      CredentialsMixin if they are not supplied by your\n      configuration or environment variables.\n    \"\"\"\n\n    @abc.abstractmethod\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        * DELIMITER '\\t'\n        \"\"\"\n        return ''\n\n    @property\n    def prune_table(self):\n        \"\"\"\n        Override to set equal to the name of the table which is to be pruned.\n        Intended to be used in conjunction with prune_column and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_column(self):\n        \"\"\"\n        Override to set equal to the column of the prune_table which is to be compared\n        Intended to be used in conjunction with prune_table and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_date(self):\n        \"\"\"\n        Override to set equal to the date by which prune_column is to be compared\n        Intended to be used in conjunction with prune_table and prune_column\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def table_attributes(self):\n        \"\"\"\n        Add extra table attributes, for example:\n\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        \"\"\"\n        return ''\n\n    @property\n    def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False\n\n    def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False\n\n    @property\n    def table_type(self):\n        \"\"\"\n        Return table type (i.e. 'temp').\n        \"\"\"\n        return ''\n\n    @property\n    def queries(self):\n        \"\"\"\n        Override to return a list of queries to be executed in order.\n        \"\"\"\n        return []\n\n    def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def prune(self, connection):\n        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        elif len(self.columns[0]) == 3:\n            # if columns is specified as (name, type, encoding) tuples\n            # possible column encodings: https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html\n            coldefs = ','.join(\n                '{name} {type} ENCODE {encoding}'.format(\n                    name=name,\n                    type=type,\n                    encoding=encoding) for name, type, encoding in self.columns\n            )\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        else:\n            raise ValueError(\"create_table() found no columns for %r\"\n                             % self.table)\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        output = self.output()\n        connection = output.connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        # update marker table\n        output.touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        logger.info(\"Inserting file: %s\", f)\n        colnames = ''\n        if len(self.columns) > 0:\n            colnames = \",\".join([x[0] for x in self.columns])\n            colnames = '({})'.format(colnames)\n\n        cursor.execute(\"\"\"\n         COPY {table} {colnames} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table=self.table,\n            colnames=colnames,\n            source=f,\n            creds=self._credentials(),\n            options=self.copy_options)\n        )\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id)\n\n    def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = lower(%s) and table_name = lower(%s) limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = lower(%s) limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()\n\n    def init_copy(self, connection):\n        \"\"\"\n        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n        \"\"\"\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)\n\n    def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)",
        "variable_definitions": null,
        "error_message": "============================= test session starts =============================\nplatform linux -- Python 3.8.10, pytest-7.4.2, pluggy-1.3.0\nrootdir: /home/huijieyan/Desktop/PyRepair/benchmarks/BugsInPy_Cloned_Repos/luigi:4\nplugins: cov-4.1.0, mock-3.11.1, timeout-2.1.0\ntimeout: 60.0s\ntimeout method: signal\ntimeout func_only: False\ncollected 1 item                                                              \n\ntest/contrib/redshift_test.py F                                         [100%]\n\n================================== FAILURES ===================================\n____________ TestS3CopyToTable.test_s3_copy_with_nonetype_columns _____________\n\nself = <contrib.redshift_test.TestS3CopyToTable testMethod=test_s3_copy_with_nonetype_columns>\nmock_redshift_target = <MagicMock name='RedshiftTarget' id='139768575303008'>\n\n    @mock.patch(\"luigi.contrib.redshift.RedshiftTarget\")\n    def test_s3_copy_with_nonetype_columns(self, mock_redshift_target):\n        task = DummyS3CopyToTableKey(columns=None)\n>       task.run()\n\ntest/contrib/redshift_test.py:337: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nluigi/contrib/redshift.py:338: in run\n    self.copy(cursor, path)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = DummyS3CopyToTableKey(table=dummy_table, columns=null)\ncursor = <MagicMock name='RedshiftTarget().connect().cursor()' id='139768574974560'>\nf = 's3://bucket/key'\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n    \n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        logger.info(\"Inserting file: %s\", f)\n        colnames = ''\n>       if len(self.columns) > 0:\nE       TypeError: object of type 'NoneType' has no len()\n\nluigi/contrib/redshift.py:356: TypeError\n============================== warnings summary ===============================\nluigi/parameter.py:28\n  /home/huijieyan/Desktop/PyRepair/benchmarks/BugsInPy_Cloned_Repos/luigi:4/luigi/parameter.py:28: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n    from collections import OrderedDict, Mapping\n\nluigi/scheduler.py:208\n  /home/huijieyan/Desktop/PyRepair/benchmarks/BugsInPy_Cloned_Repos/luigi:4/luigi/scheduler.py:208: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n    class OrderedSet(collections.MutableSet):\n\nluigi/scheduler.py:98: 29 warnings\n  /home/huijieyan/Desktop/PyRepair/benchmarks/BugsInPy_Cloned_Repos/luigi:4/luigi/scheduler.py:98: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n    fn_args = inspect.getargspec(fn)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nFAILED test/contrib/redshift_test.py::TestS3CopyToTable::test_s3_copy_with_nonetype_columns - TypeError: object of type 'NoneType' has no len()\n======================= 1 failed, 31 warnings in 0.12s ========================\n",
        "stack_trace": null,
        "test_code_blocks": [
          {
            "filename": "test/contrib/redshift_test.py",
            "test_code": "    @mock.patch(\"luigi.contrib.redshift.RedshiftTarget\")\n    def test_s3_copy_with_nonetype_columns(self, mock_redshift_target):\n        task = DummyS3CopyToTableKey(columns=None)\n        task.run()\n\n        # The mocked connection cursor passed to\n        # S3CopyToTable.copy(self, cursor, f).\n        mock_cursor = (mock_redshift_target.return_value\n                                           .connect\n                                           .return_value\n                                           .cursor\n                                           .return_value)\n\n        # `mock_redshift_target` is the mocked `RedshiftTarget` object\n        # returned by S3CopyToTable.output(self).\n        mock_redshift_target.assert_called_once_with(\n            database=task.database,\n            host=task.host,\n            update_id=task.task_id,\n            user=task.user,\n            table=task.table,\n            password=task.password,\n        )\n\n        # To get the proper intendation in the multiline `COPY` statement the\n        # SQL string was copied from redshift.py.\n        mock_cursor.execute.assert_called_with(\"\"\"\n         COPY {table} {colnames} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table='dummy_table',\n            colnames='',\n            source='s3://bucket/key',\n            creds='aws_access_key_id=key;aws_secret_access_key=secret',\n            options='')\n        )"
          }
        ],
        "raised_issue_descriptions": [
          {
            "title": "Redshift COPY fails in luigi 2.7.1 when columns are not provided",
            "content": "Running Redshift COPY jobs with columns = None to prohibit table creation fails in luigi 2.7.1 with\n\nTypeError: object of type 'NoneType' has no len()\nThe root cause seems to be https://github.com/spotify/luigi/pull/2245/files#diff-778ea3db4cccaf4de6564889c5eb670fR338\n\nA possible solution would be to change the line to\n\nif self.columns and len(self.columns) > 0:\nunless I am missing some reason to explicitly ask only for len(self.columns)."
          }
        ]
      }
    },
    {
      "id": 25,
      "buggy_code_blocks": [
        {
          "filename": "luigi/contrib/redshift.py",
          "source_code": "    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        connection = self.output().connect()\n        if not self.does_table_exist(connection):\n            # try creating table\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n        elif self.do_truncate_table():\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        logger.info(\"Inserting file: %s\", path)\n        cursor = connection.cursor()\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.output().touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()"
        }
      ],
      "features": {
        "class_definition": "class S3CopyToTable(rdbms.CopyToTable):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n\n    * Subclass and override the required attributes:\n      * `host`,\n      * `database`,\n      * `user`,\n      * `password`,\n      * `table`,\n      * `columns`,\n      * `aws_access_key_id`,\n      * `aws_secret_access_key`,\n      * `s3_load_path`.\n    \"\"\"\n\n    @abc.abstractproperty\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        \"\"\"\n        return ''\n\n    def table_attributes(self):\n        '''Add extra table attributes, for example:\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        '''\n        return ''\n\n    def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False\n\n    def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n            query = (\"CREATE TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes())\n            connection.cursor().execute(query)\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        connection = self.output().connect()\n        if not self.does_table_exist(connection):\n            # try creating table\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n        elif self.do_truncate_table():\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        logger.info(\"Inserting file: %s\", path)\n        cursor = connection.cursor()\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.output().touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n        \"\"\"\n\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n         delimiter '%s'\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, self.column_separator,\n                 self.copy_options))\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id())\n\n    def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n        query = (\"select 1 as table_exists \"\n                 \"from pg_table_def \"\n                 \"where tablename = %s limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, (self.table,))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()",
        "variable_definitions": null,
        "error_message": "=================================== FAILURES ===================================\n___________________ TestS3CopyToTable.test_s3_copy_to_table ____________________\n\nself = <contrib.redshift_test.TestS3CopyToTable testMethod=test_s3_copy_to_table>\nmock_redshift_target = <MagicMock name='RedshiftTarget' id='139901530488544'>\nmock_copy = <MagicMock name='copy' id='139901530230352'>\n\n    @mock.patch(\"luigi.contrib.redshift.S3CopyToTable.copy\")\n    @mock.patch(\"luigi.contrib.redshift.RedshiftTarget\")\n    def test_s3_copy_to_table(self, mock_redshift_target, mock_copy):\n        task = DummyS3CopyToTable()\n>       task.run()\n\ntest/contrib/redshift_test.py:55: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = DummyS3CopyToTable()\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n    \n>       path = self.s3_load_path()\nE       TypeError: 'str' object is not callable\n\nenv/lib/python3.8/site-packages/luigi/contrib/redshift.py:166: TypeError\n",
        "stack_trace": null,
        "test_code_blocks": [
          {
            "filename": "test/contrib/redshift_test.py",
            "test_code": "    @mock.patch(\"luigi.contrib.redshift.S3CopyToTable.copy\")\n    @mock.patch(\"luigi.contrib.redshift.RedshiftTarget\")\n    def test_s3_copy_to_table(self, mock_redshift_target, mock_copy):\n        task = DummyS3CopyToTable()\n        task.run()\n\n        # The mocked connection cursor passed to\n        # S3CopyToTable.copy(self, cursor, f).\n        mock_cursor = (mock_redshift_target.return_value\n                                           .connect\n                                           .return_value\n                                           .cursor\n                                           .return_value)\n\n        # `mock_redshift_target` is the mocked `RedshiftTarget` object\n        # returned by S3CopyToTable.output(self).\n        mock_redshift_target.assert_called_with(database=task.database,\n                                                host=task.host,\n                                                update_id='DummyS3CopyToTable()',\n                                                user=task.user,\n                                                table=task.table,\n                                                password=task.password)\n\n        # Check if the `S3CopyToTable.s3_load_path` class attribute was\n        # successfully referenced in the `S3CopyToTable.run` method, which is\n        # in-turn passed to `S3CopyToTable.copy` and other functions in `run`\n        # (see issue #995).\n        mock_copy.assert_called_with(mock_cursor, task.s3_load_path)\n\n        # Check the SQL query in `S3CopyToTable.does_table_exist`.\n        mock_cursor.execute.assert_called_with(\"select 1 as table_exists \"\n                                               \"from pg_table_def \"\n                                               \"where tablename = %s limit 1\",\n                                               (task.table,))\n\n        return"
          }
        ],
        "raised_issue_description": [
          {
            "title": "S3CopyToTable.s3_load_path TypeError",
            "content": "I encountered this TypeError when subclassing S3CopyToTable:\n...\nTraceback (most recent call last):\n  File \"/home/kian/workspaces/contrib/luigi/luigi/worker.py\", line 137, in run\n    new_deps = self._run_get_new_deps()\n  File \"/home/kian/workspaces/contrib/luigi/luigi/worker.py\", line 88, in _run_get_new_deps\n    task_gen = self.task.run()\n  File \"/home/kian/workspaces/contrib/luigi/luigi/contrib/redshift.py\", line 166, in run\n    path = self.s3_load_path()\nTypeError: 'str' object is not callable\nINFO: Skipping error email. Set `error-email` in the `core` section of the luigi config file to receive error emails.\nDEBUG: 1 running tasks, waiting for next task to finish\nDEBUG: Asking scheduler for work...\nINFO: Done\nINFO: There are no more tasks to run at this time\nINFO: Worker Worker(salt=532581476, workers=1, host=..., username=kian, pid=23435) was stopped. Shutting down Keep-Alive thread\n\nwhich was fixed by changing https://github.com/spotify/luigi/blob/master/luigi/contrib/redshift.py#L166 from:\npath = self.s3_load_path()\n\nto\npath = self.s3_load_path\n\n(I submitted this fix as PR #996)\n\nas per the other class properties, which as far as i can tell, aren't explicitly referenced as methods either (otherwise they too would raise the above exception?).\n\nThe snippet I used to generate the above error:\nimport luigi\n\nfrom luigi.s3 import S3Target, S3Client                                                                          \nfrom luigi.contrib.redshift import S3CopyToTable                                                                 \n\n\nclass MyS3Task(luigi.Task): \n    local_tsv = luigi.Parameter()\n    s3_load_path = luigi.Parameter()                                                                             \n    client = luigi.Parameter()\n\n    def output(self):\n        return S3Target(self.s3_load_path, client=self.client)\n\n    def run(self):\n        self.client.put(self.local_tsv, self.output().path)\n        return\n\n\nclass MyRedshiftTask(S3CopyToTable):\n    host = luigi.Parameter()\n    database = luigi.Parameter()\n    user = luigi.Parameter()\n    password = luigi.Parameter()\n    table = luigi.Parameter()\n    local_tsv = luigi.Parameter()\n\n    aws_access_key_id = luigi.Parameter()\n    aws_secret_access_key = luigi.Parameter()\n\n    columns = [(\"x\", \"INT\"),\n               (\"y\", \"INT\")]\n\n    s3_load_path = luigi.Parameter()\n    copy_options = \"IGNOREHEADER 1\"\n\n    def requires(self):\n        client = S3Client(self.aws_access_key_id, self.aws_secret_access_key)\n        return MyS3Task(s3_load_path=self.s3_load_path,\n                        local_tsv=self.local_tsv, client=client)\n\n\nif __name__ == '__main__':\n    luigi.run()\n\nwhich was run from the command line using:\npython ./luigi_example.py --local-scheduler MyRedshiftTask \\\n    --host \"<REDSHIFT ENDPOINT>:5439\" \\\n    --database \"dev\" \\\n    --user \"<USERNAME>\" \\\n    --password \"<HIDDEN>\" \\\n    --table \"test_redshift_table_5439\" \\\n    --aws-access-key-id \"<HIDDEN>\" \\\n    --aws-secret-access-key \"<HIDDEN>\" \\\n    --s3-load-path \"s3://bucket-5439/test.tsv\" \\\n    --local-tsv \"./test.tsv\" \\\n\nwhere test.tsv (tab-separated values) contained:\nx   y\n1   2\n10  20\n100 200"
          }
        ]
      }
    },
    {
      "id": 28,
      "buggy_code_blocks": [
        {
          "filename": "luigi/contrib/hive.py",
          "source_code": "    def table_exists(self, table, database='default', partition=None):\n        if partition is None:\n            stdout = run_hive_cmd('use {0}; show tables like \"{1}\";'.format(database, table))\n\n            return stdout and table in stdout\n        else:\n            stdout = run_hive_cmd(\"\"\"use %s; show partitions %s partition\n                                (%s)\"\"\" % (database, table, self.partition_spec(partition)))\n\n            if stdout:\n                return True\n            else:\n                return False"
        }
      ],
      "features": {
        "class_definition": "class HiveCommandClient(HiveClient):\n    \"\"\"\n    Uses `hive` invocations to find information.\n    \"\"\"\n\n    def table_location(self, table, database='default', partition=None):\n        cmd = \"use {0}; describe formatted {1}\".format(database, table)\n        if partition is not None:\n            cmd += \" PARTITION ({0})\".format(self.partition_spec(partition))\n\n        stdout = run_hive_cmd(cmd)\n\n        for line in stdout.split(\"\\n\"):\n            if \"Location:\" in line:\n                return line.split(\"\\t\")[1]\n\n    def table_exists(self, table, database='default', partition=None):\n        if partition is None:\n            stdout = run_hive_cmd('use {0}; show tables like \"{1}\";'.format(database, table))\n\n            return stdout and table in stdout\n        else:\n            stdout = run_hive_cmd(\"\"\"use %s; show partitions %s partition\n                                (%s)\"\"\" % (database, table, self.partition_spec(partition)))\n\n            if stdout:\n                return True\n            else:\n                return False\n\n    def table_schema(self, table, database='default'):\n        describe = run_hive_cmd(\"use {0}; describe {1}\".format(database, table))\n        if not describe or \"does not exist\" in describe:\n            return None\n        return [tuple([x.strip() for x in line.strip().split(\"\\t\")]) for line in describe.strip().split(\"\\n\")]\n\n    def partition_spec(self, partition):\n        \"\"\"\n        Turns a dict into the a Hive partition specification string.\n        \"\"\"\n        return ','.join([\"{0}='{1}'\".format(k, v) for (k, v) in\n                         sorted(six.iteritems(partition), key=operator.itemgetter(0))])",
        "variable_definitions": null,
        "error_message": "",
        "stack_trace": null,
        "test_code_blocks": [
          {
            "filename": "test/contrib/redshift_test.py",
            "test_code": "    @mock.patch(\"luigi.contrib.redshift.S3CopyToTable.copy\")\n    @mock.patch(\"luigi.contrib.redshift.RedshiftTarget\")\n    def test_s3_copy_to_table(self, mock_redshift_target, mock_copy):\n        task = DummyS3CopyToTable()\n        task.run()\n\n        # The mocked connection cursor passed to\n        # S3CopyToTable.copy(self, cursor, f).\n        mock_cursor = (mock_redshift_target.return_value\n                                           .connect\n                                           .return_value\n                                           .cursor\n                                           .return_value)\n\n        # `mock_redshift_target` is the mocked `RedshiftTarget` object\n        # returned by S3CopyToTable.output(self).\n        mock_redshift_target.assert_called_with(database=task.database,\n                                                host=task.host,\n                                                update_id='DummyS3CopyToTable()',\n                                                user=task.user,\n                                                table=task.table,\n                                                password=task.password)\n\n        # Check if the `S3CopyToTable.s3_load_path` class attribute was\n        # successfully referenced in the `S3CopyToTable.run` method, which is\n        # in-turn passed to `S3CopyToTable.copy` and other functions in `run`\n        # (see issue #995).\n        mock_copy.assert_called_with(mock_cursor, task.s3_load_path)\n\n        # Check the SQL query in `S3CopyToTable.does_table_exist`.\n        mock_cursor.execute.assert_called_with(\"select 1 as table_exists \"\n                                               \"from pg_table_def \"\n                                               \"where tablename = %s limit 1\",\n                                               (task.table,))\n\n        return"
          }
        ],
        "raised_issue_descriptions": [
          {
            "title": "hive table_exists should be case insensitive?",
            "content": "Any thoughts on this one?\\n\\nIn https://github.com/spotify/luigi/blob/master/luigi/contrib/hive.py#L141\\n(possibly here too, but we're not on CDH: https://github.com/spotify/luigi/blob/master/luigi/contrib/hive.py#L192)\\n\\nSometimes we have tables that are defined as capitalized, rather than lower case underscored names. These are easier to read in code if left capitalized, though hive is case insensitive, and will return them as lower case.\\n\\nE.g. when checking for an existing table table = 'FooBar', stdout will return with foobar and the test will fail\\n\\nThis wasn't an issue in older versions, which just checked for string \\\"does not exist\\\" or \\\"Table not found\\\" in stdout.\\n\\nWould be easy to fix using return stdout and table.lower() in stdout or return stdout and table.lower() in stdout.lower()\\n\\nLet me know your thoughts on this. I can supply a pull request if necessary.\\n\\nThanks,\\nLin."
          },
          {
            "title": "Fix #896: make table_exists case insensitive",
            "content": "Check stdout against table.lower() to avoid case issues"
          }
        ]
      }
    }
  ]
}